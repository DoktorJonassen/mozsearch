Enter your prompt here

# Project Structure

â”œâ”€ ğŸ“ docs
  â””â”€ old-vagrant-setup.md
  â””â”€ bash-scripting-cheatsheet.md
  â””â”€ a11y.md
  â””â”€ testing-checks.md
  â””â”€ nss-stuff.txt
  â””â”€ addons.txt
  â””â”€ webtest.md
  â””â”€ web-server.md
  â””â”€ searchfox-tool-cookbook.md
  â””â”€ blame.md
  â””â”€ platform-notes.txt
  â””â”€ manual-indexing.md
  â””â”€ aws.md
  â””â”€ crossref.md
  â””â”€ newrepo.md
  â””â”€ vm-docker.md
  â””â”€ analysis.md
  â””â”€ blame-design.txt
  â””â”€ output.md
  â””â”€ mach-design.txt
  â””â”€ index-directory-contents.md
  â””â”€ liquid-templating-cheatsheet.md
â”œâ”€ ğŸ“ router
  â””â”€ router.py
  â””â”€ codesearch.py
  â””â”€ logger.py
  â””â”€ crossrefs.py
  â””â”€ raw_search.py
  â””â”€ identifiers.py
â”œâ”€ ğŸ“ scripts
  â”œâ”€ ğŸ“ web-analyze
    â”œâ”€ ğŸ“ wasm-css-analyzer
      â”œâ”€ ğŸ“ src
        â””â”€ lib.rs
      â””â”€ Cargo.toml
  â””â”€ js-analyze.sh
  â””â”€ compress-outputs.sh
  â””â”€ check-helper.sh
  â””â”€ generate-config.sh
  â””â”€ find-repo-files.py
  â””â”€ indexer-logs-print.py
  â””â”€ webidl-analyze.py
  â””â”€ webtest.sh
  â””â”€ generate-other-resources-list.py
  â””â”€ indexer-logs-analyze.sh
  â””â”€ staticprefs-analyze.sh
  â””â”€ idl-analyze.sh
  â””â”€ staticprefs-analyze.py
  â””â”€ weblog-elb-fetch.sh
  â””â”€ find-objdir-files.sh
  â””â”€ css-analyze.sh
  â””â”€ replace-aliases.sh
  â””â”€ crossref.sh
  â””â”€ js-analyze.js
  â””â”€ nginx-setup.py
  â””â”€ ipdl-analyze.sh
  â””â”€ lib.py
  â””â”€ scip-analyze.sh
  â””â”€ idl-analyze.py
  â””â”€ ld-wrapper
  â””â”€ process-chrome-map.py
  â””â”€ indexer-setup.py
  â””â”€ update-fonts.sh
  â””â”€ mkindex.sh
  â””â”€ mkdirs.sh
  â””â”€ indexer-logs-fetch.sh
  â””â”€ weblog-analyze.sh
  â””â”€ objdir-mkdirs.sh
  â””â”€ output.sh
  â””â”€ rust-analyze.sh
  â””â”€ load-vars.sh
  â””â”€ generate-analsysis-files-list.sh
  â””â”€ weblog-elb-analyze.sh
  â””â”€ replace-aliases.py
  â””â”€ html-analyze.sh
  â””â”€ check-index.sh
  â””â”€ fontello-config.json
â”œâ”€ ğŸ“ static
  â”œâ”€ ğŸ“ font
    â””â”€ icons.woff2
    â””â”€ icons.woff
    â””â”€ LICENSE.txt
    â””â”€ icons.ttf
    â””â”€ icons.eot
  â”œâ”€ ğŸ“ js
    â””â”€ blame.js
    â””â”€ settings.js
    â””â”€ search.js
    â””â”€ panel.js
    â””â”€ code-highlighter.js
    â””â”€ context-menu.js
  â”œâ”€ ğŸ“ css
    â””â”€ font-icons.css
    â””â”€ icons.css
    â””â”€ mozsearch.css
  â””â”€ robots.txt
â”œâ”€ ğŸ“ tools
  â”œâ”€ ğŸ“ src
    â”œâ”€ ğŸ“ tree_sitter_support
      â””â”€ cst_tokenizer.rs
      â””â”€ mod.rs
    â”œâ”€ ğŸ“ bin
      â””â”€ scip-indexer.rs
      â””â”€ css-analyze.rs
      â””â”€ output-file.rs
      â””â”€ rust-indexer.rs
      â””â”€ ipdl-analyze.rs
      â””â”€ pipeline-server.rs
      â””â”€ crossref.rs
      â””â”€ searchfox-tool.rs
      â””â”€ merge-analyses.rs
      â””â”€ web-server.rs
    â”œâ”€ ğŸ“ abstract_server
      â””â”€ server_interface.rs
      â””â”€ remote_server.rs
      â””â”€ lazy_crossref.rs
      â””â”€ local_index.rs
      â””â”€ mod.rs
    â”œâ”€ ğŸ“ file_format
      â”œâ”€ ğŸ“ history
        â””â”€ syntax_symdex.rs
        â””â”€ syntax_files_struct.rs
        â””â”€ timeline_files_delta.rs
        â””â”€ timeline_tokens.rs
        â””â”€ rev_summaries.rs
        â””â”€ io_helpers.rs
        â””â”€ mod.rs
        â””â”€ timeline_common.rs
      â””â”€ analysis.rs
      â””â”€ analysis_manglings.rs
      â””â”€ url_map.rs
      â””â”€ merger.rs
      â””â”€ config.rs
      â””â”€ globbing_file_list.rs
      â””â”€ ontology_pointer_kind.rs
      â””â”€ ontology_mapping.rs
      â””â”€ crossref_converter.rs
      â””â”€ crossref_lookup.rs
      â””â”€ per_file_info.rs
      â””â”€ identifiers.rs
      â””â”€ doc_trees.rs
      â””â”€ mod.rs
      â””â”€ repo_data_ingestion.rs
    â”œâ”€ ğŸ“ cmd_pipeline
      â””â”€ cmd_search_text.rs
      â””â”€ cmd_webtest.rs
      â””â”€ cmd_graph.rs
      â””â”€ cmd_render.rs
      â””â”€ cmd_crossref_expand.rs
      â””â”€ cmd_augment_results.rs
      â””â”€ cmd_filter_analysis.rs
      â””â”€ cmd_format_symbols.rs
      â””â”€ cmd_fuse_crossrefs.rs
      â””â”€ cmd_search_identifiers.rs
      â””â”€ cmd_search.rs
      â””â”€ cmd_show_html.rs
      â””â”€ symbol_graph.rs
      â””â”€ cmd_cat_html.rs
      â””â”€ cmd_query.rs
      â””â”€ cmd_compile_results.rs
      â””â”€ cmd_jumpref_lookup.rs
      â””â”€ cmd_tokenize_source.rs
      â””â”€ cmd_traverse.rs
      â””â”€ cmd_batch_render.rs
      â””â”€ parser.rs
      â””â”€ interface.rs
      â””â”€ hier_graph.rs
      â””â”€ mod.rs
      â””â”€ cmd_search_files.rs
      â””â”€ cmd_merge_analyses.rs
      â””â”€ transforms.rs
      â””â”€ cmd_crossref_lookup.rs
      â””â”€ cmd_prod_filter.rs
    â”œâ”€ ğŸ“ query
      â””â”€ chew_query.rs
      â””â”€ query_core.toml
      â””â”€ mod.rs
    â”œâ”€ ğŸ“ templating
      â””â”€ liquid_exts.rs
      â””â”€ mod.rs
    â””â”€ symbol_graph_edge_kind.rs
    â””â”€ output.rs
    â””â”€ logging.rs
    â””â”€ glob_helper.rs
    â””â”€ git_ops.rs
    â””â”€ url_map_handler.rs
    â””â”€ file_utils.rs
    â””â”€ languages.rs
    â””â”€ css_analyzer.rs
    â””â”€ lib.rs
    â””â”€ blame.rs
    â””â”€ links.rs
    â””â”€ describe.rs
    â””â”€ tokenize.rs
    â””â”€ format.rs
    â””â”€ doc_trees_handler.rs
    â””â”€ url_encode_path.rs
  â”œâ”€ ğŸ“ templates
    â”œâ”€ ğŸ“ query_results
      â””â”€ line_span.liquid
      â””â”€ kind_group.liquid
      â””â”€ file_table.liquid
      â””â”€ graph_collection_root.liquid
      â””â”€ symbol_tree_table.liquid
      â””â”€ file_results.liquid
      â””â”€ graph_root.liquid
      â””â”€ text_file_root.liquid
      â””â”€ symbol_crossref_info_list_root.liquid
      â””â”€ symbol_tree_table_node.liquid
      â””â”€ facet_root.liquid
      â””â”€ facet_group.liquid
      â””â”€ symbol_tree_table_list_root.liquid
      â””â”€ symbol.liquid
      â””â”€ pathkind_group.liquid
      â””â”€ rb_root.liquid
    â””â”€ settings.liquid
    â””â”€ ontology_ingestion_explainer.liquid
    â””â”€ help_index.liquid
    â””â”€ scroll_footer.liquid
    â””â”€ navigation_panel.liquid
    â””â”€ pipeline_explainer.liquid
    â””â”€ dir_listing.liquid
    â””â”€ repo_ingestion_explainer.liquid
    â””â”€ query_results.liquid
    â””â”€ search_template.liquid
    â””â”€ footer.liquid
    â””â”€ path.liquid
    â””â”€ header_search.liquid
    â””â”€ breadcrumbs.liquid
    â””â”€ header_query.liquid
    â””â”€ panel.liquid
  â”œâ”€ ğŸ“ .cargo
    â””â”€ config.toml
  â”œâ”€ ğŸ“ languages
    â”œâ”€ ğŸ“ tokenizer_queries
      â””â”€ python.scm
      â””â”€ rust.scm
      â””â”€ cpp.scm
      â””â”€ typescript.scm
  â”œâ”€ ğŸ“ tests
    â””â”€ test_check_insta.rs
  â””â”€ Cargo.toml
â”œâ”€ ğŸ“ infrastructure
  â”œâ”€ ğŸ“ aws
    â””â”€ trigger_indexer.py
    â””â”€ index.sh
    â””â”€ send-warning-email.py
    â””â”€ send-done-email.py
    â””â”€ trigger_common.py
    â””â”€ make-crontab.py
    â””â”€ main.sh
    â””â”€ scp-while-sshed.py
    â””â”€ delete-volume.py
    â””â”€ indexer-provision.sh
    â””â”€ web-server-provision.sh
    â””â”€ send-failure-email.py
    â””â”€ warning-suppression.patterns
    â””â”€ shell-setup.sh
    â””â”€ trigger-web-server.py
    â””â”€ send-provision-email.py
    â””â”€ ssh.py
    â””â”€ channel-tool.py
    â””â”€ attach-index-volume.py
    â””â”€ terminate-indexer.py
    â””â”€ trigger_shell.py
    â””â”€ detach-volume.py
    â””â”€ web-serve.sh
    â””â”€ trigger-provision.py
    â””â”€ awslib.py
    â””â”€ upload.py
    â””â”€ upload-lambda-zips-from-outside-vm.sh
    â””â”€ mkscratch.sh
  â”œâ”€ ğŸ“ vagrant
    â””â”€ indexer-provision.sh
  â””â”€ web-server-run.sh
  â””â”€ common-provision-post.sh
  â””â”€ web-server-check.sh
  â””â”€ indexer-update.sh
  â””â”€ web-server-update.sh
  â””â”€ Dockerfile
  â””â”€ indexer-upload.sh
  â””â”€ docker-provision.sh
  â””â”€ web-server-setup.sh
  â””â”€ reblame-run.sh
  â””â”€ indexer-setup.sh
  â””â”€ indexer-provision.sh
  â””â”€ web-server-provision.sh
  â””â”€ common-provision-pre.sh
  â””â”€ indexer-run.sh
â”œâ”€ ğŸ“ trees
  â””â”€ README.md
â”œâ”€ ğŸ“ sax
  â””â”€ sax.js
  â””â”€ sax.patch
  â””â”€ LICENSE
  â””â”€ README
â”œâ”€ ğŸ“ clang-plugin
  â”œâ”€ ğŸ“ testfiles
    â”œâ”€ ğŸ“ xyz
      â””â”€ test.cpp
    â””â”€ test.cpp
  â”œâ”€ ğŸ“ from-clangd
    â””â”€ README.md
    â””â”€ HeuristicResolver.h
    â””â”€ HeuristicResolver.cpp
  â””â”€ FileOperations.h
  â””â”€ Makefile
  â””â”€ BindingOperations.h
  â””â”€ StringOperations.cpp
  â””â”€ MozsearchIndexer.cpp
  â””â”€ .clang-format
  â””â”€ StringOperations.h
  â””â”€ BindingOperations.cpp
  â””â”€ FileOperations.cpp
â”œâ”€ ğŸ“ config_defaults
  â””â”€ per-file-info.toml
  â””â”€ source_file_other_tools_panels.liquid
  â””â”€ ontology-mapping.toml
  â””â”€ source_file_info_boxes.liquid
â”œâ”€ ğŸ“ tests
  â”œâ”€ ğŸ“ searchfox
    â””â”€ setup
  â”œâ”€ ğŸ“ webtest
    â””â”€ webtest.html
    â””â”€ test_WebIDLBindings.js
    â””â”€ test_Titler.js
    â””â”€ head.js
    â””â”€ test_LineNumberInURL.js
    â””â”€ test_CopyAsMarkdown.js
    â””â”€ test_FieldLayoutForGenerated.js
    â””â”€ test_SymbolSectionInPanel.js
    â””â”€ test_MacroExpansions.js
    â””â”€ test_SpaceInFilename.js
    â””â”€ test_UTF8InFilename.js
    â””â”€ test_FieldLayoutContextMenu.js
    â””â”€ test_Panel.js
    â””â”€ test_SearchSection.js
    â””â”€ test_Breadcrumbs.js
    â””â”€ test_Search.js
    â””â”€ test_DebugUI.js
    â””â”€ test_OverloadInTemplate.js
    â””â”€ test_TreeSwitcherKeys.js
    â””â”€ test_ContextMenuKeys.js
    â””â”€ test_FieldLayoutColumns.js
  â”œâ”€ ğŸ“ tests
    â”œâ”€ ğŸ“ checks
      â”œâ”€ ğŸ“ src
        â””â”€ dummy-test-package.rs
      â”œâ”€ ğŸ“ inputs
        â”œâ”€ ğŸ“ fancy
          â”œâ”€ ğŸ“ diagram
            â”œâ”€ ğŸ“ traverse
              â”œâ”€ ğŸ“ lots_of_calls.cpp
                â””â”€ paths_between__four_left_one_right__json
              â”œâ”€ ğŸ“ big_cpp.cpp
                â””â”€ uses__thing_takeDamage__json
                â””â”€ uses__thing_takeDamage__hier__dot
                â””â”€ callees__outercat_meet__json
                â””â”€ callees__outercat_meet__dot
                â””â”€ callees__colorize__outercat_meet__dot
                â””â”€ uses__thing_takeDamage__hier__json
          â”œâ”€ ğŸ“ format-symbol
            â”œâ”€ ğŸ“ field-layout
              â”œâ”€ ğŸ“ platform_specific_field.cpp
                â””â”€ field_layout__platform_specific_field_1__json
                â””â”€ field_layout__platform_specific_field_2__json
              â”œâ”€ ğŸ“ empty.cpp
                â””â”€ field_layout__empty__json
              â”œâ”€ ğŸ“ platform_specific_size.cpp
                â””â”€ field_layout__platform_specific_size__json
              â”œâ”€ ğŸ“ non-struct.cpp
                â””â”€ field_layout__non_struct__json
              â”œâ”€ ğŸ“ bitfields.cpp
                â””â”€ field_layout__bitfields__json
              â”œâ”€ ğŸ“ field-type.cpp
                â””â”€ field_show_hide__name__json
                â””â”€ field_layout__field_type__json
              â”œâ”€ ğŸ“ holes.cpp
                â””â”€ field_layout__holes__json
              â”œâ”€ ğŸ“ big_cpp.cpp
                â””â”€ field_layout__outercat__json
              â”œâ”€ ğŸ“ multiple_inheritance.cpp
                â””â”€ field_layout__multiple_inheritance__json
              â”œâ”€ ğŸ“ vtable.cpp
                â””â”€ field_layout__vtable__json
        â”œâ”€ ğŸ“ analysis
          â”œâ”€ ğŸ“ html
            â”œâ”€ ğŸ“ script.html
              â””â”€ def_func__json
              â””â”€ def_func_for_attribute__json
              â””â”€ use_func__json
              â””â”€ use_func_in_attribute__json
              â””â”€ unknown_script__json
          â”œâ”€ ğŸ“ webidl
            â”œâ”€ ğŸ“ external-interface.webidl
              â””â”€ webidl_external_interface__json
            â”œâ”€ ğŸ“ dictionary.webidl
              â””â”€ webidl_dictionary__json
            â”œâ”€ ğŸ“ partial-interface.webidl
              â””â”€ webidl_partial_interface__json
            â”œâ”€ ğŸ“ const.webidl
              â””â”€ webidl_const__json
            â”œâ”€ ğŸ“ interface-mixin.webidl
              â””â”€ webidl_interface_mixin__json
            â”œâ”€ ğŸ“ maplike.webidl
              â””â”€ webidl_maplike__json
            â”œâ”€ ğŸ“ overload.webidl
              â””â”€ webidl_overload__json
            â”œâ”€ ğŸ“ asynciterable.webidl
              â””â”€ webidl_asynciterable__json
            â”œâ”€ ğŸ“ typedef.webidl
              â””â”€ webidl_typedef__json
            â”œâ”€ ğŸ“ includes.webidl
              â””â”€ webidl_asynciterable__json
            â”œâ”€ ğŸ“ iterable.webidl
              â””â”€ webidl_iterable__json
            â”œâ”€ ğŸ“ enum.webidl
              â””â”€ webidl_enum__json
            â”œâ”€ ğŸ“ partial-dictionary.webidl
              â””â”€ webidl_partial_dictionary__json
            â”œâ”€ ğŸ“ callback.webidl
              â””â”€ webidl_callback__json
            â”œâ”€ ğŸ“ namespace.webidl
              â””â”€ webidl_namespace__json
            â”œâ”€ ğŸ“ setlike.webidl
              â””â”€ webidl_setlike__json
            â”œâ”€ ğŸ“ method.webidl
              â””â”€ webidl_method__json
            â”œâ”€ ğŸ“ attribute.webidl
              â””â”€ webidl_attribute__json
            â”œâ”€ ğŸ“ super-interface.webidl
              â””â”€ webidl_super_interface__json
            â”œâ”€ ğŸ“ super-dictionary.webidl
              â””â”€ webidl_super_dictionary__json
            â”œâ”€ ğŸ“ type.webidl
              â””â”€ webidl_type__json
            â”œâ”€ ğŸ“ partial-namespace.webidl
              â””â”€ webidl_partial_namespace__json
          â”œâ”€ ğŸ“ rust
            â”œâ”€ ğŸ“ test_rust_dependency
              â””â”€ MyType__html
              â””â”€ MyType__json
            â”œâ”€ ğŸ“ simple.rs
              â””â”€ simple_Loader_new__html
              â””â”€ simple_Loader_new__json
          â”œâ”€ ğŸ“ java
            â”œâ”€ ğŸ“ KotlinTest.kt
              â””â”€ external_dependency__json
            â”œâ”€ ğŸ“ InlineObject.kt
              â””â”€ method_with_return_type__json
              â””â”€ method__json
          â”œâ”€ ğŸ“ js
            â”œâ”€ ğŸ“ export4.mjs
              â””â”€ exported_decls__json
            â”œâ”€ ğŸ“ export5.mjs
              â””â”€ exported_decls__json
            â”œâ”€ ğŸ“ export6.mjs
              â””â”€ exported_decls__json
            â”œâ”€ ğŸ“ import.mjs
              â””â”€ imported_symbols__json
              â””â”€ imported_symbols_use__json
            â”œâ”€ ğŸ“ export.mjs
              â””â”€ exported_use__json
              â””â”€ exported_decls__json
            â”œâ”€ ğŸ“ imported-module.mjs
              â””â”€ def_ModuleClass__json
              â””â”€ def_ModuleClass_error__json
              â””â”€ def_moduleFunc__json
              â””â”€ def_moduleConst__json
            â”œâ”€ ğŸ“ secret-madjewel.js
              â””â”€ def_secretMadjewelConst__json
            â”œâ”€ ğŸ“ root-module.mjs
              â””â”€ def_rootModuleConst__json
            â”œâ”€ ğŸ“ export3.mjs
              â””â”€ exported_decls__json
            â”œâ”€ ğŸ“ export2.mjs
              â””â”€ exported_use__json
              â””â”€ exported_decls__json
            â”œâ”€ ğŸ“ export8.mjs
              â””â”€ ignore_anonymous_function_default__json
            â”œâ”€ ğŸ“ some_javascript.js
              â””â”€ all__priv_field_num__json
              â””â”€ all__priv_field_num__html
            â”œâ”€ ğŸ“ export7.mjs
              â””â”€ exported_decls__json
          â”œâ”€ ğŸ“ urlmap
            â”œâ”€ ğŸ“ root.xhtml
              â””â”€ url_def_in_xhtml__json
              â””â”€ url_use_in_xhtml__json
            â”œâ”€ ğŸ“ chrome1.mjs
              â””â”€ no_stray_relpath__json
              â””â”€ url_use_with_relative__json
            â”œâ”€ ğŸ“ subdir
              â”œâ”€ ğŸ“ sub.mjs
                â””â”€ url_use_with_relative_parent__json
            â”œâ”€ ğŸ“ root.js
              â””â”€ url_def_in_script__json
              â””â”€ url_use_in_script__json
              â””â”€ no_stray_url__json
            â”œâ”€ ğŸ“ root.mjs
              â””â”€ url_def_in_module__json
              â””â”€ url_use_in_module__json
            â”œâ”€ ğŸ“ root.html
              â””â”€ url_def_in_html__json
              â””â”€ url_use_in_html__json
            â”œâ”€ ğŸ“ resource1.mjs
              â””â”€ url_use_with_relative__json
          â”œâ”€ ğŸ“ cpp
            â”œâ”€ ğŸ“ ForwardingTemplates.cpp
              â””â”€ TypeIndependentNewInTemplateReportedInTemplate
              â””â”€ TypeDependentNewInMethodReportedAtCallSite
              â””â”€ TypeDependentNewInTemplateReportedAtCallSite
            â”œâ”€ ğŸ“ big_header.h
              â””â”€ def_big_header__json
              â””â”€ def_big_header__html
            â”œâ”€ ğŸ“ URL_sym.cpp
              â””â”€ URL_sym__json
            â”œâ”€ ğŸ“ template_specialization.cpp
              â””â”€ some_function_called_in_partial_specialization
            â”œâ”€ ğŸ“ overs.cpp
              â””â”€ def__DoubleBase_doublePure__html
            â”œâ”€ ğŸ“ lambdas.cpp
              â””â”€ Struct0UsedInLambda
              â””â”€ Struct1UsedInLambda
              â””â”€ LambdaFields
            â”œâ”€ ğŸ“ macro.cpp
              â””â”€ TEST_MACRO_INCLUDE
              â””â”€ MULTI_LINE_MACRO
              â””â”€ EMPTY_MACRO__json
              â””â”€ NESTED_MACRO_WITH_ARG
              â””â”€ CONST_MACRO__json
              â””â”€ PER_TARGET_FUNCTION
              â””â”€ IDENT_MACRO__json
              â””â”€ NESTED_MACRO
            â”œâ”€ ğŸ“ template_shapes.cpp
              â””â”€ Foo_Project
            â”œâ”€ ğŸ“ atom_list.h
              â””â”€ YO_ATOM
              â””â”€ NESTED_YO_ATOM
              â””â”€ structured_yo_atoms_foo_string__json
            â”œâ”€ ğŸ“ bug1781178.cpp
              â””â”€ Foo_Static
              â””â”€ Foo_Simple
              â””â”€ Foo_Nested_field
              â””â”€ WithOverloads_Overloaded
              â””â”€ Foo_Project
              â””â”€ Foo_E_Waldo
              â””â”€ Foo_E
              â””â”€ internal_Read
              â””â”€ Point_IsThereOne
              â””â”€ Foo_Typedef
            â”œâ”€ ğŸ“ templates7.cpp
              â””â”€ def_InlineShouldHaveContextSym
              â””â”€ def_InlineTemplateShouldHaveContextSym
              â””â”€ def_OutOfLineShouldntHaveContextSym
              â””â”€ def_OutOfLineTemplateShouldntHaveContextSym
            â”œâ”€ ğŸ“ big_cpp.cpp
              â””â”€ structured_outercat__json
              â””â”€ include_big_header__html
              â””â”€ structured_human__json
              â””â”€ structured_superhero__json
              â””â”€ structured_thing__json
              â””â”€ structured_practicalart_beart__json
              â””â”€ structured_thing_takedamage__json
              â””â”€ structured_lessglobalcontext_decidewhethertodecide__json
              â””â”€ structured_abstractart_beart__json
              â””â”€ structured_globalcontext__json
              â””â”€ include_big_header__json
              â””â”€ include_big_header__prodjson
              â””â”€ structured_superhero_takedamage__json
              â””â”€ include_big_header__prodhtml
              â””â”€ def_thing__json
              â””â”€ structured_lessglobalcontext__json
          â”œâ”€ ğŸ“ ipdl
            â”œâ”€ ğŸ“ PTestBasic.ipdl
              â””â”€ def_PTestBasic__Hello__html
              â””â”€ def_PTestBasic__Hello__json
          â”œâ”€ ğŸ“ xpidl
            â”œâ”€ ğŸ“ xpctest_params.idl
              â””â”€ use_testOctet__json
              â””â”€ def_testOctet__json
              â””â”€ def_testOctet__html
            â”œâ”€ ğŸ“ xpctest_attributes.idl
              â””â”€ idl_stringProperty__html
              â””â”€ idl_stringProperty__json
          â”œâ”€ ğŸ“ staticprefs
            â””â”€ staticprefs__json
          â”œâ”€ ğŸ“ css
            â”œâ”€ ğŸ“ embed_css.html
              â””â”€ custom_prop__json
              â””â”€ url__json
            â”œâ”€ ğŸ“ test.css
              â””â”€ custom_prop__json
              â””â”€ url__json
        â”œâ”€ ğŸ“ blame
          â”œâ”€ ğŸ“ syntax-token-tokenize
            â””â”€ big_cpp.cpp__tokenize__file
            â””â”€ big_cpp.cpp__tokenize_outline__json
        â”œâ”€ ğŸ“ search-text
          â”œâ”€ ğŸ“ global
            â””â”€ searchfox_plain__json
            â””â”€ searchfox_boring_re__json
            â””â”€ searchfox_re_multi_o__json
        â”œâ”€ ğŸ“ web
          â”œâ”€ ğŸ“ docs
            â””â”€ docs_md__html
            â””â”€ docs_rst__html
          â”œâ”€ ğŸ“ files
            â”œâ”€ ğŸ“ test-info-boxes
              â””â”€ wpt__some_cross_global_test__html
              â””â”€ test_many_manifest_permutations__html
              â””â”€ text_custom_element_base_xul__infobox__html
              â””â”€ wpt__test_ima_sad_subtests_wpt__html
              â””â”€ wpt__test_ima_disabled_wpt__html
            â”œâ”€ ğŸ“ urlmap
              â””â”€ links_in_string__html
              â””â”€ links_in_comment__html
            â”œâ”€ ğŸ“ contextual-keyword
              â””â”€ contextual-keyword_webidl__html
              â””â”€ contextual-keyword_py__html
              â””â”€ contextual-keyword_rs__html
              â””â”€ contextual-keyword_cpp__html
              â””â”€ contextual-keyword_js__html
            â”œâ”€ ğŸ“ nav-panel
              â””â”€ wpt__test_ima_weird_meta_wpt__navpanel__html
              â””â”€ wpt__some_cross_global_test__navpanel__html
          â”œâ”€ ğŸ“ templates
            â””â”€ search_template__html
            â””â”€ help_file__html
          â”œâ”€ ğŸ“ search
            â”œâ”€ ğŸ“ overs.cpp
              â””â”€ doublePure__json
              â””â”€ triplePure__json
            â”œâ”€ ğŸ“ big_cpp.cpp
              â””â”€ practicalart_beart__json
              â””â”€ human__json
              â””â”€ practicalart__json
              â””â”€ abstractart_beart__json
              â””â”€ abstractart__json
          â”œâ”€ ğŸ“ query
            â”œâ”€ ğŸ“ parsing
              â”œâ”€ ğŸ“ simple
                â”œâ”€ ğŸ“ overs.cpp
                  â””â”€ query__parse__doublePure__context4__json
                  â””â”€ query__parse__doublePure__json
                â”œâ”€ ğŸ“ lots_of_calls.cpp
                  â””â”€ query__parse__calls_between__four_left_one_right__svg
                  â””â”€ query__parse__calls_to__four_left_and_right__svg
                â”œâ”€ ğŸ“ big_cpp.cpp
                  â””â”€ query__parse__calls_to__takeDamage__json
                  â””â”€ query__parse__calls_to__colorize__takeDamage__json
                  â””â”€ query__parse_calls_to__graoh_format__takeDamage__json
                â””â”€ default__needs_regexp_escape__json
                â””â”€ default__unquoted_dashed__json
                â””â”€ default__quoted_phrase__json
                â””â”€ default__unquoted_word__json
                â””â”€ default__quoted_dashed__json
            â”œâ”€ ğŸ“ execution
              â”œâ”€ ğŸ“ field-layout
                â””â”€ query__field_layout__outercat__json
              â”œâ”€ ğŸ“ simple
                â”œâ”€ ğŸ“ overs.cpp
                  â””â”€ query__doublePure__context4__json
                  â””â”€ query__doublePure__json
                  â””â”€ query__doublePure__context_alias4__json
          â”œâ”€ ğŸ“ dirs
            â””â”€ root_listing__html
            â””â”€ generated_listing__html
        â”œâ”€ ğŸ“ jumpref
          â”œâ”€ ğŸ“ jumpref
            â”œâ”€ ğŸ“ LightweightThemeManager.jsm
              â””â”€ ADDON_TYPE__lookup__json
            â”œâ”€ ğŸ“ simple.rs
              â””â”€ loader__needs_hard_reload__lookup__json
            â”œâ”€ ğŸ“ big_cpp.cpp
              â””â”€ practicalart__beart__lookup__json
              â””â”€ big_header__h__lookup__json
              â””â”€ header_declared_func__lookup__json
              â””â”€ abstractart__beart__lookup__json
        â”œâ”€ ğŸ“ search-files
          â””â”€ anchored_doublestar_html__json
          â””â”€ anchored_fakewpt_doublestar_combi__json
          â””â”€ anchored_star_html__json
          â””â”€ file_list_ingestion_check__json
        â”œâ”€ ğŸ“ crossref
          â”œâ”€ ğŸ“ java
            â”œâ”€ ğŸ“ JavaLibrary.java
              â””â”€ no_inverse_relationships__json
              â””â”€ links_to_java_library_constructor__json
              â””â”€ multiple_implements__json
            â”œâ”€ ğŸ“ KotlinLibrary.kt
              â””â”€ links_to_kotlin_library_constructor__json
            â””â”€ package__json
          â”œâ”€ ğŸ“ merge
            â””â”€ merge__big_cpp
          â”œâ”€ ğŸ“ identifiers
            â”œâ”€ ğŸ“ big_cpp.cpp
              â””â”€ abstractart_beart__json
              â””â”€ thing__json
          â”œâ”€ ğŸ“ expand
            â”œâ”€ ğŸ“ big_cpp.cpp
              â””â”€ outercat__json
          â”œâ”€ ğŸ“ crossref
            â”œâ”€ ğŸ“ xpctest_params.idl
              â””â”€ testOctet__json
            â”œâ”€ ğŸ“ xpctest_attributes.idl
              â””â”€ booleanProperty_getter__json
              â””â”€ booleanProperty_setter__json
            â”œâ”€ ğŸ“ big_cpp.cpp
              â””â”€ thing_mHP__json
              â””â”€ abstractart__beart__json
              â””â”€ stackartholder__json
              â””â”€ thing__json
      â””â”€ Cargo.toml
    â”œâ”€ ğŸ“ files
      â”œâ”€ ğŸ“ ipdl
        â””â”€ PTestBasic.ipdl
        â””â”€ TestBasic.cpp
      â”œâ”€ ğŸ“ cpp
        â””â”€ URL_sym.cpp
        â””â”€ contextual-keyword.cpp
      â”œâ”€ ğŸ“ test_rust_dependency
        â”œâ”€ ğŸ“ src
          â””â”€ lib.rs
        â””â”€ Cargo.toml
      â”œâ”€ ğŸ“ urlmap
        â”œâ”€ ğŸ“ subdir
          â””â”€ sub.mjs
        â””â”€ chrome2.mjs
        â””â”€ resource1.css
        â””â”€ root.xhtml
        â””â”€ chrome1.mjs
        â””â”€ chrome3.mjs
        â””â”€ mozsrc1.html
        â””â”€ chrome1.css
        â””â”€ chrome1b.mjs
        â””â”€ root.js
        â””â”€ root.mjs
        â””â”€ chrome1.html
        â””â”€ resource3.mjs
        â””â”€ root.html
        â””â”€ mozsrc2.mjs
        â””â”€ resource2.css
        â””â”€ mozsrc3.mjs
        â””â”€ resource2.mjs
        â””â”€ chrome2.css
        â””â”€ mozsrc2.css
        â””â”€ root.cpp
        â””â”€ resource1.mjs
        â””â”€ mozsrc1.mjs
        â””â”€ mozsrc1.css
        â””â”€ mozsrc1b.mjs
        â””â”€ resource1.html
      â”œâ”€ ğŸ“ field-layout
        â””â”€ field-type.cpp
        â””â”€ bitfields.cpp
        â””â”€ holes.cpp
        â””â”€ field-type-include.h
        â””â”€ empty-subclass.cpp
        â””â”€ platform_specific_field.cpp
        â””â”€ field-type.h
        â””â”€ empty.cpp
        â””â”€ platform_specific_size.cpp
        â””â”€ non-struct.cpp
        â””â”€ multiple_inheritance.cpp
        â””â”€ vtable.cpp
      â”œâ”€ ğŸ“ docs
        â”œâ”€ ğŸ“ rst_test
          â””â”€ index.rst
        â”œâ”€ ğŸ“ md_test
          â””â”€ index.md
      â”œâ”€ ğŸ“ html
        â””â”€ invalid-entity-ref.html
        â””â”€ script.xhtml
        â””â”€ script.html
      â”œâ”€ ğŸ“ ignored-js
        â”œâ”€ ğŸ“ doublestar-ignored
          â””â”€ ignored-by-doublestar.js
        â””â”€ not-ignored-after-reinclusion.js
        â””â”€ reignored-after-reinclusion.js
        â””â”€ wildcard-ignored.js
        â””â”€ explicit-ignored.js
      â”œâ”€ ğŸ“ src
        â”œâ”€ ğŸ“ test
          â”œâ”€ ğŸ“ java
            â”œâ”€ ğŸ“ sample
              â””â”€ JavaTest.java
          â”œâ”€ ğŸ“ kotlin
            â”œâ”€ ğŸ“ sample
              â””â”€ KotlinTest.kt
        â”œâ”€ ğŸ“ main
          â”œâ”€ ğŸ“ kotlin
            â”œâ”€ ğŸ“ sample
              â””â”€ InlineObject.kt
              â””â”€ KotlinLibrary.kt
          â”œâ”€ ğŸ“ java
            â”œâ”€ ğŸ“ sample
              â””â”€ JavaLibrary.java
              â””â”€ Jni.java
      â”œâ”€ ğŸ“ webidl
        â”œâ”€ ğŸ“ bindings
          â”œâ”€ ğŸ“ include
            â””â”€ BindingTestMixed1Binding.h
            â””â”€ BindingTestMixed2Binding.h
            â””â”€ BindingTestBinding.h
          â”œâ”€ ğŸ“ src
            â””â”€ BindingTest.h
            â””â”€ BindingTestMixed2Binding.cpp
            â””â”€ BindingTestMixed1Binding.cpp
            â””â”€ BindingTestMixed2.h
            â””â”€ BindingTestMixed1.h
            â””â”€ BindingTestBinding.cpp
        â””â”€ partial-namespace.webidl
        â””â”€ callback.webidl
        â””â”€ namespace.webidl
        â””â”€ BindingTest.webidl
        â””â”€ setlike.webidl
        â””â”€ method.webidl
        â””â”€ attribute.webidl
        â””â”€ BindingTestMixed2.webidl
        â””â”€ const.webidl
        â””â”€ interface-mixin.webidl
        â””â”€ external-interface.webidl
        â””â”€ dictionary.webidl
        â””â”€ partial-interface.webidl
        â””â”€ BindingTestMixin.webidl
        â””â”€ maplike.webidl
        â””â”€ overload.webidl
        â””â”€ asynciterable.webidl
        â””â”€ typedef.webidl
        â””â”€ includes.webidl
        â””â”€ iterable.webidl
        â””â”€ contextual-keyword.webidl
        â””â”€ enum.webidl
        â””â”€ partial-dictionary.webidl
        â””â”€ BindingTestMixed1.webidl
        â””â”€ super-interface.webidl
        â””â”€ super-dictionary.webidl
        â””â”€ type.webidl
      â”œâ”€ ğŸ“ mobile
        â”œâ”€ ğŸ“ androidTest
          â”œâ”€ ğŸ“ asserts
            â””â”€ inputs.html
      â”œâ”€ ğŸ“ rust
        â””â”€ weak_keyword.rs
        â””â”€ mod.rs
      â”œâ”€ ğŸ“ testing
        â”œâ”€ ğŸ“ web-platform
          â”œâ”€ ğŸ“ meta
            â”œâ”€ ğŸ“ fake-standard
              â””â”€ test_ima_weird_meta_wpt.js.ini
              â””â”€ test_ima_sad_subtests_wpt.js.ini
              â””â”€ test_ima_disabled_wpt.js.ini
          â”œâ”€ ğŸ“ tests
            â”œâ”€ ğŸ“ fake-standard
              â””â”€ test_ima_weird_meta_wpt.js
              â””â”€ test_ima_sad_subtests_wpt.js
              â””â”€ test_ima_disabled_wpt.js
      â”œâ”€ ğŸ“ js
        â””â”€ export7.mjs
        â””â”€ contextual-keyword.js
        â””â”€ export4.mjs
        â””â”€ export5.mjs
        â””â”€ README.md
        â””â”€ export6.mjs
        â””â”€ with space.js
        â””â”€ import.mjs
        â””â”€ export.mjs
        â””â”€ GCAnnotations.h
        â””â”€ Value.h
        â””â”€ export3.mjs
        â””â”€ export2.mjs
        â””â”€ with-UTF8-ãƒ•ã‚¡ã‚¤ãƒ«.js
        â””â”€ export8.mjs
      â”œâ”€ ğŸ“ mozilla
        â”œâ”€ ğŸ“ _ipdltest
          â””â”€ TestBasicParent.h
          â””â”€ TestBasicChild.h
      â”œâ”€ ğŸ“ subdir
        â””â”€ header@with,many^strange~chars.h
      â”œâ”€ ğŸ“ webtest
        â””â”€ Titler.cpp
        â””â”€ CopyAsMarkdown.cpp
        â””â”€ Webtest.cpp
        â””â”€ WebtestPathFilter.cpp
      â”œâ”€ ğŸ“ xpidl
        â””â”€ xpctest_utils.idl
        â””â”€ xpctest_cenums.idl
        â””â”€ xpctest_params.idl
        â””â”€ nsrootidl.idl
        â””â”€ nsISupports.idl
        â””â”€ xpctest_attributes.idl
        â””â”€ xpctest_bug809674.idl
        â””â”€ xpctest_interfaces.idl
        â””â”€ xpctest_returncode.idl
      â”œâ”€ ğŸ“ python
        â””â”€ contextual-keyword.py
      â”œâ”€ ğŸ“ css
        â””â”€ embed_css.html
        â””â”€ test.css
      â”œâ”€ ğŸ“ staticprefs
        â”œâ”€ ğŸ“ bindings
          â””â”€ StaticPrefList_test.h
          â””â”€ StaticPrefList_test2.h
        â””â”€ StaticPrefsConsumer.cpp
        â””â”€ StaticPrefList.yaml
      â”œâ”€ ğŸ“ third_party
        â””â”€ a_file
      â”œâ”€ ğŸ“ invalid-files
        â””â”€ unterminated-rust-str.rs
      â””â”€ .clang-format
      â””â”€ chrome.ini
      â””â”€ mochitest-common.ini
      â””â”€ bug1781178.cpp
      â””â”€ bug1588908.js
      â””â”€ LightweightThemeManager.jsm
      â””â”€ Cargo.toml
      â””â”€ bug1449291.cpp
      â””â”€ tricky_symbol_names.cpp
      â””â”€ bug1446220_unicode.html
      â””â”€ secret-madjewel.js
      â””â”€ atom_list.h
      â””â”€ root-module.mjs
      â””â”€ templates4.cpp
      â””â”€ test_many_manifest_permutations.js
      â””â”€ test_talosconfig_browser_config.json
      â””â”€ gzip-colliding-file
      â””â”€ implicit.cpp
      â””â”€ some_ini.ini
      â””â”€ using.cpp
      â””â”€ lots_of_calls.cpp
      â””â”€ macro.cpp
      â””â”€ templates6.cpp
      â””â”€ nsTArray.h
      â””â”€ mochitest-alt-pref.ini
      â””â”€ mochitest.ini
      â””â”€ ForwardingTemplates.cpp
      â””â”€ browser.ini
      â””â”€ nsISupports.h
      â””â”€ README.md
      â””â”€ big_header.h
      â””â”€ template_specialization.cpp
      â””â”€ enummacro.cpp
      â””â”€ .eslintignore
      â””â”€ templates3.cpp
      â””â”€ templates_nsTArray.cpp
      â””â”€ overs.cpp
      â””â”€ bug1432300.cpp
      â””â”€ imported-module.mjs
      â””â”€ lambdas.cpp
      â””â”€ GeckoApp.java
      â””â”€ templates2.cpp
      â””â”€ template_shapes.cpp
      â””â”€ test_custom_element_base.xul
      â””â”€ AddonUpdateChecker.jsm
      â””â”€ templates5.h
      â””â”€ templates6.h
      â””â”€ templates7.cpp
      â””â”€ bug1435345.cpp
      â””â”€ gzip-colliding-file.gz
      â””â”€ some_python.py
      â””â”€ simple.rs
      â””â”€ spaces  are bad.txt
      â””â”€ runnables.cpp
      â””â”€ test_DOMWindowCreated_chromeonly.html
      â””â”€ big_cpp.cpp
      â””â”€ atom_magic.h
      â””â”€ some_javascript.js
      â””â”€ enummacro.h
      â””â”€ xpidl_cpp_consumer.cpp
      â””â”€ jni.cpp
      â””â”€ templates5.cpp
      â””â”€ simple.cpp
      â””â”€ templates1.cpp
      â””â”€ multiline.cpp
    â”œâ”€ ğŸ“ metadata
      â””â”€ wpt-manifest.json
      â””â”€ README.md
      â””â”€ test2.chrome-map.json
      â””â”€ wpt-mozilla-manifest.json
      â””â”€ test.chrome-map.json
      â””â”€ test-info-all-tests.json
      â””â”€ doc-trees.json
      â””â”€ bugzilla-components.json
      â””â”€ wpt-metadata-summary.json
    â”œâ”€ ğŸ“ mc-analysis
      â”œâ”€ ğŸ“ mozilla
        â”œâ”€ ğŸ“ _ipdltest
          â””â”€ TestBasicParent.h
          â””â”€ TestBasicChild.h
      â”œâ”€ ğŸ“ ipdl
        â””â”€ TestBasic.cpp
      â”œâ”€ ğŸ“ __GENERATED__
        â”œâ”€ ğŸ“ ipdl
          â””â”€ PTestBasicParent.cpp
          â””â”€ PTestBasic.cpp
          â””â”€ PTestBasicChild.cpp
        â”œâ”€ ğŸ“ ipc
          â”œâ”€ ğŸ“ ipdl
            â”œâ”€ ğŸ“ _ipdlheaders
              â”œâ”€ ğŸ“ mozilla
                â”œâ”€ ğŸ“ _ipdltest
                  â””â”€ PTestBasic.h
                  â””â”€ PTestBasicChild.h
                  â””â”€ PTestBasicParent.h
    â”œâ”€ ğŸ“ mc-generated
      â”œâ”€ ğŸ“ ipc
        â”œâ”€ ğŸ“ ipdl
          â”œâ”€ ğŸ“ _ipdlheaders
            â”œâ”€ ğŸ“ mozilla
              â”œâ”€ ğŸ“ _ipdltest
                â””â”€ PTestBasic.h
                â””â”€ PTestBasicChild.h
                â””â”€ PTestBasicParent.h
      â”œâ”€ ğŸ“ ipdl
        â””â”€ PTestBasicParent.cpp
        â””â”€ PTestBasicChild.cpp
        â””â”€ PTestBasic.cpp
    â””â”€ setup
    â””â”€ refresh-mozilla-central-stuff
  â””â”€ webtest-config.json
  â””â”€ tree-list.js
  â””â”€ config.json
  â””â”€ searchfox-config.json
  â””â”€ help.html
â”œâ”€ ğŸ“ extension
  â””â”€ background.js
  â””â”€ manifest.json
â”œâ”€ ğŸ“ tree-configs
  â””â”€ README.md
â””â”€ LICENSE
â””â”€ README.md
â””â”€ run-docker.sh
â””â”€ Vagrantfile
â””â”€ .eslintrc.js
â””â”€ CODE_OF_CONDUCT.md
â””â”€ .prettierrc
â””â”€ flake.nix
â””â”€ Makefile
â””â”€ .travis.yml
â””â”€ CONTRIBUTING.md
â””â”€ .pre-commit-config.yaml
â””â”€ TODO


# Project Files

- docs/old-vagrant-setup.md
- docs/bash-scripting-cheatsheet.md
- docs/a11y.md
- docs/testing-checks.md
- docs/nss-stuff.txt
- docs/addons.txt
- docs/webtest.md
- docs/web-server.md
- docs/searchfox-tool-cookbook.md
- docs/blame.md
- docs/platform-notes.txt
- docs/manual-indexing.md
- docs/aws.md
- docs/crossref.md
- docs/newrepo.md
- docs/vm-docker.md
- docs/analysis.md
- docs/blame-design.txt
- docs/output.md
- docs/mach-design.txt
- docs/index-directory-contents.md
- docs/liquid-templating-cheatsheet.md
- LICENSE
- README.md
- router/router.py
- router/codesearch.py
- router/logger.py
- router/crossrefs.py
- router/raw_search.py
- router/identifiers.py
- run-docker.sh
- scripts/js-analyze.sh
- scripts/compress-outputs.sh
- scripts/check-helper.sh
- scripts/generate-config.sh
- scripts/find-repo-files.py
- scripts/indexer-logs-print.py
- scripts/webidl-analyze.py
- scripts/webtest.sh
- scripts/generate-other-resources-list.py
- scripts/indexer-logs-analyze.sh
- scripts/staticprefs-analyze.sh
- scripts/idl-analyze.sh
- scripts/staticprefs-analyze.py
- scripts/weblog-elb-fetch.sh
- scripts/find-objdir-files.sh
- scripts/css-analyze.sh
- scripts/web-analyze/wasm-css-analyzer/src/lib.rs
- scripts/web-analyze/wasm-css-analyzer/Cargo.toml
- scripts/replace-aliases.sh
- scripts/crossref.sh
- scripts/js-analyze.js
- scripts/nginx-setup.py
- scripts/ipdl-analyze.sh
- scripts/lib.py
- scripts/scip-analyze.sh
- scripts/idl-analyze.py
- scripts/ld-wrapper
- scripts/process-chrome-map.py
- scripts/indexer-setup.py
- scripts/update-fonts.sh
- scripts/mkindex.sh
- scripts/mkdirs.sh
- scripts/indexer-logs-fetch.sh
- scripts/weblog-analyze.sh
- scripts/objdir-mkdirs.sh
- scripts/output.sh
- scripts/rust-analyze.sh
- scripts/load-vars.sh
- scripts/generate-analsysis-files-list.sh
- scripts/weblog-elb-analyze.sh
- scripts/replace-aliases.py
- scripts/html-analyze.sh
- scripts/check-index.sh
- scripts/fontello-config.json
- static/robots.txt
- static/font/icons.woff2
- static/font/icons.woff
- static/font/LICENSE.txt
- static/font/icons.ttf
- static/font/icons.eot
- static/js/blame.js
- static/js/settings.js
- static/js/search.js
- static/js/panel.js
- static/js/code-highlighter.js
- static/js/context-menu.js
- static/css/font-icons.css
- static/css/icons.css
- static/css/mozsearch.css
- tools/src/symbol_graph_edge_kind.rs
- tools/src/output.rs
- tools/src/logging.rs
- tools/src/tree_sitter_support/cst_tokenizer.rs
- tools/src/tree_sitter_support/mod.rs
- tools/src/glob_helper.rs
- tools/src/bin/scip-indexer.rs
- tools/src/bin/css-analyze.rs
- tools/src/bin/output-file.rs
- tools/src/bin/rust-indexer.rs
- tools/src/bin/ipdl-analyze.rs
- tools/src/bin/pipeline-server.rs
- tools/src/bin/crossref.rs
- tools/src/bin/searchfox-tool.rs
- tools/src/bin/merge-analyses.rs
- tools/src/bin/web-server.rs
- tools/src/git_ops.rs
- tools/src/url_map_handler.rs
- tools/src/file_utils.rs
- tools/src/languages.rs
- tools/src/css_analyzer.rs
- tools/src/lib.rs
- tools/src/blame.rs
- tools/src/abstract_server/server_interface.rs
- tools/src/abstract_server/remote_server.rs
- tools/src/abstract_server/lazy_crossref.rs
- tools/src/abstract_server/local_index.rs
- tools/src/abstract_server/mod.rs
- tools/src/links.rs
- tools/src/describe.rs
- tools/src/file_format/analysis.rs
- tools/src/file_format/analysis_manglings.rs
- tools/src/file_format/url_map.rs
- tools/src/file_format/history/syntax_symdex.rs
- tools/src/file_format/history/syntax_files_struct.rs
- tools/src/file_format/history/timeline_files_delta.rs
- tools/src/file_format/history/timeline_tokens.rs
- tools/src/file_format/history/rev_summaries.rs
- tools/src/file_format/history/io_helpers.rs
- tools/src/file_format/history/mod.rs
- tools/src/file_format/history/timeline_common.rs
- tools/src/file_format/merger.rs
- tools/src/file_format/config.rs
- tools/src/file_format/globbing_file_list.rs
- tools/src/file_format/ontology_pointer_kind.rs
- tools/src/file_format/ontology_mapping.rs
- tools/src/file_format/crossref_converter.rs
- tools/src/file_format/crossref_lookup.rs
- tools/src/file_format/per_file_info.rs
- tools/src/file_format/identifiers.rs
- tools/src/file_format/doc_trees.rs
- tools/src/file_format/mod.rs
- tools/src/file_format/repo_data_ingestion.rs
- tools/src/tokenize.rs
- tools/src/cmd_pipeline/cmd_search_text.rs
- tools/src/cmd_pipeline/cmd_webtest.rs
- tools/src/cmd_pipeline/cmd_graph.rs
- tools/src/cmd_pipeline/cmd_render.rs
- tools/src/cmd_pipeline/cmd_crossref_expand.rs
- tools/src/cmd_pipeline/cmd_augment_results.rs
- tools/src/cmd_pipeline/cmd_filter_analysis.rs
- tools/src/cmd_pipeline/cmd_format_symbols.rs
- tools/src/cmd_pipeline/cmd_fuse_crossrefs.rs
- tools/src/cmd_pipeline/cmd_search_identifiers.rs
- tools/src/cmd_pipeline/cmd_search.rs
- tools/src/cmd_pipeline/cmd_show_html.rs
- tools/src/cmd_pipeline/symbol_graph.rs
- tools/src/cmd_pipeline/cmd_cat_html.rs
- tools/src/cmd_pipeline/cmd_query.rs
- tools/src/cmd_pipeline/cmd_compile_results.rs
- tools/src/cmd_pipeline/cmd_jumpref_lookup.rs
- tools/src/cmd_pipeline/cmd_tokenize_source.rs
- tools/src/cmd_pipeline/cmd_traverse.rs
- tools/src/cmd_pipeline/cmd_batch_render.rs
- tools/src/cmd_pipeline/parser.rs
- tools/src/cmd_pipeline/interface.rs
- tools/src/cmd_pipeline/hier_graph.rs
- tools/src/cmd_pipeline/mod.rs
- tools/src/cmd_pipeline/cmd_search_files.rs
- tools/src/cmd_pipeline/cmd_merge_analyses.rs
- tools/src/cmd_pipeline/transforms.rs
- tools/src/cmd_pipeline/cmd_crossref_lookup.rs
- tools/src/cmd_pipeline/cmd_prod_filter.rs
- tools/src/format.rs
- tools/src/query/chew_query.rs
- tools/src/query/query_core.toml
- tools/src/query/mod.rs
- tools/src/doc_trees_handler.rs
- tools/src/url_encode_path.rs
- tools/src/templating/liquid_exts.rs
- tools/src/templating/mod.rs
- tools/templates/settings.liquid
- tools/templates/ontology_ingestion_explainer.liquid
- tools/templates/help_index.liquid
- tools/templates/scroll_footer.liquid
- tools/templates/navigation_panel.liquid
- tools/templates/pipeline_explainer.liquid
- tools/templates/dir_listing.liquid
- tools/templates/repo_ingestion_explainer.liquid
- tools/templates/query_results.liquid
- tools/templates/search_template.liquid
- tools/templates/footer.liquid
- tools/templates/path.liquid
- tools/templates/header_search.liquid
- tools/templates/breadcrumbs.liquid
- tools/templates/header_query.liquid
- tools/templates/query_results/line_span.liquid
- tools/templates/query_results/kind_group.liquid
- tools/templates/query_results/file_table.liquid
- tools/templates/query_results/graph_collection_root.liquid
- tools/templates/query_results/symbol_tree_table.liquid
- tools/templates/query_results/file_results.liquid
- tools/templates/query_results/graph_root.liquid
- tools/templates/query_results/text_file_root.liquid
- tools/templates/query_results/symbol_crossref_info_list_root.liquid
- tools/templates/query_results/symbol_tree_table_node.liquid
- tools/templates/query_results/facet_root.liquid
- tools/templates/query_results/facet_group.liquid
- tools/templates/query_results/symbol_tree_table_list_root.liquid
- tools/templates/query_results/symbol.liquid
- tools/templates/query_results/pathkind_group.liquid
- tools/templates/query_results/rb_root.liquid
- tools/templates/panel.liquid
- tools/.cargo/config.toml
- tools/languages/tokenizer_queries/python.scm
- tools/languages/tokenizer_queries/rust.scm
- tools/languages/tokenizer_queries/cpp.scm
- tools/languages/tokenizer_queries/typescript.scm
- tools/Cargo.toml
- tools/tests/test_check_insta.rs
- Vagrantfile
- infrastructure/aws/detach-volume.py
- infrastructure/aws/web-serve.sh
- infrastructure/aws/trigger-provision.py
- infrastructure/aws/awslib.py
- infrastructure/aws/upload.py
- infrastructure/aws/upload-lambda-zips-from-outside-vm.sh
- infrastructure/aws/mkscratch.sh
- infrastructure/aws/attach-index-volume.py
- infrastructure/aws/warning-suppression.patterns
- infrastructure/aws/index.sh
- infrastructure/aws/send-warning-email.py
- infrastructure/aws/send-done-email.py
- infrastructure/aws/trigger_common.py
- infrastructure/aws/make-crontab.py
- infrastructure/aws/main.sh
- infrastructure/aws/scp-while-sshed.py
- infrastructure/aws/delete-volume.py
- infrastructure/aws/indexer-provision.sh
- infrastructure/aws/web-server-provision.sh
- infrastructure/aws/send-failure-email.py
- infrastructure/aws/shell-setup.sh
- infrastructure/aws/trigger-web-server.py
- infrastructure/aws/trigger_indexer.py
- infrastructure/aws/send-provision-email.py
- infrastructure/aws/ssh.py
- infrastructure/aws/channel-tool.py
- infrastructure/aws/terminate-indexer.py
- infrastructure/aws/trigger_shell.py
- infrastructure/web-server-run.sh
- infrastructure/common-provision-post.sh
- infrastructure/vagrant/indexer-provision.sh
- infrastructure/web-server-check.sh
- infrastructure/indexer-update.sh
- infrastructure/web-server-update.sh
- infrastructure/Dockerfile
- infrastructure/indexer-upload.sh
- infrastructure/docker-provision.sh
- infrastructure/web-server-setup.sh
- infrastructure/reblame-run.sh
- infrastructure/indexer-setup.sh
- infrastructure/indexer-provision.sh
- infrastructure/web-server-provision.sh
- infrastructure/common-provision-pre.sh
- infrastructure/indexer-run.sh
- .eslintrc.js
- CODE_OF_CONDUCT.md
- .prettierrc
- trees/README.md
- flake.nix
- Makefile
- .travis.yml
- CONTRIBUTING.md
- sax/README
- sax/LICENSE
- sax/sax.js
- sax/sax.patch
- .pre-commit-config.yaml
- clang-plugin/BindingOperations.h
- clang-plugin/StringOperations.cpp
- clang-plugin/MozsearchIndexer.cpp
- clang-plugin/Makefile
- clang-plugin/FileOperations.h
- clang-plugin/testfiles/xyz/test.cpp
- clang-plugin/testfiles/test.cpp
- clang-plugin/.clang-format
- clang-plugin/StringOperations.h
- clang-plugin/BindingOperations.cpp
- clang-plugin/FileOperations.cpp
- clang-plugin/from-clangd/README.md
- clang-plugin/from-clangd/HeuristicResolver.h
- clang-plugin/from-clangd/HeuristicResolver.cpp
- TODO
- config_defaults/per-file-info.toml
- config_defaults/source_file_other_tools_panels.liquid
- config_defaults/ontology-mapping.toml
- config_defaults/source_file_info_boxes.liquid
- tests/tree-list.js
- tests/searchfox/setup
- tests/webtest-config.json
- tests/webtest/head.js
- tests/webtest/test_LineNumberInURL.js
- tests/webtest/test_CopyAsMarkdown.js
- tests/webtest/test_FieldLayoutForGenerated.js
- tests/webtest/test_SymbolSectionInPanel.js
- tests/webtest/test_MacroExpansions.js
- tests/webtest/test_SpaceInFilename.js
- tests/webtest/test_UTF8InFilename.js
- tests/webtest/test_FieldLayoutContextMenu.js
- tests/webtest/test_WebIDLBindings.js
- tests/webtest/webtest.html
- tests/webtest/test_Titler.js
- tests/webtest/test_Panel.js
- tests/webtest/test_SearchSection.js
- tests/webtest/test_Breadcrumbs.js
- tests/webtest/test_Search.js
- tests/webtest/test_DebugUI.js
- tests/webtest/test_OverloadInTemplate.js
- tests/webtest/test_TreeSwitcherKeys.js
- tests/webtest/test_ContextMenuKeys.js
- tests/webtest/test_FieldLayoutColumns.js
- tests/config.json
- tests/searchfox-config.json
- tests/tests/checks/src/dummy-test-package.rs
- tests/tests/checks/inputs/fancy/diagram/traverse/lots_of_calls.cpp/paths_between__four_left_one_right__json
- tests/tests/checks/inputs/fancy/diagram/traverse/big_cpp.cpp/uses__thing_takeDamage__json
- tests/tests/checks/inputs/fancy/diagram/traverse/big_cpp.cpp/uses__thing_takeDamage__hier__dot
- tests/tests/checks/inputs/fancy/diagram/traverse/big_cpp.cpp/callees__outercat_meet__json
- tests/tests/checks/inputs/fancy/diagram/traverse/big_cpp.cpp/callees__outercat_meet__dot
- tests/tests/checks/inputs/fancy/diagram/traverse/big_cpp.cpp/callees__colorize__outercat_meet__dot
- tests/tests/checks/inputs/fancy/diagram/traverse/big_cpp.cpp/uses__thing_takeDamage__hier__json
- tests/tests/checks/inputs/fancy/format-symbol/field-layout/platform_specific_field.cpp/field_layout__platform_specific_field_1__json
- tests/tests/checks/inputs/fancy/format-symbol/field-layout/platform_specific_field.cpp/field_layout__platform_specific_field_2__json
- tests/tests/checks/inputs/fancy/format-symbol/field-layout/empty.cpp/field_layout__empty__json
- tests/tests/checks/inputs/fancy/format-symbol/field-layout/platform_specific_size.cpp/field_layout__platform_specific_size__json
- tests/tests/checks/inputs/fancy/format-symbol/field-layout/non-struct.cpp/field_layout__non_struct__json
- tests/tests/checks/inputs/fancy/format-symbol/field-layout/bitfields.cpp/field_layout__bitfields__json
- tests/tests/checks/inputs/fancy/format-symbol/field-layout/field-type.cpp/field_show_hide__name__json
- tests/tests/checks/inputs/fancy/format-symbol/field-layout/field-type.cpp/field_layout__field_type__json
- tests/tests/checks/inputs/fancy/format-symbol/field-layout/holes.cpp/field_layout__holes__json
- tests/tests/checks/inputs/fancy/format-symbol/field-layout/big_cpp.cpp/field_layout__outercat__json
- tests/tests/checks/inputs/fancy/format-symbol/field-layout/multiple_inheritance.cpp/field_layout__multiple_inheritance__json
- tests/tests/checks/inputs/fancy/format-symbol/field-layout/vtable.cpp/field_layout__vtable__json
- tests/tests/checks/inputs/analysis/html/script.html/def_func__json
- tests/tests/checks/inputs/analysis/html/script.html/def_func_for_attribute__json
- tests/tests/checks/inputs/analysis/html/script.html/use_func__json
- tests/tests/checks/inputs/analysis/html/script.html/use_func_in_attribute__json
- tests/tests/checks/inputs/analysis/html/script.html/unknown_script__json
- tests/tests/checks/inputs/analysis/webidl/external-interface.webidl/webidl_external_interface__json
- tests/tests/checks/inputs/analysis/webidl/dictionary.webidl/webidl_dictionary__json
- tests/tests/checks/inputs/analysis/webidl/partial-interface.webidl/webidl_partial_interface__json
- tests/tests/checks/inputs/analysis/webidl/const.webidl/webidl_const__json
- tests/tests/checks/inputs/analysis/webidl/interface-mixin.webidl/webidl_interface_mixin__json
- tests/tests/checks/inputs/analysis/webidl/maplike.webidl/webidl_maplike__json
- tests/tests/checks/inputs/analysis/webidl/overload.webidl/webidl_overload__json
- tests/tests/checks/inputs/analysis/webidl/asynciterable.webidl/webidl_asynciterable__json
- tests/tests/checks/inputs/analysis/webidl/typedef.webidl/webidl_typedef__json
- tests/tests/checks/inputs/analysis/webidl/includes.webidl/webidl_asynciterable__json
- tests/tests/checks/inputs/analysis/webidl/iterable.webidl/webidl_iterable__json
- tests/tests/checks/inputs/analysis/webidl/partial-dictionary.webidl/webidl_partial_dictionary__json
- tests/tests/checks/inputs/analysis/webidl/enum.webidl/webidl_enum__json
- tests/tests/checks/inputs/analysis/webidl/callback.webidl/webidl_callback__json
- tests/tests/checks/inputs/analysis/webidl/namespace.webidl/webidl_namespace__json
- tests/tests/checks/inputs/analysis/webidl/setlike.webidl/webidl_setlike__json
- tests/tests/checks/inputs/analysis/webidl/method.webidl/webidl_method__json
- tests/tests/checks/inputs/analysis/webidl/attribute.webidl/webidl_attribute__json
- tests/tests/checks/inputs/analysis/webidl/super-dictionary.webidl/webidl_super_dictionary__json
- tests/tests/checks/inputs/analysis/webidl/super-interface.webidl/webidl_super_interface__json
- tests/tests/checks/inputs/analysis/webidl/partial-namespace.webidl/webidl_partial_namespace__json
- tests/tests/checks/inputs/analysis/webidl/type.webidl/webidl_type__json
- tests/tests/checks/inputs/analysis/rust/test_rust_dependency/MyType__html
- tests/tests/checks/inputs/analysis/java/KotlinTest.kt/external_dependency__json
- tests/tests/checks/inputs/analysis/rust/test_rust_dependency/MyType__json
- tests/tests/checks/inputs/analysis/java/InlineObject.kt/method_with_return_type__json
- tests/tests/checks/inputs/analysis/rust/simple.rs/simple_Loader_new__html
- tests/tests/checks/inputs/analysis/java/InlineObject.kt/method__json
- tests/tests/checks/inputs/analysis/rust/simple.rs/simple_Loader_new__json
- tests/tests/checks/inputs/analysis/js/export4.mjs/exported_decls__json
- tests/tests/checks/inputs/analysis/js/imported-module.mjs/def_ModuleClass__json
- tests/tests/checks/inputs/analysis/js/imported-module.mjs/def_ModuleClass_error__json
- tests/tests/checks/inputs/analysis/js/imported-module.mjs/def_moduleFunc__json
- tests/tests/checks/inputs/analysis/js/imported-module.mjs/def_moduleConst__json
- tests/tests/checks/inputs/analysis/js/export6.mjs/exported_decls__json
- tests/tests/checks/inputs/analysis/js/export5.mjs/exported_decls__json
- tests/tests/checks/inputs/analysis/js/import.mjs/imported_symbols__json
- tests/tests/checks/inputs/analysis/js/export.mjs/exported_use__json
- tests/tests/checks/inputs/analysis/js/import.mjs/imported_symbols_use__json
- tests/tests/checks/inputs/analysis/js/export.mjs/exported_decls__json
- tests/tests/checks/inputs/analysis/urlmap/root.xhtml/url_def_in_xhtml__json
- tests/tests/checks/inputs/analysis/urlmap/root.xhtml/url_use_in_xhtml__json
- tests/tests/checks/inputs/analysis/js/secret-madjewel.js/def_secretMadjewelConst__json
- tests/tests/checks/inputs/analysis/js/root-module.mjs/def_rootModuleConst__json
- tests/tests/checks/inputs/analysis/urlmap/chrome1.mjs/url_use_with_relative__json
- tests/tests/checks/inputs/analysis/urlmap/chrome1.mjs/no_stray_relpath__json
- tests/tests/checks/inputs/analysis/js/export3.mjs/exported_decls__json
- tests/tests/checks/inputs/analysis/js/export2.mjs/exported_use__json
- tests/tests/checks/inputs/analysis/js/export2.mjs/exported_decls__json
- tests/tests/checks/inputs/analysis/urlmap/subdir/sub.mjs/url_use_with_relative_parent__json
- tests/tests/checks/inputs/analysis/js/export8.mjs/ignore_anonymous_function_default__json
- tests/tests/checks/inputs/analysis/urlmap/root.js/no_stray_url__json
- tests/tests/checks/inputs/analysis/urlmap/root.js/url_def_in_script__json
- tests/tests/checks/inputs/analysis/urlmap/root.js/url_use_in_script__json
- tests/tests/checks/inputs/analysis/urlmap/root.mjs/url_use_in_module__json
- tests/tests/checks/inputs/analysis/js/some_javascript.js/all__priv_field_num__json
- tests/tests/checks/inputs/analysis/js/some_javascript.js/all__priv_field_num__html
- tests/tests/checks/inputs/analysis/urlmap/root.mjs/url_def_in_module__json
- tests/tests/checks/inputs/analysis/js/export7.mjs/exported_decls__json
- tests/tests/checks/inputs/analysis/urlmap/root.html/url_use_in_html__json
- tests/tests/checks/inputs/analysis/urlmap/root.html/url_def_in_html__json
- tests/tests/checks/inputs/analysis/urlmap/resource1.mjs/url_use_with_relative__json
- tests/tests/checks/inputs/analysis/cpp/template_shapes.cpp/Foo_Project
- tests/tests/checks/inputs/analysis/cpp/ForwardingTemplates.cpp/TypeIndependentNewInTemplateReportedInTemplate
- tests/tests/checks/inputs/analysis/cpp/ForwardingTemplates.cpp/TypeDependentNewInMethodReportedAtCallSite
- tests/tests/checks/inputs/analysis/cpp/big_header.h/def_big_header__json
- tests/tests/checks/inputs/analysis/cpp/ForwardingTemplates.cpp/TypeDependentNewInTemplateReportedAtCallSite
- tests/tests/checks/inputs/analysis/cpp/big_header.h/def_big_header__html
- tests/tests/checks/inputs/analysis/cpp/template_specialization.cpp/some_function_called_in_partial_specialization
- tests/tests/checks/inputs/analysis/cpp/URL_sym.cpp/URL_sym__json
- tests/tests/checks/inputs/analysis/cpp/lambdas.cpp/Struct0UsedInLambda
- tests/tests/checks/inputs/analysis/cpp/lambdas.cpp/LambdaFields
- tests/tests/checks/inputs/analysis/cpp/overs.cpp/def__DoubleBase_doublePure__html
- tests/tests/checks/inputs/analysis/cpp/lambdas.cpp/Struct1UsedInLambda
- tests/tests/checks/inputs/analysis/cpp/macro.cpp/TEST_MACRO_INCLUDE
- tests/tests/checks/inputs/analysis/cpp/macro.cpp/MULTI_LINE_MACRO
- tests/tests/checks/inputs/analysis/cpp/atom_list.h/YO_ATOM
- tests/tests/checks/inputs/analysis/cpp/macro.cpp/EMPTY_MACRO__json
- tests/tests/checks/inputs/analysis/cpp/atom_list.h/NESTED_YO_ATOM
- tests/tests/checks/inputs/analysis/cpp/macro.cpp/NESTED_MACRO_WITH_ARG
- tests/tests/checks/inputs/analysis/cpp/atom_list.h/structured_yo_atoms_foo_string__json
- tests/tests/checks/inputs/analysis/cpp/macro.cpp/CONST_MACRO__json
- tests/tests/checks/inputs/analysis/cpp/macro.cpp/PER_TARGET_FUNCTION
- tests/tests/checks/inputs/analysis/cpp/macro.cpp/IDENT_MACRO__json
- tests/tests/checks/inputs/analysis/cpp/macro.cpp/NESTED_MACRO
- tests/tests/checks/inputs/analysis/cpp/bug1781178.cpp/Foo_Typedef
- tests/tests/checks/inputs/analysis/ipdl/PTestBasic.ipdl/def_PTestBasic__Hello__html
- tests/tests/checks/inputs/analysis/ipdl/PTestBasic.ipdl/def_PTestBasic__Hello__json
- tests/tests/checks/inputs/analysis/cpp/bug1781178.cpp/Point_IsThereOne
- tests/tests/checks/inputs/analysis/cpp/templates7.cpp/def_OutOfLineTemplateShouldntHaveContextSym
- tests/tests/checks/inputs/analysis/cpp/bug1781178.cpp/Foo_Static
- tests/tests/checks/inputs/analysis/cpp/bug1781178.cpp/Foo_Simple
- tests/tests/checks/inputs/analysis/cpp/bug1781178.cpp/Foo_Nested_field
- tests/tests/checks/inputs/analysis/cpp/bug1781178.cpp/WithOverloads_Overloaded
- tests/tests/checks/inputs/analysis/cpp/bug1781178.cpp/Foo_Project
- tests/tests/checks/inputs/analysis/cpp/bug1781178.cpp/Foo_E_Waldo
- tests/tests/checks/inputs/analysis/cpp/templates7.cpp/def_OutOfLineShouldntHaveContextSym
- tests/tests/checks/inputs/analysis/cpp/bug1781178.cpp/Foo_E
- tests/tests/checks/inputs/analysis/cpp/bug1781178.cpp/internal_Read
- tests/tests/checks/inputs/analysis/cpp/templates7.cpp/def_InlineShouldHaveContextSym
- tests/tests/checks/inputs/analysis/cpp/templates7.cpp/def_InlineTemplateShouldHaveContextSym
- tests/tests/checks/inputs/analysis/cpp/big_cpp.cpp/structured_thing__json
- tests/tests/checks/inputs/analysis/cpp/big_cpp.cpp/structured_practicalart_beart__json
- tests/tests/checks/inputs/analysis/xpidl/xpctest_params.idl/use_testOctet__json
- tests/tests/checks/inputs/analysis/cpp/big_cpp.cpp/structured_thing_takedamage__json
- tests/tests/checks/inputs/analysis/xpidl/xpctest_params.idl/def_testOctet__json
- tests/tests/checks/inputs/analysis/cpp/big_cpp.cpp/structured_superhero__json
- tests/tests/checks/inputs/analysis/cpp/big_cpp.cpp/structured_lessglobalcontext_decidewhethertodecide__json
- tests/tests/checks/inputs/analysis/xpidl/xpctest_params.idl/def_testOctet__html
- tests/tests/checks/inputs/analysis/cpp/big_cpp.cpp/structured_abstractart_beart__json
- tests/tests/checks/inputs/analysis/cpp/big_cpp.cpp/structured_globalcontext__json
- tests/tests/checks/inputs/analysis/cpp/big_cpp.cpp/include_big_header__json
- tests/tests/checks/inputs/analysis/cpp/big_cpp.cpp/structured_human__json
- tests/tests/checks/inputs/analysis/cpp/big_cpp.cpp/include_big_header__prodjson
- tests/tests/checks/inputs/analysis/cpp/big_cpp.cpp/structured_superhero_takedamage__json
- tests/tests/checks/inputs/analysis/cpp/big_cpp.cpp/include_big_header__prodhtml
- tests/tests/checks/inputs/analysis/cpp/big_cpp.cpp/def_thing__json
- tests/tests/checks/inputs/analysis/cpp/big_cpp.cpp/structured_lessglobalcontext__json
- tests/tests/checks/inputs/analysis/cpp/big_cpp.cpp/structured_outercat__json
- tests/tests/checks/inputs/analysis/cpp/big_cpp.cpp/include_big_header__html
- tests/tests/checks/inputs/analysis/xpidl/xpctest_attributes.idl/idl_stringProperty__html
- tests/tests/checks/inputs/analysis/staticprefs/staticprefs__json
- tests/tests/checks/inputs/analysis/xpidl/xpctest_attributes.idl/idl_stringProperty__json
- tests/tests/checks/inputs/blame/syntax-token-tokenize/big_cpp.cpp__tokenize__file
- tests/tests/checks/inputs/analysis/css/embed_css.html/custom_prop__json
- tests/tests/checks/inputs/blame/syntax-token-tokenize/big_cpp.cpp__tokenize_outline__json
- tests/tests/checks/inputs/analysis/css/embed_css.html/url__json
- tests/tests/checks/inputs/analysis/css/test.css/custom_prop__json
- tests/tests/checks/inputs/analysis/css/test.css/url__json
- tests/tests/checks/inputs/web/files/nav-panel/wpt__test_ima_weird_meta_wpt__navpanel__html
- tests/tests/checks/inputs/web/files/nav-panel/wpt__some_cross_global_test__navpanel__html
- tests/tests/checks/inputs/web/files/test-info-boxes/wpt__some_cross_global_test__html
- tests/tests/checks/inputs/web/files/urlmap/links_in_comment__html
- tests/tests/checks/inputs/web/files/test-info-boxes/test_many_manifest_permutations__html
- tests/tests/checks/inputs/web/files/test-info-boxes/text_custom_element_base_xul__infobox__html
- tests/tests/checks/inputs/web/files/test-info-boxes/wpt__test_ima_sad_subtests_wpt__html
- tests/tests/checks/inputs/web/files/test-info-boxes/wpt__test_ima_disabled_wpt__html
- tests/tests/checks/inputs/web/files/urlmap/links_in_string__html
- tests/tests/checks/inputs/web/docs/docs_md__html
- tests/tests/checks/inputs/web/docs/docs_rst__html
- tests/tests/checks/inputs/web/files/contextual-keyword/contextual-keyword_webidl__html
- tests/tests/checks/inputs/web/files/contextual-keyword/contextual-keyword_py__html
- tests/tests/checks/inputs/web/templates/search_template__html
- tests/tests/checks/inputs/web/files/contextual-keyword/contextual-keyword_rs__html
- tests/tests/checks/inputs/web/templates/help_file__html
- tests/tests/checks/inputs/web/files/contextual-keyword/contextual-keyword_cpp__html
- tests/tests/checks/inputs/web/files/contextual-keyword/contextual-keyword_js__html
- tests/tests/checks/inputs/search-text/global/searchfox_plain__json
- tests/tests/checks/inputs/web/search/overs.cpp/doublePure__json
- tests/tests/checks/inputs/web/search/overs.cpp/triplePure__json
- tests/tests/checks/inputs/web/query/parsing/simple/default__unquoted_word__json
- tests/tests/checks/inputs/web/query/parsing/simple/default__quoted_dashed__json
- tests/tests/checks/inputs/web/query/execution/field-layout/query__field_layout__outercat__json
- tests/tests/checks/inputs/web/query/parsing/simple/default__needs_regexp_escape__json
- tests/tests/checks/inputs/web/query/parsing/simple/default__unquoted_dashed__json
- tests/tests/checks/inputs/web/query/parsing/simple/default__quoted_phrase__json
- tests/tests/checks/inputs/web/query/parsing/simple/lots_of_calls.cpp/query__parse__calls_to__four_left_and_right__svg
- tests/tests/checks/inputs/web/query/parsing/simple/overs.cpp/query__parse__doublePure__context4__json
- tests/tests/checks/inputs/web/query/parsing/simple/overs.cpp/query__parse__doublePure__json
- tests/tests/checks/inputs/web/query/parsing/simple/lots_of_calls.cpp/query__parse__calls_between__four_left_one_right__svg
- tests/tests/checks/inputs/web/query/parsing/simple/big_cpp.cpp/query__parse_calls_to__graoh_format__takeDamage__json
- tests/tests/checks/inputs/web/query/parsing/simple/big_cpp.cpp/query__parse__calls_to__takeDamage__json
- tests/tests/checks/inputs/web/query/parsing/simple/big_cpp.cpp/query__parse__calls_to__colorize__takeDamage__json
- tests/tests/checks/inputs/web/query/execution/simple/overs.cpp/query__doublePure__context4__json
- tests/tests/checks/inputs/web/query/execution/simple/overs.cpp/query__doublePure__json
- tests/tests/checks/inputs/web/query/execution/simple/overs.cpp/query__doublePure__context_alias4__json
- tests/tests/checks/inputs/web/dirs/generated_listing__html
- tests/tests/checks/inputs/web/search/big_cpp.cpp/practicalart_beart__json
- tests/tests/checks/inputs/web/search/big_cpp.cpp/human__json
- tests/tests/checks/inputs/web/search/big_cpp.cpp/practicalart__json
- tests/tests/checks/inputs/web/dirs/root_listing__html
- tests/tests/checks/inputs/web/search/big_cpp.cpp/abstractart_beart__json
- tests/tests/checks/inputs/web/search/big_cpp.cpp/abstractart__json
- tests/tests/checks/inputs/search-text/global/searchfox_boring_re__json
- tests/tests/checks/inputs/search-text/global/searchfox_re_multi_o__json
- tests/tests/checks/inputs/search-files/anchored_doublestar_html__json
- tests/tests/checks/inputs/search-files/anchored_fakewpt_doublestar_combi__json
- tests/tests/checks/inputs/search-files/anchored_star_html__json
- tests/tests/checks/inputs/search-files/file_list_ingestion_check__json
- tests/tests/checks/inputs/jumpref/jumpref/LightweightThemeManager.jsm/ADDON_TYPE__lookup__json
- tests/tests/checks/inputs/crossref/java/KotlinLibrary.kt/links_to_kotlin_library_constructor__json
- tests/tests/checks/inputs/crossref/merge/merge__big_cpp
- tests/tests/checks/inputs/crossref/java/package__json
- tests/tests/checks/inputs/crossref/java/JavaLibrary.java/no_inverse_relationships__json
- tests/tests/checks/inputs/crossref/java/JavaLibrary.java/links_to_java_library_constructor__json
- tests/tests/checks/inputs/crossref/java/JavaLibrary.java/multiple_implements__json
- tests/tests/checks/inputs/crossref/identifiers/big_cpp.cpp/abstractart_beart__json
- tests/tests/checks/inputs/crossref/identifiers/big_cpp.cpp/thing__json
- tests/tests/checks/inputs/crossref/crossref/big_cpp.cpp/thing_mHP__json
- tests/tests/checks/inputs/crossref/crossref/xpctest_params.idl/testOctet__json
- tests/tests/checks/inputs/crossref/crossref/big_cpp.cpp/abstractart__beart__json
- tests/tests/checks/inputs/crossref/crossref/big_cpp.cpp/stackartholder__json
- tests/tests/checks/inputs/crossref/crossref/big_cpp.cpp/thing__json
- tests/tests/checks/inputs/crossref/expand/big_cpp.cpp/outercat__json
- tests/tests/checks/inputs/jumpref/jumpref/simple.rs/loader__needs_hard_reload__lookup__json
- tests/tests/checks/inputs/crossref/crossref/xpctest_attributes.idl/booleanProperty_getter__json
- tests/tests/checks/inputs/crossref/crossref/xpctest_attributes.idl/booleanProperty_setter__json
- tests/tests/checks/Cargo.toml
- tests/tests/checks/inputs/jumpref/jumpref/big_cpp.cpp/practicalart__beart__lookup__json
- tests/tests/checks/inputs/jumpref/jumpref/big_cpp.cpp/big_header__h__lookup__json
- tests/tests/checks/inputs/jumpref/jumpref/big_cpp.cpp/header_declared_func__lookup__json
- tests/tests/checks/inputs/jumpref/jumpref/big_cpp.cpp/abstractart__beart__lookup__json
- tests/tests/setup
- tests/tests/metadata/wpt-manifest.json
- tests/tests/metadata/README.md
- tests/tests/metadata/test2.chrome-map.json
- tests/tests/metadata/wpt-mozilla-manifest.json
- tests/tests/metadata/test.chrome-map.json
- tests/tests/metadata/test-info-all-tests.json
- tests/tests/metadata/doc-trees.json
- tests/tests/metadata/bugzilla-components.json
- tests/tests/metadata/wpt-metadata-summary.json
- tests/tests/files/enummacro.h
- tests/tests/files/some_javascript.js
- tests/tests/files/xpidl_cpp_consumer.cpp
- tests/tests/files/some_python.py
- tests/tests/files/gzip-colliding-file.gz
- tests/tests/files/.clang-format
- tests/tests/files/test_talosconfig_browser_config.json
- tests/tests/files/templates4.cpp
- tests/tests/files/root-module.mjs
- tests/tests/files/mochitest-common.ini
- tests/tests/files/test_rust_dependency/src/lib.rs
- tests/tests/files/chrome.ini
- tests/tests/files/bug1781178.cpp
- tests/tests/files/cpp/URL_sym.cpp
- tests/tests/files/cpp/contextual-keyword.cpp
- tests/tests/files/bug1588908.js
- tests/tests/files/LightweightThemeManager.jsm
- tests/tests/files/Cargo.toml
- tests/tests/files/bug1449291.cpp
- tests/tests/files/tricky_symbol_names.cpp
- tests/tests/files/bug1446220_unicode.html
- tests/tests/files/secret-madjewel.js
- tests/tests/files/atom_list.h
- tests/tests/files/test_many_manifest_permutations.js
- tests/tests/files/gzip-colliding-file
- tests/tests/files/test_rust_dependency/Cargo.toml
- tests/tests/files/implicit.cpp
- tests/tests/files/some_ini.ini
- tests/tests/files/templates2.cpp
- tests/tests/files/templates_nsTArray.cpp
- tests/tests/files/subdir/header@with,many^strange~chars.h
- tests/tests/files/nsISupports.h
- tests/tests/files/README.md
- tests/tests/files/nsTArray.h
- tests/tests/files/field-layout/field-type.h
- tests/tests/files/field-layout/empty.cpp
- tests/tests/files/field-layout/field-type-include.h
- tests/tests/files/field-layout/empty-subclass.cpp
- tests/tests/files/field-layout/platform_specific_field.cpp
- tests/tests/files/field-layout/platform_specific_size.cpp
- tests/tests/files/docs/rst_test/index.rst
- tests/tests/files/field-layout/non-struct.cpp
- tests/tests/files/field-layout/bitfields.cpp
- tests/tests/files/field-layout/field-type.cpp
- tests/tests/files/field-layout/holes.cpp
- tests/tests/files/field-layout/multiple_inheritance.cpp
- tests/tests/files/field-layout/vtable.cpp
- tests/tests/files/mozilla/_ipdltest/TestBasicParent.h
- tests/tests/files/mochitest-alt-pref.ini
- tests/tests/files/mozilla/_ipdltest/TestBasicChild.h
- tests/tests/files/docs/md_test/index.md
- tests/tests/files/mochitest.ini
- tests/tests/files/ForwardingTemplates.cpp
- tests/tests/files/src/main/java/sample/JavaLibrary.java
- tests/tests/files/src/test/java/sample/JavaTest.java
- tests/tests/files/src/main/java/sample/Jni.java
- tests/tests/files/src/test/kotlin/sample/KotlinTest.kt
- tests/tests/files/src/main/kotlin/sample/KotlinLibrary.kt
- tests/tests/files/src/main/kotlin/sample/InlineObject.kt
- tests/tests/files/html/script.html
- tests/tests/files/html/invalid-entity-ref.html
- tests/tests/files/html/script.xhtml
- tests/tests/files/ignored-js/reignored-after-reinclusion.js
- tests/tests/files/ignored-js/wildcard-ignored.js
- tests/tests/files/ignored-js/explicit-ignored.js
- tests/tests/files/ignored-js/not-ignored-after-reinclusion.js
- tests/tests/files/ignored-js/doublestar-ignored/ignored-by-doublestar.js
- tests/tests/files/browser.ini
- tests/tests/files/mobile/androidTest/asserts/inputs.html
- tests/tests/files/webidl/iterable.webidl
- tests/tests/files/webidl/contextual-keyword.webidl
- tests/tests/files/webidl/bindings/src/BindingTestMixed2Binding.cpp
- tests/tests/files/webidl/bindings/src/BindingTestMixed1Binding.cpp
- tests/tests/files/webidl/bindings/src/BindingTestMixed2.h
- tests/tests/files/webidl/external-interface.webidl
- tests/tests/files/webidl/bindings/src/BindingTestMixed1.h
- tests/tests/files/webidl/dictionary.webidl
- tests/tests/files/webidl/bindings/src/BindingTest.h
- tests/tests/files/webidl/bindings/src/BindingTestBinding.cpp
- tests/tests/files/webidl/partial-interface.webidl
- tests/tests/files/webidl/BindingTestMixin.webidl
- tests/tests/files/webidl/const.webidl
- tests/tests/files/webidl/interface-mixin.webidl
- tests/tests/files/webidl/maplike.webidl
- tests/tests/files/webidl/overload.webidl
- tests/tests/files/webidl/asynciterable.webidl
- tests/tests/files/webidl/typedef.webidl
- tests/tests/files/webidl/includes.webidl
- tests/tests/files/webidl/enum.webidl
- tests/tests/files/webidl/partial-dictionary.webidl
- tests/tests/files/webidl/BindingTestMixed2.webidl
- tests/tests/files/webidl/BindingTest.webidl
- tests/tests/files/webidl/callback.webidl
- tests/tests/files/webidl/namespace.webidl
- tests/tests/files/webidl/setlike.webidl
- tests/tests/files/webidl/method.webidl
- tests/tests/files/webidl/attribute.webidl
- tests/tests/files/rust/weak_keyword.rs
- tests/tests/files/webidl/BindingTestMixed1.webidl
- tests/tests/files/rust/mod.rs
- tests/tests/files/webidl/super-interface.webidl
- tests/tests/files/webidl/super-dictionary.webidl
- tests/tests/files/webidl/type.webidl
- tests/tests/files/webidl/partial-namespace.webidl
- tests/tests/files/webidl/bindings/include/BindingTestMixed2Binding.h
- tests/tests/files/webidl/bindings/include/BindingTestMixed1Binding.h
- tests/tests/files/webidl/bindings/include/BindingTestBinding.h
- tests/tests/files/big_header.h
- tests/tests/files/testing/web-platform/meta/fake-standard/test_ima_weird_meta_wpt.js.ini
- tests/tests/files/testing/web-platform/meta/fake-standard/test_ima_disabled_wpt.js.ini
- tests/tests/files/testing/web-platform/meta/fake-standard/test_ima_sad_subtests_wpt.js.ini
- tests/tests/files/testing/web-platform/tests/fake-standard/test_ima_weird_meta_wpt.js
- tests/tests/files/testing/web-platform/tests/fake-standard/test_ima_sad_subtests_wpt.js
- tests/tests/files/testing/web-platform/tests/fake-standard/test_ima_disabled_wpt.js
- tests/tests/files/template_specialization.cpp
- tests/tests/files/enummacro.cpp
- tests/tests/files/.eslintignore
- tests/tests/files/templates3.cpp
- tests/tests/files/overs.cpp
- tests/tests/files/bug1432300.cpp
- tests/tests/files/js/with space.js
- tests/tests/files/js/export4.mjs
- tests/tests/files/js/export5.mjs
- tests/tests/files/js/README.md
- tests/tests/files/js/export6.mjs
- tests/tests/files/js/import.mjs
- tests/tests/files/js/export.mjs
- tests/tests/files/js/GCAnnotations.h
- tests/tests/files/js/Value.h
- tests/tests/files/js/export3.mjs
- tests/tests/files/js/export2.mjs
- tests/tests/files/js/with-UTF8-ãƒ•ã‚¡ã‚¤ãƒ«.js
- tests/tests/files/js/export8.mjs
- tests/tests/files/js/contextual-keyword.js
- tests/tests/files/js/export7.mjs
- tests/tests/files/imported-module.mjs
- tests/tests/files/lambdas.cpp
- tests/tests/files/GeckoApp.java
- tests/tests/files/webtest/Titler.cpp
- tests/tests/files/webtest/CopyAsMarkdown.cpp
- tests/tests/files/webtest/Webtest.cpp
- tests/tests/files/webtest/WebtestPathFilter.cpp
- tests/tests/files/ipdl/PTestBasic.ipdl
- tests/tests/files/templates6.cpp
- tests/tests/files/ipdl/TestBasic.cpp
- tests/tests/files/lots_of_calls.cpp
- tests/tests/files/macro.cpp
- tests/tests/files/urlmap/chrome3.mjs
- tests/tests/files/urlmap/mozsrc1.html
- tests/tests/files/urlmap/chrome2.mjs
- tests/tests/files/urlmap/resource1.css
- tests/tests/files/urlmap/root.xhtml
- tests/tests/files/urlmap/chrome1.mjs
- tests/tests/files/urlmap/subdir/sub.mjs
- tests/tests/files/urlmap/chrome1.css
- tests/tests/files/urlmap/chrome1b.mjs
- tests/tests/files/urlmap/root.js
- tests/tests/files/urlmap/root.mjs
- tests/tests/files/urlmap/chrome1.html
- tests/tests/files/urlmap/resource3.mjs
- tests/tests/files/urlmap/root.html
- tests/tests/files/urlmap/mozsrc2.mjs
- tests/tests/files/urlmap/resource2.css
- tests/tests/files/urlmap/mozsrc3.mjs
- tests/tests/files/urlmap/resource2.mjs
- tests/tests/files/urlmap/chrome2.css
- tests/tests/files/urlmap/mozsrc2.css
- tests/tests/files/urlmap/root.cpp
- tests/tests/files/urlmap/resource1.mjs
- tests/tests/files/urlmap/mozsrc1.mjs
- tests/tests/files/urlmap/mozsrc1.css
- tests/tests/files/urlmap/mozsrc1b.mjs
- tests/tests/files/urlmap/resource1.html
- tests/tests/files/template_shapes.cpp
- tests/tests/files/test_custom_element_base.xul
- tests/tests/files/AddonUpdateChecker.jsm
- tests/tests/files/templates5.h
- tests/tests/files/templates6.h
- tests/tests/files/using.cpp
- tests/tests/files/templates7.cpp
- tests/tests/files/bug1435345.cpp
- tests/tests/files/simple.rs
- tests/tests/files/xpidl/xpctest_cenums.idl
- tests/tests/files/xpidl/xpctest_params.idl
- tests/tests/files/xpidl/nsrootidl.idl
- tests/tests/files/xpidl/nsISupports.idl
- tests/tests/files/xpidl/xpctest_attributes.idl
- tests/tests/files/xpidl/xpctest_bug809674.idl
- tests/tests/files/xpidl/xpctest_interfaces.idl
- tests/tests/files/xpidl/xpctest_returncode.idl
- tests/tests/files/xpidl/xpctest_utils.idl
- tests/tests/files/python/contextual-keyword.py
- tests/tests/files/staticprefs/StaticPrefsConsumer.cpp
- tests/tests/files/staticprefs/StaticPrefList.yaml
- tests/tests/files/staticprefs/bindings/StaticPrefList_test.h
- tests/tests/files/staticprefs/bindings/StaticPrefList_test2.h
- tests/tests/files/spaces  are bad.txt
- tests/tests/files/runnables.cpp
- tests/tests/files/test_DOMWindowCreated_chromeonly.html
- tests/tests/files/big_cpp.cpp
- tests/tests/files/atom_magic.h
- tests/tests/files/css/embed_css.html
- tests/tests/files/css/test.css
- tests/tests/files/third_party/a_file
- tests/tests/files/invalid-files/unterminated-rust-str.rs
- tests/tests/files/jni.cpp
- tests/tests/files/templates5.cpp
- tests/tests/files/simple.cpp
- tests/tests/files/templates1.cpp
- tests/tests/files/multiline.cpp
- tests/tests/mc-generated/ipc/ipdl/_ipdlheaders/mozilla/_ipdltest/PTestBasic.h
- tests/tests/mc-generated/ipc/ipdl/_ipdlheaders/mozilla/_ipdltest/PTestBasicChild.h
- tests/tests/mc-generated/ipc/ipdl/_ipdlheaders/mozilla/_ipdltest/PTestBasicParent.h
- tests/tests/mc-generated/ipdl/PTestBasic.cpp
- tests/tests/mc-generated/ipdl/PTestBasicChild.cpp
- tests/tests/mc-generated/ipdl/PTestBasicParent.cpp
- tests/tests/mc-analysis/mozilla/_ipdltest/TestBasicParent.h
- tests/tests/mc-analysis/mozilla/_ipdltest/TestBasicChild.h
- tests/tests/mc-analysis/ipdl/TestBasic.cpp
- tests/tests/mc-analysis/__GENERATED__/ipc/ipdl/_ipdlheaders/mozilla/_ipdltest/PTestBasic.h
- tests/tests/mc-analysis/__GENERATED__/ipc/ipdl/_ipdlheaders/mozilla/_ipdltest/PTestBasicChild.h
- tests/tests/mc-analysis/__GENERATED__/ipc/ipdl/_ipdlheaders/mozilla/_ipdltest/PTestBasicParent.h
- tests/tests/mc-analysis/__GENERATED__/ipdl/PTestBasic.cpp
- tests/tests/mc-analysis/__GENERATED__/ipdl/PTestBasicChild.cpp
- tests/tests/mc-analysis/__GENERATED__/ipdl/PTestBasicParent.cpp
- tests/tests/refresh-mozilla-central-stuff
- tests/help.html
- extension/background.js
- extension/manifest.json
- tree-configs/README.md

## docs/old-vagrant-setup.md
```
## Deprecated Vagrant Setup

There are almost no circumstances in which you'd want to use this Vagrant setup,
and with any luck it will bit-rot into obsolescence and we can delete this doc
and the Vagrant file, but... here you go:

### Setting up the VM

We use Vagrant to setup a virtual machine.  This may be the most frustrating part of
working with Searchfox.  If you can help provide better/more explicit instructions
for your platform, please do!

#### Linux

Important note: In order to expose the Searchfox source directory into the VM, we
need to be able to export it via NFS.  If you are using a FUSE-style filesystem
like `eCryptFS` which is a means of encrypting your home directory, things will not
work.  You will need to move searchfox to a partition that's a normal block device
(which includes LUKS-style encrypted partitions, etc.)

##### Ubuntu

```shell
# make sure the apt package database is up-to-date
sudo apt update
# vagrant will also install vagrant-libvirt which is the vagrant provider we use.
# virt-manager is a UI that helps inspect that your VM got created
# The rest are related to enabling libvirt and KVM-based virtualization
sudo apt install vagrant virt-manager qemu libvirt-daemon-system libvirt-clients

git clone https://github.com/mozsearch/mozsearch
cd mozsearch
git submodule update --init
vagrant up
```
##### Other Linux
Note: VirtualBox is an option on linux, but not recommended.

1. [install Vagrant](https://www.vagrantup.com/downloads.html).
2. Install libvirt via [vagrant-libvirt](https://github.com/vagrant-libvirt/vagrant-libvirt).
   Follow the [installation instructions](https://github.com/vagrant-libvirt/vagrant-libvirt#installation).
  - Note that if you didn't already have libvirt installed, then a new `libvirt`
    group may just have been created and your existing logins won't have the
    permissions necessary to talk to the management socket.  If you do
    `exec su -l $USER` you can get access to your newly assigned group.
  - See troubleshooting below if you have problems.

Once that's installed:
```shell
git clone https://github.com/mozsearch/mozsearch
cd mozsearch
git submodule update --init
vagrant up
```

If vagrant up times out in the "Mounting NFS shared folders..." step, chances
are that you cannot access nfs from the virtual machine.

Under stock Fedora 31, you probably need to allow libvirt to access nfs:

```
firewall-cmd --permanent --add-service=nfs --zone=libvirt
firewall-cmd --permanent --add-service=rpc-bind --zone=libvirt
firewall-cmd --permanent --add-service=mountd --zone=libvirt
firewall-cmd --reload
```

#### OS X and Windows

Note: The current Homebrew version of Vagrant is currently not able to use the most
recent version of VirtualBox so it's recommended to install things directly via their
installers.

1. [install Vagrant](https://www.vagrantup.com/downloads.html).
2. Figure out the right virtualization option for you.
  - OS X:
    - Are you on an M1 mac?  Then you probably need to get a license for Parallels
      and use it, maybe.  And then you can do `vagrant plugin install vagrant-parallels`
      below.
    - Maybe get a license for parallels anyways?
    - Otherwise do the virtualbox thing below.
  - Windows,  Visit the [VirtualBox downloads page](https://www.virtualbox.org/wiki/Downloads) and
    follow the instructions for your OS.  You do not need and should not install
    any extra extensions.  You only need the Open Source piece and should avoid
    installing anything closed source or with a commercial license.

Then clone Mozsearch and provision a Vagrant instance:
```
git clone https://github.com/mozsearch/mozsearch
cd mozsearch
git submodule update --init

# If using VirtualBox; if using Parallels, install `vagrant-parallels`
vagrant plugin install vagrant-vbguest
vagrant up
```

### Once vagrant up has started...

The last step will take some time (10 or 15 minutes on a fast laptop)
to download a lot of dependencies and build some tools locally.  **Note
that this step can fail!**  Say, if you're at a Mozilla All-Hands and the
network isn't exceedingly reliable.  In particular, if you are seeing
errors related to host resolution and you have access to a VPN, it may
be advisable to connect to the VPN.

A successful provisioning run will end with `mv update-log provision-update-log-2`.

In the event of failure you will want to run
`vagrant destroy` to completely delete the VM and then
run `vagrant up` again to re-create it.  The base image gets cached on
your system, so you'll save ~1GB of download, but all the Ubuntu package
installation will be re-done.

After `vagrant up` completes, ssh into the VM as follows. From this point
onward, all commands should be executed inside the VM.

```
vagrant ssh
```

At this point, your Mozsearch git directory has been mounted into a
shared folder at `/vagrant` in the VM. Any changes made from inside or
outside the VM will be mirrored to the other side. Generally I find it
best to edit code outside the VM, but any commands to build or run
scripts must run inside the VM.

```

## docs/bash-scripting-cheatsheet.md
```
This file attempts to provide the basics of bash scripting as relevant to
searchfox's automation.  Note that _all_ scripts use bash, not sh, which means
we have additional tricks available.

Nothing here is authoritative or exhaustive.  If you want those, check out
these links:
- [Bash Sheet](https://mywiki.wooledge.org/BashSheet) - A quick one page reference.
- [Bash Guide](https://mywiki.wooledge.org/BashGuide) - Detailed (useful)
  documentation.
- [Bash FAQ](https://mywiki.wooledge.org/BashFAQ) and specific pages which focus
  on best practices, edge cases, and common mistakes, although there's also
  [Practices](https://mywiki.wooledge.org/BashGuide/Practices) from the Bash
  Guide if you're not already dealing with something that's broken.
  - [`Arguments`](https://mywiki.wooledge.org/Arguments)
  - [`Quotes`](https://mywiki.wooledge.org/Quotes)
  - [`What is the difference between test, [ and [[ ?`](https://mywiki.wooledge.org/BashFAQ/031)
  - [`How do I do string manipulations in bash?`](https://mywiki.wooledge.org/BashFAQ/100)
  - [`How can I use parameter expansion? (and more)!`](https://mywiki.wooledge.org/BashFAQ/073)

It's probably a good idea to read the
[Quotes](https://mywiki.wooledge.org/Quotes) and
[Arguments](https://mywiki.wooledge.org/Arguments) pages if you're touching
anything related to variables.

## Error Handling

We use the following initialization stanza in all bash scripts at the top:
```bash
set -x # Show commands
set -eu # Errors/undefined vars are fatal
set -o pipefail # Check all commands in a pipeline
```

## Argument Handling

Because we set that undefined variables are fatal, it's not okay to reference
a positional argument like `$4` unless it's a mandatory argument, and ideally
after checking the number of arguments.

### Checking the number of arguments

Exact match:
```bash
if [[ $# -ne 1 ]]; then
    echo "Usage: $0 <ARG>"
    echo " e.g.: $0 example-arg"
    exit 1
fi
```

Minimum count:
```bash
if [[ $# -lt 2 ]]
then
    echo "Usage: $0 arg-1 arg-2 [optional-arg]"
    exit 1
fi
```

### Checking whether variables are empty

If `[[` is used instead of `[` there's no need to quote the variable.  Note that
because of `set -eu`, this will still error if the variable is not defined.  See
the next section for how to handle that.

```bash
if [[ $defined_var_that_may_be_empty ]]; then
    # logic to run if the variable was non-empty
fi
```

### Dealing with optional arguments

Default an argument to something if unset or empty (the `:` makes it handle
empty in addition to unset):
```bash
NAME=${4:-default_value}
```

This also works if you want to normalize an omitted argument to being empty:
```bash
NAME=${4:-}
```

## Checking the file-system

Check whether a file exists and is a regular file.

```bash
if [[ -f $PATH ]]; then
    # logic to run if the file existed
fi
```

Check if it doesn't exist or isn't a regular file.

```bash
if [[ ! -f $PATH ]]; then
    # logic to run if the file didn't exist / wasn't a file
fi
```

Other related tests:
* `-f` is a file (not a directory or something weird)
* `-x` is an executable file
* `-d` is a directory
* `-e` is any kind of file
* `-h` is a symlink

```bash
if [[ -d $PATH ]]; then
    # logic to run if the dir existed and was a dir
fi
```

Check if it doesn't exist or isn't a directory.

```bash
if [[ ! -d $PATH ]]; then
    # logic to run if the dir didn't exist or wasn't a file.
fi
```

If you don't care if something is a directory or weird thing, use `-e`.

## Commands and Escaping Arguments
It's still probably a good idea to read the
[Quotes](https://mywiki.wooledge.org/Quotes) and
[Arguments](https://mywiki.wooledge.org/Arguments) pages if you're touching
anything related to this.  But here are important highlights.

### Nesting quotes does not do what you think it does.

As documented at [Quoting Happens Before PE](https://mywiki.wooledge.org/Arguments#Quoting_Happens_Before_PE)
if you put single quotes inside a double quote to try and escape something that
you know will be passed to another shell invocation, the single quotes will be
escaped as content, which is probably not what you were trying to do.  Example:

```bash
testfoo='bar' # the use of single-quotes doesn't matter here
set -x # Show commands
$Â echo "'$testfoo'"
+ echo ''\''bar'\''' #
'bar'
```

If you're thinking about doing this because you're using `parallel`, see the
section on `parallel`.

### Double-quotes stops globbing but not variable expansion.
Using a wildcard that you don't want globbed because you're passing it to
`find`?  Wrap it in double-quotes, and you can still use variables!
```bash
"*.json foo-${VAR}-*.json"
```

### For loops are bad - while loops are good

See https://mywiki.wooledge.org/BashFAQ/001 but the basic idea is that instead
of doing:

```bash
# THIS IS THE BAD EXAMPLE DON'T DO THIS BECAUSE IF THERE ARE SPACES IN THE FILE
# NAME IT WILL BE PARSED AS TWO SEPARATE TOKENS, NOT ONE, AND THEN YOU WILL HAVE
# A BAD TIME.
for file in $(find . -type f | sort -r); do
  gzip -f "$file"
  touch -r "$file".gz "$file"
done
```

you want to do:

```bash
find . -type f | sort -r | while read -r file; do
  gzip -f "$file"
  touch -r "$file".gz "$file"
done
```

because the for loop will tokenize things incorrectly.

### GNU parallel and its processing

GNU parallel does use a shell in each of its invocations.  So shell parsing
will happen both in the invocation of parallel and each of its sub-invocations.

#### Quoting

Passing `-q` to parallel will cause it to escape everything it passes to the
shell.  This is necessary in cases where arguments contain characters like `;`
which the shell will interpret and aren't automatically escaped by parallel.

The `-q` option should be used instead of attempting to embed quotes within
quotes, which https://mywiki.wooledge.org/Arguments#Quoting_Happens_Before_PE
tells us will end badly.

Parallel has a `--shellquote` argument that can be used to generate a quoted
version of a parallel command so that `-q` doesn't need to be used (which could
preclude some shell magic).

See https://www.gnu.org/software/parallel/parallel_tutorial.html#Quoting for
more info.

#### Environment Variables

If we want something like an exported `RUST_BACKTRACE` to be propagated into the
command run by parallel, we need to pass `--env RUST_BACKTRACE` or use the
`env_parallel` helper to propagate the entire environment.

```

## docs/a11y.md
```
# Accessibility

This document is very much still under construction.

## Testing

### Software

Use NVDA from https://www.nvaccess.org/.  The current choice is made based on
internal usage in Mozilla as well as use in the world at large.

Relevant docs:
- https://marcozehe.wordpress.com/articles/how-to-use-nvda-and-firefox-to-test-your-web-pages-for-accessibility/
- https://webaim.org/articles/nvda/

### Smoke Test Steps

- Bring up a source listing page.
- Press NVDA + space to enter virtual buffer mode.
- Press 't' to select the table.
- Use ctrl + alt + (arrow keys) to move around the table.  Confirm that the
  table layout matches the design below, with column 1 being blame, column 2
  being line numbers, and column 3 being the contents of the source lines.
- In the first column where there is blame data, press the 'enter' key to toggle
  the blame pop-up open.
- Press the down arrow to read the first line of the blame pop-up.  Successive
  down arrow presses should read the rest of the blame lines.

## Design

### Source Listing

The source lists are exposed as a 3-column table using ARIA roles.
1. Blame button, toggling it toggles display of the blame popup.
2. Line number.
3. Source code for the given line.

Here's an example of a 1-row source listing:
```html
<div id="file" class="file" role="table">
  <div role="row" class="source-line-with-number">
    <div role="cell"><div class="blame-strip c1" data-blame="a015fb538abbbdcbfe404c320a8fefdc07a6a54e#%#1" role="button" aria-label="blame" aria-expanded="false"></div></div>
    <div id="l1" role="cell" class="line-number">1</div>
    <code role="cell" class="source-line" id="line-1"><span class="syn_comment" >/* -*- Mode: C++; tab-width: 2; indent-tabs-mode: nil; c-basic-offset: 2 -*- */</span>
  </code>
</div>
```

```

## docs/testing-checks.md
```
# Searchfox Testing

## Overview: Snapshot Testing

The primary testing mechanism both for development and detecting failures in
production is use of [insta](https://insta.rs/)-based snapshot testing.

Test cases are defined as pipelines to the `searchfox-tool` which are run
against the on-disk index and against the web-server.  The expected output of
the commands is checked into the repository, and tests confirm that the output
of test runs matches the expected output.  When changes are expected, a "review"
mode can be used to run the tests and see a diff of the changes in output that
the code author can confirm are expected and desired, and then the code reviewer
can confirm the diffs of the changes to the repository.

For the searchfox "tests" repository, the inputs and expected outputs can be
found at:
- https://github.com/mozsearch/mozsearch/tree/master/tests/tests/checks/inputs
- https://github.com/mozsearch/mozsearch/tree/master/tests/tests/checks/snapshots

For the mozilla-central production configuration, the inputs and expected
outputs can be found at:
- https://github.com/mozsearch/mozsearch-mozilla/tree/master/mozilla-central/checks/inputs
- https://github.com/mozsearch/mozsearch-mozilla/tree/master/mozilla-central/checks/snapshots

This mechanism was introduced in
[bug 1707282](https://bugzilla.mozilla.org/show_bug.cgi?id=1707282) and much of
the design rationale and longer term goals can be found in the bug there,
combined with comments and discussion in the
[initial PR](https://github.com/mozsearch/mozsearch/pull/422).

### Normalizations

Production repositories will inherently change over time.  In particular,
production blame and coverage data will continually change.  To this end, we
have implemented a `prod-filter` command that is used in our production checks
in order to:
- Strip .cov-strip elements.
- Strip .blame-strip elements.
- Replace line numbers with N
- Replace data-i values with "NORM".

## Purpose of the Checks

### mozsearch "tests" checks

These are regression tests that also may be used as a place one might be able to
quickly check what the current data representations look like.

Any changes to the indexing process should either be resulting in (expected)
changes to existing checks, or new checks should be introduced that cover the
new functionality.

Check coverage need not be exhaustive.  It's preferable that changes to the
expectations be something that a human can feasibly review in their entirety.
If you find yourself adding excessively verbose check expectations, it might
be appropriate to add functionality to searchfox-tool commands so that the
output can be filtered and/or automated invariant-checks can be performed
internally.

### production mozilla-central checks

The mozilla-central checks are intended to detect when changes in
mozilla-central break specific aspects of searchfox support.  For example,
changes in the auto-generated naming scheme for the C++ IPC bindings or C++
XIPDL or rust XPIDL support could break that specific class of support.  So we
pick specific real types and check for their expected definitions and related
state.

In the event the checks fail, the indexer fails and the previous web-server will
continue to run with its stale content.  An email will be sent to the searchfox
list, letting the maintainers know.  Usually regressions like this are simple
and straightforward, which means that it's not too bad to address the change.

Because we currently choose real types, there's also the potential for false
positives if the thing we chose gets renamed/refactored.  Again, this is
potentially quite quick to address.

#### Data Catch 22

Previously, our checks were just expecting the presence of a specific symbol in
a specific file, not matching exact output.  This made it easy to update the
check script without having any dependency on data that's only available on a
failed indexer.

Unfortunately, we now do have a data dependency, and our current indexer
behavior is to stop the indexer on failure, which makes it harder to get the
data off of it.

XXX the "release" condition for review versus fail didn't land that was proposed
at https://github.com/mozsearch/mozsearch/pull/425#issuecomment-898745607 and
that should help address this problem.  It's mainly a question of making sure
that we propagate `INSTA_FORCE_PASS=1` to the invocation of check-index.sh
for non-release builds.  As discussed below, it could be good to also propagate
the additional resulting files that `cargo insta review` could then process.

#### Needed Enhancements

We likely have additional work to do like to have the failed indexer generate a
tarball of its deviation from expectations and to upload this to S3.  Or to
fulfill the original plan of having non-release channel builds not fail

## Updating Checks

### mozsearch "tests" repo

Inside the VM, cd to `/vagrant` and then run:
```
make review-test-repo
```

This will trigger the [cargo insta](https://insta.rs/docs/cli/) review
mechanism which will make the appropriate changes to the repository for you to
commit.

### production mozilla-central checks

#### Setup

Note: You don't have to do things the following way; the underlying mechanism is
reasonably straightforward, but this is the current approach used by asuth and
what he will copy and paste from.

Check out mozsearch-mozilla as `config` inside your mozsearch checkout:
```
git clone https://github.com/mozsearch/mozsearch-mozilla.git config
```

This will also expose it at `/vagrant/config` inside the VM because of the (NFS)
mount in use.  Note that the VM will also have made its own checkout at
`~/config` but that directory isn't exposed outside the VM and so isn't useful.

You probably will then want to add your own fork as a remote.  For example,
assuming you are asuth, you would do:
```
git remote add gh-asuth git@github.com:asutherland/mozsearch-mozilla.git
```

#### Updating

Outside the VM, make sure you have an up-to-date copy of the default branch and
then branch from that.
```shell
# change into the mozsearch-mozilla checkout
cd config
# get off of any existing branch
git checkout master
# update the default branch
git pull origin master
# make our new branch
git checkout -b update-checks
```

Inside the VM:
```shell
# change into the mozsearch-mozilla checkout dir in the VM
cd /vagrant/config
# run the checks from this repo against the current state of
# https://
./review-build-check-results.sh config1.json mozilla-central release
```

Then, outside the VM, commit the changes to the branch and create a pull
request and submit it.

#### Updating checks on a stopped AWS indexer due to local (disk) check failure

Currently, in the event the indexer has a check failure, it will stop before
starting the web-server, which means you can't use the
`review-build-check-results.sh` script from your local machine to talk to a web
server.

In this case if you login, you can run the following to be able to reproduce the
failures experienced by the indexer run:

```shell
# mount the index to ~/index as documented in aws.md
sudo mount /dev/`lsblk | grep 300G | cut -d" " -f1` /index
# make index-scratch paths valid again
sudo ln -s /index/interrupted /mnt/index-scratch

export MOZSEARCH_PATH=~/mozsearch
export CONFIG_REPO=~/config
$MOZSEARCH_PATH/scripts/check-index.sh /index/interrupted/config.json mozilla-central "filesystem" ""
```

If we want to update the checks in the config, we can re-run with
`INSTA_FORCE_PASS=1` like so:

```shell
INSTA_FORCE_PASS=1 $MOZSEARCH_PATH/scripts/check-index.sh /index/interrupted/config.json mozilla-central "filesystem" ""
```

If we want to review these changes on the machine, we can do:
```shell
cargo insta review --workspace-root=$CONFIG_REPO
```

Now we've updated the mozsearch-mozilla repo checked out at ~/config on the
server, but we still need to get that committed to github somehow, and you
almost certainly don't want the indexer server to have access to your normal
github creds (although I guess we could give the indexer its own login?), so
the easiest thing to do is run the following locally to copy the changed
contents to your local machine (after doing `cargo insta review` above):

```shell
export INSTANCE=<you gotta get this from ssh.py or channel-tool.py>
# note this assumes mozilla-central; change as appropriate
infrastructure/aws/scp-while-sshed.py $INSTANCE 'config/mozilla-central/checks/snapshots/*' config/mozilla-central/checks/snapshots
# if you changed any of the inputs, you'll want to run this too:
infrastructure/aws/scp-while-sshed.py $INSTANCE 'config/mozilla-central/checks/inputs/*' config/mozilla-central/checks/inputs
```

```

## docs/nss-stuff.txt
```
0. Need to compile with:
     USE_64=1 make nss_build_all

1. The Linux.mk file hard codes gcc as the compiler. Need to fix CC and CCC.
2. Need to set CCC to $CXX.

```

## docs/addons.txt
```
Design for add-ons repo:
- Will download the latest version of every public add-on.
- Will unzip the XPI.
- Directory structure will be:
    `${prefix1}/${prefix2}/${addonId}/${addonName}/${fileId}/...`
- Will unzip any subsidiary jar files within the addon.

This will probably be quite large in the end. Many gigabytes.
I'll only allow full-text searching--no analysis. That wouldn't be
very useful and it would be wasteful of memory and disk.
There also won't be any blame information.

I'll tar all this up at the end and upload it to AWS, like all the
other stuff.

```

## docs/webtest.md
```
# Webtest

Webtest is a testing command for the searchfox UI part.

## Running tests

Webtest can be run by the following command inside the docker image.

```
make webtest
```

This does the following:
  * Build the tests repo and searchfox repo (`tests/webtest-config.json`)
  * Install geckodriver
  * Download Firefox
  * Start geckodriver
  * Run the `searchfox-tools`s `webtest` command, which loads and runs the tests in headless browser
  * Stop geckodriver

If the repositories are ready, you can also directly run `./scripts/webtest.sh`, which does the above, excluding the build for the repositories.

```
./scripts/webtest.sh
```

`webtest.sh` takes an optional filter arguments, to specify which test to run.
It's a substring-match on the test path.

```
./scripts/webtest.sh FILTER

# examples
./scripts/webtest.sh tests/webtest/test_Search.js
./scripts/webtest.sh test_Search.js
./scripts/webtest.sh Search
```

## Structure

Webtest consists of the following parts:
  * [Firefox](https://www.mozilla.org/en-US/firefox/) (automatically downloaded into `/vagrant/mozsearch-firefox`)
  * [geckodriver](https://github.com/mozilla/geckodriver) (automatically installed)
  * `searchfox-tool` command `webtest`: Command to initiate the test and analyze logs
  * `tests/webtest/webtest.html`: The top-level frame for the test, loaded in the browser
  * `tests/webtest/head.js`: A test harness and utility, loaded into `webtest.html`

## Test files

Test files should be put inside `tests/webtest/` directory, and named `test_*.js`.

Simple test file looks like the following:

```js
"use strict";

add_task(async function test_Header() {
  await TestUtils.loadPath("/");

  const h1 = frame.contentDocument.querySelector("h1");

  is(h1.textContent, "Welcome to Searchfox [testing]",
    "The header is there");
});
```

The test file is loaded into the `webtest.html`'s top-level frame.
`webtest.html` has an iframe for loading searchfox page, and `frame` global variable
holds the reference to it.

## Global variables

Test harness and utility functions are made similar to mochitest-browser style,
in order to make it easier for people who's familiar with the mozilla-central code.

Functions provided by the test harness:

  * `add_test(func)`: add single subtest, with as async function
  * `registerCleanupFunction(func)`: add a function to perfom at the end of the current subtest

Assertion functions:

  * `ok(condition, "Description of the check")`: tests a value for its truthfulness
  * `is(actual, expected, "Description of the check")`:  compares two values (using `Object.is`)
  * `isnot(actual, expected, "Description of the check")`: opposite of `is()`
  * `waitForCondition(condition, "Description of the check")`: wait until `condition` becomes true

Utility functions:

  * `TestUtils.loadPage(path)`: load the specified path of searchfox page into the iframe
  * `TestUtils.shortenSearchTimeouts()`: Shorten the timeout value for query and history, in order to shorten the time taken by tests
  * `TestUtils.setFeatureGate(name, value)`: set the feature gate value. e.g. `TestUtils.setFeatureGate("semanticInfo", "release")`  (this opens the settings page)
  * `TestUtils.resetFeatureGate(name)`: undo `TestUtils.setFeatureGate`

Other global variables:

  * `frame`: the iframe which loads the searchfox page

Misc:

  * `info(msg)`: print "INFO" message

For the complete list, please see `head.js`.

## Running from browser

In case you want to debug the test, webtest can also be run from the browser.

  1. Build the tests repo and searchfox repo (`make build-webtest-repo`)
  2. Open `http://localhost:16995/tests/webtest/webtest.html` (The private browsing mode is recommended, in order to avoid interferring with the settings)
  3. Open Web Console
  4. Run `TestHarness.loadTest(TES_PATH);`.  e.g. `TestHarness.loadTest("tests/webtest/test_Search.js");`

The log is printed to the console.

```

## docs/web-server.md
```
# Web server

Mozsearch serves pages in three ways:

* Nginx, for static resources and the current versions of source files.
* Python server, for search results.
* Rust, for blame information and historical versions of files.

All requests first go to the Nginx server. Based on the URL, it may
router the request to the Python or Rust servers, each of which runs
on its own port. Eventually search results should be moved to the Rust
server for performance.

The `scripts/nginx-setup.py` script generates the configuration file
for Nginx.

```

## docs/searchfox-tool-cookbook.md
```
searchfox-tool is both a rust binary that we build that can be directly run
against live servers and local index data, as well as the underlying mechanism
of the [testing mechanism](testing-checks.md).

This document is intended to be a repository of searchfox-tool command lines
that we've used as a copy-and-paste reference and starting point for similar
explorations.

## searchfox-tool parsing oddities

searchfox-tool is built around the idea of building pipelines, and its primary
use-case is the test check mechanism that doesn't directly involve a shell, so
it very unusually requires you to provide it with a single quoted argument which
it will then apply shell rules to.

As an example, all of the following will work:

A single argument, this is fine!
```
searchfox-tool --help
```

Even though the intent is for this to be 2 arguments, we have to quote them into
a single argument.
```
searchfox-tool 'query --help'
# `searchfox-tool query --help` however, would not!  you get an error!
```

## searchfox-tool is self-documenting

Run the following to get a list of subcommands you can chain together:
```
searchfox-tool '--help'
```

Once you know a subcommand, you can get help on it.  For example, choosing
"query":
```
searchfox-tool 'query --help'
```

## Cookbook Proper

### Dumping crossref info from an identifier on a web-server shell
```
~/mozsearch/tools/target/release/searchfox-tool '--server=/home/ubuntu/index/config.json
--tree=mozilla-central search-identifiers ServiceWorkerManager::SendNotificationClickEvent | crossref-lookup'
```

```
~/mozsearch/tools/target/release/searchfox-tool '--server=/home/ubuntu/index/config.json
--tree=mozilla-central search-identifiers PClientSourceParent::SendPClientSourceOpConstructor | crossref-lookup' | jq .
```

### Dumping crossref info from a symbol on a web-server shell


```
~/mozsearch/tools/target/release/searchfox-tool '--server=/home/ubuntu/index/config.json
--tree=mozilla-central crossref-lookup "_ZN7mozilla3dom18ClientSourceParent7StartOpEONS0_23ClientOpConstructorArgsE"' | jq .
```

### Graph traversal without rendering on a web-server shell

```
~/mozsearch/tools/target/release/searchfox-tool '--server=/home/ubuntu/index/config.json
--tree=mozilla-central search-identifiers ServiceWorkerManager::SendNotificationClickEvent | crossref-lookup | traverse --edge=uses --max-depth=2' | jq .
```

```
RUST_LOG=trace ~/mozsearch/tools/target/release/searchfox-tool '--server=/home/ubuntu/index/config.json
--tree=mozilla-central search-identifiers ClientSource::Focus | crossref-lookup | traverse --edge=uses --max-depth=4'
```

### Graphing on a web-server shell

```
~/mozsearch/tools/target/release/searchfox-tool '--server=/home/ubuntu/index/config.json
--tree=mozilla-central search-identifiers mozilla::GetContentWin32kLockdownEnabled | crossref-lookup | traverse --edge=uses --max-depth=9 | graph --format=svg'
```

```
~/mozsearch/tools/target/release/searchfox-tool '--server=/home/ubuntu/index/config.json
--tree=mozilla-central search-identifiers GetLiveWin32kLockdownState | crossref-lookup | traverse --edge=uses --max-depth=9 | graph --format=svg'
```



### Diffing Query Results

While investigating aspects of queries that hit limits because of non-intuitive
results, a `--diff` flag was added to query that allows it to be used in a
pipeline like `searchfox-tool 'query foo | query --diff foot'` to diff the
change in results.  This was not immediately useful for 2 reasons:
1. The "bounds" fields generated a ton of noise because the substring lengths
   were different.  So `--normalize` was added to compensate for that.
2. Our search result hit lists are normally returned as arrays where the JSON
   diff algorithm correctly treats the ordering as important.  So `--dictify`
   was added as a means of transforming `[{ "path": "foo", ... }]` to
   `{ "foo": { "path": "foo", ... }}` which the JSON diff algorithm then applies
   set/map semantics to, which is what we as humans like.

So the specific example was for "new xmlhttprequest" with and without the
trailing "t".  In this case, we were seeing different result counts, which was
surprising because you would expect a situation that is not showing evidence of
hitting a result limit based on the understood logging output.  (It of course
turns out that we were hitting a result limit count and the misleading counts
were due to aggregating based on the path first.)

```
searchfox-tool 'query "new xmlhttpreques" | query --diff --normalize --dictify "new xmlhttprequest"'
```

### Test Server Text Search
```
RUST_LOG=trace ./searchfox-tool '--server=/home/vagrant/index/config.json --tree=tests search-text searchfox'
```


### Graphing Test Server contents

From inside the VM:
```
./searchfox-tool '--server=/home/vagrant/index/config.json --tree=tests search-identifiers outerNS::OuterCat::meet | crossref-lookup | traverse | graph --format=svg' > /vagrant/pretty.svg
```

```

## docs/blame.md
```
# Blame

In order to serve up blame information quickly, mozsearch caches blame
data for every version of every file in a repository. Here's how it works:

Suppose a file has three revisions:

```
rev1:
abc
def
ghi

rev2:
abc
def2
ghi

rev3:
abc
ghij
```

Then mozsearch will generate three "blame revisions" in a new repository:

```
blame-rev1:
rev1
rev1
rev1

blame-rev2:
rev1
rev2
rev1

blame-rev3:
rev1
rev3
```

Every revision in the original repository has a corresponding blame
revision. The blame version of the file will have one line for every
line in the original file. This line will contain the revision ID of
the revision in the original repository that introduced that line.

This data is stored in its own git repository. This repository, called
the blame repository, has exactly the same file structure as the
original repository. Each commit in this repository corresponds to a
commit in the original repository. Commit messages in the blame
repository give the revision they correspond to in the original
repository.

Let's imagine you want to get blame for a file `${f}` at revision
`${rev}`. First, find the revision `${blame_rev}` in the blame
repository that corresponds to `${rev}` (the web server keeps a
mapping between revisions in memory for this purpose). Then find
`${f}` in the blame repository at revision `${blame_rev}`. Finally,
show these two files side-by-side and you're done.

Mozsearch uses the `tools/src/bin/build-blame` tool to generate a blame
repository. Generating cached blame information is pretty
slow. However, each indexing run only needs to generate new blame
revisions for each new revision that has appeared in the original
repository since the last indexing. Typically the blame repository is
about the same size as the original repository since it compresses
very well with git's delta compression.

```

## docs/platform-notes.txt
```
WINDOWS

One-time:
Need mozilla-build.
Need visual studio community edition.
Install git, cmake, ninja.
  git: https://git-scm.com/download/win
  cmake: https://cmake.org/files/v3.5/cmake-3.5.1-win32-x86.msi
  ninja: https://github.com/ninja-build/ninja/releases
Install clang, clang-cl. Need to install from source.
  Follow instructions at https://developer.mozilla.org/en-US/docs/Mozilla/Developer_guide/Build_Instructions/Building_Firefox_on_Windows_with_clang-cl
  However, need to run cmake with -DCMAKE_INSTALL_PREFIX=/c/llvm-install and run ninja install

Every time:
Install mozsearch.
Build mozsearch indexer plugin.
Figure out how to get index volume attached to Windows machine.
Compile with the indexer plugin.


MAC

https://github.com/tpoechtrager/osxcross

```

## docs/manual-indexing.md
```
## Manually Running Test Repo Tests

### Build Necessary Tools

The first step is to build all the statically compiled parts of
Mozsearch:

```
# This clang plugin analyzes C++ code and is written in C++.
cd /vagrant/clang-plugin
make

# The Rust code is stored here. We do a release build since our scripts
# look in tools/target/release to find binaries.
cd /vagrant/tools
cargo build --release
```

### Testing locally using the "tests" repository

Mozsearch chooses what to index using a set of configuration
files. There is a test configuration inside the Mozsearch `tests`
directory. We'll use this configuration for testing. However, Mozilla
code indexing is done using the
[mozsearch-mozilla](https://github.com/mozsearch/mozsearch-mozilla)
repository.

The `config.json` file is the most important part of the
configuration. It contains metadata about the trees to be indexed. For
example, it describes where the files are stored, whether there is a
git repository that backs the files to be indexed, and whether there
is blame information available.

Mozsearch stores all the indexed information in a directory called the
index. This directory contains a full-text search index, a map from
symbol names to where they appear, a list of all files, and symbol
information for each file.

The first step in indexing is to run the `indexer-setup.sh`
script. This script sets up the directory structure for the index. In
some cases, it will also download the repositories that will be
indexed. In the case of the test repository, though, all the files are
already available. From the VM, run the following command to create
the index directory at `~/index`.

```
mkdir ~/index
/vagrant/infrastructure/indexer-setup.sh /vagrant/tests config.json ~/index
```

Now it's time to index! To do that, run the `indexer-run.sh`
script. It will compile and index all the C++ and Rust files and
also do whatever indexing is needed on JS, IDL, and IPDL files.

```
/vagrant/infrastructure/indexer-run.sh /vagrant/tests ~/index
```

Now is a good time to look through the `~/index/tests` directory to
look at all the index files that were generated. To begin serving web
requests, we can start the server as follows:

```
# Creates a configuration file for nginx. The last path gives the location
# where log files are stored.
/vagrant/infrastructure/web-server-setup.sh /vagrant/tests config.json ~/index ~

# Starts the Python and Rust servers needed for Mozsearch.
/vagrant/infrastructure/web-server-run.sh /vagrant/tests ~/index ~
```

At this point, you should be able to visit the server, which is
running on port 80 inside the VM and port 16995 outside the VM. Visit
`http://localhost:16995/` to do so.

## Indexing Mozilla code locally

DOCUMENTATION DISCLAIMER: This content was extracted from
[the primary README](../README.md) because it's not something you're likely to
want to do and the searchfox developers haven't done the below in a while.
Please see the section "Testing changes against mozilla-central" there instead.
If you really want to run things locally, be aware that the current default VM
configuration is probably far too limiting and you will want to change it
locally so that the VM can use more of your processor cores, more RAM, more
disk, etc.

Although it can take a long time, it's sometimes necessary to index
the Mozilla codebase to test changes to Searchfox. How to do that
depends on what you want to test.

If you are making changes to the clang-plugin, you should first follow the steps
to run a try build from the primary readme.

### Testing basic changes

Note: You can also just do `make build-mozilla-repo` in `/vagrant` to have it
idempotently do the following for you.

```
# Clone the Mozilla configuration into ~/mozilla-config, if you haven't
# already done so. (If you are testing clang-plugin changes, you will
# already have done this and made modifications to mozilla-central/setup,
# so no need to clone again).
git clone https://github.com/mozsearch/mozsearch-mozilla ~/mozilla-config

# Manually edit the ~/mozilla-config/config.json to remove trees you don't
# care about (probably NSS and comm-central). Make sure to remove any trailing
# commas if they're not valid JSON!
nano ~/mozilla-config/config.json

# Make a new index directory.
mkdir ~/mozilla-index

# This step will download copies of the Mozilla code and blame information,
# along with the latest taskcluster artifacts, so it may be slow.
/vagrant/infrastructure/indexer-setup.sh ~/mozilla-config config.json ~/mozilla-index

# This step involves unpacking the taskcluster artifacts, and indexing a lot of
# code, so it will be slow!
/vagrant/infrastructure/indexer-run.sh ~/mozilla-config ~/mozilla-index
```

Note: By default, `indexer-setup.sh` keeps the contents of the working
directory (in the example above, that's `~/mozilla-index`). In case you want
to delete the contents of the working directory, define CLEAN_WORKING=1
when calling `indexer-setup.sh`.

### Indexing m-c try builds locally

See [the main README](../README.md) section on how to run a try job.  Once it's
completed, make note of the hg revision and then continue with the following:

* In the vagrant instance, run the following command in `/vagrant/`:
```
TRYPUSH_REV=<40-char-rev-hash> make trypush
```
This will clone the Mozilla configuration into ~/mozilla-config, and
generate a reduced config that has just the mozilla-central tree, but
use the code and artifacts from your try push when building the index.
It will build the index into a `~/trypush-index` folder to keep it separate
from any `~/mozilla-index` folders you might have lying around.
It's very similar to the operations described in the next section
which will build an index using the latest mozilla-central version with
searchfox artifacts.

### Locally indexing a try push

If you are not hacking on Searchfox itself, but just want to build a local
index of changes to mozilla-central (e.g. you are reviewing a complex
patchset, and want to have a Searchfox instance with those patches applied)
follow the same steps as described in the "Testing clang-plugin changes"
section above, except obviously you don't need to make any changes to
the clang-plugin, but just include the patches you care about in the try
push.

```

## docs/aws.md
```
# AWS deployment

Mozsearch supports being deployed to AWS. Incoming requests are
handled by an Elastic Load Balancer instance. Using ELB means that the
web server machines don't need to know anything about the TLS
certificate being used. ELB takes care of that. The load balancer
directs each incoming request to a "target group", which consists
of a single EC2 machine that handles web serving. The target group
is chosen based on the code repository that the request is directed
to. As of this writing, for example, the mozilla-central repository
is handled by the "release1-target" target group, while the mozilla-beta
repository is handled by the "release2-target" target group.
The mapping from repository to target group is set manually by path
routing rules in the load balancer configuration.

AWS Lambda tasks run each day to start indexing of all the
trees. These jobs start up EC2 instances to perform the indexing. Each
indexing instance takes care of the repos from a single config file.
So there will be one indexer instance processing the repos in
[config1.json](https://github.com/mozsearch/mozsearch-mozilla/blob/master/config1.json),
another instance processing the repos in
[config2.json](https://github.com/mozsearch/mozsearch-mozilla/blob/master/config2.json),
etc. The indexing instances have an extra Elastic Block Store volume attached
where the index will be stored. The following paragraphs explain the
lifecycle of a single indexer and its web server; the lifecycle applies
to each indexer instance.

Note that as of this writing, config1.json, config2.json, config4.json, and config5.json
are processed via the above-described Lambda task/indexer every day.
config3.json contains "archived" repositories (ones
which are not getting any more code updates). This one is not run
via a daily Lambda task, and need to be triggered manually if an
update is desired (generally not, since the code isn't changing).
Updates for this config should only be needed if the generated HTML
changes significantly.

The indexing instance downloads all the Git repositories from an
Amazon S3 bucket. It updates these repositories to the latest version
of the code. It also downloads the most recent blame repository from
an S3 bucket and incrementally builds blame revisions corresponding to
the new git revisions. When this is done, the updated repositories
(including blame) are uploaded back to the same S3 bucket. Following
that, the normal indexing process commences.

When indexing is complete, the indexer instance unmounts and detaches
the EBS volume containing the index. It does this using the AWS
API. Then the indexer uses the API to start a new EC2 instance for web
serving, passing it the name of the indexer instance as an
argument. The web server instance attaches and mounts the index volume
and starts serving web pages.

The indexer instance waits for the web server to report that it is
up and running (by polling the /status.txt URL, which is updated by
the web server processes). Then it updates the ELB
target group to point to the new web server instead of the old
one. Finally, it shuts down and destroys any old web server instances
and index volumes. Finally, the indexer instance terminates itself.

## Logging into the AWS console

The AWS console allows you to manually control AWS resources. To log
in, you need to be a member of the
[searchfox-aws](https://people.mozilla.org/a/searchfox-aws/)
Mozillians access group.

### Fast way

The following is a perma-link that the slow way will also get you:
https://mozilla-aws.awsapps.com/start/#/saml/custom/653057761566%20%28Searchfox%20%28Mozilla%29%29/MzI5NTY3MTc5NDM2X2lucy0xMjRlYmMyMGY2ZGZhOTBkX3AtYTlkNGEyNTlkMjNhMjRkYw%3D%3D

### Slow way

Once you are a member, you can use your Mozilla SSO authentication to
log in to AWS by going to https://mozilla-aws.awsapps.com/start. Once you get past the
SSO authentication, you'll be asked to pick a role - the "AdministratorAccess"
role is almost always the one you will want, as it gives you access to make
changes whereas the other ones are read-only type roles.  Note that this URL
has changed with the retirement of the "maws" infrastructure.

### Once you're logged in.

After you've logged in, you need to [change the AWS region in the top right
corner](http://docs.aws.amazon.com/awsconsolehelpdocs/latest/gsg/getting-started.html#select-region). The
region for Searchfox is "US West (Oregon)". Now you should be able to
select EC2 from Services and see the list of EC2 machines running. The
tags on the machine can be useful in selecting a particular instance that
you might be looking for.

## Setting up AWS locally

Mozsearch uses a lot of scripts that use the AWS API to start and stop
indexing, provision servers, etc. It is recommended that you run these
scripts **outside** the VM, as that is where the commands below have been tested
and the SSO flows involve opening a web browser which is at best awkward from
inside the VM/docker instance.

We previously used `maws`, but consistent with the docs at
https://mozilla-hub.atlassian.net/l/cp/GH1kL2zg this mechanism has been retired
due to changes at our SSO provider.  In these docs, searchfox always falls under
"Mozilla IT".  If you have an existing maws setup and want to know how to
upgrade it, see the next markdown header section.

The first step is to install the AWS CLI v2.  Amazon intentionally does not
make this available as a package we can install via PIP, although there is a
third-party packaging available as a python package, that seems a bit like a
scary avenue for compromise, so I'm going to suggest:
- Linux:
  - Ubuntu:
    - &gt;= 24.04: The "awscli" package is gone and you have to install a snap:
      `sudo snap install aws-cli --classic`
    - &gt;= 23.04: The "awscli" package is v2 and can be installed via apt.
  - Debian: Yeah, v2 is available on some versions, `apt show awscli` and see if
    the major version is >= 2.  Keep upgrading until that's true.
  - Nix: the devShell in our flake.nix provides awscli2. Just call `nix develop`.
- OS X:
  - In the #iam channel, using "brew" was recommended.
- Everything else, or if the above didn't work for you:
  - https://docs.aws.amazon.com/cli/latest/userguide/getting-started-install.html

Now when you run `aws --version` you should see something like (this is what I
see on Ubuntu 23.04):
```
$Â aws --version
aws-cli/2.9.19 Python/3.11.4 Linux/6.2.0-32-generic source/x86_64.ubuntu.23 prompt/off
```

Now we are going to login to SSO.  We are going to run `aws configure sso` and
enter the following values.  Note that there's also a variation of this command,
`aws configure sso-session` and my eyes glazed over when trying to understand
the differences.  If you care, the docs are at
https://docs.aws.amazon.com/cli/latest/userguide/sso-configure-profile-token.html
and https://mozilla-hub.atlassian.net/l/cp/GH1kL2zg also I think tries to explain
the difference.

- For `SSO session name (Recommended):` enter `mozilla`
- For `SSO start URL [None]:` enter `https://mozilla-aws.awsapps.com/start`
- For `SSO region [None]:` enter `us-west-2`
- This will now attempt to open a web page and show a code that you should
  confirm is also present in the browser window.  (This ensures you're granting
  access to the CLI and not some other simultaneously popped up rogue auth
  request.)  Verify the code and press the button on the web page to advance.
- This will now ask "Allow botocore-client-mozilla to access your data?" and
  you should hit the allow button.
- You should now be done with the webpage, moving back to the CLI:
- It should indicate the AWS account id for searchfox is available to you,
  probably only that one.  If you're not sure which one to use (I'm avoiding
  listing the account id here), do the "Logging into the AWS console" section
  guidance above which will show you a list of account IDs and their actual
  account names.
- It will ask you which role you want to use.  Pick "AdministratorAccess".
- For `CLI default client Region [None]:` enter `us-west-2` again.
- For `CLI default output format [None]:` I just hit enter.
- For `CLI profile name [AdministratorAccess-###]:` I suggest using `searchfox`.

The CLI should then print something like:
```
To use this profile, specify the profile name using --profile, as shown:

aws s3 ls --profile searchfox
```

Run that command, and we should see something like the following:
```
$Â aws s3 ls --profile searchfox
2020-06-27 09:19:43 indexer-logs
2022-08-06 13:48:44 searchfox-web-logs
2020-06-27 19:22:01 searchfox.repositories
```

Because we need the boto3 lib and a few other things, we're going to set up a
python venv now.  See https://packaging.python.org/en/latest/guides/installing-using-pip-and-virtual-environments/
for more on installing venv, but on ubuntu you should be able to run
`apt install python3-venv`.

> Note for Nix users: you don't need to setup a venv, the right Python packages
> are provided by our devShell.

```
# RUN THESE COMMANDS OUTSIDE THE VM!

python3 -m venv env
# if you chose a different profile name above, use that instead!
echo "export AWS_PROFILE=searchfox" >> env/bin/activate
# "." is the same as "source"
. env/bin/activate
pip install boto3 rich
# Make sure that we have an up-to-date version of certifi for certificate
# validation.  See comments in build-lambda-indexer-start.sh for more context.
pip install --upgrade certifi
```

Okay, now we're going to just validate that SSO credential refresh mechanism
works.  Run `aws sso login` and repeat the prompt flow where you have to:
- On the first page, verify that the code in the browser matches the code in the
  CLI and allow it.
- Allow "botocore-client-mozilla" access again.

Now we can run `infrastructure/aws/channel-tool.py list` or any other command.
But please run that one to make sure you see a list of our nodes, etc.

## Upgrading an existing maws configuration

The easiest option is to just `rm -rf env` the venv you previously created and
start over, especially if you just upgraded your distribution and pip may be in
a weird state.

That said, it's also possible you may have some weird system cruft built-up that
may need addressing, especially if you ever set things up before we started to
use venvs.  In that case you may do something like run "aws" and see something
like `-bash: /PATH/TO/.local/bin/aws: cannot execute: required file not found`.
In that case a fun thing to do is `rm ~/.local/bin/aws*`.  On bash, you will
likely then need to invoke `hash -r` so bash realizes those files have been
removed.

If you are deeply attached to your current venv, then you can:

```
# use the venv; we'll assume this is active from here on out
. env/bin/activate

# uninstall the old maws mechanism, you'll need to say yes.
pip uninstall mozilla-aws-cli-mozilla

# uninstall v1 awscli (binary "aws"), you'll need to say yes
pip uninstall awscli
```

Now go back to the previous section and install awscli.

## SSHing into AWS machines

To SSH into an EC2 instance, you will need to obtain the private key
file for Searchfox. Once you have the key, ensure that the permissions
are set so that it is not world-readable. Put it (or a symlink to it) at
`~/.aws/private_key.pem`. The ssh script below will check this location
and use this as the identity file if it exists.

Now you can connect to an instance as follows:

```
infrastructure/aws/ssh.py
```

This command will print a list of instances that you can connect to as
well as details about them. Select an instance ID (starting with `i-`)
and connect to it:

```
infrastructure/aws/ssh.py i-955af89
```

## Lambda

The AWS Lambda task uses a cron-style scheduler to run once a day for all
configN jobs except config3 which runs once a week on Saturdays (which is the
day our scripts run maintenance, so it's the right day to pick when doing things
less than daily).  config3 previously ran only when manually triggered but this
resulted in both configuration bit-rot as well a tendency for config3 to be out
of date with regard to the other indexers which cause actual visible problems
when static resources changed.

### Automated-ish Updates

If you just want to update the existing daily lambda jobs for the release jobs,
you can:

- Inside the vagrant VM:
  - `cd /vagrant`
  - `./infrastructure/aws/build-lambda-zips-from-inside-vm.sh`
    - This will produce 5 zips files in `/vagrant` matching `lambda-release*.zip`
- Outside the vagrant VM where you have active credentials so that `ssh.py`
  works.  This will upload the zips and delete them after each successful upload
  so there should be no zip files left over.
  - `./infrastructure/aws/upload-lambda-zips-from-outside-vm.sh`

### Lambda Details / Manual Updates

The task that runs can be generated by running the following command inside
the Vagrant VM instance:

```
./infrastructure/aws/build-lambda-indexer-start.sh \
  https://github.com/mozsearch/mozsearch \
  https://github.com/mozsearch/mozsearch-mozilla \
  config1.json \
  master \
  release1
```

The first three arguments are links to the repositories containing
Mozsearch and the configuration to use. The fourth argument is a branch
name. When scripts check out Mozsearch or the configuration
repository, they will check out this branch. The last argument is used
to determine which ELB target group will be updated. The `release1`
argument updates the `release1-target` target group (which might
control, for example, `example.com`). The `dev` argument updates the
`dev-target` target group (which might control, for example,
`dev.example.com`).

When the script finishes, it generates a file, `/tmp/lambda.zip`, that
can be uploaded to AWS Lambda using the AWS control panel. To update
an existing lambda task, select that task from the AWS Lambda console,
scroll down to the "Function code" section, and select "Upload a .zip file"
from the Actions menu. Save your changes and that should be all that
you need.

If you're setting up a new Lambda task for a new channel:
- Select "Create Function" from the AWS Lambda console.
  - Give it a name similar to the others (`start-<channel>-indexer`),
  - Select Python 3.9 for the Runtime
  - Expand "Change default execution role" and the "Use an existing role" radio
    button, then select the existing `lambda_indexer_start_role` for the
    execution role. This gives the task permissions to create indexer instances.
  - Press the "Create function" button to create the rule.
- On the function's page (you may have to click on the newly created rule or you
  may end up there automatically):
  - On the "Code" tab which you should already be looking at:
    - Click "Edit" by runtime settings and set the Handler to
     `lambda-indexer-start.start` (this refers to the `start` function in the
     `lambda-indexer-start.py` file inside the generated `lambda.zip`).
    - Hit "Save" to get back to the code tab.
  - On the "Configuration" time tab (which you will need to click on):
    - On the "General configuration" vertical sub-tab:
      - Click the "Edit" button for "general configuration" section so we can
        edit the "Timeout" setting.
      - The default timeout is 3 seconds which is way too short.  Increase that
        to 1 minute.
  - At the top of the UI in the "Function Overview" box you can add a cron
    trigger by pressing the "Add trigger" button and selecting "EventBridge
    (CloudWatch Events)" as a source from the drop-down and select one of the
    existing "everyday" rules or create a new one as appropriate.

## Triggering indexing manually

### Via the AWS UI

From the Lambda UI's list of functions you can:
- Select the function you want to run by clicking on it and bringing up its UI.
- Click on the "Test" tab.
- Click on the "Test Event" section's "Test" button.  If you wait about 12
  seconds an AJAX UI should do a spinny thing and then show a green
  "Execution result: succeeded".  If you see a failure and if it mentions a
  timeout, then that means you probably forgot to increase the timeout from the
  default of 3 seconds when creating the function.  Update it now!

### Locally

It's fairly easy to trigger an indexing job manually from your local
computer. To do so, run the following from within the Python virtual environment:

```
infrastructure/aws/trigger_indexer.py \
  https://github.com/some-user/mozsearch \
  https://github.com/some-user/mozsearch-mozilla \
  some-config.json \
  some-development-branch \
  dev
```

The arguments here are the same as those to
`build-lambda-indexer-start.sh`. In this example, the new index would
appear on `dev.example.com` and it would use Mozsearch code from the
`some-development-branch` branch.

Note that the .zip file created for AWS Lambda in the previous section
merely includes a copy of the `trigger_indexer.py` script, which it
invokes when the task runs.

## Creating additional development channels

If many developers are working on features concurrently, it might be
useful to set up additional channels so they can test on AWS without
stepping on each others' toes. In order to create a channel, the
following steps need to be done in the AWS console:

1. Decide on a name for the new channel. These instructions will use
   `foo` as the name.
2. In the EC2 console, go to the Load Balancers section and create
   a new Load Balancer (of type Application Load Balancer). Give it
   a name like `foo-lb`.  The non-default settings needed are:
- Listeners: add listeners for both HTTP and HTTPS
- Availability Zones: select all three availability zones
- Certificate: use the wildcard certificate for `*.searchfox.org`
  from ACM.
- Security group: Select the load-balancer security group
- Target group: Create a new target group with name `foo-target`
3. After creating the new Load Balancer, copy the DNS name from
   the description tab (something like `foo-lb-123456789.us-west-2.elb.amazonaws.com`)
4. Go to the Route 53 console, and under the `searchfox.org` Hosted
   Zone, add a new Record Set with the following properties:
- Name: `foo` (it will append `.searchfox.org` automatically)
- Type: A - IPv4 address
- Alias: Yes
- Alias Target: the DNS name copied from the Load Balancer. Note that
  it will automatically prepend `dualstack.` to the name.

That's it! After this is set up, you can trigger an indexer run
using the `foo` channel (instead of `dev` or `release`) and it
will show up at https://foo.searchfox.org once it is complete.

## Creating additional release channels

If more release channels are required (usually because we want to
host even more repos and the existing indexers/web-servers are
nearing their capacity limits), the process is a little different
than that for creating additional development channels as described
above. There is only one load balancer for all release channels,
so you don't have to create one. However, you do need to create a
new target group. Make sure it starts with the string "release" as
this is handled specially within parts of the Mozsearch codebase.

Once you've created a new target group, you can kick off an indexer
and/or set up a lambda task for this channel using your desired
config file. The only other step required is to modify the `release-lb`
load balancer to direct requests for those new repos to the appropriate
target group. Do this by selecting the `release-lb` load balancer in
the AWS EC2 console, going to the listeners tab, and editing the rules.
Note that you need to edit the rules for both HTTP and HTTPS manually.
The rule editor is fairly self-explanatory, just add new rules
in the (ordered) list to redirect requests for the new repos to the
new target group.

## Provisioning and cloud init

The EC2 instances for indexing and web serving are started using a
custom Amazon Machine Image (AMI). This is the disk image used for
booting the machine. These AMIs are based off Ubuntu 20.04, but
additional software has been installed for all the basic dependencies,
like clang for the indexing machine and nginx for the web server.

The AMIs also contain the Ubuntu cloud init package, which allows a
custom shell script to be passed to the machines using the Amazon API
when the instances are created. The shell script runs after the
machine boots up. Mozsearch uses the shell script to pass in
parameters like the branch, channel, and configuration repository. The
`trigger_index.py` and `trigger-web-server.py` scripts generate the
custom shell scripts that are sent to indexing and web server
instances.

New AMIs need to be built every time a dependency changes (if a newer
version of Clang is required, for example). We've also recently started
re-provisioning whenever we update Cargo.toml dependencies so that the
update process is less likely to fail due to download failures (which
was happening frequently enough that we started doing this.)

Dependencies that aren't handled by the build system need to be expressed in
our shell scripts:
- infrastructure/aws/indexer-provision.sh: AWS-specific dependencies/setup for
  the indexing process.  This runs before the normal indexer provisioning script
  in order to perform setup like resizing the EBS boot partition.
- infrastructure/indexer-provision.sh: Dependencies for indexing both in the
  local dev VM and on an AWS instance.
- infrastructure/aws/web-server-provision.sh: AWS-specific dependencies/setup
  for the indexing process.  This will tend to be a subset of the indexer setup
  because there's less to run on the web-server and we also don't give the web
  server an IAM role so it can't do as much infrastructure manipulation.  (These
  things must be done on behalf of the web-server by the indexer that is
  starting the web-server.)
- infrastructure/web-server-provision.sh: Dependences for web-serving in the
  local dev VM and on an AWS instance.  Because the dev VM will also run the
  indexer provisioning scripts, this script should ideally avoid doing redundant
  work.  However, it's not required for this script to succeed if it's run a
  second time itself; we no longer support re-provisioning the dev VM manually.
  (Instead, the VM should be destroyed and rebuilt.)

Generating a new AMI should now be largely automated thanks to the work on
[bug 1747289](https://bugzilla.mozilla.org/show_bug.cgi?id=1747289).
However, there are a set of manual steps that need to be taken, see below.

To re-provision the indexer AMI, run the following:

```
infrastructure/aws/trigger-provision.py indexer \
  infrastructure/aws/indexer-provision.sh \
  infrastructure/common-provision-pre.sh \
  infrastructure/indexer-provision.sh \
  infrastructure/common-provision-post.sh
```

For web serving, use this command:

```
infrastructure/aws/trigger-provision.py web-server \
  infrastructure/aws/web-server-provision.sh \
  infrastructure/common-provision-pre.sh \
  infrastructure/web-server-provision.sh \
  infrastructure/common-provision-post.sh
```

The `trigger-provision.py` script starts a new EC2 instance and uses
cloud-init to run the given provisioner shell scripts in it. These
scripts:
- Install all the required dependencies.
- Create a new AMI image named `{indexer/web-server}-YEAR-MONTH-DAY-HOUR-MINUTE`
  (well, that's the template).
- Wait for the image to be created; an S3 snapshot needs to be performed and
  this takes on the order of 10 minutes.
- Tag the new image with "indexer" or "web-server" as appropriate.
- remove the tag from the old image.
- send an email about success/failure
  - Disclaimer: Depending on when provisioning fails, it's possible that the
    system state will mean that it's not possible for a failure email to be
    sent.

In the event of failure, the EC2 instance will shut itself down via `shutdown`
with a 10 minute delay which means that you can inspect the failure by canceling
shutdown with `sudo shutdown -c` if you log in before shutdown, or by restarting
the instance if the instance has already shut down.  The `ssh.py` command will
offer to start the instance if it's stopped, so no extra steps are required.

### Still-Required Manual Steps

The following will continue to need to be done eventually, at least until
more automation is put in place.
1. The old AMIs need to be deleted.  Each AMI uses S3 storage and has an
   associated (low) cost, and we don't really need more than one backup or even
   a backup after a successful indexing run, so it's likely best to delete the
   old AMIs a few days after provisioning.
   - Deregistering is accomplished by:
     - Going to the EC2 console and clicking on "AMIs" under the "Images"
       heading to get a list of current AMIs.
     - Click on the AMI you think you want to delete.  Because of the date-based
       naming scheme, this should be an AMI with an older name.
     - Confirm that the AMI is not currently tagged for use.  Specifically,
       there should be no tags listed, resulting in "No tags found" being
       displayed.
     - Click the "Actions" button up at the top of the pane and select
       "Deregister AMI".
   - You shouldn't need to worry about any side-effects on existing EC2
     instances because they effectively fork a copy-on-write version of the AMI
     at startup.
2. The volume snapshots corresponding to the old AMIs need to be deleted. As
   with the AMI, the volume snapshot uses has an ongoing cost.
   AWS automatically prevents you from deleting snapshots that are associated
   with a still-active AMI, so the easiest way to purge unused snapshots is to:
   - Go to the snapshots pane (under "Elastic Block Store" heading in the EC2
     console.
   - Select all the snapshots
   - From the actions menu, select delete.
   - Confirm the deletion as requested
   - This will fail to delete some snapshots (because they are currently in
     use by some AMI) and delete all the unused ones.
   - Verify that the number of snapshots remaining is equal to the number of
     AMIs (as of this writing at least, each AMI generates one volume snapshot).

## Updating the machine after startup

Some dependencies change too often to require a new image, so they are
installed every time an instance boots. These include Rust and
Spidermonkey, since the Gecko build process will fail if they are out
of date. Additionally, a current version of Mozsearch and the
configuration repository must be installed when each instance is
started.

The provisioner scripts automatically install a `~ubuntu/update.sh`
script that downloads/builds this software. This script is run by the
custom cloud-init script when each instance is started.

## Error handling

When the indexer instance is created, a crontab file is installed that
runs an error handling script after 6 hours. The presumption is that
any indexing job taking more than 6 hours must have failed. The error
handling script uses Amazon Simple Email Service to send an email
notifying the Searchfox email list that indexing failed. Then it shuts
down (but does not destroy) the EC2 indexer instance. The instance can
be debugged by starting it up again from the AWS console and logging
into it via ssh.

Even on successful runs, the index log is grepped for warning lines,
and an email is sent to the searchfox mailing list containing these
warnings. Warnings are "recoverable errors" in that the indexing completed
with a new deployment, but some part of the functionality may be missing
due to an error that needs fixing. The complete log is uploaded to
the `indexer-logs` S3 bucket, so if additional context is needed for the
warnings, you can download the complete log from there and inspect it.
The name of the log is the timestamp at completion, suffixed with the
channel (e.g. `release`) and the file stem of the config file used.

## Debugging errors

If an error occurs, the email sent to the searchfox mailing list will
contain some of the log output. The log in the email may make it obvious
what the root cause was. If not, you may have to start up the indexer
instance using the EC2 web console, and then SSH in to it to examine
the log in more detail and/or inspect other state to debug the problem.
After SSH'ing to the indexer, you should run the command:
```
sudo mount /dev/`lsblk | grep 300G | cut -d" " -f1` /index
```
to re-mount the data volume. This will allow you to inspect the state
on the data volume as well as run additional commands for debugging
purposes, or to test a fix.

Because the AWS indexing jobs now use a scratch-disk and that's relevant
for the indexing process, when the indexer aborts, it moves the contents
of `/mnt/index-scratch` under an `interrupted` directory on the above
mount point.  So the in-progress indexing data can be found at
`/index/interrupted` after the above mount.  In order to make paths sane
again, you can run the command:
```
sudo ln -s /index/interrupted /mnt/index-scratch
```
to provide the same effective path mapping.  Note that you wouldn't want
to restart indexing under this regime as `/mnt/index-scratch` would
be backed by IO-bound S3. If you *do* want to do I/O intensive work
in this state, you can move the interrupted state back to a local
disk by running:
```
$HOME/mozsearch/infrastructure/aws/mkscratch.sh
mv /index/interrupted/* /mnt/index-scratch/ # may take a long time
```

The shell scripts that run during indexing
generally require some environment variables to be set; you can set
up the main ones by sourcing the load-vars.sh script like so:
```
export MOZSEARCH_PATH=$HOME/mozsearch
# Replace the last two arguments with the appropriate config file
# and repo that errored out
source $MOZSEARCH_PATH/scripts/load-vars.sh $HOME/config/config.json mozilla-central
```

After the debugging is complete, or even if no SSHing is required,
it is important to terminate the indexer and delete the incomplete
index volume, otherwise they will sit around forever and eat up money.
You can terminate the indexer either through the EC2 web console, or
by running
```
infrastructure/aws/terminate-indexer.py <instance-id>
infrastructure/aws/delete-volume.py <volume-id>
```
from within your local searchfox venv (see the above section
on setting up AWS locally). The terminate-indexer.py script or the
web console will let you know the volume ID of the volume to delete.

```

## docs/crossref.md
```
# Cross-referencing analysis data

Once analysis results for individual files have been generated, these
results need to be combined to make it possible to link from one file
to another. This is the job of the cross-referencer (located in
`tools/src/bin/crossref.rs`). It reads in target records from every
analysis file and records them in a hashtable. The hashtable maps the
symbol name to every target record with that symbol. Finally, the
hashtable is written to a pair of files `${index}/${tree_name}/crossref` and
`${index}/${tree_name}/crossref-extra`.  The `-extra` variant stores data
payloads that are large enough that they impact our ability to perform a
memory-mapped binary search of `crossref` file.

The general structure of hit records is:

```
{<kind>: [{"path": <file-path>, "lines": [{"lno": <lineno>, "line": <text-of-line>}, ...]}, ...]}
```

The values for `<kind>` are Declarations, Definitions, Uses,
Assignments, IDL, and Callees.

The `<text-of-line>` contains the text of the given line, with leading and
trailing spaces stripped.  An example entry in this file looks like:

```
!_ZN19nsISupportsPRUint647SetDataEm
:{"Declarations":[{"lines":[{"line":"NS_IMETHOD SetData(uint64_t aData) = 0;","lno":830},{"line":"NS_IMETHOD SetData(uint64_t aData) override; \\","lno":842}],"path":"__GENERATED__/dist/include/nsISupportsPrimitives.h"}],"Definitions":[{"lines":[{"line":"nsSupportsPRUint64::SetData(uint64_t aData)","lno":371}],"path":"xpcom/ds/nsSupportsPrimitives.cpp"}],"IDL":[{"lines":[{"line":"attribute uint64_t data;","lno":129}],"path":"xpcom/ds/nsISupportsPrimitives.idl"}],"Uses":[{"lines":[{"line":"wrapper->SetData(mWindowID);","lno":72}],"path":"dom/audiochannel/AudioChannelService.cpp"},{"lines":[{"line":"wrapper->SetData(mID);","lno":8925}],"path":"dom/base/nsGlobalWindow.cpp"},{"lines":[{"line":"ret->SetData(gBrowserTabsRemoteStatus);","lno":1004}],"path":"toolkit/xre/nsAppRunner.cpp"}]}
```

The file is sorted.

The first line is the symbol name and the second line is a JSON object
describing all the target records for that symbol.  Each line begins with an
indicator character that identifies the type of line.

More details from https://bugzilla.mozilla.org/show_bug.cgi?id=1702916:
- `crossref` continues to be newline-delimited.
- Each line in `crossref` gets a prefix indicating what's on the line:
  - `!`: An Identifier follows.
  - `:`: Inline-stored JSON for the preceding line's identifier (which must be an identifier).
  - `@`: Externally-stored JSON in `crossref-extra`.  The entirety of the line (eliding the trailing newline) should be `@${offsetOfJsonOpeningCurlyBrace.toString(16)} ${lengthIncludingNewline.toString(16)}`.  The offset and length (including newline) are represented in hexadecimal (without preceding `0x`) and separated by a space.  The choice of hex is for information density purposes while still being human readable.  Because I'll be augmenting `searchfox-tool` to directly perform any lookups people would otherwise use UNIX tools for, I think this should be fine.
- Although it seems like this would support having comment lines, we won't support
  these, at least not initially, as it would complicate the bisection logic which
  benefits from being able to depend on things being written in pairs.
- `crossref-extra` also ends up looking like `crossref` for the sake of ease of debugging.  It's newline delimited and will include (useless) `!Identifier` lines preceding each long JSON line.  The JSON lines also get `:` prefixed onto them even though the offsets in `crossref` will not include the leading `:`.
  - The rationale here is that it seems nice if someone wants to build a naive script / grep command invocation that they can just point it at both files and they'll get a result without having to deal with the offset indirection by requiring the second line to start with `:` and ignore the `@` second lines.
- The initial arbitrary line length cutoff will be 3k based on the statistics I gathered from comment 0 and because if we assume 4k page sizes that means in any 4k page we should then still be able to find an identifier (although the binary search will likely be naive about page alignment issues which means it would probably be happier with a constant that's less than 2k).  I'm sure one could write a nice shell script to brute force some practical legwork.  Or we could vary the constant randomly every day and gather the performance characteristics, etc. etc.  I'm not super concerned, I just want rust-based lookups.

### Identifiers file

In addition, an identifiers file is generated that is used for
`id:`-style searches. It appears at
`${index}/${tree_name}/identifiers`. For each target record, the
`pretty` name of the identifier is broken into components by splitting
on `:` and `.`. Given a `pretty` name of `A::B::C`, lines are
generated for `A::B::C`, `B::C`, and `C` (since these are the things
people might search on). The line has the form:

```
<qualified-name-suffix> <symbol-name>
```

This file is sorted (case insensitively). When the user searches for a
qualified name `Abc::Def`, the web server will use binary search to
find all lines starting with `Abc::Def`. Then it looks up the
corresponding symbols in the crossref file and combines those results.

### Jumps file

Finally, a `jumps` file is also generated. This file is used when
generating the "Goto XYZ" context menu items. We only generate one of
these items if there is exactly one definition of a given symbol. For
all such symbols, we generate a line in the `jumps` file of the
following form:

```["<symbol-name>","<definition-path>",<definition-lineno>,"<definition-pretty-name>"]```

The pretty name comes from the `pretty` property of the single target
record for the definition.

```

## docs/newrepo.md
```
# Adding a new repository

The basic steps needed to add a new repository to searchfox are listed below. This assumes as a prerequisite
that you have set up the Vagrant VM as documented in the [top level README](../README.md).

## 1. Create a tarball with the git repo

This is as simple as running `git clone` to clone the repo locally, and then running `tar` and `lz4`
to make an lz4-compressed tarball.  Note that it's best to do this inside your mozsearch folder so
that it's automatically mirrored inside the vagrant VM instance for other steps below. For example:

```
cd $MOZSEARCH
git clone https://github.com/mozilla/glean git
tar cf - git | lz4 - glean.tar.lz4
```

If you're cloning a hg repo, use cinnabar:

```
git clone -b branches/default/tip hg::https://hg.mozilla.org/hgcustom/version-control-tools git
pushd git
git branch -m master
git config fetch.prune true
popd
tar cf - git | lz4 - version-control-tools.tar.lz4
```

## 2. Create a tarball of the blame repo

To create the blame repo, you can manually run the `build-blame` tool. If your git repo has hg metadata that
git-cinnabar can access, that will also be included into the blame repo. If you don't have git-cinnabar
installed at all, set `CINNABAR=0` in your environment before running the `build-blame` tool. You can run
this step inside or outside the Vagrant VM, wherever you prefer. The instructions assume you're inside
the VM because that's usually where the rust code is built and run.

```
cd /vagrant
pushd tools && cargo +nightly build --release && popd
mkdir blame
pushd blame && git init . && popd
tools/target/release/build-blame ./git ./blame # this might take a while, depending on your repo size
tar cf - blame | lz4 - glean-blame.tar.lz4
```

## 3. Upload the two tarballs to the S3 bucket

This uses the `upload.py` script, and assumes you have appropriate AWS permissions (see the section on
[Setting up AWS access locally](aws.md#setting-up-aws-locally)). These commands should be run in whatever
environment you have AWS access with (typically outside the vagrant VM). If you do not have AWS access,
please contact one of the searchfox maintainers who can do the upload for you so you can finish debugging
and testing the setup.

```
cd $MOZSEARCH
infrastructure/aws/upload.py ./glean.tar.lz4 searchfox.repositories glean.tar.lz4
infrastructure/aws/upload.py ./glean-blame.tar.lz4 searchfox.repositories glean-blame.tar.lz4
```

The above commands don't provide progress output. You can equivalently do the upload with
the aws CLI tool, which does provide progress output. Make sure to set the permissions:
```
cd $MOZSEARCH
aws s3 cp ./glean.tar s3://searchfox.repositories/glean.tar --acl public-read
aws s3 cp ./glean-blame.tar s3://searchfox.repositories/glean-blame.tar --acl public-read
```

## 4. Update the mozsearch-mozilla repo

For this you need to clone the [mozsearch-mozilla](https://github.com/mozsearch/mozsearch-mozilla) repo. For
convenience in step 5 below, it's best to do this inside the vagrant VM instance, like so:

```
cd /home/vagrant
git clone https://github.com/mozsearch/mozsearch-mozilla mozilla-config
```

and then modify the `config.json` file with an entry for new repo. A basic one might look like this:

```json
    "glean": {
      "index_path": "$WORKING/glean",
      "files_path": "$WORKING/glean/glean",
      "git_path": "$WORKING/glean/glean",
      "git_blame_path": "$WORKING/glean/glean-blame",
      "github_repo": "https://github.com/mozilla/glean",
      "objdir_path": "$WORKING/glean/objdir",
      "codesearch_path": "$WORKING/glean/livegrep.idx",
      "codesearch_port": 8088
    }
```

A few things to note:
* The `codesearch_port` should be unique in the file, so increment by one compared to whatever the last entry in the file is.
* Watch your commas! This is JSON, so the last entry should not be followed by a comma.
* If the repo contains git submodules, you need to set `walk_submodules` to `False`. Otherwise the codesearch text indexing tool will attempt to index submodules and fail.

You also need to create a folder for your repo, with the `setup`, `build`, `upload`, and `find-repo-files` scripts. You can
look at the existing folders for other repos for inspiration. Copy-pasting from something like the `glean` repo will probably
be a good start, although the `build` step may need to be modified depending on whether or not your repo is buildable and
produces useful artifacts on the searchfox indexing instance.

Try to ensure that the `setup` script avoids unnecessary re-work for subsequent invocations on a "dirty" tree.
In other words, the default operation of the `indexer-setup.sh` script will not clean the working directory, so
artifacts may left behind from a previous run of `indexer-setup.sh`, and the repo's `setup` script should try
to reuse those artifacts where possible, rather than failing.

Finally, update the top-level `help.html` file to include a link to your new repo as well.

## 5. Test and debug

Assuming you did step 4 inside the vagrant VM, you can use the `build-mozilla-repo` target in the Makefile to test out
and debug the indexing of your new repository. However this will do a lot of work, including all the mozilla-central
indexing, which you probably don't want as it takes a lot of time. So first edit the `/home/vagrant/mozilla-config/config.json`
file to just have the entry for your new repo (i.e. delete all the other entries). Also ensure the `default_tree` field
at the top of the file points to your repo. Then:

```
cd /vagrant
make build-mozilla-repo
```

This will spew out a lot of output as it does stuff, and either end in an error (which you will need to debug), or deploy
the web server in your vagrant VM which you will be able to access from http://localhost:16995/.

Once any issues are debugged, push a PR with your changes to the `mozsearch-mozilla` repo.

## 6. Update load balancer

If the repo is being added to a config file OTHER than config1.json, it will need an entry in the load balancer. This is
what tells AWS to route requests for this repo to the web-server that hosts it. Setting this up requires AWS access,
and is usually done via the web console:
- Log in to the AWS console at aws.sso.mozilla.org
- Go to the EC2 service, and then the "Load balancers" page from the sidebar.
- Select the "release-lb" balancer, and then edit the rules for the HTTP listener.
- Add a new rule (or edit an existing one) such that the path for your repo is forwarded to the appropriate release target group. Use the existing rules as guides. Note that each rule has a limit of 5 condition values (i.e. repos), which is why they sometimes spill over into new rules even though they have the same target.
- Repeat the previous step for the HTTPS listener on the "release-lb" balancer.

```

## docs/vm-docker.md
```
# Using Docker with Searchfox

## Debugging Provisioning Failure

Did the `docker build` step fall over?

You can run the following to get a list of images:
```
docker image ls
```

This should list the known images sorted by creation time, with the most recent
image being at the top.  You'll want to copy the `IMAGE ID`.  You can then run
a shell in the container via the following, replacing `$IMAGE_ID` with the
relevant image id.

```
docker run -it --entrypoint bash $IMAGE_ID
```

```

## docs/analysis.md
```
# Analysis JSON

All data from the semantic analysis of a source file is dumped out to
JSON. For a file with path `${path}` from the repository root, the
analysis data is stored at `${index}/${tree_name}/analysis/${path}`.

The analysis data is broken into records. Each record corresponds to
an identifier in the original source code. Each line of the file
contains a record. (Technically the file is not JSON itself, but a
series of JSON objects delimited by line breaks.)

Analysis records are currently generated from:
* `scripts/js-analyze.js` for the JavaScript analysis.
* `scripts/idl-analyze.py` for IDL files.
* `clang-plugin/MozsearchIndexer.cpp` for C++ files.
* `tools/src/bin/rust-indexer.rs` for Rust files.

Analysis records may also be downloaded from Taskcluster for
mozilla-central builds (this can be viewed as an optimization to
avoid rebuilding mozilla-central as part of indexing). In fact,
records may be downloaded for multiple platforms, in which case
the `scripts/merge-analyses.py` script is used to combine the
different analyses from different platforms for a given source
file.

Analysis records are consumed from Rust code in
`tools/src/analysis.rs`.

There are two kinds of records: sources and targets. Each source
record generates one or more context menu items when the user clicks
on an identifier. Each target record corresponds to a place in the
source code where things are to be found. For the most part, there
will be one source record and one target record for a given
identifier. However, in the case of C++ inheritance, there may be one
source record and multiple target records.

Here is an example analysis. First, some JavaScript code:

```
let x = {a: 1};
dump(x.a);
```

Then its analysis:

```
{"loc":"1:4-5","source":1,"syntax":"def,prop","pretty":"property x","sym":"#x"}
{"loc":"1:4","target":1,"kind":"def","pretty":"x","sym":"#x"}
{"loc":"1:9-10","source":1,"syntax":"def,prop","pretty":"property a","sym":"#a"}
{"loc":"1:9","target":1,"kind":"def","pretty":"a","sym":"#a"}
{"loc":"1:9-10","source":1,"syntax":"def,prop","pretty":"property x.a","sym":"x#a"}
{"loc":"1:9","target":1,"kind":"def","pretty":"x.a","sym":"x#a"}
{"loc":"2:0-4","source":1,"syntax":"use,prop","pretty":"property dump","sym":"#dump"}
{"loc":"2:0","target":1,"kind":"use","pretty":"dump","sym":"#dump"}
{"loc":"2:5-6","source":1,"syntax":"use,prop","pretty":"property x","sym":"#x"}
{"loc":"2:5","target":1,"kind":"use","pretty":"x","sym":"#x"}
{"loc":"2:7-8","source":1,"syntax":"use,prop","pretty":"property a","sym":"#a"}
{"loc":"2:7","target":1,"kind":"use","pretty":"a","sym":"#a"}
{"loc":"2:7-8","source":1,"syntax":"use,prop","pretty":"property x.a","sym":"x#a"}
{"loc":"2:7","target":1,"kind":"use","pretty":"x.a","sym":"x#a"}
```

### Locations

In JavaScript, there is one source and one target for each
identifier. Both kinds of records include a location, of the form
`${lineno}:${colno}` for targets and
`${lineno}:${start_colno}-${end_colno}` for sources.

### Symbols

Both kinds of nodes also contain a `sym` property, which is how
sources are linked to targets. Source nodes are allowed to contain a
comma-delimited list of symbols. The results for all of these symbols
are combined in search results. Target records can only contain a
single symbol.

In JavaScript, the symbol can take three forms:

* `${file_index}-${var_index}`. All local variables are assigned a number
within the file. In addition, each JS file has a unique number assigned to it.
Combining them, we can generate a unique symbol for every variable in the
repository. Note that top-level variables are considered properties of the
global object, so they use the next form.

* `#prop`. A property `prop` of an object is given the symbol `#prop`. Properties
used in different files will get the same symbol, since they might be for the
same object.

* `object#prop`. In some cases, mozsearch is able to infer a static name for the
object in which a property lives. It can do this for object literals
(i.e., `let x = {prop: ...};`) as well as cases like `Foo.method = ...;`.
These names aren't always useful, but they often are. In all such cases, mozsearch
also generates analysis records for the bare property names (`#prop`).

There are a variety of symbol names for C++ code:

* For functions, the mangled name of the function is used. This allows mozsearch to
distinguish overloads.

* For local variables, the symbol is `V_${variable_location_hash}_${variable_name_hash}`,
where the location is the hash of the filename and line where the variable is declared.

* For anonymous types, the symbol is `T_${type_location_hash}`.

* For named types, the symbol is `T_${qualified_type_name}`.

* For anonymous namespaces, the symbol is `NS_${namespace_location_hash}`.

* For named namespaces, the symbol is `NS_${qualified_namespace_name}`.

* For fields of structs, unions, and classes, the symbol is
`F_<${record_symbol}>_${field_index}`, where `${record_symbol}` uses
the recursively defined symbol name and `${field_index}` is the
index of the field starting from 0.

* For enumeration constants, the symbol is
`F_<${enum_symbol}>_${constant_name}`.

* For files (for includes), the symbol prefix is `FILE_` and the following
  normalization rules apply.  See `mangleFile` in `MozsearchIndexer.cpp` for
  more rationale,
  - Any character that is not alphanumeric, `_` or `/` gets uppercase hex
    escaped with a prefix of `@`.  So "path/foo.h" becomes `FILE_path/foo@2Eh`.
  - Generated files get an extra prefix of `${PLATFORM}@` is addition to the
    `__GENERATED__/` path prefix.  So for the example of "path/foo.h" if it were
    generated on windows, the symbol would be
    `FILE_windows@__GENERATED__/path/foo@2Eh`.
    - The platform prefix is reusing the logic from `mangleLocation` which was
      intended to avoid hash collisions for things that aren't equivalent, but
      this probably could be re-thought to factor in types and whether
      merge-analyses ends up merging things.  Right now it creates diverging
      symbols that potentially don't need to diverge.


Note that in some cases, the symbol for a C++ identifier might vary from one
platform to another. For example, a function signature that includes `uint64_t`
produces a different mangled name on macOS and Linux. In such cases, if
analysis records are generated for multiple platforms, the final source records will
contain the symbols from all the platforms merged into a comma-separated list. The
`scripts/merge-analyses.py` script is used to do this.

Also, in many cases (notably for C++ and JS code), local variables have their
target records omitted, and their source records have the `no_crossref`
property. Since local variables are not referenced outside of a very narrow
context, this optimization helps to avoid bloating the index files with a
lot of unnecessary entries.

### Sources

A source record additionally contains a `syntax` property, a `pretty` property,
an optional `no_crossref` property, and an optional `nestingRange` property.

The `syntax` property describes how the identifier should be syntax highlighted.
It is a comma-delimited list of strings. Currently the only strings that have any
effect are:

* `def`, `decl`, and `idl` all cause the identifier to be shown in bold.
* `type` causes the identifier to be shown in a different color.

The `pretty` property is used to generate the context menu items for
the identifier. It should contain a human-readable description like
`constructor nsDocShell::nsDocShell` or `property
SessionStore.getTabState`.

The `no_crossref` property, if set, always has a value of `1`, and indicates
that this identifier will have no target records and does not participate
in cross-referencing.

The `nestingRange` property, if present, contains a value analogous to a
Clang SourceRange, with a string representation of "line1:col1-line2:col2" where
lines are 1-based and columns are 0-based.  This currently powers
"position:sticky" source code display so that when you are inside hierarchically
nested definitions you can immediately understand where you are and what the
scope is without needing to manually scroll up.

Ideally, the nesting range's two points are the start of the token creating a
nested block and the start of the token ending a nesting block.  ("{" and "}"
in C++ and the 'b' in "begin" for Pascal.  This is consistent with how Clang's
AST representations.)  However, when an analyzer doesn't have an exact AST to
work with (ex: rust as of writing this), we may do our best to simply specify a
conservative range of lines covering the children of a definition.

In cases where we have accurate nestingRange information, we may be able to do
neat tricks like highlight the area between braces or implement code folding.

#### Experimental / in flux
These fields, like the "structured" record type, are in flux as part of work
on the fancy branch.

A `type` may be emitted which is a string representation of the compiler's
understanding of the type/return type of something.  This will include
qualifiers like const/it's a pointer/it's a reference.

A `typesym` symbol may be emitted when the type corresponds to a type indexed by
searchfox (which will inherently not include qualifiers unless we're talking
about method signatures).

### Targets

Target records additionally contain a `kind` property, a `pretty` property,
and optionally `context`, `contextsym`, and `peekRange` properties.

The `kind` property should be one of `use`, `def`, `decl`, `assign`,
or `idl`. This property determines whether the identifier will appear
under the "Uses", "Definitions", "Declarations", "Assignments", or
"IDL" category of the search results page.

The `pretty` property is also used for the context menu. If a target
record is the only `def` target for a given symbol, then the context
menu for any source records with that symbol will contain a `Go to
${pretty}` entry, where `${pretty}` is the target's `pretty` property.

The `context` and `contextsym` properties capture an enclosing context
for the identifier, such as the enclosing function. These are used to
link to the context in search results that include the target record.

The `peekRange` property is a range of lines that appears to be
currently unused.

### Structured Records

Structured records are an attempt to provide richer information about types and
their relationships.  This is an evolving area of Searchfox, and is subject to
change.  This expected change also informs the design of the record format,
which is just a wrapper around an opaque JSON structure as far as `analysis.rs`
is concerned.  (All other record types are flat with a well known set of
keys/values.)

We emit structured records at the point of their definition.  Structured
records reference other structured records by their searchfox symbol identifier.
A structured record itself won't embed child types (even if they're not visible
outside the type) but instead reference them (which may involve searchfox
generating identifiers that have no meaning outside of searchfox, like is done
for locals).

Because of the realities of compilation, we know a parent class won't
necessarily know all of its subclasses, so all type references in emitted
records will generally only be upwards/sideways, never downwards.  We depend on
cross-referencing for determining sets of children/etc.

So for a class, we might expect the structured record in the analysis file to
contain:
- A list of its known super-classes.
- A list of its fields and members.

But it would not contain:
- A list of its known sub-classes.  This will be determined by
  cross-referencing.

#### Bytes and CharUnits

Clang defines a "Character Units" type
[`CharUnits`](https://clang.llvm.org/doxygen/classclang_1_1CharUnits.html#details)
that basically means bytes.  Searchfox just calls them bytes and assumes an
8-bit byte because strings are such a large part of the Firefox codebase that
calling bytes "char units" just adds terrifying confusion.

#### Formal Hierarchy:

Raw record info.  These are attributes that will be found in the analysis files.
- `pretty`: The pretty name/identifier for this structured symbol info.
- `sym`: The searchfox symbol for this symbol.
- `kind`: A string with one of the following values:
  - `file`: This is a synthetic symbol for the file (`FILE_normalized_blah`) for
    purposes like diagramming where we always operate in symbol space.  Keep in
    mind that we also store per-file info in `concise-per-file-info.json` and
    which corresponds to `ConcisePerFileInfo` structures.  That representation
    is intended for file-centric queries, but we can absolutely mirror data from
    that rep into this rep when needed/appropriate.
  - `enum`: XPIDL enums do this at least
  - `class`
  - `struct`
  - `union`
  - `method`: It's a method on a class/struct.  It will have `overrides`.
  - `function`: A boring function.  No `overrides`.
  - `field`: A member of a class/struct.  Right now the field record has minimal
    info with the intent being that the data canonically lives on the parent
    symbol and this just provides the `parentsym` necessary to get to that info.
    But this potentially needs more thought.  TODO: Think more on this!
  - `ipc`: An IPC function where there's a send method and a recv method.
  - `namespace`: We currently don't emit this for structured records from the
    C++ indexer, but we probably want to start to; currently adding this for
    tree-sitter based tokenization where it's potentially useful to be able to
    refer to namespaces.
- `parentsym`: For methods and fields, the symbol of the record to which they
  belong.  The current intent is that this is not populated for namespace
  purposes.  (That is, for a class "Bar" in namespace "foo" with pretty name
  "foo::Bar", "Bar" would not have a parentsym.)  The rationale is that we
  expect there to be a ton of stuff in any given namespace and we already have
  means of looking up the contents of a namespace via the `identifiers` table.
  This may want to evolve in the future, however.  Note that this attribute is
  not currently used for any cross-referencing, it's just meta.  (And note that
  target records' `contextsym` should frequently be the same when it's not just
  a namespace.)
- `slotOwner`: For bindings, an optional `StructuredBindingSlotInfo` as found in
  `bindingSlots` but in the opposite direction and identifying the owning symbol.
- `implKind`: Assume to be "impl" if not present.  Reasonable values:
  - `idl`: This is the semantic definition in XPIDL, IPDL, WebIDL, etc.
  - `binding`: Ex: WebIDL binding glue.  Auto-generated and perhaps of interest
    but of less interst than the `idl` or the `impl`.  Note that in cases like
    XPIDL, the binding machinery may be invisible for searchfox's context menus
    and the like.
  - `impl`: By default, most things will be "impl".  But when WebIDL/etc. are
    involved this will be the actual implementation.
- `sizeBytes`: Size in bytes.  Not present for method/function.
- `bindingSlots`: For binding definitions, an array of `StructuredBindingSlotInfo`:
  - `slotKind`: See `BindingSlotKind`
  - `slotLang`: See `BindingSlotLang`
  - `ownerLang`: See `BindingOwnerLang`
  - `sym`
- `ontologySlots`: For semantics introduced via `ontology-mapping.toml`, an
  array of `OntologySlotInfo`:
  - `slotKind`: See `OntologySlotKind`
  - `syms`: An array of target symbols.  Because of cases like
    RunnableConstructor this needs to be a set and not just a singular sym.
- `supers`: For class-like symbols, an array of:
  - `sym`: The searchfox symbol for this super.
  - `props`: An array of strings whose presence indicates a semantic attribute:
    - `virtual`: It's a virtual base class if present.
- `methods`: For class-like symbols, an array of:
  - `pretty`: The pretty name/identifier for this method.
  - `sym`: The searchfox symbol for this method.
  - `props`: An array of strings whose presence indicates a semantic attribute:
    - `static`: It's a static method (implies not "instance").
    - `instance`: It's a method on the instance (implies not "static").
    - `virtual`: It's a virtual method.
    - `user`: It's user-provided.
    - `defaulted`: It's defaulted per C++0x, AKA someone did `= default`.
    - `deleted`: It's deleted per C++0x, AKA someone did `= delete`.
    - `constexpr`: It's marked (C++11) constexpr!
- `fields`: For data-structure-like symbols, an array of:
  - `pretty`: The pretty name/identifier for this field.
  - `sym`: The searchfox symbol for this field.
  - `type`: Compiler's string representation of the type.  This is still
    somewhat experimental and the same thing we emit for source records.
  - `typesym`: The searchfox symbol for the type of this field.
  - `offsetBytes`: Byte offset of the field within this immediate structure.
  - `bitPositions`: Only present in bit-fields.  Object with the following
    properties whose names are derived from the AST dumper:
    - `begin`
    - `width`
  - `sizeBytes`: Only present in non-bit-fields.  The size of the fieldin bytes.
- `overrides`: For methods, an array of method signatures that are overridden.
  - `sym`: The searchfox symbol for the referenced method.
- `props`: For methods, an array of strings whose presence indicates a semantic
  attribute.  These are the same as the props under a class-like symbol's
  `methods` array.
  - `static`: It's a static method (implies not "instance").
  - `instance`: It's a method on the instance (implies not "static").
  - `virtual`: It's a virtual method.
  - `user`: It's user-provided.
  - `defaulted`: It's defaulted per C++0x, AKA someone did `= default`.
  - `deleted`: It's deleted per C++0x, AKA someone did `= delete`.
  - `constexpr`: It's marked (C++11) constexpr!

Attributes added/updated by cross-referencing:
- `subclasses`: Derived from `supers`.
  - `pretty`
  - `sym`
- `overriddenBy`: Derived from `overrides`.
  - `pretty`
  - `sym`


Attributes optionally added by merging (see more on this below).  These will
only be present when the structured records differed between platforms.  If
every platform had the same structured record contents, that representation is
left as-is.
- `variants`: A list of structured record objects with the above
  (pre-cross-referencing) attributes.  Each of these records will also have a
  `platforms` Array of string platform names that had these attributes.
- `platforms`: Array of string platform names whose structured records were the
  same and chosen to be the canonical variant.

#### Merging of Structured Records

Note that there is also documentation in `merge-analyses.rs` alongside the code
implementing this logic which may be more straightforward to understand.

Merging is performed by:
- Hashing all of the structured records for a given symbol so that we can detect
  equivalent structured records.  (The records don't include the platform name
  at the time.)
- Checking if all the structured records were the same, and if so, just spitting
  out the singleton record as-is.
- If the records differed, which will frequently be the case at the time of
  having written this where we built 32-bit ARM builds in addition to the 64-bit
  builds for Windows/OS X/Linux, we arbitrarily pick a record to be the
  "canonical" structured record "variant".
  - Currently this is the last record we saw because this will never be the
    32-bit ARM record with our current config where arm gets listed first.
- The canonical variant ends up looking exactly like it would have without
  merging except we add a `variants` attribute which is a list of all of the
  other records we saw (consolidated by hashing).  Every variant (including the
  top-level canonical variant) gets a `platforms` attribute that is just an
  Array of Strings that are the platform names as used by searchfox.


### C++ inheritance

C++ inheritance is one of the most tricky issues to deal with in an
analysis like this. When the user clicks on a method call and
searches, should they find method definitions/calls for other classes?
If so, how should this work? Mozsearch is pretty naive here, but it
generally is good enough to find an over-approximation of the desired
results.

Mangled C++ names include the concrete class on which a method is
defined. So each symbol is automatically tagged with the name of the
class in which it's defined.

First let's consider method calls. Consider a method call
`obj->f`. The mangled name of `f` will include the class name of `obj`
(or, when it doesn't implement `f` itself, the first type above it in
the inheritance chain that does define `f`).  In this case, mozsearch
generates one source and one target record. The symbol in both records
is for `f`'s mangled name.

When a method `T::f` is defined, mozsearch finds all the methods that
`f` overrides in `T`'s direct and indirect superclasses (including via
multiple inheritance). It generates a single source record whose `sym`
property contains all of these symbols in a comma-delimited
list. Consequently, searching from this method definition will find
all calls to this implementation of `f`, either directly through `T`
or via supertypes of `T` (which may invoke this `f` through dynamic
dispatch).

We also generate one target record for each override of `f` in `T`'s
superclasses. This way, searching from any method call, either through
`T` or some superclass of `T` that might dispatch to this `f` via
dynamic dispatch, will find this `f`.

One flaw in this system is that it doesn't consider the full shape of
the inheritance tree. Consider this case:

```
class A {
  virtual void f() { return 1; }
};

class B : public A {};

class C : public A {
  virtual void f() override { return 2; } // Y
};

B* b;
b->f(); // Z
```

We will generate a source record at the line marked Z with
one symbol, for `A::f`. At line Y, we generate a target record for
`C::f` that has symbols for `C::f` and `A::f`. Consequently, searching
at line Z will find `C::f`. However, it's not actually possible for
`b->f()` to call `C::f`. A better technique would recognize this. That
is future work.

### Multiple passes over a single file

The clang plugin for indexing C++ files will typically analyze a given
header file many times. In some cases, the analysis records generated
for the header file will differ from one time to another. This happens
most often in the case of macros and templates. Different files that
include a header will instantiate a macro or template in different
ways, which may produce different analysis results.

To compensate for this issue, the clang plugin checks to see if an
analysis file already exists before writing one out. If one does
exist, it reads it in and merges its data with the new data. The merge
happens by combining the new and old records in a single vector,
sorting the vector (using an arbitrary sort order), and removing
duplicates. This new list of records is then written to disk. During
this time, the file is kept locked to avoid issues with parallel
compilation.

```

## docs/blame-design.txt
```
Basic idea:
I'll build a data structure that's basically a hashtable.
The key will be a hash of some sort.
The value will be a LineInfo.
A LineInfo will have:
  changeset (when the line was last changed)
  prev (hash of LineInfo for the previous time the line was changed)

However, I also want to be able to reconstitute the entire file at the time
of the last change. Getting the contents of a file at a given revision
is a fast operation in Git. But I also need to get the blame for that
entire file. That means possibly following lots of links. Also, what if
a bunch of lines were added in unrelated places?

The correct way to get blame is to walk backwards in the log for the
given file, I think. A larger file might have ~2000 commits to it.
Getting the blame is easier with --incremental. It shows what's actually going on.

What if I just compute the blame for every revision of the file
simultaneously? Then users could just move a slider to decide which
rev they're interested in, and the results would be immediate.
I'd have to store more data, potentially, but it doesn't seem like
that much in total. And maybe I could compress it somehow.
For each line of each revision, I would store the rev. Git already
stores the data itself, so there would be no need for that.
Storing a revision is 20 bytes. So, for a 15000-line file with
2000 commits, that would be 572MB. That's too big I guess.
I wonder how big it would be if I gzipped it?

Let's say I already have this information for one version of a file.
Now I want to store it for a later version. I can look at the diff
that transforms to the new version. The new version is just the old
revlist, except any + lines in the diff are now set to the diff rev.
So I could store it as a range of lines from the old revlist, then some
new lines, then another range of lines from the old revlist, etc.

Another idea: what if I just stored the revlist for every 50 revs
or so. Then I could apply diffs from there the way that git blame
already does. The difference is that I'd never be more than 50 revs
from where I want to be. That would reduce my storage overhead by
a factor of 50. Now I'd be storing 11MB for nsDocShell, even without
any other compression.

Perhaps I should also store the complete list of revs to the file.
That would also speed things up considerably I think, and it shouldn't be
too big.

How do I deal with merge commits?
I need to linearize the commits in some way. That's fine, and I should
try to do it in the same order as git blame. But I also need to handle
the actual ordering correctly. Let's say I get to a rev. I need to use
the blame info for its predecessor rather than just whatever rev I
did most recently. What if it has multiple predecessors?

  b
 / \
a   d <-- HEAD
 \ /
  c

How do I compute blame at "d"? The contents of the files could be
completely different. I guess the merge will sort of tell me
what happened? Not really. I guess the diff will tell me something.

The diff for the merge commit will show nothing if there were no
merge conflicts. Otherwise it will show a multi-way diff.
So I want to take the blame data from both incoming revs.
I also need some kind of "base" rev to work off of.
So I need to find the common ancestor of these commits and get
the blame data for that. Then I can do a 3-way merge.
In the case of conflicts, I'll arbitrarily go with one side
or the other. Then I need to apply the merge diff, ignoring
the side that I ignored before.

When I build the database, can I include more info to make
this easier?


Idea: What if I make a new git repo that looks just like the
original except that the line data is replaced by revs from
the original repository? Then I would just need to keep a mapping
between original revs and the transformed revs. Git would do
whatever compression it normally does, which is typically
quite good. I could even make it handle renames and deletion
in the same way, perhaps?

Can I do the transformation based on the data itself, or do I
need to use diffs? And if I need to use diffs, how would I
transform merge commits?

Perhaps I could use git write-tree and git commit-tree and git reset
to create the commits in the new repo. That would make it easier to do
merge commits without invoking a merge tool.

Still, how do I generate the merged tree? I would find the parent
files in the transformed tree, as well as the LCA file. I'd really
like to ask git to do a merge and provide a 3-way diff to make it
happen. I wonder if that is possible.

I will have to read out diffs from the tree and do merges myself,
using them. The merge commit includes the LCA blob as well as the
branch blobs. I can look these up in the transformed tree and
apply the same changes to them. It shouldn't be too difficult.

Another idea:
Most merges do not have conflicts. In that case, I can just
do git merge-file --ours on the incoming blame files and I'm
done. If there *are* conflicts, perhaps I could fall back
to calling git blame? I estimate about .5% of commits would hit
this (~700).

Is there a way that I can use git blame --incremental to
get the data I need? It works pretty quickly if you just
want the top-most commits, which is all I think I want.
However, I would need to cut it off once I had reached
the point where I lose interest.

Say I want to compute blame for a given rev and I already
have blame for the parents. I run git blame --incremental
and stop it when it outputs a commit that happened before
one of the parents (inclusive). Generally, the only commit
this could be is the given one, I think.

But how do I use this? I don't know how many lines were
overwritten by this commit.


IDEA:
If I want to see the effect of a given changeset, I can do
  git blame --incremental rev^..rev -- filename
It will show me the lines changed in that rev and blame everything
else on rev^. It runs very quickly, even for old changesets.
But how do I handle merges with this technique?

If I have a changeset R with parents P1, ..., Pn, then I need to run
  git blame --incremental ^P1 ^P2 ... ^Pn R -- filename
Then the lines will be blamed to either R or else to P1, ..., Pn.
I can copy the blame information from P1, ..., Pn based on this.

Basic steps:
Figure out which files were modified by the commit.
For each file F:
  For each parent, get the blame for F by fetching it from the blame repo (or from a cache).
  Call git blame --incremental on the file
  Update the blame in the blame repo based on it

```

## docs/output.md
```
# HTML output

The final stage of indexing is to output a static HTML file for:
- Every source file via `tools/src/bin/output-file.rs`
- Every directory, linking to subdirectories and source files via
  `cmd_batch_render.rs` with `dir_listing.liquid`.
- The search file template used by `router/router.py` via
  `cmd_batch_render.rs` with `search_template.liquid`.
  (The template is little more than the HTML UI boilerplate, a place to inline
  the JSON-style results object, and a "load" listener to trigger the JS logic
  to render the results.)
- The `help.html` file at the root of the output tree.
  `scripts/output-help.html` wraps the contents of the config tree's `help.html`
  in the HTML boilerplate of the UI so that the standard search bar is at the
  top of the page.

### Indexed Source Files

This code lives in `tools/src/bin/output-file.rs` and `tools/src/output.rs`. The
main formatting loop is in `tools/src/format.rs`. The inputs to this process
are:

* The original source code, either from the file system (for the
  current version) or from version control (for historical versions).
* Blame information from the blame repository.
* Analysis records generated for the given file.
* Jump information generated by the cross referencer.

The original code is tokenized using one of two hand-coded tokenizers
(both in `tools/src/tokenize.rs`). One tokenizer recognizes C-like
languages (JS, C++, IDL, Python) and the other recognizes tag-based
languages (HTML, XML).

The central loop in `format.rs` iterates over tokens.  When it finds
an identifier token, it outputs markup for all text between the
previous identifier and this one. Then it checks if this identifier
has an analysis source record for the token's location. If it does,
then it adds `data-` attributes to the markup that describe what the
context menu should do for that identifier. Regardless, the markup
colors the identifier based on whether it's a reserved word as well as
the `syntax` property on the source record (if there is one).

### Blame diffs

The output code also has the ability to show annotated commit
diffs. These diffs are generated dynamically by the web server when
the user requests an annotated diff. The diff is generated by running
`git diff -U100000`. All the lines forming the "new" version of the
file are also run through `format.rs` to syntax highlight them
(although there are no analysis records available). The "old" `-`
lines are then merged in at the right locations and the appropriate
blame information is fetched for unchanged and `-` lines.

```

## docs/mach-design.txt
```
Directories used during indexing:
- Source directory (gecko-dev)
- Hg source dir (mozilla-central)
- Blame output (gecko-blame)
- Objdir
- Analysis files
- Output files
- Output dir listings
- Help file
- repo-files, repo-dirs, js-files, idl-files
- objdir-files, objdir-dirs
- crossrefs, jumps, identifiers
- Codesearch index (one per repo now!)

New S3 index uploads:
Web server can download its own copy of the repos just as the index server does (also via S3).
Where will blame be generated? In a separate stage, I guess?
Don't need objdir.
Need analysis files.
Output files and dir listings can be generated on web server before it's switched in.
Crossref needs to be done after we have all the analysis files.
Generating repo-files, etc. is easy, so that can be done independently on each server.
Need per-repo codesearch index.

Features:
1. Configure location of directories (index dir, config repo) and save persistently
2. Download all the S3 inputs

3. Update the repos
4. Build blame

5. Build/analyze C++ code
6. Analyze JS
7. Analyze IDL
8. Build crossref
9. Build codesearch index
A. Output files

Ideas:
If the repo has updated, then I can automatically clobber everything (or suggest doing so).
Have a command to update the repos and blame.
Have a command to update all the data for a particular file (C++, JS, or IDL). What about crossref/codesearch?

Commands:

mach config
  Ask questions about the locations of various things.

mach fetch
  Download all the S3 data

mach update
  Update all the repos and (optionally) re-index blame.

mach build [file/dir]
  Do steps 5 through 9. If param is given, do one of steps 5, 6, or 7. If param is given, can optionally do 8, 9.

mach serve
  Do step A and start server. Ideally just the rust server, which would be able to do everything.

```

## docs/index-directory-contents.md
```
# Index Directory Contents

If you look inside an index directory (ex: `~/index/mozilla-central` on an AWS
server), these are the files and sub-trees you may find, and who put them there.
Simpler configurations with fewer platforms will be simpler.

Note that many of the files referenced here are from the
https://github.com/mozsearch/mozsearch-mozilla configuration repository, most
specifically referencing the "mozilla-central" tree in its config1.json.

Directories:
- `analysis`: Directory hierarchy that directly corresponds to the paths exposed
  by the searchfox UI as a single unified namespace where objdir files are
  folded into `__GENERATED__`.  Each file has the name of its corresponding
  source/generated file and contains JSON analysis data.  It is populated by
  the indexing process.  For mozilla-central and similar builds, some of the
  indexing (ex: C++) occurs on the taskcluster build machine and is inherently
  per-platform, with `process-gecko-analysis.sh` using the per-platform
  `process-tc-artifacts.sh` to process the per-platform data and then
  `collapse-generated-files.sh` and `merge-analyses.rs` to merge the
  per-platform data into merged analysis files.
- `description`: Searchfox Directory hierarchy with per-file text files that
  contain extracted summaries from files extracted by `describe.rs` using
  heuristics that usually involve extracting the contents of comments found
  in the file.  These summaries are produced by `output-file.rs` as a
  byproduct of writing the HTML for the files to disk during the
  `output.sh` stage of `mkindex.sh`.
- `dir`: HTML files for the directory listings for each file.  It forms a
  parallel hierarchy to the `file` directory.  This is necessary because the
  directory HTML files are placed at `index.html` inside each directory which
  would collide if there is a source file with that name.  The nginx config
  uses a lookup sequence to make this work.  Produced by `cmd_batch_render.rs`
  with `dir_listing.liquid`.
- `file`: HTML files for each source/generated file.  Produced by
  `output-file.rs` from the source/generated file itself, the corresponding
  analysis file found under `analysis/`, the `jumps` all-files aggregate file,
  the `derived-per-file-info.json` all-files aggregate file, the
  corresponding per-file aggregate file found under `per-file-info/`.
- `gecko-blame`: git repository containing pre-computed per-file blame/annotate
  info built by `build-blame.rs`.  The directory name is specific per repository
  configuration.
- `gecko-dev`: Source git repository (using git-cinnabar).  The directory name
  is specific per repository configuration.  The indexed revision will be the
  currently checked out working directory.
- `objdir`: The conceptual source directory for things found under
  `__GENERATED__` in Searchfox's unified namespace.  When `output-file.rs` is
  generating an HTML file for `__GENERATED__/foo`, `foo` will be found under
  this directory.
- `objdir-*`: Leftover per-platform files from `process-gecko-analysis.sh`,
  probably just rust "save-analysis" files.  Most files are destructively
  consumed or moved during the process.  These leftovers are retained for
  debugging purposes.
- `detailed-per-file-info`: Directory hierarchy like `analysis` for storing
  detailed per-file information in a JSON file that's too large to put in the
  single aggregate `concise-per-file-info.json` file or not useful for summary
  purposes.
- `templates`: Holds `search.html` produced by `cmd_batch_render.rs` with
  `search_template.liquid` so that `router.py` can inline the JSON search
  results from a query.

Files:
- `all-dirs`: `repo-dirs` and `objdir-dirs` concatenated together by
  `find-objdir-files.sh` after deriving `objdir-dirs`.  Exists for the benefit
  of crossref for now.
- `all-files`: `repo-files` and `objdir-files` concatenated together by
  `find-objdir-files.sh` and shuffled after deriving `objdir-files`.  This
  exists so that `output.sh` can use `--pipe-part` which needs a real file on
  disk rather than a pipe from dynamically `cat`-ing the source files.  Also now
  the crossref script consumes this file.
- `analysis-dirs-*.list`: `find -type d` for each per-platform analysis
  directory.  Produced by the per-platform `process-tc-artifacts.sh` script and
  concatenated into the unified list by `process-gecko-analysis.sh`.  The
  paths are all relative to the per-platform directory and so don't actually
  include the platform-specific path.
- `analysis-dirs.list`: A concatenated list of the above per-platform lists
  created by `process-gecko-analysis.sh` and unique-ified, used to ensure the
  appropriate directories are created in the `analysis/` tree for the script's
  invocation of `merge-analyses` with stdout redirection (which can't mkdir -p
  itself).
- `analysis-files-*.list`: `find -type f` for each per-platform analysis
  directory.  Produced by the per-platform `process-tc-artifacts.sh` script and
  concatenated into the unified list by `process-gecko-analysis.sh`.  The
  paths are all relative to the per-platform directory and so don't actually
  include the platform-specific path.
- `analysis-files.list`: A concatenated list of the above per-platform lists
  created by `process-gecko-analysis.sh` and unique-ified and then passed to a
  `parallel` invocation of `merge-analyses` with stdout redirection.  Note that
  generated files are handled separately and use `generated-files.list`.
- `android-armv7.*`: A bunch of per-platform files downloaded by
  `fetch-tc-artifacts.sh` that we retain for debugging
  `process-gecko-analysis.sh`.
- `bugzilla-components.json`: Downloaded by `fetch-tc-artifacts.sh` and
  integrated into per-file information by `derive-per-file-info.rs` when invoked
  by `crossref.sh`.
- `crossref`: The big database produced by `crossref.rs` that has all the
  per-symbol information that gets returned by (symbol) search results by
  `router.py` after first mapping from pretty human names to machine symbol
  names using `identifiers`.  See [crossref.md](crossref.md) for more info.
- `concise-per-file-info.json`: Produced by `derive-per-file-info.rs` when
  invoked by `crossref.sh`.
- `downloads.lst`: List of curl download commands accumulated by
  `fetch-tc-artifacts.sh` so that it can run them in parallel.
- `generated-files-*.list`: `find -type f` for each per-platform generated-files
  directory.  Produced by the per-platform `process-tc-artifacts.sh` script and
  concatenated into the unified list by `process-gecko-analysis.sh`.  The
  paths are all relative to the per-platform directory and so don't actually
  include the platform-specific path.
- `generated-files.list`:  A concatenated list of the above per-platform lists
  created by `process-gecko-analysis.sh` and unique-ified and then passed to a
  `parallel` invocation of `collapse-generated-files.sh` which takes on
  responsibility for running `mkdir -p` directly and so doesn't need a `-dirs`
  variant of this list.
- `help.html`: The file you see at the root of the searchfox UI that is
  basically the HTML contents of the config repo's `help.html` with all the
  searchfox UI scaffolding wrapped around it by `cmd_batch_render.rs` with
  `help_index.liquid` just like is done for `templates/search.html`.
- `identifiers`: A text file mapping pretty human-readable symbol names to
  machine-readable (AKA mangled C++) symbol names.  Generated by `crossref.rs`
  and part of `router.py`'s search logic.  See [crossref.md](crossref.md) for
  more info.
- `idl-files`: A list of all the '.idl' files in the tree produced by
  `find-repo-files.py` found and that the per-config `repo_files.py` didn't
  veto.  Used by `idl-analyze.sh` to know what files to process when invoked by
  `mkindex.sh`.
- `ipdl-files`: A list of all the '.ipdl' files in the tree produced by
  `find-repo-files.py` found and that the per-config `repo_files.py` didn't
  veto.  Used by `ipdl-analyze.sh` to know what files to process when invoked by
  `mkindex.sh`.
- `ipdl-includes`: A list of all the '.ipdlh' files in the tree produced by
  `find-repo-files.py` found and that the per-config `repo_files.py` didn't
  veto.  Used by `idl-analyze.sh` to know what files to process when invoked by
  `mkindex.sh`.
- `js-files`: A list of all the '.js' files in the tree produced by
  `find-repo-files.py` found and that the per-config `repo_files.py` didn't
  veto.  Used by `js-analyze.sh` to know what files to process when invoked by
  `mkindex.sh`.
- `jumps`: Lookup table that maps from machine symbol names to their canonical
  definition point.  Produced by `crossref.rs` and consumed by `output-file.rs`
  so that the context menus can in the HTML files can generate definition links
  without having to involve any server queries.  See [crossref.md](crossref.md)
  for more info.
- `linux64.*`: A bunch of per-platform files downloaded by
  `fetch-tc-artifacts.sh` that we retain for debugging
  `process-gecko-analysis.sh`.
- `livegrep.idx`: This is the output file generated by the `codesearch`
   invocation in `build-codesearch.py` and contains the full-text index that the
    `codesearch` tool uses to do full-text search. It gets loaded by the
    `codesearch` invocation on the web-server instance, in
    `router/codesearch.py`.
- `macosx64.*`: A bunch of per-platform files downloaded by
  `fetch-tc-artifacts.sh` that we retain for debugging
  `process-gecko-analysis.sh`.
- `macosx64-aarch64.*`: A bunch of per-platform files downloaded by
  `fetch-tc-artifacts.sh` that we retain for debugging
  `process-gecko-analysis.sh`.
- `objdir-dirs`: A list of the directories found under `objdir/` for scripting
  and indexing purposes using in a bunch of places.  This is necessary because
  source files are exposed via the UI at `/PATH` and come from `gecko-dev/PATH`
  and generated files are exposed at `/__GENERATED__/PATH` and come from
  `objdir/PATH`.  Produced by `find-objdir-files.sh` which is invoked by
  `mkindex.sh` early in the indexing process.
- `objdir-files`: File variant of `objdir-dirs`, see above for more info.
- `repo-dirs`: A list of the directories that correspond to source files tracked
  by revision control produced by `find-repo-files.py` which actually runs
  `git ls-files` so if you don't check your files into git they won't show up.
  As with `objdir-dirs`, this needs to exist because of the split between source
  files and generated files.
- `repo-files`: File variant of `repo-dirs`, see above for more info.
- `target.json`: Downloaded by `resolve-gecko-revs.sh` as part of the process
  of identifying the most recent successful searchfox indexing jobs run on
  taskcluster for the given channel/tree.  Taskclusters' routes mechanism means
  that the most recent job will be exposed via both its specific revision and
  "latest", so if we fetch the "latest" version, we'll get a real revision in
  this JSON file and can then use it to make sure all other fetched results come
  from the exact same revision.
- `test-info-all-tests.json`: Downloaded by `fetch-tc-artifacts.sh` and
  integrated into per-file information by `derive-per-file-info.rs` when invoked
  by `crossref.sh`.
- `win64.*`: A bunch of per-platform files downloaded by
  `fetch-tc-artifacts.sh` that we retain for debugging
  `process-gecko-analysis.sh`.
- `wpt-metadata-summary.json`: Downloaded by `fetch-tc-artifacts.sh` and
  integrated into per-file information by `derive-per-file-info.rs` when invoked
  by `crossref.sh`.

```

## docs/liquid-templating-cheatsheet.md
```
# Liquid Templating Cheatsheet

Mozsearch uses the [liquid crate](https://crates.io/crates/liquid) which is a
rust implementation of the liquid templating language.  Documentation for the
[JS](https://liquidjs.com/tutorials/intro-to-liquid.html) and the canonical
[Ruby](https://shopify.github.io/liquid/) implementations are available and are
quite good, but note that [liquid-rust](https://github.com/cobalt-org/liquid-rust)
currently does not implement all of the blocks/tags/filters that are documented
at those implementations.  liquid was chosen from the limited pool of rust
templating engines that also had JS implementations available so that we could
use the same logic on both the server and client if needed.

## Liquid Core Semantics

### Blocks, Tags, Outputs, and Filters

Tags look like `{% foo %}`.  Outputs look like `{{ some_var }}`.  liquid-rust
adds an additional concept of "blocks" which are tags which can have other tags
conceptually nested inside them, like `{% for %}` which gets paired with
`{% endfor %}`.   Outputs can also include filters which use a pipe-like syntax
that looks like `{{ my_mixed_case_string || downcase }}`.

### Truthy / Falsy / Empty / Blank

Under the canonical ruby semantics, the only things that are falsy are `false`
and `nil`.  Empty strings, empty arrays, and empty objects are all truthy!  If
you would like sane results, you need to compare against the magic literals of
`empty` and/or `blank`.

There is logic [in the core method value_eq](https://github.com/cobalt-org/liquid-rust/blob/7b767eea877990ae96a6761b9ed74db8baab8f9e/crates/core/src/model/value/view.rs#L287-L291)
which checks if the LHS or RHS is an explicit ["state"](https://github.com/cobalt-org/liquid-rust/blob/7b767eea877990ae96a6761b9ed74db8baab8f9e/crates/core/src/model/value/state.rs)
and in that case uses the "query_state" method to check the semantics of the
value for the given state; for example [the str ValueView has this logic](https://github.com/cobalt-org/liquid-rust/blob/7b767eea877990ae96a6761b9ed74db8baab8f9e/crates/core/src/model/scalar/mod.rs#L553-L560).  This provides the mechanism for "truthy" (used by the
[if block's existence condition](https://github.com/cobalt-org/liquid-rust/blob/30ad5c4e3f84f918c1be46215187bcbb5ebde37d/crates/lib/src/stdlib/blocks/if_block.rs#L341))
as well as for the "[default](https://liquidjs.com/filters/default.html)" filter's
logic which checks for falsy or blank, not just falsy.

The difference between `empty` and `blank` is that `blank` will trim a string before
checking if it's empty.  So `"" == empty` and `"" == blank` but for `" "` we find
that `" " != empty` and `" " == blank`.

#### Is a string blank / empty?

Want to check if a string is exactly `""`?  Then use `my_var == empty`.  Do you
also want to act like it's empty if it's only whitespace?  Then use
`my_var == blank`.

#### Is an array empty?

Use `empty`.  Do `my_array_val == empty`.

### Is an object empty?

Want to check if an object dictionary has no keys/values?  Use `empty`.  Do
`my_obj == empty`.  Be aware that if you try and access a nonexistent property
of an object you will get an IndexError, so in many cases you actually want to
be using the `contains` operator instead; see the next section for more info.

### Does an object have a specific property?

liquid-rust currently does not have support for "EmptyDrop" as documented at
https://shopify.github.io/liquid/basics/types/ so if you try and access a
property that does not exist, you will get an IndexError.  If EmptyDrop support
existed, in theory it would just return something that is equal to empty, but
the ruby implementation doesn't seem to explicitly test for this behavior, so it
may just be an implementation artifact.

In any event, you can use the "contains" operator to do `my_obj contains "key"`
to check if there's a key property.  The canonical documentation for contains at
https://shopify.github.io/liquid/basics/operators/ is a little confusing because
the way it discusses object makes it sound like you can't do this, but I think
it's just saying it can only test string equality / check for keys and is not
capable of doing structural equality tests.

### Other Conditional Logic

#### There's a "contains" operator

In a conditional you can do things like `my_array contains "foo"` in an if tag
like `{% if my_array contains "foo" %}`.  As noted above about objects, this
also works for objects, so `my_obj contains "foo"` should return true for an
underlying object that looks like `{ "foo": ... }` in JSON.

#### There are "and" and "or" operators that operate right-to-left

You can't use parentheses or otherwise nest things.  It's like the rightmost
pair is fully nested in parentheses.  Don't even think about asking about
short-circuiting.

### Types

#### Arrays are zero-based and indexed with square brackets

`my_array_val[0]` is the first item in the array.  Usually you would use a
[for](https://liquidjs.com/tags/for.html) block tag, not directly subscript
things.  Subscripting makes sense for tuple types, such as when iterating over
an object/map.

### Whitespace Control

Putting a `-` character on the inside of a tag indicates to strip the whitespace
on that side of the tag.  So `{{-` /`-}}` can be used instead of `{{`/`}}` and
`{%-`/`-%}` can be used instead of `{%`/`%}`.  Note that you can make the
decision independently for the opening and closing tags.

If stripping all whitespaces is not suitable, the other option to control the
whitespace is to put newlines and spaces inside the `{{`/`}}` and `{%`/`%}`:

```
<div id="content" %{
     if expanded
     %}aria-expanded="true" aria-hidden="false"%{
     else
     %}aria-expanded="false" aria-hidden="true"%{
     endif %}>
```

## Liquid-Rust Supported Tags / Filters

### Built-in

This list is derived from examining the source's [blocks](https://github.com/cobalt-org/liquid-rust/blob/master/crates/lib/src/stdlib/blocks/mod.rs),
[tags](https://github.com/cobalt-org/liquid-rust/blob/master/crates/lib/src/stdlib/tags/mod.rs),
and [filters](https://github.com/cobalt-org/liquid-rust/blob/master/crates/lib/src/stdlib/filters/mod.rs) modules for the stdlib.

- Blocks:
  - capture
  - case
  - comment
  - for
  - if
  - ifchanged (only outputs its rendered contents if they've changed since the
    last outputted value, starting from the base case)
  - raw
  - tablerow
  - unless
- Tags
  - assign
  - break
  - continue
  - cycle
  - decrement
  - include (note: "render" is not supported right now!!)
  - increment
- Filters
  - abs
  - append
  - at_least
  - at_most
  - capitalize
  - ceil
  - compact
  - concat
  - date
  - default
  - divided_by
  - downcase
  - escape
  - escape_once
  - first
  - floor
  - join
  - last
  - lstrip
  - map
  - minus
  - modulo
  - newline_to_br
  - plus
  - prepend
  - reverse
  - remove
  - remove_first
  - replace
  - replace_first
  - round
  - rstrip
  - size: Number of elements in an array or object, length for strings.
  - slice
  - split
  - strip
  - strip_newlines
  - sort
  - sort_natural
  - strip_html
  - times
  - truncate
  - truncate_words
  - uniq
  - upcase
  - url_decode
  - url_encode
  - where

### Mozsearch additions

- Filters
  - compact_pathlike: Remove excess whitespace in a path-like string.  Compacts
    `" foo /  bar/ baz "` to `"foo/bar/baz"`.
  - ensure_bug_url: If we're given something that's clearly a link, pass it
    through as-is, but if it's not a bug, format it into a proper bug tracker
    link, which by default is (or may still be hardcoded to be) BMO.
  - fileext: Extracts the file extension from a path string, defaulting to the
    empty string if there is no file extension.  (Note that this does not use
    the "default" mechanism!)
  - json: Render the given value to JSON
  - `strip_prefix_or_empty`: Takes an argument which is a prefix to attempt to
    remove.  If the string started with the prefix, the prefix-stripped string
    is returned.  If the string did not start with the prefix, an empty string
    is returned.  We probably could also have just returned the false value but
    idiomatically it's probably better to have explicit checks against `empty`
    everywhere to reduce confusion.

```

## LICENSE
```
Mozilla Public License, version 2.0

1. Definitions

1.1. "Contributor"

     means each individual or legal entity that creates, contributes to the
     creation of, or owns Covered Software.

1.2. "Contributor Version"

     means the combination of the Contributions of others (if any) used by a
     Contributor and that particular Contributor's Contribution.

1.3. "Contribution"

     means Covered Software of a particular Contributor.

1.4. "Covered Software"

     means Source Code Form to which the initial Contributor has attached the
     notice in Exhibit A, the Executable Form of such Source Code Form, and
     Modifications of such Source Code Form, in each case including portions
     thereof.

1.5. "Incompatible With Secondary Licenses"
     means

     a. that the initial Contributor has attached the notice described in
        Exhibit B to the Covered Software; or

     b. that the Covered Software was made available under the terms of
        version 1.1 or earlier of the License, but not also under the terms of
        a Secondary License.

1.6. "Executable Form"

     means any form of the work other than Source Code Form.

1.7. "Larger Work"

     means a work that combines Covered Software with other material, in a
     separate file or files, that is not Covered Software.

1.8. "License"

     means this document.

1.9. "Licensable"

     means having the right to grant, to the maximum extent possible, whether
     at the time of the initial grant or subsequently, any and all of the
     rights conveyed by this License.

1.10. "Modifications"

     means any of the following:

     a. any file in Source Code Form that results from an addition to,
        deletion from, or modification of the contents of Covered Software; or

     b. any new file in Source Code Form that contains any Covered Software.

1.11. "Patent Claims" of a Contributor

      means any patent claim(s), including without limitation, method,
      process, and apparatus claims, in any patent Licensable by such
      Contributor that would be infringed, but for the grant of the License,
      by the making, using, selling, offering for sale, having made, import,
      or transfer of either its Contributions or its Contributor Version.

1.12. "Secondary License"

      means either the GNU General Public License, Version 2.0, the GNU Lesser
      General Public License, Version 2.1, the GNU Affero General Public
      License, Version 3.0, or any later versions of those licenses.

1.13. "Source Code Form"

      means the form of the work preferred for making modifications.

1.14. "You" (or "Your")

      means an individual or a legal entity exercising rights under this
      License. For legal entities, "You" includes any entity that controls, is
      controlled by, or is under common control with You. For purposes of this
      definition, "control" means (a) the power, direct or indirect, to cause
      the direction or management of such entity, whether by contract or
      otherwise, or (b) ownership of more than fifty percent (50%) of the
      outstanding shares or beneficial ownership of such entity.


2. License Grants and Conditions

2.1. Grants

     Each Contributor hereby grants You a world-wide, royalty-free,
     non-exclusive license:

     a. under intellectual property rights (other than patent or trademark)
        Licensable by such Contributor to use, reproduce, make available,
        modify, display, perform, distribute, and otherwise exploit its
        Contributions, either on an unmodified basis, with Modifications, or
        as part of a Larger Work; and

     b. under Patent Claims of such Contributor to make, use, sell, offer for
        sale, have made, import, and otherwise transfer either its
        Contributions or its Contributor Version.

2.2. Effective Date

     The licenses granted in Section 2.1 with respect to any Contribution
     become effective for each Contribution on the date the Contributor first
     distributes such Contribution.

2.3. Limitations on Grant Scope

     The licenses granted in this Section 2 are the only rights granted under
     this License. No additional rights or licenses will be implied from the
     distribution or licensing of Covered Software under this License.
     Notwithstanding Section 2.1(b) above, no patent license is granted by a
     Contributor:

     a. for any code that a Contributor has removed from Covered Software; or

     b. for infringements caused by: (i) Your and any other third party's
        modifications of Covered Software, or (ii) the combination of its
        Contributions with other software (except as part of its Contributor
        Version); or

     c. under Patent Claims infringed by Covered Software in the absence of
        its Contributions.

     This License does not grant any rights in the trademarks, service marks,
     or logos of any Contributor (except as may be necessary to comply with
     the notice requirements in Section 3.4).

2.4. Subsequent Licenses

     No Contributor makes additional grants as a result of Your choice to
     distribute the Covered Software under a subsequent version of this
     License (see Section 10.2) or under the terms of a Secondary License (if
     permitted under the terms of Section 3.3).

2.5. Representation

     Each Contributor represents that the Contributor believes its
     Contributions are its original creation(s) or it has sufficient rights to
     grant the rights to its Contributions conveyed by this License.

2.6. Fair Use

     This License is not intended to limit any rights You have under
     applicable copyright doctrines of fair use, fair dealing, or other
     equivalents.

2.7. Conditions

     Sections 3.1, 3.2, 3.3, and 3.4 are conditions of the licenses granted in
     Section 2.1.


3. Responsibilities

3.1. Distribution of Source Form

     All distribution of Covered Software in Source Code Form, including any
     Modifications that You create or to which You contribute, must be under
     the terms of this License. You must inform recipients that the Source
     Code Form of the Covered Software is governed by the terms of this
     License, and how they can obtain a copy of this License. You may not
     attempt to alter or restrict the recipients' rights in the Source Code
     Form.

3.2. Distribution of Executable Form

     If You distribute Covered Software in Executable Form then:

     a. such Covered Software must also be made available in Source Code Form,
        as described in Section 3.1, and You must inform recipients of the
        Executable Form how they can obtain a copy of such Source Code Form by
        reasonable means in a timely manner, at a charge no more than the cost
        of distribution to the recipient; and

     b. You may distribute such Executable Form under the terms of this
        License, or sublicense it under different terms, provided that the
        license for the Executable Form does not attempt to limit or alter the
        recipients' rights in the Source Code Form under this License.

3.3. Distribution of a Larger Work

     You may create and distribute a Larger Work under terms of Your choice,
     provided that You also comply with the requirements of this License for
     the Covered Software. If the Larger Work is a combination of Covered
     Software with a work governed by one or more Secondary Licenses, and the
     Covered Software is not Incompatible With Secondary Licenses, this
     License permits You to additionally distribute such Covered Software
     under the terms of such Secondary License(s), so that the recipient of
     the Larger Work may, at their option, further distribute the Covered
     Software under the terms of either this License or such Secondary
     License(s).

3.4. Notices

     You may not remove or alter the substance of any license notices
     (including copyright notices, patent notices, disclaimers of warranty, or
     limitations of liability) contained within the Source Code Form of the
     Covered Software, except that You may alter any license notices to the
     extent required to remedy known factual inaccuracies.

3.5. Application of Additional Terms

     You may choose to offer, and to charge a fee for, warranty, support,
     indemnity or liability obligations to one or more recipients of Covered
     Software. However, You may do so only on Your own behalf, and not on
     behalf of any Contributor. You must make it absolutely clear that any
     such warranty, support, indemnity, or liability obligation is offered by
     You alone, and You hereby agree to indemnify every Contributor for any
     liability incurred by such Contributor as a result of warranty, support,
     indemnity or liability terms You offer. You may include additional
     disclaimers of warranty and limitations of liability specific to any
     jurisdiction.

4. Inability to Comply Due to Statute or Regulation

   If it is impossible for You to comply with any of the terms of this License
   with respect to some or all of the Covered Software due to statute,
   judicial order, or regulation then You must: (a) comply with the terms of
   this License to the maximum extent possible; and (b) describe the
   limitations and the code they affect. Such description must be placed in a
   text file included with all distributions of the Covered Software under
   this License. Except to the extent prohibited by statute or regulation,
   such description must be sufficiently detailed for a recipient of ordinary
   skill to be able to understand it.

5. Termination

5.1. The rights granted under this License will terminate automatically if You
     fail to comply with any of its terms. However, if You become compliant,
     then the rights granted under this License from a particular Contributor
     are reinstated (a) provisionally, unless and until such Contributor
     explicitly and finally terminates Your grants, and (b) on an ongoing
     basis, if such Contributor fails to notify You of the non-compliance by
     some reasonable means prior to 60 days after You have come back into
     compliance. Moreover, Your grants from a particular Contributor are
     reinstated on an ongoing basis if such Contributor notifies You of the
     non-compliance by some reasonable means, this is the first time You have
     received notice of non-compliance with this License from such
     Contributor, and You become compliant prior to 30 days after Your receipt
     of the notice.

5.2. If You initiate litigation against any entity by asserting a patent
     infringement claim (excluding declaratory judgment actions,
     counter-claims, and cross-claims) alleging that a Contributor Version
     directly or indirectly infringes any patent, then the rights granted to
     You by any and all Contributors for the Covered Software under Section
     2.1 of this License shall terminate.

5.3. In the event of termination under Sections 5.1 or 5.2 above, all end user
     license agreements (excluding distributors and resellers) which have been
     validly granted by You or Your distributors under this License prior to
     termination shall survive termination.

6. Disclaimer of Warranty

   Covered Software is provided under this License on an "as is" basis,
   without warranty of any kind, either expressed, implied, or statutory,
   including, without limitation, warranties that the Covered Software is free
   of defects, merchantable, fit for a particular purpose or non-infringing.
   The entire risk as to the quality and performance of the Covered Software
   is with You. Should any Covered Software prove defective in any respect,
   You (not any Contributor) assume the cost of any necessary servicing,
   repair, or correction. This disclaimer of warranty constitutes an essential
   part of this License. No use of  any Covered Software is authorized under
   this License except under this disclaimer.

7. Limitation of Liability

   Under no circumstances and under no legal theory, whether tort (including
   negligence), contract, or otherwise, shall any Contributor, or anyone who
   distributes Covered Software as permitted above, be liable to You for any
   direct, indirect, special, incidental, or consequential damages of any
   character including, without limitation, damages for lost profits, loss of
   goodwill, work stoppage, computer failure or malfunction, or any and all
   other commercial damages or losses, even if such party shall have been
   informed of the possibility of such damages. This limitation of liability
   shall not apply to liability for death or personal injury resulting from
   such party's negligence to the extent applicable law prohibits such
   limitation. Some jurisdictions do not allow the exclusion or limitation of
   incidental or consequential damages, so this exclusion and limitation may
   not apply to You.

8. Litigation

   Any litigation relating to this License may be brought only in the courts
   of a jurisdiction where the defendant maintains its principal place of
   business and such litigation shall be governed by laws of that
   jurisdiction, without reference to its conflict-of-law provisions. Nothing
   in this Section shall prevent a party's ability to bring cross-claims or
   counter-claims.

9. Miscellaneous

   This License represents the complete agreement concerning the subject
   matter hereof. If any provision of this License is held to be
   unenforceable, such provision shall be reformed only to the extent
   necessary to make it enforceable. Any law or regulation which provides that
   the language of a contract shall be construed against the drafter shall not
   be used to construe this License against a Contributor.


10. Versions of the License

10.1. New Versions

      Mozilla Foundation is the license steward. Except as provided in Section
      10.3, no one other than the license steward has the right to modify or
      publish new versions of this License. Each version will be given a
      distinguishing version number.

10.2. Effect of New Versions

      You may distribute the Covered Software under the terms of the version
      of the License under which You originally received the Covered Software,
      or under the terms of any subsequent version published by the license
      steward.

10.3. Modified Versions

      If you create software not governed by this License, and you want to
      create a new license for such software, you may create and use a
      modified version of this License if you rename the license and remove
      any references to the name of the license steward (except to note that
      such modified license differs from this License).

10.4. Distributing Source Code Form that is Incompatible With Secondary
      Licenses If You choose to distribute Source Code Form that is
      Incompatible With Secondary Licenses under the terms of this version of
      the License, the notice described in Exhibit B of this License must be
      attached.

Exhibit A - Source Code Form License Notice

      This Source Code Form is subject to the
      terms of the Mozilla Public License, v.
      2.0. If a copy of the MPL was not
      distributed with this file, You can
      obtain one at
      http://mozilla.org/MPL/2.0/.

If it is not possible or desirable to put the notice in a particular file,
then You may include the notice in a location (such as a LICENSE file in a
relevant directory) where a recipient would be likely to look for such a
notice.

You may add additional accurate notices of copyright ownership.

Exhibit B - "Incompatible With Secondary Licenses" Notice

      This Source Code Form is "Incompatible
      With Secondary Licenses", as defined by
      the Mozilla Public License, v. 2.0.

```

## README.md
```
# Mozsearch

Mozsearch is the backend for the [Searchfox](https://searchfox.org)
code indexing tool. Searchfox runs inside AWS, but you can develop on
Searchfox locally using Vagrant.

## Docker Setup For Local Development

We've moved from using Vagrant to docker for development because it's comparably
painless and because, especially on linux, it's great for the searchfox container
to be able to use all your CPU cores and only use memory when it needs it
(instead of all the time).

That said, if you want to use something else, that's fine.  If you look at our
docker scripts, they're just a bunch of shell scripts we run under Ubuntu and
they're basically the same as they were under Vagrant.  And we also just run
those scripts on top of a basic Ubuntu AMI for provisioning our instances.  If
you can run Ubuntu under a VM, you can run Searchfox in there.

Could you run searchfox outside of a container/VM?  Probably?  But, dependencies
are a hassle.

### Install docker.io

Can you already run "docker" on your command line and have it work?  Then you
can skip this section!

> [!WARNING]
> If you're running docker on macOS, make sure the Rosetta option is disabled
> in the Docker Desktop settings, otherwise you may run into segfaults on
> installing rust components. See https://github.com/rust-lang/rustup/issues/3902
> and friends.

#### Important Docker Licensing Notes

Docker has changed their licensing of their "Docker Desktop" packages.  We do
not recommend using Docker Desktop; you don't need it as long as you already
are able to run Ubuntu directly or under (para)virtualization like WSL2.

If you do want to use it and you work at Mozilla, you should acquire a license
through Service Desk before doing anything else.  If you already have a license,
that's fine too.

#### Alternative to Docker: Podman

Podman can be used almost as a drop-in replacement to Docker. Just make sure
that:
- You have a podman wrapper or symlink named docker in your PATH, the scripts
  call docker extensively.
- You set the environment variable PODMAN_USERNS="keep-id" âˆ’ or the equavalent
  option in containers.conf. The source repository is bind-mounted inside the
  container and the user in the container gets the same UID/GID as the caller.
  This makes sure that the vagrant user in the container can read/write inside
  the bind mount at /vagrant.

> Note for Nix users: the devShell in flake.nix provides both of those requirements.

#### Installing on macOS/OS X

I don't think anyone has tried this yet, but it seems like there are a variety
of options.  Here are some I've just briefly researched:
- Use podman.  https://podman.io/docs/installation explains how to use QEMU on
  macOS.
- Use Colima: https://github.com/abiosoft/colima
- Use lima, the thing that colima wraps: https://github.com/lima-vm/lima

If you use a thing and it works, please let us know and we can update these
docs.  Or better yet, you can submit a pull request for this doc!

#### Installing on WSL2 on Windows 10 or Windows 11

First, you need Ubuntu installed under WSL2.  If you already have a sufficiently
up-to-date version (I would suggest Ubuntu 22.04 or later), you can skip this
next step.

From the windows store, install "Ubuntu 22.04.1 LTS" or whatever updated version
there is.  Then I launched the "Terminal" app (which you may need to also
install from the Windows store).  I used the drop-down arrow to the right of the
add a tab "+" button and picked "Ubuntu 22.04.1 LTS".  This launched an
installer that ran for ~30 seconds and then prompted me to be okay to switch
back to the terminal.  Specific steps:
- I picked my language (â€œEnglishâ€)
- I picked my username and entered a user password
- On the â€œWSL configuration optionsâ€ I left things at their default (â€œ/mnt/â€ as the mount
  location, empty mount option, and â€œenable host generationâ€ and â€œenable resolv.confâ€
  generation checked.)
- I chose the reboot option.
- After the reboot, the terminal showed a terminal logged in as my user.

Now let's keep going through the next sections.

#### Installing docker.io on WSL2 and Linux

Run the following commands in your normal Linux terminal on Linux, and under
WSL2 on Windows, use the Terminal app with your "Ubuntu 22.04.1 LTS" tab open.

- `sudo apt update` to update the package registry, you may need to enter your
  password because youâ€™re using sudo.
- `sudo apt install -y docker.io`
- `sudo adduser $USER docker` to add yourself to the docker group.
- Unfortunately you wonâ€™t effectively be in the group until you are running
  under a fresh login.  There are 2 easy ways to do this and 1 annoying way:
  1. If you are using windows terminal, you can close the existing tab and open
     a new tab.  In the new tab, you should then see â€œdockerâ€ when you run
     `groups`.
  2. Run `su - $USER` and this will dump you in a freshly logged in shell where
     you should see â€œdockerâ€ when you run `groups`.
  3. Logout and login again.
- Under WSL2: `sudo update-alternatives --config iptables` and pick the
  â€œiptables-legacyâ€ option, probably by hitting 1 and hitting enter.  This
  switches iptables to legacy mode.  This was necessary to make docker.io happy
  under WSL2 for me.

#### Installing deps that will make VS Code happy

VS Code is a very nice editor and it hooks up quite nicely to rust-analyzer via
the language server protocol (LSP).  Rust-analyzer needs to run somewhere and
VS code's remote support is quite excellent.  I personally do the following on
these platforms:
- On Windows, I install the following dependencies in the "Ubuntu 22.04.1 LTS"
  install and then connect to it via VS code's support for connecting to WSL(2)
  instances.  This results in rust-analyzer running in the WSL2 instance but not
  inside the docker instance.  (I've never tried having it run inside docker;
  that might not be a bad idea?)
- On linux I just run VS code locally with no remote connections which means
  rust-analyzer is running locally (and outside of the docker instance).  Having
  the following things installed is still useful.

Installation steps:
- Go to https://rustup.rs/ and copy and paste its command into your terminal.
  What could go wrong?
- `sudo apt install -y clang pkg-config libssl-dev cmake`

### Check out mozsearch

- Change into whatever directory you want to keep your â€œmozsearchâ€ checkout in.
- `git clone https://github.com/mozsearch/mozsearch.git` to check out searchfox
  if you havenâ€™t already
- `git submodule update --init --recursive` to check out the relevant submodules

### Now you can build and run the mozsearch docker container!

- Run `./build-docker.sh` to provision and setup the â€œVMâ€ / container.  This
  will run our shell scripts to create multiple layers of filesystem.  Note that
  the "livegrep" build phase may seem to hang for several minutes at one point
  when it's looking for boost dependencies and this is unfortunately expected.
  Don't ctrl-c out (although it's harmless if you do, you can re-run the command
  later).
- Run `./run-docker.sh` to start up the VM/container.  It will shutdown when you
  exit from the shell so leave the shell running to keep the web-server
  accessible.
- If you want more shells inside the VM, you can invoke `./run-docker.sh` from
  other terminal tabs and they will connect to that same running container.
  Note that if you log out of the first invocation that actually started the
  container, it will close all the other terminals.

Those scripts have default container, image, and volume names ("searchfox",
"searchfox", "searchfox-vol") that can be overridden.  You would usually only do
this if you want multiple orthogonal searchfox checkouts on the same machine.
(But do note that currently the script does not make it possible to override the
decision to use port 16995, so you can't run both containers at the same time.)

## Instant Fun with the Test Repo

Once you are able to successfully `./run-docker.sh`, once you are inside at that
shell, you can do the following:

```
cd /vagrant
make build-test-repo
```

The above process will:
- Build necessary tools.
- Setup the indexer for the test repo.
- Run the indexer for the test repo.
- Run [test checks](docs/testing-checks.md) against the indexer output.
- Setup the webserver for the test repo.
- Run the webserver for the test repo.
- Run [test checks](docs/testing-checks.md) against the web server.  These are
  the same checks we ran against the indexer above plus several that are
  specific to the web-server.

After that, you can connect to http://localhost:16995/ and see Searchfox at work!

Once you've done that, you might want to read the
[Manual Indexing doc](docs/manual-indexing.md) for more details on what's
happening under the hood when you run the above make rule.


### Testing UI

You can also test the UI part of searchfox by the following inside the docker:

```
make webtest
```

If you want to run specific tests, you can run the following, where the
`FILTER` is a substring-match against the test file's path, which

```
./scripts/webtest.sh FILTER
```

For more details, please refer [the webtest document](docs/webtest.md).

### Testing locally with blame using the "searchfox" test config

The `tests` configuration defined at `tests/config.json` is very helpful, but it
isn't configured to use a blame repository or generate blame UI output.  If
you're making changes that affect the blame UI or might interact with it, it
helps to test with it!

The `searchfox` configuration defined at `tests/searchfox-config.json` exists
for this purpose.  It indexes the entirety of the repository.  It can be built
via the `Makefile` by invoking the following to build the index at
`~/searchfox-index` (whereas `tests` is built at `~/index`).

```
make build-searchfox-repo
```

Note that you will need to do a couple things for this to work right:
- You need to make sure any changes you've made to the searchfox repository are
  committed to git.  `output-file.rs` depends on the blame repository having
  lines that match up exactly with the state of the source files checked out
  from git or it can panic because of accessing beyond the end of vectors.  (The
  blame data will also be wrong.)
- You need to make sure the blame repository has been updated.  The Makefile
  will take care of this for you, but if you're running `indexer-run.sh`
  manually without first running `indexer-setup.sh`, you may experience
  problems.

Also note that this will terminate any previously running `tests` web servers
even though the indexes live at different directories (`~/index` versus
`~/searchfox-index`). If you find that you want both the `tests` and `searchfox`
configurations to be served at the same time, you can add a new configuration
file and update these docs and submit a pull requests.  Thanks in advance!

## Testing changes against mozilla-central

If you are making more extensive changes to searchfox, it's usually advisable to
test them against mozilla-central before landing them.  While it's possible to
do this locally, the normal way to do this is:
- If you have made any changes to the in-tree indexing process, such as the
  clang plugin, run the relevant try jobs using the mozilla-central try
  infrastructure.  If you haven't made any changes, you can skip this step and
  the AWS indexing job will just reuse mozilla-central's most recently nightly
  searchfox data.
- Run an AWS indexing job using `trigger_indexer.py`.

Details below.

### Running mozilla-central try builds for changes to in-tree indexing

For testing changes to the clang-plugin, run these steps, followed by the
steps in the next section.

* Make your changes to the build/clang-plugin/mozsearch-plugin/ folder
  in mozilla-central, and push them to try. Ensure that your try push has
  all the searchfox jobs as well as the bugzilla-components job. The following
  try syntax will accomplish this:
```
./mach try fuzzy --full -q "'searchfox" -q "'bugzilla-component"
```
* Record the full hg revision hash (40 characters) of the try push.

### Triggering an AWS indexer run

An important precondition is that you need to be a member of the "searchfox-aws"
mozillians.org group in order to have the access rights to do the following.  We
are happy to add Mozillians to this group who are actively interested in
contributing to searchfox.  Please reach out in #searchfox on
https://chat.mozilla.org/

- First, follow the [Searchfox AWS docs](docs/aws.md) to ensure you have your
  credentials working in general and that you can run the
  `infrastructure/aws/ssh.py` command and successfully get a list of active VMs.
  In particular, you will probably need to type:
  - `. env/bin/activate`
  - `aws sso login` (this assumes your "env/bin/activate" script above sets the
    `AWS_PROFILE` env variable; in our docs referenced above we added that to
    the activate script).
- Push your changes to your mozsearch branch and your mozsearch-mozilla branch,
  if appropriate.
  - It's usually a good idea to explicitly make sure you've saved all your
    buffers, that `git status` shows no uncommitted changes, that
    `make build-test-repo` runs successfully, and that
    `git push -f REMOTE BRANCH` says all the commits are already there.
- Pick what "channel" you are going to use.  Generally, the right answer is the
  "dev" channel, which will display its results at https://dev.searchfox.org/
  but it's possible to use and create other channels.  You can see if anyone
  already has a server up on the "dev" channel by running the
  `infrastructure/aws/ssh.py` script.
- Pick what config file you are going to use.  Normally this is "config1.json"
  which includes the mozilla-central repo and a few other repositories that
  don't have all the bells and whistles turned on.  You can use a different
  config file or edit config1.json to not contain repositories you aren't
  interested in to make things go faster.
  - You don't have to have your own branch of mozsearch-mozilla!  You can use
    https://github.com/mozsearch/mozsearch-mozilla and it's fine that it doesn't
    have a branch with the name of your development branch.  The scripts will
    automatically fall back to the default branch.
- If you didn't run a custom m-c try job, you can edit the following:
```
infrastructure/aws/trigger_indexer.py \
  https://github.com/some-user/mozsearch \
  https://github.com/some-user/mozsearch-mozilla \
  config1.json \
  some-development-branch \
  dev
```
- If you did run a custom m-c try job, the only difference is the addition of a
  `--setenv TRYPUSH_REV=full-40char-hash` to to the command.  Using a truncated
  hash won't work because, unlike the hg/git command line, the taskcluster
  server can't/won't expand revisions and searchfox doesn't apply any transforms
  at this time.  So this looks like:
```
infrastructure/aws/trigger_indexer.py \
  --setenv TRYPUSH_REV=full-40char-hash \
  https://github.com/some-user/mozsearch \
  https://github.com/some-user/mozsearch-mozilla \
  config1.json \
  some-development-branch \
  dev
```
- The author of the HEAD commit of the mozsearch branch that gets checked out
  will receive an email when indexing completes or when it fails.  This means
  that if you are testing changes that you've only made to mozsearch-mozilla,
  you will likely need to create a silly change to the mozsearch repo.
  - If your indexing run failed, the indexer will move the current state of its
    scratch SSD to the durable S3 storage and then stop itself.  The `ssh.py`
    command will restart the indexer for you when you connect to it.  Then you
    will need to investigate what went wrong.  See the section on Debugging
    errors in our [AWS docs](docs/aws.md) for more info.
  - If your indexing run succeeded, that means the indexer successfully kicked
    off a web-server.  You should be able to connect to the searchfox UI at
    https://dev.searchfox.org/ or whatever the name of the channel you used was.
    You should also be able to use `infrastructure/aws/ssh.py` to connect to the
    web-server and explore the contents of the built index under `~/index`.
- Once the indexer starts, [it generates a log file in `~/index-log`](infrastructure/aws/main.sh) on the instance.  You can track the progress by logging into the instance with `ssh.py` and running `tail -f ~/index-log`.  [The log file is copied to `/index/index-log`](infrastructure/aws/index.sh) once the indexing completes.
- When you are done with any of the above severs, you can use
  `infrastructure/aws/terminate-indexer.py` to destroy the VM which will also
  clean up any S3 storage the index used.  You can find which servers are yours
  via the `ssh.py` script, making sure to pay attention to the "channel" tag;
  you don't want to terminate any of the release servers!

## Background on Mozsearch indexing

The Mozsearch indexing process has three main steps, depicted here:

![Indexing diagram](/docs/indexing.png?raw=true)

Here are these steps in more detail:

* A language-specific analysis step. This step processes C++, Rust,
  JavaScript, and IDL files. For each input file, it generates a
  line-delimited JSON file as output. Each line of the output file
  corresponds to an identifier in the input file. The line contains a
  JSON object describing the identifier (the symbol that it refers to,
  whether it's a use or a def, etc.). More information on the analysis
  format can be found in the [analysis
  documentation](docs/analysis.md).

* Full-text index generation. This step generates a single large index
  file, `livegrep.idx`. This self-contained file can be used to do
  regular expression searches on every text file in the input. The
  index is generated by the `codesearch` tool, which is part of
  [Livegrep](https://github.com/livegrep/livegrep). The same
  `codesearch` tool is used by the web server to search the index.

* Blame generation. This step takes a git repository as input and
  generates a "blame repository" as output. Every revision in the
  original repository has a corresponding blame revision. The blame
  version of the file will have one line for every line in the
  original file. This line will contain the revision ID of the
  revision in the original repository that introduced that line. This
  format makes it very fast to look up the blame for an arbitrary line
  at an arbitrary revision. More information is available on
  [blame caching](docs/blame.md).

Once all these intermediate files have been generated, a
cross-referencing step merges all of the symbol information into a set
of summary files: `crossref`, `jumps`, and `identifiers`. These files
are used for answering symbol lookup queries in the web server and for
generating static HTML pages. More detail is available on
[cross-referencing](docs/crossref.md).

After all the steps above, Mozsearch generates one static HTML file
for every source file. These static HTML pages are served in response
to URLs like
`https://searchfox.org/mozilla-central/source/dir/foobar.cpp`. Most
requests are for URLs of this type. Generating the HTML statically
makes it very quick for the web server frontend (nginx) to serve these
requests.

HTML generation takes as input the analysis JSON. It uses this data to
syntax highlight the code more effectively (so that it can color types
differently from variables, and definitions differently from uses). It
also uses the analysis JSON, as well as the `jumps` file, to generate
the context menu information for each identifier. In addition, the
blame repository is used to generate HTML for the blame strip.

## More background

* Production Reference
  * [Adding new repos](docs/newrepo.md)
  * [AWS Production Infrastructure Details and Runbooks](docs/aws.md)
* Implementation Documentation
  * [Analysis](docs/analysis.md)
  * [Blame caching](docs/blame.md)
  * [Cross-referencing](docs/crossref.md)
  * [Index Directory Contents](docs/index-directory-contents.md)
  * [HTML output](docs/output.md)
  * [Testing](docs/testing-checks.md)
  * [Web serving](docs/web-server.md)
* Cheatsheets
  * [Bash scripting cheatsheet](docs/bash-scripting-cheatsheet.md)
  * [Liquid templating cheatsheet](docs/liquid-templating-cheatsheet.md)
  * [searchfox-tool cookbook](docs/searchfox-tool-cookbook.md)

```

## router/router.py
```
#!/usr/bin/env python3

from __future__ import absolute_import
import http.server
from socketserver import ForkingMixIn
import urllib
import sys
import os
import os.path
import json
import re
import subprocess
import signal
import time
import errno
import traceback
import collections

import crossrefs
import identifiers
import codesearch
from logger import log
from raw_search import RawSearchResults

# Our historical limits are somewhat limiting, let's add a brief and poorly
# but excitingly named scaling factor for our limits.
EXTREME_FACTOR = 4

def index_path(tree_name):
    return config['trees'][tree_name]['index_path']

# Simple globbing implementation, except ^ and $ are also allowed.
def parse_path_filter(filter):
    filter = filter.replace('(', '\\(')
    filter = filter.replace(')', '\\)')
    filter = filter.replace('|', '\\|')
    filter = filter.replace('.', '\\.')

    def star_repl(m):
        if m.group(0) == '*':
            return '[^/]*'
        else:
            return '.*'
    filter = re.sub(r'\*\*|\*', star_repl, filter)

    filter = filter.replace('?', '.')

    def repl(m):
        s = m.group(1)
        components = s.split(',')
        s = '|'.join(components)
        return '(' + s + ')'
    filter = re.sub('{([^}]*)}', repl, filter)

    return filter

key_remapping = { 'uses': 'Uses', 'defs': 'Definitions', 'assignments': 'Assignments',
                  'decls': 'Declarations', 'idl': 'IDL', 'aliases': 'Aliases',
                  'callees': None, 'field-member-uses': None }

def merge_defs_from_symbols_as(tree_name, mix_target, symbol_names, as_key):
    '''
    Helper for `expand_keys` to build an aggregate path hit list to be stored
    as `as_key` in the `mix_target` consisting of the definitions for each
    provided symbol name, augmented with some kind of hacky hint for the UI so
    it can know to generate a search link for each specific type.
    '''
    # Do not do anything if there's too many results!
    # This was previously 50 but nsIAsyncInputStream has 60 subclasses.
    if len(symbol_names) >= 100:
        return

    aggr_defs = []
    for symbol_name in symbol_names:
        info = crossrefs.lookup_single_symbol(tree_name, symbol_name)
        if info is None or 'defs' not in info:
            continue

        defs = info['defs']
        for path_hit in defs:
            path_hit['lines'][0]['upsearch'] = 'symbol:' + symbol_name
            aggr_defs.append(path_hit)

    if len(aggr_defs):
        if as_key in mix_target:
            mix_target[as_key].extend(aggr_defs)
        else:
            mix_target[as_key] = aggr_defs


def expand_keys(tree_name, new_keyed, traverse_relations=True, depth=0):
    '''
    Converts to the old Uses/Definitions/Assignments/Declarations/IDL rep
    from the new uses/defs/assignments/decls/idl rep, dropping 'callees'
    entries.  Performs the mutation in-place which also means keys that aren't
    re-mapped are passed through untouched.

    ## New relation-traversing support!

    To help address the regression in the handling of overridden methods, we
    now will also investigate the "meta" field and induce synthetic keys
    ["Overrides", "Overridden By", "Superclasses", "Subclasses"] if
    `traverse_relations` is set to True.

    Our general UX goal (operating within the existing "search-not-sorch" data
    model) is:
    - If showing a method which has overrides:
      - We will show an "Overridden By" section whose hits will be the
        definitions of the overrides and exposes a "(search using this symbol)"
        upsell.
    - If showing a method which is itself an override of something else:
      - We will show an "Overrides" section whose hits will be the definitions
        of the thing we are overriding and upsells "(search using this symbol)".
    - We do the same thing as the above for "Superclasses" and "Subclasses".
    - If there will be more than 100 results, we don't attempt to show anything
      out of concern for overwhelming the server.  (merge_defs_from_symbols_as
      is where the hard-coded constant lives, plus comments.)

    ## New IDL binding slot support!

    XPIDL and IPDL analysis have been updated to canonically emit only their own
    `XPIDL_foo` and `IPDL_foo` symbols with the C++ bindings and send/recv pairs
    expressed as slots.  To this end we now process the meta 'bindingSlots'
    array property and the singular 'slotOwner' property.
    '''
    for new_name, old_name in key_remapping.items():
        if new_name in new_keyed:
            # just drop records that the old names don't know how to handle.
            if old_name is None:
                new_keyed.pop(new_name)
            else:
                new_keyed[old_name] = new_keyed.pop(new_name)

    if 'meta' in new_keyed:
        if traverse_relations:
            # lookup_merging will have wrapped the value into a list
            meta_arr = new_keyed.pop('meta')
            for meta in meta_arr:
                if 'overrides' in meta:
                    merge_defs_from_symbols_as(tree_name, new_keyed, [x['sym'] for x in meta['overrides']], 'Overrides')
                if 'overriddenBy' in meta:
                    # Currently this derived relationship only includes the symbol
                    # name, as opposed to the overrides cases which is an obj with
                    # { sym, pretty }.
                    merge_defs_from_symbols_as(tree_name, new_keyed, meta['overriddenBy'], 'Overridden By')
                if 'supers' in meta:
                    merge_defs_from_symbols_as(tree_name, new_keyed, [x['sym'] for x in meta['supers']], 'Superclasses')
                if 'subclasses' in meta:
                    # This is also a derived relationship with only the symbol.
                    merge_defs_from_symbols_as(tree_name, new_keyed, meta['subclasses'], 'Subclasses')
                if 'slotOwner' in meta:
                    if meta['slotOwner']['ownerLang'] == 'idl':
                        # The owner is an IDL parent, make sure we're showing the IDL (def) as IDL.
                        merge_defs_from_symbols_as(tree_name, new_keyed, [meta['slotOwner']['sym']], 'IDL')
                    else:
                        # The owner is a definition in a foreign language
                        merge_defs_from_symbols_as(tree_name, new_keyed, [meta['slotOwner']['sym']], 'Definitions')
                if 'bindingSlots' in meta:
                    # Show IDL binding slots as definitions.
                    # But show non-IDL binding slots as bindings.
                    merge_defs_from_symbols_as(tree_name, new_keyed, [x['sym'] for x in meta['bindingSlots'] if x['ownerLang'] == 'idl'], 'Definitions')
                    merge_defs_from_symbols_as(tree_name, new_keyed, [x['sym'] for x in meta['bindingSlots'] if x['ownerLang'] != 'idl'], 'Bindings')
        else:
            del new_keyed['meta']

    return new_keyed

def escape_regex(searchString):
    # a version of re.escape that doesn't escape every non-ASCII character,
    # and therefore doesn't mangle utf-8 encoded characters.
    # https://bugzilla.mozilla.org/show_bug.cgi?id=1446220
    return re.sub(r"[(){}\[\].*?|^$\\+-]", r"\\\g<0>", searchString)

def parse_search(searchString):
    pieces = searchString.split(' ')
    result = {}
    for i in range(len(pieces)):
        if pieces[i].startswith('path:'):
            result['pathre'] = parse_path_filter(pieces[i][len('path:'):])
        elif pieces[i].startswith('pathre:'):
            result['pathre'] = pieces[i][len('pathre:'):]
        elif pieces[i].startswith('context:'):
            # Require the context to be an integer <= 10.
            try:
                # This may throw.
                context_lines = int(pieces[i][len('context:'):])
                context_lines = max(0, context_lines)
                context_lines = min(10, context_lines)
                result['context_lines'] = context_lines
            except:
                pass
        elif pieces[i].startswith('symbol:'):
            # This used to normalize "." into "#", presumably because of an
            # assumption that people might construct the symbols by hand, but
            # I don't think this ever caught on and this only would have worked
            # for symbols as generated by `js-analyze.js` but the new JS support
            # via SCIP does not create human-friendly symbols and this mapping
            # is destructive to SCIP symbols in a needless way.
            result['symbol'] = ' '.join(pieces[i:])[len('symbol:'):].strip()
        elif pieces[i].startswith('re:'):
            result['re'] = (' '.join(pieces[i:]))[len('re:'):]
            break
        elif pieces[i].startswith('text:'):
            result['re'] = escape_regex((' '.join(pieces[i:]))[len('text:'):])
            break
        elif pieces[i].startswith('id:'):
            result['id'] = pieces[i][len('id:'):]
        else:
            result['default'] = escape_regex(' '.join(pieces[i:]))
            break

    return result

def is_trivial_search(parsed):
    if 'symbol' in parsed:
        return False

    for k in parsed:
        if k == 'context_lines':
            continue
        if len(parsed[k]) >= 3:
            return False

    return True

class SearchResults(object):
    def __init__(self):
        self.results = []
        self.qualified_results = []
        self.count_limit_hit = False
        self.work_limit_hit = False

        self.pathre = None
        self.compiled = {}

    def set_path_filter(self, path):
        if not path or path == '.*':
            self.pathre = None
            return

        try:
            self.pathre = re.compile(path, re.IGNORECASE)
        except re.error:
            # In case the pattern is not a valid RE, treat it as literal string.
            self.pathre = re.compile(re.escape(path), re.IGNORECASE)

    def add_results(self, results):
        self.results.append(results)

    def add_qualified_results(self, qual, results, modifier):
        self.qualified_results.append((qual, results, modifier))

    max_count = 1000 * EXTREME_FACTOR
    max_work = 1000 * EXTREME_FACTOR
    path_precedences = ['normal', 'thirdparty', 'test', 'generated']
    key_precedences = ["Files", "IDL", "Definitions", "Declarations", "Bindings",
        "Aliases", "Overrides", "Overridden By", "Superclasses", "Subclasses",
        "Assignments", "Uses", "Textual Occurrences"]

    def categorize_path(self, path):
        '''
        Given a path, decide whether it's "normal"/"test"/"generated".  These
        are the 3 top-level groups by which results are categorized.

        These hard-coded heuristics will continue to exist for this "search"
        endpoint but are being obsoleted in favor of the definitions processed
        by `repo_data_ingestion.rs`.
        '''
        def is_test(p):
            # Except a few exceptions, all other paths contain the substring
            # 'test', so we can exit early in case it is not present.
            if '/unit/' in p or '/androidTest/' in p or p.startswith('LayoutTests/'):
                return True
            if 'test' not in p:
                return False
            return ('/test/' in p or '/tests/' in p or '/mochitest/' in p or 'testing/' in p or
                    '/jsapi-tests/' in p or '/reftests/' in p or '/reftest/' in p or
                    '/crashtests/' in p or '/crashtest/' in p or
                    '/googletest/' in p or '/gtest/' in p or '/gtests/' in p or
                    '/imptests/' in p)

        if '__GENERATED__' in path:
            return 'generated'
        elif path.startswith('third_party/'):
            return "thirdparty"
        elif is_test(path):
            return 'test'
        else:
            return 'normal'

    def compile_result(self, kind, qual, pathr, line_modifier):
        '''
        Given path-binned results of a specific analysis `kind` for a
        pretty symbol (`qual`), categorize the path into generated/test/normal
        and nest the results under a [pathkind, qkind, path] nested key
        hierarchy where the values are an array of crossref.rs `SearchResult`
        json results plus the line_modifier fixup hack.

        Path filtering requested via `set_path_filter` is performed at this
        stage.

        line_modifier is a (closed-over) fixup function that was passed in to
        add_qualified_results that's provided the given `line`.  It's only ever
        used by identifier_search in order to fixup "bounds" to compensate for
        prefix searches.
        '''
        if qual:
            qkind = '%s (%s)' % (kind, qual)
        else:
            qkind = kind

        path = pathr['path']
        lines = pathr['lines']

        pathkind = self.categorize_path(path)

        if self.pathre and not self.pathre.search(path):
            return

        # compiled is a map {pathkind: {qkind: {path: [(lines, line_modifier)]}}}
        kind_results = self.compiled.setdefault(pathkind, collections.OrderedDict()).setdefault(qkind, {})
        path_results = kind_results.setdefault(path, ([], line_modifier))
        path_results[0].extend(lines)

    def sort_compiled(self):
        '''
        Traverse the `compiled` state in `path_precedences` order, and then
        its "qkind" children in their inherent order (which is derived from
        the use of `key_precedences` by `get()`), transforming and propagating
        the results, applying a `max_count` result limit.

        Additional transformations that are performed:
        - result de-duplication is performed so that a given (path, line) tuple
          can only be emitted once.  Because of the intentional order of
          `key_precedences` this means that semantic matches should preclude
          their results from being duplicated in the more naive text search
          results.
        - line_modifier's bounds fixups as mentioned in `compile_result` are
          applied which helps the bolding logic in the display logic on the
          (web) client.
        '''
        count = 0

        line_hash = {}

        result = collections.OrderedDict()
        for pathkind in self.path_precedences:
            for qkind in self.compiled.get(pathkind, []):
                paths = list(self.compiled[pathkind][qkind].keys())
                paths.sort()
                for path in paths:
                    # see `compile_result` docs for line_modifier above.
                    (lines, line_modifier) = self.compiled[pathkind][qkind][path]
                    lines.sort(key=lambda l: l['lno'])
                    lines_out = []
                    for line in lines:
                        lno = line['lno']
                        key = (path, lno)
                        if key in line_hash:
                            continue
                        line_hash[key] = True
                        if line_modifier:
                            line_modifier(line)
                        lines_out.append(line)
                        count += 1
                        if count == self.max_count:
                            self.count_limit_hit = True
                            break
                    else:
                        # if there were no lines, as is the case for files, still
                        # count this as 1 result.
                        count += 1
                        if count == self.max_count:
                            self.count_limit_hit = True
                            # we're not in a loop here so no need to break here.

                    if lines_out or qkind == 'Files':
                        l = result.setdefault(pathkind, collections.OrderedDict()).setdefault(qkind, [])
                        l.append({'path': path, 'lines': lines_out})
                    if count == self.max_count:
                        break
                if count == self.max_count:
                    break
            if count == self.max_count:
                break

        return result

    def get(self, work_limit):
        '''
        Work-limiting/result-bounding logic to process the returned results,
        capping them based on some heuristics.  Limiting is performed for each
        "key" type (AKA analysis kind), with the harder result limit occurring
        in `sort_compiled` where a hard result limit `max_count` is enforced.

        See `compile_result` and `sort_compiled` for more info.
        '''
        # compile_result will categorize each path that it sees.
        # It will build a list of paths indexed by pathkind, qkind.
        # Later I'll iterate over this, remove dupes, sort, and keep the top ones.

        self.qualified_results.sort(key=lambda x: x[0])
        for kind in self.key_precedences:
            work = 0
            for (qual, results, line_modifier) in self.qualified_results:
                if work > self.max_work and work_limit:
                    self.work_limit_hit = True
                    log('WORK LIMIT HIT')
                    break
                for pathr in results.get(kind, []):
                    self.compile_result(kind, qual, pathr, line_modifier)
                    work += 1

            for results in self.results:
                for pathr in results.get(kind, []):
                    self.compile_result(kind, None, pathr, None)
                    work += 1

        r = self.sort_compiled()
        return r

# What's the maximum number of files to return to the user?
FILE_RESPONSE_LIMIT = 1000 * EXTREME_FACTOR
# Since we do end up categorizing the files, let's actually get a lot more
# results from `search_files` and apply that filtering later.  Honestly, I'm
# not sure we even need to apply this limit, but there are currently 347k m-c
# files, so baby steps are probably appropriate.
FILE_PRE_FILTER_RESPONSE_LIMIT = FILE_RESPONSE_LIMIT * 8

def search_files(tree_name, path):
    t = time.time()
    # Note that while `all-files` currently exists, it's also intentionally
    # shuffled and so it's probably more sane to just use both of these files
    # for now.
    pathFile = os.path.join(index_path(tree_name), 'repo-files')
    objdirFile = os.path.join(index_path(tree_name), 'objdir-files')
    try:
        # We set the locale to make grep much faster.
        results = subprocess.check_output(['grep', '-Eih', path, pathFile, objdirFile], env={'LC_CTYPE': 'C'}, universal_newlines=True)
    except subprocess.CalledProcessError:
        return ([], False)
    results = results.strip().split('\n')
    results = [ {'path': f, 'lines': []} for f in results ]
    log('  search_files "%s" - %f', path, time.time() - t)
    limit_hit = len(results) > FILE_RESPONSE_LIMIT
    return (results[:FILE_PRE_FILTER_RESPONSE_LIMIT], limit_hit)

def demangle(sym):
    try:
        return subprocess.check_output(['c++filt', '--no-params', sym], universal_newlines=True).strip()
    except subprocess.CalledProcessError:
        return sym

def identifier_search(search, tree_name, needle, complete, fold_case):
    t = time.time()
    needle = re.sub(r'\\(.)', r'\1', needle)

    pieces = re.split(r'\.|::', needle)
    # If the last segment of the search needle is too short, return no results
    # because we're worried that would return too many results.
    if not complete and len(pieces[-1]) < 3:
        return {}

    # Fixup closure for use by add_qualified_results to reduce the range of the
    # match's bounds to the prefix that was included in the search needle from
    # the full bounds of the search result.  (So if the search was "foo::bar"
    # and we matched "foo::bartab" and "foo::barhat", the idea I guess is that
    # only the "bar" portion would be highlighted assuming the bounds
    # previously were referencing "bartab" and "barhat".)
    def line_modifier(line):
        if 'bounds' in line:
            (start, end) = line['bounds']
            end = start + len(pieces[-1])
            line['bounds'] = [start, end]

    ids = identifiers.lookup(tree_name, needle, complete, fold_case)
    for (i, (qualified, sym)) in enumerate(ids):
        if i > 500:
            break

        q = demangle(sym)
        if q == sym:
            q = qualified

        results = expand_keys(tree_name, crossrefs.lookup_merging(tree_name, sym))
        search.add_qualified_results(q, results, line_modifier)
    log('  identifier_search "%s" - %f', needle, time.time() - t)

def get_json_search_results(tree_name, query):
    try:
        search_string = query['q'][0]
    except:
        search_string = ''

    try:
        fold_case = query['case'][0] != 'true'
    except:
        fold_case = True

    try:
        regexp = query['regexp'][0] == 'true'
    except:
        regexp = False

    try:
        path_filter = query['path'][0]
    except:
        path_filter = ''

    parsed = parse_search(search_string)

    # Should we just be leaving this in parsed?
    context_lines = 0
    if 'context_lines' in parsed:
        context_lines = parsed['context_lines']

    if path_filter:
        parsed['pathre'] = parse_path_filter(path_filter)

    if regexp:
        if 'default' in parsed:
            del parsed['default']
        if 're' in parsed:
            del parsed['re']
        parsed['re'] = search_string

    if 'default' in parsed and len(parsed['default']) == 0:
        del parsed['default']

    if is_trivial_search(parsed):
        results = {}
        return json.dumps(results)

    title = search_string
    if not title:
        title = 'Files ' + path_filter

    search = SearchResults()

    work_limit = False
    hit_timeout = False
    hit_limits = []

    if 'symbol' in parsed:
        search.set_path_filter(parsed.get('pathre'))
        symbols = parsed['symbol']
        title = 'Symbol ' + symbols
        search.add_results(expand_keys(tree_name, crossrefs.lookup_merging(tree_name, symbols)))
    elif 're' in parsed:
        path = parsed.get('pathre', '.*')
        (substr_results, timed_out, codesearch_limit_hit) = codesearch.search(parsed['re'], fold_case, path, tree_name, context_lines)
        search.add_results({'Textual Occurrences': substr_results})
        hit_timeout |= timed_out
        if codesearch_limit_hit:
            hit_limits.append('fulltext search hit limit')
    elif 'id' in parsed:
        search.set_path_filter(parsed.get('pathre'))
        identifier_search(search, tree_name, parsed['id'], complete=True, fold_case=fold_case)
    elif 'default' in parsed:
        work_limit = True
        path = parsed.get('pathre', '.*')
        (substr_results, timed_out, codesearch_limit_hit) = codesearch.search(parsed['default'], fold_case, path, tree_name, context_lines)
        search.add_results({'Textual Occurrences': substr_results})
        hit_timeout |= timed_out
        if codesearch_limit_hit:
            hit_limits.append('fulltext search hit limit')
        if 'pathre' not in parsed:
            (file_results, file_limit_hit) = search_files(tree_name, parsed['default'])
            if file_limit_hit:
                hit_limits.append('file pre-filter limit')
            search.add_results({'Files': file_results})

            identifier_search(search, tree_name, parsed['default'], complete=False, fold_case=fold_case)
    elif 'pathre' in parsed:
        path = parsed['pathre']
        (file_results, file_limit_hit) = search_files(tree_name, path)
        if file_limit_hit:
            hit_limits.append("file")
        search.add_results({'Files': file_results})
    else:
        assert False
        results = {}

    search_get_t = time.time()
    results = search.get(work_limit)
    log('  search.get() - %f', time.time() - search_get_t)

    if search.count_limit_hit:
        hit_limits.append('result count limit')
    if search.work_limit_hit:
        hit_limits.append('work limit')

    results['*title*'] = title
    results['*timedout*'] = hit_timeout
    results['*limits*'] = hit_limits
    return json.dumps(results)

def identifier_sorch(search, tree_name, needle, complete, fold_case):
    needle = re.sub(r'\\(.)', r'\1', needle)

    pieces = re.split(r'\.|::', needle)
    # If the last segment of the search needle is too short, return no results
    # because we're worried that would return too many results.
    if not complete and len(pieces[-1]) < 3:
        return

    # Fixup closure for use by add_qualified_results to reduce the range of the
    # match's bounds to the prefix that was included in the search needle from
    # the full bounds of the search result.  (So if the search was "foo::bar"
    # and we matched "foo::bartab" and "foo::barhat", the idea I guess is that
    # only the "bar" portion would be highlighted assuming the
    # previously were referencing "bartab" and "barhat".)
    def line_modifier(line):
        if 'bounds' in line:
            (start, end) = line['bounds']
            end = start + len(pieces[-1])
            line['bounds'] = [start, end]

    ids = identifiers.lookup(tree_name, needle, complete, fold_case)
    for (i, (qualified, sym)) in enumerate(ids):
        if i > 500:
            break

        q = demangle(sym)
        if q == sym:
            q = qualified

        sym_data = crossrefs.lookup_single_symbol(tree_name, sym)
        if sym_data:
            # XXX we could pass line_modifier here and have it be used; the
            # logic probably still holds.  OTOH, having the full symbol that
            # matched by prefix doesn't seem like the end of the world.
            search.add_symbol(sym, sym_data)

def get_json_sorch_results(tree_name, query):
    '''
    New RawSearchResults variant.  Initially supports 'symbol:', 'id:' and
    default queries that only perform identifier searches and filename searches
    (no fulltext).
    '''
    try:
        search_string = query['q'][0]
    except:
        search_string = ''

    try:
        fold_case = query['case'][0] != 'true'
    except:
        fold_case = True

    try:
        regexp = query['regexp'][0] == 'true'
    except:
        regexp = False

    try:
        path_filter = query['path'][0]
    except:
        path_filter = ''

    parsed = parse_search(search_string)

    if path_filter:
        parsed['pathre'] = parse_path_filter(path_filter)

    if regexp:
        if 'default' in parsed:
            del parsed['default']
        if 're' in parsed:
            del parsed['re']
        parsed['re'] = search_string

    if 'default' in parsed and len(parsed['default']) == 0:
        del parsed['default']

    if is_trivial_search(parsed):
        results = {}
        return json.dumps(results)

    title = search_string
    if not title:
        title = 'Files ' + path_filter

    search = RawSearchResults()

    work_limit = False
    hit_timeout = False
    hit_limits = []

    if 'symbol' in parsed:
        search.set_path_filter(parsed.get('pathre'))
        symbols = parsed['symbol']
        title = 'Symbol ' + symbols
        for symbol in symbols.split(','):
            sym_data = crossrefs.lookup_single_symbol(tree_name, symbol)
            if sym_data:
                search.add_symbol(symbol, sym_data)
    elif 'id' in parsed:
        search.set_path_filter(parsed.get('pathre'))
        identifier_sorch(search, tree_name, parsed['id'], complete=True, fold_case=fold_case)
    elif 'default' in parsed:
        work_limit = True
        path = parsed.get('pathre', '.*')
        #(substr_results, timed_out) = codesearch.search(parsed['default'], fold_case, path, tree_name)
        #search.add_results({'Textual Occurrences': substr_results})
        #hit_timeout |= timed_out
        if 'pathre' not in parsed:
            (file_results, file_limit_hit) = search_files(tree_name, parsed['default'])
            if file_limit_hit:
                hit_limits.append('file pre-filter limit')

            search.add_paths(file_results)

            identifier_sorch(search, tree_name, parsed['default'], complete=False, fold_case=fold_case)
    else:
        assert False
        results = {}

    results = search.get()

    results['*title*'] = title
    results['*timedout*'] = hit_timeout
    results['*limits*'] = hit_limits
    return json.dumps(results)

class Handler(http.server.SimpleHTTPRequestHandler):
    def do_GET(self):
        pid = os.fork()
        if pid:
            # Parent process
            log('request(handled by %d) %s', pid, self.path)

            timedOut = [False]
            def handler(signum, frame):
                log('timeout %d, killing', pid)
                timedOut[0] = True
                os.kill(pid, signal.SIGKILL)
            signal.signal(signal.SIGALRM, handler)
            signal.alarm(120)

            t = time.time()
            while True:
                try:
                    (pid2, status) = os.waitpid(pid, 0)
                    break
                except OSError as e:
                    if e.errno != errno.EINTR: raise e

            failed = timedOut[0]
            if os.WIFEXITED(status) and os.WEXITSTATUS(status) != 0:
                log('error pid %d - %f', pid, time.time() - t)
                failed = True
            else:
                log('finish pid %d - %f', pid, time.time() - t)

            if failed:
                self.send_response(504)
                self.end_headers()
        else:
            # Child process
            try:
                self.process_request()
                os._exit(0)
            except:
                e = traceback.format_exc()
                log('exception\n%s', e)
                os._exit(1)

    def log_request(self, *args):
        pass

    def _wrap_sorch_results(self, tree_name, query):
        '''
        Commonalities around sorch results and URI wrappers like "symbol" that
        are just a specialized sorch.
        '''
        j = get_json_sorch_results(tree_name, query)
        if 'json' in self.headers.get('Accept', ''):
                self.generateJson(j)
        else:
            j = j.replace("</", "<\\/").replace("<script", "<\\script").replace("<!", "<\\!")
            template = os.path.join(index_path(tree_name), 'templates/sorch.html')
            self.generateWithTemplate({'{{BODY}}': j, '{{TITLE}}': 'Search'}, template)

    def process_request(self):
        url = urllib.parse.urlparse(self.path)
        path_elts = url.path.split('/')

        # Strip any extra slashes.
        path_elts = [ elt for elt in path_elts if elt != '' ]

        if len(path_elts) >= 2 and path_elts[1] == 'search':
            tree_name = path_elts[0]
            query = urllib.parse.parse_qs(url.query)
            j = get_json_search_results(tree_name, query)
            if 'json' in self.headers.get('Accept', ''):
                self.generateJson(j)
            else:
                j = j.replace("</", "<\\/").replace("<script", "<\\script").replace("<!", "<\\!")
                template = os.path.join(index_path(tree_name), 'templates/search.html')
                self.generateWithTemplate({'{{BODY}}': j, '{{TITLE}}': 'Search'}, template)
        elif len(path_elts) >= 2 and path_elts[1] == 'sorch':
            tree_name = path_elts[0]
            query = urllib.parse.parse_qs(url.query)
            self._wrap_sorch_results(tree_name, query)
        # "symbol" is a variant on "define", but whereas "define" creates a
        # redirect, "symbol" is equivalent to source with "q=symbol:ORIGINAL_Q"
        elif len(path_elts) >= 2 and path_elts[1] == 'symbol':
            tree_name = path_elts[0]
            orig_query = urllib.parse.parse_qs(url.query)
            symbol = orig_query['q'][0]
            new_query = { 'q': [ 'symbol:' + symbol ]}
            self._wrap_sorch_results(tree_name, new_query)
        elif len(path_elts) >= 2 and path_elts[1] == 'define':
            tree_name = path_elts[0]
            query = urllib.parse.parse_qs(url.query)
            symbol = query['q'][0]
            results = expand_keys(tree_name, crossrefs.lookup_merging(tree_name, symbol), False)
            definition = results['Definitions'][0]
            filename = definition['path']
            lineno = definition['lines'][0]['lno']
            url = '/' + tree_name + '/source/' + filename + '#' + str(lineno)

            self.send_response(301)
            self.send_header("Location", url)
            self.end_headers()
        else:
            return http.server.SimpleHTTPRequestHandler.do_GET(self)

    def generateJson(self, data):
        databytes = data.encode('utf-8')

        self.send_response(200)
        self.send_header("Vary", "Accept")
        self.send_header("Content-type", "application/json;charset=utf-8")
        self.send_header("Content-Length", str(len(databytes)))
        self.end_headers()

        self.wfile.write(databytes)

    def generateWithTemplate(self, replacements, templateFile):
        output = open(templateFile).read()
        for (k, v) in replacements.items():
            output = output.replace(k, v)

        databytes = output.encode('utf-8')

        self.send_response(200)
        self.send_header("Vary", "Accept")
        self.send_header("Content-type", "text/html;charset=utf-8")
        self.send_header("Content-Length", str(len(databytes)))
        self.end_headers()

        self.wfile.write(databytes)

config_fname = sys.argv[1]
status_fname = sys.argv[2]

config = json.load(open(config_fname))

os.chdir(config['mozsearch_path'])

crossrefs.load(config)
codesearch.load(config)
identifiers.load(config)

# We *append* to the status file because other server components
# also write to this file when they are done starting up, and we
# don't want to clobber those messages.
with open(status_fname, "a") as status_out:
    status_out.write("router.py loaded\n")

class ForkingServer(ForkingMixIn, http.server.HTTPServer):
    pass

server_address = ('', 8000)
httpd = ForkingServer(server_address, Handler)
httpd.serve_forever()

```

## router/codesearch.py
```
#!/usr/bin/env python3

from __future__ import absolute_import
import json
import sys
import socket
import os
import os.path
import time
from logger import log

# A workaround until https://github.com/grpc/grpc/pull/37666 gets merged.
import warnings
warnings.filterwarnings("ignore", "Protobuf gencode version 5.27.2 is older than the runtime version 5.28.2", UserWarning)

import grpc
from src.proto import livegrep_pb2
from src.proto import livegrep_pb2_grpc

def collateMatches(matches):
    paths = {}
    for m in matches:
        # For results in the "mozilla-subrepo" repo, which is the mozilla/
        # subfolder of comm-central, we need to adjust the path to reflect
        # the fact that it's in the subfolder.
        path = m.path
        if m.tree == 'mozilla-subrepo':
            path = 'mozilla/' + path

        line = {
            'lno': m.line_number,
            'bounds': [m.bounds.left, m.bounds.right],
            'line': m.line
        }

        if len(m.context_before):
            # The before context is provided in reverse order which is not what
            # we want.
            before = list(m.context_before)
            # This does not return the list, so it's on its own line.
            before.reverse()
            line['context_before'] = before
        if len(m.context_after):
            line['context_after'] = list(m.context_after)

        paths.setdefault(path, []).append(line)
    results = [ {'path': p, 'icon': '', 'lines': paths[p]} for p in paths ]
    return results

def do_search(host, port, pattern, fold_case, file, context_lines):
    t = time.time()
    use_file = []
    # file is now a repeatd arg; if we pass an empty array, no constraints are
    # applied.  For efficiency, we convert a useless wildcard of '.*' to just be
    # no constraint.
    if file and file != '.*':
        use_file.append(file)
    query = livegrep_pb2.Query(line = pattern, file = use_file, fold_case = fold_case,
                               context_lines = context_lines)
    log('QUERY %s', repr(query).replace('\n', ', '))

    channel = grpc.insecure_channel('{0}:{1}'.format(host, port))
    grpc_stub = livegrep_pb2_grpc.CodeSearchStub(channel)
    result = grpc_stub.Search(query) # maybe add a timeout arg here?
    channel.close()

    matches = collateMatches(result.results)
    log('  codesearch result with %d line matches across %d paths - %f : %s',
        len(result.results), len(matches), time.time() - t,
        repr(result.stats).replace('\n', ', '))
    return (matches, livegrep_pb2.SearchStats.ExitReason.Name(result.stats.exit_reason) == 'TIMEOUT',
            livegrep_pb2.SearchStats.ExitReason.Name(result.stats.exit_reason) == 'MATCH_LIMIT')

def daemonize(args):
    # Spawn a process to start the daemon
    pid = os.fork()
    if pid:
        # Parent
        return

    # Double fork
    pid = os.fork()
    if pid:
        os._exit(0)

    pid = os.fork()
    if pid:
        os._exit(0)

    si = open('/dev/null', 'r')
    so = open('/dev/null', 'a+')
    se = open('/dev/null', 'a+')
    os.dup2(si.fileno(), sys.stdin.fileno())
    os.dup2(so.fileno(), sys.stdout.fileno())
    os.dup2(se.fileno(), sys.stderr.fileno())

    os.execvp(args[0], args)

def stop_codesearch(data):
    log('Stopping codesearch on port %d', data['codesearch_port'])
    os.system("pkill -f '^codesearch.+localhost:%d '" % (data['codesearch_port']))

def startup_codesearch(data):
    log('Starting codesearch on port %d', data['codesearch_port'])

    use_threads = 4
    try:
        # Defined to return None if "undetermined", so we handle that but also
        # are prepared for things to throw.
        maybe_count = os.cpu_count()
        # Limit us to 8 cores primarily to avoid the Vagrant VM getting too
        # resource hungry.  We may tend to want to give it as many cores as
        # possible for rust compilation, but our current EC2 core max is 8.
        if maybe_count is not None:
            use_threads = min(8, maybe_count)
    except:
        pass

    args = ['codesearch', '-grpc', 'localhost:' + str(data['codesearch_port']),
            '--noreuseport',
            '-load_index', data['codesearch_path'],
            # Note that because multiple threads are involved, this limit
            # potentially will not return the same results every time it is run
            # and that's okay.  But because of our app-level caching, it ends
            # up that we will usually only run one exact query once.
            '-max_matches', '4000',
            '-threads', f'{use_threads}',
            # We set the timeout to 30 seconds up from 10 seconds because our
            # caching policy requires our searches to be deterministic in the
            # face of I/O slowness.  Note that this differs from the
            # non-determinism of the "max_matches" limit which is acceptable.
            # We do expect to have addressed this problem by ensuring the cache
            # is fully loaded via vmtouch before serving begins in earnest.
            '-timeout', '30000',
            '-context_lines', '0']
    # Dump our arguments to the log so someone investigating things can just
    # kill the server and then copy and paste the arguments to run it
    # non-daemonized.  (Unfortunately, we don't have a way to get at its output
    # otherwise because of how we daemonize it.)
    log(' '.join(args))

    daemonize(args)
    # Sleep a teeny bit to let the server have some exclusive time to spin up
    # before any siblings start to race it.
    time.sleep(0.1)

def try_info_request(host, port):
    infoq = livegrep_pb2.InfoRequest()

    channel = grpc.insecure_channel('{0}:{1}'.format(host, port))
    grpc_stub = livegrep_pb2_grpc.CodeSearchStub(channel)
    result = grpc_stub.Info(infoq) # maybe add a timeout arg here?
    channel.close()

def wait_for_codesearch(data, max_tries=200):
    '''Wait for the codesearch server to become available/responsive.'''

    tries = 0
    while tries < max_tries:
        tries += 1
        try:
            try_info_request('localhost', data['codesearch_port'])
            break
        except Exception as e:
            # sleep a little to give the server time to make progress
            time.sleep(0.1)
    log('Server on port %d found alive after %d tries', data['codesearch_port'], tries)

def search(pattern, fold_case, path, tree_name, context_lines):
    data = tree_data[tree_name]

    try:
        return do_search('localhost', data['codesearch_port'], pattern, fold_case, path, context_lines)
    except Exception as e:
        log('Got exception: %s', repr(e))
        if e.code() != grpc.StatusCode.UNAVAILABLE:
            # TODO: better job of surfacing the error back to the user. This might be e.g.
            # a grpc.StatusCode.INVALID_ARGUMENT if say the `pattern` is a malformed regex
            return ([], False, False)

        # If the exception indicated a connection failure, try to restart the server and search
        # again.
        stop_codesearch(data)
        startup_codesearch(data)
        try:
            return do_search('localhost', data['codesearch_port'], pattern, fold_case, path, context_lines)
        except Exception as e:
            log('Got exception after restarting codesearch: %s', repr(e))
            # TODO: as above, do a better job of surfacing the error back to the user.
            return ([], False, False)


def load(config, stop=True, start=True, only_tree_name=None):
    global tree_data
    tree_data = {}
    for tree_name in config['trees']:
        if only_tree_name and tree_name != only_tree_name:
            continue
        tree_data[tree_name] = {
            'codesearch_path': config['trees'][tree_name]['codesearch_path'],
            'codesearch_port': config['trees'][tree_name]['codesearch_port'],
        }
        # Start the daemon during loading. If it dies we will restart it lazily
        # during the search function, but that should be rare. This avoids a
        # race condition where search() can get invoked multiple times in quick
        # succession by separate queries, resulting in the daemon getting started
        # multiple times.
        if stop:
            stop_codesearch(tree_data[tree_name])
        if start:
            startup_codesearch(tree_data[tree_name])
            wait_for_codesearch(tree_data[tree_name])

if __name__ == '__main__':
    '''(Re)start or stop all the codesearch instances for the given config file.

    Usage:
    codesearch.py CONFIG.JSON start [only_tree_name]
    codesearch.py CONFIG.JSON stop [only_tree_name]
    '''
    stop = True
    start = True
    only_tree_name = None
    if sys.argv[2] == 'stop':
        start = False
    if len(sys.argv) > 3:
        only_tree_name = sys.argv[3]

    config = json.load(open(sys.argv[1]))
    load(config, stop=stop, start=start, only_tree_name=only_tree_name)

```

## router/logger.py
```
from __future__ import absolute_import
from __future__ import print_function
import multiprocessing
import sys
import datetime
import os

lock = multiprocessing.Lock()

def log(msg, *args):
    now = datetime.datetime.now()
    pid = os.getpid()
    lock.acquire()
    print('%s/pid=%d - %s' % (str(now), pid, msg % args))
    sys.stdout.flush()
    lock.release()

```

## router/crossrefs.py
```
# Functions for working with the `crossref` and `crossref-extra` cross-reference
# files documented in `crossref.md`.

from __future__ import absolute_import
import json
import sys
import mmap
import os.path
from logger import log

repo_data = {}

def load(config):
    global repo_data

    for repo_name in config['trees']:
        log('Loading %s', repo_name)
        index_path = config['trees'][repo_name]['index_path']

        inline_mm = None
        with open(os.path.join(index_path, 'crossref')) as f:
            try:
                inline_mm = mmap.mmap(f.fileno(), 0, prot=mmap.PROT_READ)
            except ValueError as e:
                log('Failed to mmap crossref file for %s: %s', repo_name, str(e))
                pass

        extra_mm = None
        with open(os.path.join(index_path, 'crossref-extra')) as f:
            try:
                extra_mm = mmap.mmap(f.fileno(), 0, prot=mmap.PROT_READ)
            except ValueError as e:
                log('Failed to mmap crossref file for %s: %s', repo_name, str(e))
                pass

        repo_data[repo_name] = (inline_mm, extra_mm)

NEWLINE_ORD = ord('\n')
ID_START_ORD = ord('!')
INLINE_STORED_STR = ':'
EXTERNALLY_STORED_STR = '@'

def get_id_line(mm, pos):
    '''
    Given a memory map and a position, expand from `pos` to find the identifier
    line (`!` prefixed) that covers the position.  Returns (the identifier,
    the offset of the `!` from the start of the identifier line, the offset of
    the newline ending the identifier line).

    `pos` is either inside an identifier line or a payload line that follows an
    identifier line, so we always walk backwards until we find an identifier.
    We should never need to walk forward (to find the start of the identifier
    line) because the result of any comparison should always tell the bisection
    to bisect in the positive direction (because the file is sorted), which
    should then find the subsequent record (if that's the one we're looking
    for, etc.).
    '''
    # We could the trailing newline as part of the record, so back up a char.
    if mm[pos] == NEWLINE_ORD:
        pos -= 1

    start = end = pos

    # Scan backwards until we hit the start of the file (where the first line
    # must be an identifier) or we hit a newline and the character following
    # the newline is the identifier prefix of `!`.
    while start > 0:
        if mm[start - 1] == NEWLINE_ORD:
            if mm[start] == ID_START_ORD:
                break
            else:
                # We're hitting a ":" and we need to reset end to this newline
                end = start - 1
                # and we want to keep going...
        start -= 1

    # Start should now be pointing at the `!` of the identifier line.

    size = mm.size()
    while end < size and mm[end] != NEWLINE_ORD:
        end += 1

    # end should now be pointing at the trailing newline.

    # Skip the leading `!` and decode the utf-8 encoded symbol
    line_sym = mm[start+1:end].decode('utf-8')
    return (line_sym, start, end)

def bisect_for_payload(mm, search_sym):
    '''
    Bisect the mmap to look for an exact symbol match `sym`, and returning the
    payload line which may be either inline JSON or external offsets to be
    retrieved from another map.
    '''

    first = 0
    count = mm.size()
    while count > 0:
        step = int(count / 2)
        pos = first + step

        (line_sym, line_start, line_end) = get_id_line(mm, pos)

        if line_sym == search_sym:
            ## Exact Match!
            mm.seek(line_end + 1)
            payload_line = mm.readline().decode('utf-8')
            return payload_line
        elif line_sym < search_sym:
            ## Bisect latter half
            # We might as well exclude the payload line we're skipping as well.
            # Because payload lines are intentionally limited during the
            # creation of `crossref`, we know this should fault an acceptable
            # number of pages which may have already been pre-fetched.
            next_newline = mm.find(b'\n', line_end + 1)
            if next_newline != -1:
                first = next_newline + 1
            else:
                # If there was no newline, then we're at the end and we might
                # as well stop.
                return None
            # Halve count and also subtract off the parts of the identifier line
            # and payload line we're skipping.  `first` is now effectively
            # `original_first + step + value_length` whereas `pos` is still
            # `original_first + step`.  So `first - pos` = `value_length`
            count -= step + (first - pos)
        else:
            ## Bisect first half
            # Halve count and subtract off the part of the identifier line that
            # we can eliminate from consideration.
            count = step - (pos - line_start)

    return None

def lookup_raw(tree_name, sym):
    '''
    Look up the given symbol from `crossref` and parse and return the resulting
    JSON as objects.
    '''
    (inline_mm, extra_mm) = repo_data[tree_name]

    if not inline_mm:
        return None

    payload = bisect_for_payload(inline_mm, sym)
    if not payload:
        return None

    if payload[0] == INLINE_STORED_STR:
        return json.loads(payload[1:])
    elif payload[0] != EXTERNALLY_STORED_STR:
        # Fail if we're seeing something other than an external ref.
        return None

    (braceOffset, lengthWithNewline) = payload[1:].split(' ')
    (braceOffset, lengthWithNewline) = (int(braceOffset, 16), int(lengthWithNewline, 16))

    # exclude the newline
    data = extra_mm[braceOffset:(braceOffset + lengthWithNewline - 1)]

    result = json.loads(data)
    return result

def lookup_merging(tree_name, symbols):
    '''
    Split `symbols` on commas, and lookup all of the requested symbols, merging
    their results.
    '''
    symbols = symbols.split(',')

    results = {}
    for symbol in symbols:
        result = lookup_raw(tree_name, symbol)
        if result is None:
            # we used to bail here, but it turns out this is actually a real
            # correctness problem when searching for XPIDL JS methods that don't
            # exist.
            continue

        for (k, v) in result.items():
            if k == 'callees':
                continue
            # expand_keys now expects aggregated meta, so wrap the meta obj.
            if k == 'meta':
                v = [v]
            results[k] = results.get(k, []) + v

    return results

def lookup_single_symbol(tree_name, symbol):
    '''
    Look up a single symbol, returning its results dict if it existed or None
    if it didn't exist.
    '''
    return lookup_raw(tree_name, symbol)

```

## router/raw_search.py
```
import re

class RawSearchResults(object):
    '''
    Alternate version of SearchResults that attempts to leave information in its
    underlying structured representation rather that performing consolidation
    for presentation purposes.

    The primary complication is that all of our data related to a symbol is
    actually file-centric and we want to shard these different pieces of a
    fragment by normal/test/generated.
    '''

    def __init__(self):
        self.paths = [];
        self.symbols = {}

        self.pathre = None

    # directly extracted from SearchResults
    def categorize_path(self, path):
        '''
        Given a path, decide whether it's "normal"/"test"/"generated".  These
        are the 3 top-level groups by which results are categorized.

        These are hardcoded heuristics that probably could be better defined
        in the `config.json` metadata, with a means for trees like gecko to be
        able to leverage in-tree build meta-information like moz.build and the
        various mochitest.ini files, etc.
        '''
        def is_test(p):
            # Except /unit/, all other paths contain the substring 'test', so we can exit early
            # in case it is not present.
            if '/unit/' in p:
                return True
            if 'test' not in p:
                return False
            return ('/test/' in p or '/tests/' in p or '/mochitest/' in p or 'testing/' in p or
                    '/jsapi-tests/' in p or '/reftests/' in p or '/reftest/' in p or
                    '/crashtests/' in p or '/crashtest/' in p or
                    '/googletest/' in p or '/gtest/' in p or '/gtests/' in p or
                    '/imptests/' in p)

        if '__GENERATED__' in path:
            return 'generated'
        elif is_test(path):
            return 'test'
        else:
            return 'normal'

    # directly extracted from SearchResults
    def set_path_filter(self, path):
        if not path or path == '.*':
            self.pathre = None
            return

        try:
            self.pathre = re.compile(path, re.IGNORECASE)
        except re.error:
            # In case the pattern is not a valid RE, treat it as literal string.
            self.pathre = re.compile(re.escape(path), re.IGNORECASE)

    def add_paths(self, paths):
        self.paths.extend(paths)

    def add_symbol(self, raw_sym, data):
        '''
        Given a symbol in crossrefs representation, process it into our output
        representation.  This mainly means splitting `data` into
        normal/test/generated groups.
        '''
        if raw_sym in self.symbols:
            # XXX the symbol should only be added once, so ignore if it's
            # already in there.
            return

        sym_info = {}
        sym_info['symbol'] = raw_sym

        hits_by_pathkind = sym_info['hits'] = {}
        # the goal here is to go from the flattened tuple of [kind, path, lines]
        # to [pathkind, kind, path, lines].  We are able to reuse the
        # (path, lines) pair at the end, so it's really just the structure of
        # [pathkind, kind] that we need to create.
        for kind, path_line_pairs in data.items():
            if kind == 'meta':
                sym_info['meta'] = path_line_pairs
                continue
            if kind == 'callees':
                sym_info['callees'] = path_line_pairs
                continue
            if kind == 'field-member-uses':
                sym_info['field-member-uses'] = path_line_pairs
                continue

            for path_lines in path_line_pairs:
                path = path_lines['path']
                lines = path_lines['lines']

                # skip this path if it's filtered out.
                if self.pathre and not self.pathre.search(path):
                    continue

                pathkind = self.categorize_path(path)
                hits_by_kind = hits_by_pathkind.get(pathkind, None)
                if hits_by_kind is None:
                    hits_by_kind = hits_by_pathkind[pathkind] = {}

                kind_path_lines_list = hits_by_kind.get(kind, None)
                if kind_path_lines_list is None:
                    kind_path_lines_list = hits_by_kind[kind] = []
                # we can reuse the path_line pairs.
                kind_path_lines_list.append(path_lines)

        self.symbols[raw_sym] = sym_info

    def get(self):
        '''
        Produce a JSON-able final result.  Our current schema looks like:
        - files: List of String paths.
        - fulltext: Currently null because we don't return those results.
        - semantic: A dictionary of what we're calling SymbolResult things keyed
          by their raw symbol.

        Each SymbolResult is a dictionary that has the following members.  This
        is where additional info and meta-info about symbols will be found in
        the future.
        - symbol: The raw symbol.
        - pretty: The prettified version of the symbol.
        - hits: dict with keys:
          - normal/test/generated: (pathkind) dict with keys:
            - uses/defs/assignments/decls/idl/conumes: (kind): list of
              with keys:
              - path
              - lines:
                - lno
                - line
                - bounds
                - contextsym
                - context
                - peekLines
        '''

        results = {}
        results['files'] = self.paths
        results['semantic'] = self.symbols

        return results

```

## router/identifiers.py
```
from __future__ import absolute_import
from __future__ import print_function
import json
import sys
import mmap
import os.path
from logger import log

repo_data = {}

def load(config):
    global repo_data

    for repo_name in config['trees']:
        log('Loading identifiers for %s', repo_name)
        index_path = config['trees'][repo_name]['index_path']

        mm = None
        with open(os.path.join(index_path, 'identifiers')) as f:
            try:
                mm = mmap.mmap(f.fileno(), 0, prot=mmap.PROT_READ)
            except ValueError as e:
                log('Failed to mmap identifiers file for %s: %s', repo_name, str(e))
                pass

        repo_data[repo_name] = mm

def get_line(mm, pos):
    if mm[pos] == ord('\n'):
        pos -= 1

    start = end = pos

    while start > 0 and mm[start - 1] != ord('\n'):
        start -= 1

    size = mm.size()
    while end < size and mm[end] != ord('\n'):
        end += 1

    # Identifiers files are written from Rust and are in utf-8
    return mm[start:end].decode('utf-8')

def bisect(mm, needle, upper_bound):
    needle = needle.upper()

    first = 0
    count = mm.size()
    while count > 0:
        step = int(count / 2)
        pos = first + step

        line = get_line(mm, pos).upper()
        if line < needle or (upper_bound and line == needle):
            first = pos + 1
            # Effectively: count = count - int(count/2) - 1
            # In other words: count = step; count -= 1;
            # Compensating for moving first a byte forward.
            count -= step + 1
        else:
            count = step

    return first

def lookup(tree_name, needle, complete, fold_case):
    mm = repo_data[tree_name]

    if not mm:
        return []

    first = bisect(mm, needle, False)
    last = bisect(mm, needle + '~', True)

    result = []
    mm.seek(first)
    while mm.tell() < last:
        # Identifiers files are written from Rust and are in utf-8
        line = mm.readline().decode('utf-8').strip()
        pieces = line.split(' ')
        suffix = pieces[0][len(needle):]
        if ':' in suffix or '.' in suffix or (complete and suffix):
            continue
        if not fold_case and not pieces[0].startswith(needle):
            continue
        result.append(pieces[0:2])

    return result

if __name__ == '__main__':
    load(json.load(open(sys.argv[1])))
    print(lookup(sys.argv[2], sys.argv[3]))

```

## run-docker.sh
```
#!/usr/bin/env bash

# This command tries to start up your searchfox docker container, creating it
# from the image created in `build.docker.sh` if it doesn't already exist, and
# ensure you get a distinct bash shell in the container even if you run this
# script in multiple tabs[1].
#
# This command will attempt to generate automatic mounts for any symlinks found
# in the "trees" subdirectory.  This is intended as a means of letting you
# index local trees by symlinking them.
#
# 1: We definer the container as running a canonical bash shell.  When we run
# `docker container start CONTAINER` this spins up the shell, but you don't get
# your terminal bound to the shell until we run `docker attach CONTAINER`.
# However, if we naively run `docker attach CONTAINER` in 2 separate terminals,
# you'll just end up seeing the same single underlying bash instance mirrored to
# both terminals.  This is almost certainly not what you want, so we will
# instead run an additional bash shell in the container.

# ==============================================================================

# If you have multiple searchfox checkouts on your machine and you want to be
# able to have them operate in isolation, you can define the variables below.
# I would suggest adding them to the appropriate env/bin/activate script that
# you use; `activate` is for bash but there are different suffixed files for
# other shell types.
CONTAINER_NAME=${SEARCHFOX_DOCKER_CONTAINER_NAME:-searchfox}
IMAGE_NAME=${SEARCHFOX_DOCKER_IMAGE_NAME:-searchfox}
# Note that the volume is optional!  Also, we suffix it with "-vol" because I
# saw it in the docs and that seems reasonable to avoid having everything be
# named like exactly the same.
VOLUME_NAME=${SEARCHFOX_DOCKER_VOLUME_NAME:-searchfox-vol}

THIS_DIR=$(pwd)
# For consistency we mount the source dir at /vagrant still
INSIDE_CONTAINER_DIR=/vagrant

# connect to this port on your computer
OUTSIDE_CONTAINER_PORT=16995
# which is served at this port inside the container
INSIDE_CONTAINER_PORT=80

SHELL=bash

container_exists() {
    docker container inspect ${CONTAINER_NAME} &> /dev/null
}

volume_exists() {
    docker volume inspect ${VOLUME_NAME} &> /dev/null
}

if container_exists; then
    CONTAINER_STATE=$(docker container inspect ${CONTAINER_NAME} | jq -r '.[0].State.Status')
    # If it's already running, run a new bash command inside the container
    if [[ $CONTAINER_STATE == "running" ]]; then
        docker exec -it ${CONTAINER_NAME} ${SHELL}
    else # start the (already created) container and attach to its canonical shell
        # this will print out our container name if we don't redirect stdout
        docker container start ${CONTAINER_NAME} > /dev/null
        docker attach ${CONTAINER_NAME}
    fi
else
    # build list of additional mounts; we use process substitution because
    # piping would create a subshell; see https://mywiki.wooledge.org/BashFAQ/024
    #
    # Note that the following currently works, but it doesn't work like I
    # intended it to work.  Like, the symlink ends up working because we end up
    # mounting the actual path at its true path in the real world.  I, uh,
    # don't understand what's actually going on, although I'm guessing it
    # doesn't like trying to create a mount-point with a mount-point and is
    # ignoring the target we're specifying and using the source.
    LINKMOUNTS=()
    while read -r link; do
      LINKMOUNTS+=( --mount type=bind,source=$(readlink -f ${link}),target=/vagrant/${link} )
    done < <(find trees -type l)

    # Mount the home directory volume if it exists.  The docker docs say that
    # if there is anything already at that location prior to us passing this
    # directive, it will be copied into the volume.
    VOLMOUNTS=()
    if volume_exists; then
      VOLMOUNTS+=( --mount type=volume,source=${VOLUME_NAME},target=/home/vagrant )
    fi

    # flags:
    # - `-it`: `i` is interactive, `t` is allocate a pseudo-tty
    # - `--name`: controls the name that is used to refer to the container for other
    #   commands.  For example, `docker container stop $NAME` and
    #   `docker container rm $NAME`.
    # - `--mount`: lets us bind the current directory into the container.
    # - `-p`: specifies the port mapping to expose the nginx web-server (when
    #   running; it doesn't automatically run!) on localhost port 16995.
    docker run \
        --platform linux/amd64 \
        -it \
        --name $CONTAINER_NAME \
        --mount type=bind,source=${THIS_DIR},target=${INSIDE_CONTAINER_DIR} \
        "${LINKMOUNTS[@]}" \
        "${VOLMOUNTS[@]}" \
        -p ${OUTSIDE_CONTAINER_PORT}:${INSIDE_CONTAINER_PORT} \
        ${IMAGE_NAME} \
        ${SHELL}
fi

```

## scripts/js-analyze.sh
```
#!/usr/bin/env bash

set -x # Show commands
set -eu # Errors/undefined vars are fatal
set -o pipefail # Check all commands in a pipeline

if [ $# -ne 2 ]
then
    echo "Usage: js-analyze.sh config-file.json tree_name"
    exit 1
fi

CONFIG_FILE=$(realpath $1)
TREE_NAME=$2

# Required by std::wcsrtombs, used in os.file.redirect.
export LC_CTYPE=C.UTF-8

# Add line number for the file list with `nl`, which is used as a global
# fileIndex, and used for local variable symbols.
#
# NOTE: The index conflicts with html-analyze.sh.
#       Currently this is not a problem because the index is used only for
#       local variables with no_crossref symbols.
cat $INDEX_ROOT/js-files | nl -w1 -s " " | \
    parallel --jobs 8 --pipe --halt 2 \
    js -f $MOZSEARCH_PATH/scripts/js-analyze.js -- \
    $MOZSEARCH_PATH $FILES_ROOT $INDEX_ROOT/analysis
echo $?

```

## scripts/compress-outputs.sh
```
#!/usr/bin/env bash

set -x # Show commands (parallel does all the heavy lifting, so not spammy)
set -eu # Errors/undefined vars are fatal
set -o pipefail # Check all commands in a pipeline

# Compress all of our HTML and raw-analysis files which end up being very large
# and only exist to be served via nginx and therefore can be pre-compressed,
# resulting in a net win for everyone.
#
# Limitations in our use of try_files by nginx and the `gzip_static` logic mean
# that:
# - In cases where we're using try_files we need to have the normal
#   non-gz-suffixed file on disk for try_files to be able to identify what
#   directory the file is in from its search list.  This need not be the
#   original file, so we can be tricky and just create a zero-length version of
#   the file using `touch` because no one will read it.
# - The gzipped versions of the files need to just have the straightforward .gz
#   suffix.
#
# If you were thinking "What if the tree actually contains both FOO and FOO.gz?"
# then you win a prize.
#
# It turns out mozilla-central has exactly this case in the guise of
# `devtools/client/styleeditor/test/simple.css` and its gzipped twin
# `devtools/client/styleeditor/test/simple.css.gz`.  Right now (pre this patch)
# if you view the ".gz" file you'll see a useless single gibberish line of text.
#
# Our fix:
# - We observe that if we process the existing "FOO.gz" file before "FOO", then
#   when FOO's gzipping overwrites "FOO.gz", there's no harm because it's
#   overwriting a zero-length file that only needed to exist so its filename
#   existed.
# - We write the list of files to disk and then use gzip to perform 2 separate
#   processing passes: first all .gz-suffixed files, then all the rest.
# - We use `gzip -f` so that gzip doesn't care when it overwrites the zero
#   length file.
# - We note that although we try and be responsible with the timestamps in this
#   script, they don't actually matter at all because they're the timestamps for
#   generated HTML files and don't have timestamps corresponding to the
#   underlying revision controlled files.  (Also, the timestamp that gets served
#   to users is that of the gzip file anyways and so the overwrite case doesn't
#   do any harm there either.)

function compress_dir_with_touch {
  echo "Compressing files in $1 with zero-length marker for try_files"
  pushd $1
  find . -type f > ../compress-file-list
  # It's possible there won't be matches, in which case grep will exit with 1,
  # which we want to eat.
  { egrep '\.gz$' ../compress-file-list || test $? = 1; } | parallel --halt now,fail=1 'gzip -f {} && touch -r {}.gz {}'
  { egrep -v '\.gz$' ../compress-file-list || test $? = 1; } | parallel --halt now,fail=1 'gzip -f {} && touch -r {}.gz {}'
  rm -f ../compress-file-list
  popd
}

# gzip all the files in a directory that aren't already gzipped.  Ignore files
# that are already gzipped.
function idempotently_gzip_dir_no_touch {
  echo "Compressing files in $1 without zero-length marker"
  pushd $1
  find . -type f > ../compress-file-list
  { egrep -v '\.gz$' ../compress-file-list || test $? = 1; } | parallel --halt now,fail=1 'gzip -f {}'
  rm -f ../compress-file-list
  popd
}

compress_dir_with_touch "${INDEX_ROOT}/dir/"
compress_dir_with_touch "${INDEX_ROOT}/analysis/"

```

## scripts/check-helper.sh
```
#!/usr/bin/env bash

set -x # Show commands
set -eu # Errors/undefined vars are fatal
set -o pipefail # Check all commands in a pipeline

if [[ $# -ne 4 ]]
then
    set +x  # Turn off echoing of commands and output only relevant things to avoid bloating logfiles
    echo "usage: $0 <check-disk> <server-url> <searchfox user exposed path> <canonical symbol name as found in data-symbols>"
    echo "Set CHECK_SKIP_DATA_SYMBOL=1 in the environment to skip the check for the symbol in the generated HTML, currently"
    echo "needed for zero-length symbols or analysis data that otherwise doesn't get attached to source-code tokens."
    exit 1
fi

CHECK_DISK=$1
CHECK_SERVER_URL=$2
SEARCHFOX_PATH=$3
SYMBOL_NAME=$4

function fail {
  set +x  # Turn off echoing of commands and output only relevant things to avoid bloating logfiles
  echo -e "=========================================\n    FAILED INDEXING INTEGRITY CHECK\nError: $1\n========================================="
  exit "${2-1}"  ## Return a code specified by $2 or 1 by default.
}

if [[ $CHECK_DISK ]]
then
  # We now gzip these files as part of the indexing process prior to running
  # these checks.  Note that because we use the nginx try_files mechanism
  # probing for the non .gz versions we also need to create zero-length versions
  # of the files which defeats zgrep's magic failover to trying the .gz suffixed
  # version of the file.
  LOCAL_ANALYSIS_PATH=$INDEX_ROOT/analysis/$SEARCHFOX_PATH.gz
  LOCAL_HTML_PATH=$INDEX_ROOT/file/$SEARCHFOX_PATH.gz

  # The analysis file should exist.
  if [[ ! -f $LOCAL_ANALYSIS_PATH ]]; then
    # logic to run if the file didn't exist / wasn't a file
    set +x  # Turn off echoing of commands and output only relevant things to avoid bloating logfiles
    echo "The expected analysis file $LOCAL_ANALYSIS_PATH does not exist!"
    # TODO: Maybe do a secondary check here so that we can report when the
    # analysis file has ended up in a per-platform directory if the path started
    # with __GENERATED__.
    exit 1
  fi

  # GREP NOTE!
  # We use "-m1" to return only the first match for what we're looking for.
  # This is primarily done to avoid filling up the indexing log with unnecessary
  # spam, but it also helps performance.  It does also impact curl, see the curl
  # note below.

  # Look for a target record that contains the symbol name.  (Source records may
  # currently contain multiple symbol names, in which case we wouldn't want to
  # look for the closing quote or would need to allow for it to be also be a
  # comma.)
  zgrep -m1 "\"sym\":\"$SYMBOL_NAME\"" "$LOCAL_ANALYSIS_PATH" || fail "No symbol: $SYMBOL_NAME in analysis in $LOCAL_ANALYSIS_PATH"

  # The output file should exist.
  if [[ ! -f "$LOCAL_ANALYSIS_PATH" ]]; then
    # logic to run if the file didn't exist / wasn't a file
    set +x  # Turn off echoing of commands and output only relevant things to avoid bloating logfiles
    echo "The expected analysis file $LOCAL_ANALYSIS_PATH does not exist!"
    exit 1
  fi

  if [[ -z ${CHECK_SKIP_DATA_SYMBOL:-} ]]; then
    # The output file should explicitly reference the symbol name as part of
    # `data-symbols`.  We allow for a comma or closing quote.
    zegrep -m1 -e "data-symbols=\"$SYMBOL_NAME[\",]" "$LOCAL_HTML_PATH"  || fail "No symbol: $SYMBOL_NAME in HTML in $LOCAL_HTML_PATH"
  fi

  # Note: It would be neat to check the crossref database here, but the file
  # gets very large and it's much more efficient to just ask the webserver.
fi

if [[ $CHECK_SERVER_URL ]]
then
  SERVER_ANALYSIS_URL=${CHECK_SERVER_URL}${TREE_NAME}/raw-analysis/${SEARCHFOX_PATH}
  SERVER_HTML_URL=${CHECK_SERVER_URL}${TREE_NAME}/source/${SEARCHFOX_PATH}
  SERVER_SYMBOL_SEARCH_URL=${CHECK_SERVER_URL}${TREE_NAME}/search?q=symbol:${SYMBOL_NAME}

  # CURL NOTE!
  # When curl is piped or process substitution is used (which also creates a
  # pipe) and the reader of that pipe (grep/egrep/jq) closes the pipe early,
  # curl will by default report an error about failing to write to its buffer.
  #
  # We don't actually care about this, so we silence it via "-s".  And in
  # general we don't care if curl fails, as we're looking for positive presence
  # of specific output in the returned data.

  # Curl flags we intentionally use:
  # -f: Fail on server error codes rather than returning the error document.
  # -s: Silence the progress bar and error output.

  # Check the analysis file exists and contains the expected symbol using the
  # same check from the disk case.
  grep -m1 "\"sym\":\"$SYMBOL_NAME\"" <( curl -fs "$SERVER_ANALYSIS_URL" ) || fail "No symbol: $SYMBOL_NAME in analysis served at $SERVER_ANALYSIS_URL"

  if [[ -z ${CHECK_SKIP_DATA_SYMBOL:-} ]]; then
    # Check the HTML file exists and contains the expected symbol using the same
    # check from the disk case.
    egrep -m1 -e "data-symbols=\"$SYMBOL_NAME[\",]" <( curl -fs "$SERVER_HTML_URL" ) || fail "No symbol: $SYMBOL_NAME in HTML served at $SERVER_HTML_URL"
  fi

  # This JQ expression looks for a definition that has the searchfox path as its
  # path.  Explanation:
  # - to_entries converts the dictionary that looks like
  #   { normal, generated, tests } into [{key: normal, value: ...}]
  JQ_FIND_DEF_EXPR="to_entries"
  # - This returns the Definitions hitlist for the given group or an empty array
  #   if there was no set of Definitions.
  JQ_FIND_DEF_EXPR+=" | map(.value.Definitions? // [])"
  # - flatten merges all of the definitions hitlists together into one list.
  JQ_FIND_DEF_EXPR+=" | flatten"
  # - This produces a list of booleans indicating whether the given hitlist
  #   contained the given path.
  JQ_FIND_DEF_EXPR+=" | map(.path == \"$SEARCHFOX_PATH\")"
  # - any returns true if there were any true values.
  JQ_FIND_DEF_EXPR+=" | any"

  jq "$JQ_FIND_DEF_EXPR" <( curl -fs -H "Accept: application/json" "$SERVER_SYMBOL_SEARCH_URL" )  || fail "No symbol: $SYMBOL_NAME in search results from $SERVER_SYMBOL_SEARCH_URL"
fi

set +x  # Turn off echoing of commands and output only relevant things to avoid bloating logfiles
echo -e "=========================================\nSUCCESS: Integrity check passed for $SYMBOL_NAME in $SEARCHFOX_PATH\n========================================="

```

## scripts/generate-config.sh
```
#!/usr/bin/env bash

set -x # Show commands
set -eu # Errors/undefined vars are fatal
set -o pipefail # Check all commands in a pipeline

if [ $# != 3 ]
then
    echo "usage: $0 <config-repo-path> <config-file-name> <working-path>"
    exit 1
fi

MOZSEARCH_PATH=$(readlink -f $(dirname "$0")/..)

CONFIG_REPO=$(readlink -f $1)
CONFIG_INPUT="$2"
WORKING=$(readlink -f $3)

CONFIG_FILE=$WORKING/config.json

export MOZSEARCH_PATH
export CONFIG_REPO
export WORKING
envsubst < $CONFIG_REPO/$CONFIG_INPUT > $CONFIG_FILE

```

## scripts/find-repo-files.py
```
#!/usr/bin/env python3

import collections
import json
import runpy
import os
import sys

from lib import run

config_repo = sys.argv[1]
config = json.load(open(sys.argv[2]))
tree_name = sys.argv[3]

# Dynamically import the repo_files.py script from the tree's scripts in the
# config repo.
try:
    repo_files = runpy.run_path(os.path.join(config_repo, tree_name, 'repo_files.py'))
except FileNotFoundError:
    # For simplicity allow the tree config to not have the script, in which case
    # we fall back to some default behaviour.
    repo_files = {}

tree_config = config['trees'][tree_name]
tree_repo = tree_config['files_path']
lines = run(['git', 'ls-files', '-z', '--recurse-submodules'], cwd=tree_repo).split(b'\0')
if len(lines) == 0:
    # find . -type f -printf '%P\n'
    lines = run(['/usr/bin/find', '.', '-type', 'f', '-printf', '%P\n'], cwd=tree_repo).splitlines()

if 'modify_file_list' in repo_files:
    lines = repo_files['modify_file_list'](lines, config=tree_config)

files = []
js = []
html = []
css = []
idl = []
webidl = []
ipdl = []
staticprefs = []

dirs = collections.OrderedDict()
ipdl_dirs = collections.OrderedDict()

for line in lines:
    path = line.strip()
    if not path:
        continue
    path = path.decode()

    # NOTE: `path` is raw filename, which can contain any character allowed by
    # the OS and the file system.
    #
    # For safety, ignore the path with characters that may have special meaning
    # in the HTML context or the shell command context.
    # They are not allowed on Windows, but allowed on other OS.
    for c in ['"', '<', '>', '\\']:
        if c in path:
            continue

    fullpath = os.path.join(tree_repo, path)

    elts = path.split('/')
    for i in range(len(elts)):
        sub = '/'.join(elts[:i])
        if sub and sub not in dirs:
            dirs[sub] = True

    files.append(path + '\n')

    (_, ext) = os.path.splitext(path)
    if ext == '.idl':
        if 'filter_idl' in repo_files:
            if not repo_files['filter_idl'](path):
                continue

        idl.append(path + '\n')

    if ext == '.webidl':
        if 'filter_webidl' in repo_files:
            if not repo_files['filter_webidl'](path):
                continue

        webidl.append(path + '\n')

    if ext in ['.ipdl', '.ipdlh']:
        if 'filter_ipdl' in repo_files:
            if not repo_files['filter_ipdl'](path):
                continue

        ipdl.append(path + '\n')

        dir = '/'.join(elts[:-1])
        ipdl_dirs[dir] = True

    if ext in ['.js', '.jsm', '.mjs', '.xml', '.xul', '.inc']:
        if 'filter_js' in repo_files:
            if not repo_files['filter_js'](path):
                continue

        js.append(path + '\n')

    if ext in ['.html', '.xhtml']:
        if 'filter_html' in repo_files:
            if not repo_files['filter_html'](path):
                continue

        html.append(path + '\n')

    if ext in ['.css']:
        if 'filter_css' in repo_files:
            if not repo_files['filter_css'](path):
                continue

        css.append(path + '\n')

    name = os.path.basename(path)

    if name == 'StaticPrefList.yaml':
        staticprefs.append(path + '\n')

index_path = tree_config['index_path']
open(os.path.join(index_path, 'repo-files'), 'w').writelines(files)
open(os.path.join(index_path, 'repo-dirs'), 'w').writelines([d + '\n' for d in dirs])
open(os.path.join(index_path, 'js-files'), 'w').writelines(js)
open(os.path.join(index_path, 'html-files'), 'w').writelines(html)
open(os.path.join(index_path, 'css-files'), 'w').writelines(css)
open(os.path.join(index_path, 'idl-files'), 'w').writelines(idl)
open(os.path.join(index_path, 'webidl-files'), 'w').writelines(webidl)
open(os.path.join(index_path, 'ipdl-files'), 'w').writelines(ipdl)
open(os.path.join(index_path, 'ipdl-includes'), 'w').write(' '.join(['-I ' + d for d in ipdl_dirs]))
open(os.path.join(index_path, 'staticprefs-files'), 'w').writelines(staticprefs)

```

## scripts/indexer-logs-print.py
```
#!/usr/bin/env python3

# Consumes the NDJSON fed to us by the indexer-logs-analyze.sh script which is
# an exciting bunch of shell data that eventually ends up as JSON that it's just
# most practical to actual be using a high level language on at this point.

import datetime
import json
import sys

from rich import box, print
from rich.console import Console
from rich.markup import escape
from rich.table import Table
from rich.tree import Tree
from rich.traceback import install

class IndexerLogStdinPrinter:
    def __init__(self):
        self.data_by_tree = {}

    def consume(self, fp):
        for line in fp.readlines():
            data = json.loads(line)
            if 'tree' not in data:
                continue

            tree = data['tree']
            dtime = data['time'] = datetime.datetime.fromisoformat(data['time'])

            if tree not in self.data_by_tree:
                tree_data = {
                    'tree': tree,
                    'data': [],
                    'first': dtime,
                    'last': dtime,
                }
                self.data_by_tree[tree] = tree_data
            else:
                tree_data = self.data_by_tree[tree]
            data['dur'] = ''
            if len(tree_data['data']):
                prev_data = tree_data['data'][-1]
                prev_data['dur'] = data['time'] - prev_data['time']
            tree_data['data'].append(data)
            if data['time'] > tree_data['last']:
                tree_data['last'] = data['time']

    def print(self):
        root_tree = Tree('Trees')
        for tree_name, tree_data in self.data_by_tree.items():
            name_with_dur = f"{tree_name} - {tree_data['last'] - tree_data['first']}"
            tree_tree = root_tree.add(tree_name)

            table = Table(box=box.SIMPLE)
            table.add_column('script')
            table.add_column('time since start')
            table.add_column('apparent duration')

            for data in tree_data['data']:
                table.add_row(
                    data['script'],
                    f"{data['time'] - tree_data['first']}",
                    f"{data['dur']}",
                )

            tree_tree.add(table)
        print(root_tree)

if __name__ == '__main__':
    install(show_locals=True)
    cmd = IndexerLogStdinPrinter()
    cmd.consume(sys.stdin)
    cmd.print()

```

## scripts/webidl-analyze.py
```
#!/usr/bin/env python3

# See the comment at the top of idl-analyze.py file for the details about
# how IDL indexing works.

import json
import os.path
import re
import sys

import WebIDL

# local path => records.
analysis_map = {}

# local path => interface name => member name => member symbols.
cpp_analysis_map = {}


def parse_mangled(mangled):
    def parse_inner(idents, inner):
        if inner[0] == 'E':
            return True

        m = re.match(r'[LK]?([0-9]+)([a-zA-Z0-9_]+)', inner)
        if not m:
            return False

        length = int(m.group(1))
        idents.append(m.group(2)[:length])

        return parse_inner(idents, m.group(2)[length:])

    if mangled[:3] != '_ZN':
        return

    idents = []
    if not parse_inner(idents, mangled[3:]):
        return

    return idents


def capitalize(name):
    return name[0].upper() + name[1:]


class CppSymbolMemberItem:
    '''Represents single member's C++ symbols

    Single member can have multiple symbols for the following reasons:
      * method can have overloads
      * signature can be different between architecture

    If there are N overloads, there will be one binding C++ function
    (generated code) and N C++ impls (non-generated code), for each
    architecture.

    All overload impls are supposed to br called from single binding function.
    '''

    def __init__(self):
        # A list of C++ binding function symbols.
        self.binding_syms = []

        # A list of C++ implementation function symbols.
        self.impl_syms = []

    def add_binding(self, sym):
        '''Add a binding's C++ symbol.'''

        if sym in self.binding_syms:
            # Other architecture had the same symbol
            return

        self.binding_syms.append(sym)

    def add_impl(self, sym):
        '''Add a implementation's C++ symbol.'''

        if sym in self.impl_syms:
            # Other architecture had the same symbol
            return

        self.impl_syms.append(sym)

    def merge(self, other):
        self.binding_syms += other.binding_syms
        self.impl_syms += other.impl_syms


class CppSymbolsBuilder:
    '''Build the C++ symbol map'''

    def __init__(self, cpp_symbols):
        self.cpp_symbols = cpp_symbols

        # The interface name of the current C++ binding function.
        self.current_iface_name = None

        # The member name of the current C++ binding function.
        self.current_member_name = None

        # The CppSymbolMemberItem for the member for the
        # current C++ binding function.
        self.current_member_item = None

    @staticmethod
    def parse_sym(sym):
        '''Given C++ symbol, return the class/interface name and member name.
        The '_Binding' suffix is not removed.

        If the symbol doesn't match the binding or implementation's pattern,
        returns None for both.
        '''

        if not sym:
            return None, None

        if not sym.startswith('_Z'):
            return None, None

        idents = parse_mangled(sym)
        if not idents or len(idents) != 4:
            return None, None

        # All bindings and impls are directly inside mozilla::dom namespace,
        # with `_Binding` suffix added for bindings and no suffix for impls.
        if idents[0] != 'mozilla' or idents[1] != 'dom':
            return None, None

        return idents[2], idents[3]

    def maybe_add_binding(self, sym):
        '''If given symbol is C++ binding function, add it.'''

        iface_name, member_name = self.parse_sym(sym)
        if iface_name is None:
            return
        if not iface_name.endswith('_Binding'):
            return
        iface_name = iface_name.replace('_Binding', '')

        per_iface = self.cpp_symbols.setdefault(iface_name, {})
        if member_name in per_iface:
            member_item = per_iface[member_name]
        else:
            member_item = CppSymbolMemberItem()
            per_iface[member_name] = member_item

        self.current_iface_name = iface_name
        self.current_member_name = member_name
        self.current_member_item = member_item

        member_item.add_binding(sym)

    def maybe_add_impl(self, sym, contextsym):
        '''If given symbol is C++ implementation function for the
        current C++ binding function, add it.'''

        context_iface_name, context_member_name = self.parse_sym(contextsym)
        if context_iface_name is None:
            return
        if not context_iface_name.endswith('_Binding'):
            return
        context_iface_name = context_iface_name.replace('_Binding', '')

        if self.current_iface_name != context_iface_name:
            return
        if self.current_member_name != context_member_name:
            return

        iface_name, member_name = self.parse_sym(sym)

        if iface_name is None:
            return
        if iface_name != context_iface_name:
            return

        if context_member_name.startswith('get_'):
            name = capitalize(context_member_name[4:])
            expected = [name, "Get" + name]
        elif context_member_name.startswith('set_'):
            name = capitalize(context_member_name[4:])
            expected = ["Set" + name]
        else:
            name = capitalize(context_member_name)
            expected = [name]

        if member_name not in expected:
            return

        self.current_member_item.add_impl(sym)


def read_cpp_analysis_one(path, cpp_symbols):
    '''Read given analysis file and collect C++ symbols for generated code.'''

    if not os.path.exists(path):
        return

    try:
        lines = open(path).readlines()
    except IOError as e:
        return

    builder = CppSymbolsBuilder(cpp_symbols)

    for line in lines:
        try:
            j = json.loads(line.strip())
        except ValueError as e:
            print('Syntax error in JSON file', path, line.strip(), file=sys.stderr)
            raise e

        if 'target' not in j:
            continue

        kind = j['kind']

        if kind in ('decl', 'def'):
            builder.maybe_add_binding(j['sym'])

        elif kind == 'use':
            builder.maybe_add_impl(j['sym'], j.get('contextsym', ''))


def read_cpp_analysis(analysis_root, local_path, bindings_local_path):
    '''Read analysis files for given WebIDL file and collect C++ symbols
    for generated code.'''

    base = os.path.basename(local_path)
    (idl_name, suffix) = os.path.splitext(base)
    header_name = idl_name + 'Binding.h'
    cpp_name = idl_name + 'Binding.cpp'
    cpp_symbols = {}

    if bindings_local_path:
        # Override the paths for bindings for testing.
        p = os.path.join(analysis_root, bindings_local_path, 'include', header_name)
        read_cpp_analysis_one(p, cpp_symbols)
        p = os.path.join(analysis_root, bindings_local_path, 'src', cpp_name)
        read_cpp_analysis_one(p, cpp_symbols)
    else:
        generated_dir = os.path.join(analysis_root, '__GENERATED__')
        header_local_path = os.path.join('dist', 'include', 'mozilla', 'dom', header_name)
        cpp_local_path = os.path.join('dom', 'bindings', cpp_name)

        p = os.path.join(generated_dir, header_local_path)
        read_cpp_analysis_one(p, cpp_symbols)
        p = os.path.join(generated_dir, cpp_local_path)
        read_cpp_analysis_one(p, cpp_symbols)

        for name in os.listdir(generated_dir):
            if name.startswith('__'):
                p = os.path.join(generated_dir, name, header_local_path)
                read_cpp_analysis_one(p, cpp_symbols)
                p = os.path.join(generated_dir, name, cpp_local_path)
                read_cpp_analysis_one(p, cpp_symbols)

    return cpp_symbols


def get_binary_name(target):
    binary_name = target.getExtendedAttribute('BinaryName')
    if not binary_name:
        return None
    return binary_name[0]


def cpp_method_name(method, name):
    '''Return the C++ pretty name for this method.'''
    if isinstance(method, WebIDL.IDLConstructor):
        return '_constructor'

    binary_name = get_binary_name(method)
    if binary_name:
        return binary_name
    return name


def cpp_getter_name(attr, name):
    '''Return the C++ pretty name for this getter.'''
    binary_name = get_binary_name(attr)
    if binary_name:
        return 'get_' + binary_name
    return 'get_' + name


def cpp_setter_name(attr, name):
    '''Return the C++ pretty name for this setter.'''
    binary_name = get_binary_name(attr)
    if binary_name:
        return 'set_' + binary_name
    return 'set_' + name


def IDLToCIdentifier(name):
    return name.replace('-', '_')


def cpp_dictionary_field_name(name):
    '''Return the C++ pretty name for this dictionary field.'''
    return 'm' + name[0].upper() + IDLToCIdentifier(name[1:])


def to_loc_with(lineno, colno, name_len):
    return f'{lineno}:{colno}-{colno + name_len}'


def to_loc(location, name):
    location.resolve()
    return to_loc_with(location._lineno, location._colno, len(name))


def get_records(target):
    '''Get the record list for given IDLObject's file.'''
    local_path = target.location.filename
    if local_path in analysis_map:
        return analysis_map[local_path]

    records = []
    analysis_map[local_path] = records
    return records


def emit_source(records, loc, syntax, pretty_prefix, pretty, sym):
    '''Emit AnalysisSource record.'''
    records.append({
        'loc': loc,
        'source': 1,
        'syntax': syntax,
        'pretty': f'IDL {pretty_prefix} {pretty}',
        'sym': sym,
    })


def emit_target(records, loc, kind, pretty, sym):
    '''Emit AnalysisTarget record.'''
    records.append({
        'loc': loc,
        'target': 1,
        'kind': kind,
        'pretty': pretty,
        'sym': sym,
    })


def emit_structured(records, loc, kind, pretty, sym,
                    slots=None, supers=None,
                    methods=None, fields=None):
    '''Emit AnalysisStructured record.'''
    record = {
        'loc': loc,
        'structured': 1,
        'pretty': pretty,
        'sym': sym,
        'kind': kind,
        'implKind': 'idl',
    }
    if slots:
        record['bindingSlots'] = slots
    if supers:
        record['supers'] = supers
    if methods:
        record['methods'] = methods
    if fields:
        record['fields']= fields

    records.append(record)


def handle_simple_type(records, target):
    '''Emit analysis record for simple IDLType subclasses.'''

    if isinstance(target.name, str):
        name = target.name
    else:
        name = target.name.name
    loc = to_loc(target.location, name)
    pretty = name
    idl_sym = f'WEBIDL_{name}'

    emit_source(records, loc, 'type', 'type', pretty, idl_sym)
    emit_target(records, loc, 'use', pretty, idl_sym)


def handle_type(records, target):
    '''Emit analysis record for IDLType subclasses.'''

    if isinstance(target, WebIDL.IDLNullableType):
        handle_type(records, target.inner)
        return
    if isinstance(target, WebIDL.IDLSequenceType):
        handle_type(records, target.inner)
        return
    if isinstance(target, WebIDL.IDLRecordType):
        handle_type(records, target.keyType)
        handle_type(records, target.inner)
        return
    if isinstance(target, WebIDL.IDLObservableArrayType):
        handle_type(records, target.inner)
        return
    if isinstance(target, WebIDL.IDLPromiseType):
        handle_type(records, target.inner)
        return
    if isinstance(target, WebIDL.IDLUnionType):
        for m in target.memberTypes:
            handle_type(records, m)
        return
    if isinstance(target, WebIDL.IDLBuiltinType):
        return

    assert isinstance(target, WebIDL.IDLUnresolvedType) or \
        isinstance(target, WebIDL.IDLTypedefType) or \
        isinstance(target, WebIDL.IDLCallbackType) or \
        isinstance(target, WebIDL.IDLWrapperType)

    handle_simple_type(records, target)


def handle_argument(records, target):
    '''Emit analysis record for IDLArgument in methods.'''

    handle_type(records, target.type)


def append_slot(slots, kind, lang, impl_kind, sym):
    record = {
        'slotKind': kind,
        'slotLang': lang,
        'ownerLang': 'idl',
        'sym': sym,
    }
    if impl_kind is not None:
        record['implKind'] = impl_kind
    slots.append(record)


def handle_method(records, iface_name, methods, cpp_symbols, target):
    '''Emit analysis record for IDLMethod in interface or namespace.'''

    name = target.identifier.name
    loc = to_loc(target.identifier.location, name)
    pretty = f'{iface_name}::{name}'
    idl_sym = f'WEBIDL_{iface_name}_{name}'
    cpp_item = cpp_symbols.get(cpp_method_name(target, name), None)
    js_sym = f'#{name}'
    is_constructor = name == 'constructor'

    emit_source(records, loc, 'idl', 'method', pretty, idl_sym)
    emit_target(records, loc, 'idl', pretty, idl_sym)

    slots = []
    if cpp_item:
        for sym in cpp_item.binding_syms:
            append_slot(slots, 'method', 'cpp', 'binding', sym)
        for sym in cpp_item.impl_syms:
            append_slot(slots, 'method', 'cpp', 'impl', sym)
    append_slot(slots, 'method', 'js', None, js_sym)

    emit_structured(records, loc, 'method', pretty, idl_sym,
                    slots=slots)

    # Before resolve, overloads are represented as separate IDLMethod.
    assert len(target._overloads) == 1

    overload = target._overloads[0]
    if not is_constructor:
        handle_type(records, overload.returnType)
    for arg in overload.arguments:
        handle_argument(records, arg)

    methods.append({
        'pretty': pretty,
        'sym': idl_sym,
        'props': [],
    })


def handle_attribute(records, iface_name, fields, cpp_symbols, target):
    '''Emit analysis record for IDLAttribute in interface or namespace.'''

    name = target.identifier.name
    loc = to_loc(target.identifier.location, name)
    pretty = f'{iface_name}::{name}'
    idl_sym = f'WEBIDL_{iface_name}_{name}'
    getter_cpp_item = cpp_symbols.get(cpp_getter_name(target, name), None)
    js_sym = f'#{name}'

    emit_source(records, loc, 'idl', 'attribute', pretty, idl_sym)
    emit_target(records, loc, 'idl', pretty, idl_sym)

    slots = []
    if getter_cpp_item:
        for sym in getter_cpp_item.binding_syms:
            append_slot(slots, 'getter', 'cpp', 'binding', sym)
        for sym in getter_cpp_item.impl_syms:
            append_slot(slots, 'getter', 'cpp', 'impl', sym)
    if not target.readonly:
        setter_cpp_item = cpp_symbols.get(cpp_setter_name(target, name), None)
        if setter_cpp_item:
            for sym in  setter_cpp_item.binding_syms:
                append_slot(slots, 'setter', 'cpp', 'binding', sym)
            for sym in  setter_cpp_item.impl_syms:
                append_slot(slots, 'setter', 'cpp', 'impl', sym)
    append_slot(slots, 'attribute', 'js', None, js_sym)

    emit_structured(records, loc, 'field', pretty, idl_sym,
                    slots=slots)

    handle_type(records, target.type)

    fields.append({
        'pretty': pretty,
        'sym': idl_sym,
        'props': [],
    })


def handle_const(records, iface_name, fields, cpp_symbols, target):
    '''Emit analysis record for IDLConst in interface or namespace.'''

    name = target.identifier.name
    loc = to_loc(target.identifier.location, name)
    pretty = f'{iface_name}::{name}'
    idl_sym = f'WEBIDL_{iface_name}_{name}'
    cpp_item = cpp_symbols.get(name, None)
    js_sym = f'#{name}'

    emit_source(records, loc, 'idl', 'const', pretty, idl_sym)
    emit_target(records, loc, 'idl', pretty, idl_sym)

    slots = []
    if cpp_item:
        for sym in cpp_item.binding_syms:
            append_slot(slots, 'const', 'cpp', None, sym)
    append_slot(slots, 'const', 'js', None, js_sym)

    emit_structured(records, loc, 'field', pretty, idl_sym,
                    slots=slots)

    handle_type(records, target.type)

    fields.append({
        'pretty': pretty,
        'sym': idl_sym,
        'props': [],
    })


def handle_super(records, supers, target):
    '''Emit analysis record for references in super interface
    or super dictionary.'''

    name = target.identifier.name
    loc = to_loc(target.identifier.location, name)
    pretty = name
    idl_sym = f'WEBIDL_{name}'

    emit_source(records, loc, 'type', 'class', pretty, idl_sym)
    emit_target(records, loc, 'use', pretty, idl_sym)

    supers.append({
        'sym': idl_sym,
    })


def handle_maplike_or_setlike_or_iterable(records, iface_name, target):
    '''Emit analysis record for IDLMaplikeOrSetlike, IDLIterable,
    or IDLAsyncIterable in interface.'''

    name = target.identifier.name.replace('__', '')
    loc = to_loc(target.identifier.location, name)
    pretty = f'{iface_name}::{target.maplikeOrSetlikeOrIterableType}'
    idl_sym = f'WEBIDL_{iface_name}_{target.maplikeOrSetlikeOrIterableType}'

    if target.maplikeOrSetlikeOrIterableType == 'maplike':
        cpp_sym = f'NS_mozilla::dom::{iface_name}_Binding::MaplikeHelpers'
    elif target.maplikeOrSetlikeOrIterableType == 'setlike':
        cpp_sym = f'NS_mozilla::dom::{iface_name}_Binding::SetlikeHelpers'
    elif target.maplikeOrSetlikeOrIterableType == 'iterable':
        cpp_sym = f'NS_mozilla::dom::{iface_name}Iterator_Binding'
    elif target.maplikeOrSetlikeOrIterableType == 'asynciterable':
        cpp_sym = f'NS_mozilla::dom::{iface_name}AsyncIterator_Binding'
    else:
        print(f'warning: WebIDL: Unknown maplikeOrSetlikeOrIterableType: {target.maplikeOrSetlikeOrIterableType}',
              file=sys.stderr)

    emit_source(records, loc, 'idl', 'method', pretty, idl_sym)
    emit_target(records, loc, 'idl', pretty, idl_sym)

    slots = []
    append_slot(slots, 'class', 'cpp', None, cpp_sym)

    emit_structured(records, loc, 'method', pretty, idl_sym,
                    slots=slots)

    if target.keyType:
        handle_type(records, target.keyType)
    if target.valueType:
        handle_type(records, target.valueType)
    if hasattr(target, 'argList'):
        for arg in target.argList:
            handle_argument(records, arg)


def handle_interface_or_namespace(records, target, mixin_consumers_map=None):
    '''Emit analysis record for IDLInterface, IDLInterfaceMixin, IDLNamespace,
    or IDLPartialInterfaceOrNamespace.'''

    is_mixin = isinstance(target, WebIDL.IDLInterfaceMixin)

    name = target.identifier.name
    loc = to_loc(target.identifier.location, name)
    pretty = name
    idl_sym = f'WEBIDL_{name}'
    if not is_mixin:
        cpp_sym = f'NS_mozilla::dom::{name}_Binding'
    js_sym = f'#{name}'

    if not is_mixin:
        local_path = target.location.filename
        cpp_analysis = cpp_analysis_map.get(local_path, None)
        if cpp_analysis is None:
            print('warning: WebIDL: No C++ analysis data found for', local_path, file=sys.stderr)
            cpp_symbols = {}
        else:
            cpp_symbols = cpp_analysis.get(name, {})
    else:
        cpp_symbols = {}
        for iface in mixin_consumers_map.get(name, []):
            iface_name = iface.identifier.name
            local_path = iface.location.filename
            cpp_analysis = cpp_analysis_map.get(local_path, None)
            if cpp_analysis is None:
                print('warning: WebIDL: No C++ analysis data found for', local_path, file=sys.stderr)
            else:
                iface_cpp_symbols = cpp_analysis.get(iface_name, {})
                for prop, item in iface_cpp_symbols.items():
                    if prop not in cpp_symbols:
                        cpp_symbols[prop] = CppSymbolMemberItem()

                    cpp_symbols[prop].merge(item)

    emit_source(records, loc, 'idl', 'class', pretty, idl_sym)
    emit_target(records, loc, 'idl', pretty, idl_sym)

    slots = []
    if not is_mixin:
        append_slot(slots, 'class', 'cpp', None, cpp_sym)
    append_slot(slots, 'interface_name', 'js', None, js_sym)

    if not is_mixin:
        supers = []
        if hasattr(target, 'parent') and target.parent:
            handle_super(records, supers, target.parent)
    else:
        supers = None

    methods = []
    fields = []
    for member in target.members:
        if isinstance(member.identifier.location, WebIDL.BuiltinLocation):
            continue

        if isinstance(member, WebIDL.IDLMethod):
            handle_method(records, name, methods, cpp_symbols, member)
        elif isinstance(member, WebIDL.IDLAttribute):
            handle_attribute(records, name, methods, cpp_symbols, member)
        elif isinstance(member, WebIDL.IDLConst):
            handle_const(records, name, methods, cpp_symbols, member)
        elif isinstance(member, WebIDL.IDLMaplikeOrSetlike):
            handle_maplike_or_setlike_or_iterable(records, name, member)
        elif isinstance(member, WebIDL.IDLIterable):
            handle_maplike_or_setlike_or_iterable(records, name, member)
        elif isinstance(member, WebIDL.IDLAsyncIterable):
            handle_maplike_or_setlike_or_iterable(records, name, member)
        else:
            print(f'warning: WebIDL: Unknown member production: {member.__class__.__name__}',
                  file=sys.stderr)

    emit_structured(records, loc, 'class', pretty, idl_sym,
                    slots=slots, supers=supers,
                    methods=methods, fields=fields)


def handle_dictionary_field(records, dictionary_name, dictionary_cpp_sym,
                            fields, target):
    '''Emit analysis record for IDLArgument in dictionary.'''

    name = target.identifier.name
    loc = to_loc(target.identifier.location, name)
    pretty = f'{dictionary_name}.{name}'
    idl_sym = f'WEBIDL_{dictionary_name}_{name}'
    cpp_sym = f'F_<{dictionary_cpp_sym}>_{cpp_dictionary_field_name(name)}'
    js_sym = f'#{name}'

    emit_source(records, loc, 'idl', 'field', pretty, idl_sym)
    emit_target(records, loc, 'idl', pretty, idl_sym)

    slots = []
    append_slot(slots, 'attribute', 'cpp', None, cpp_sym)
    append_slot(slots, 'attribute', 'js', None, js_sym)

    emit_structured(records, loc, 'field', pretty, idl_sym,
                    slots=slots)

    handle_type(records, target.type)

    fields.append({
        'pretty': pretty,
        'sym': idl_sym,
        'props': [],
    })


def handle_dictionary(records, target):
    '''Emit analysis record for IDLDictionary or IDLPartialDictionary.'''

    name = target.identifier.name
    loc = to_loc(target.identifier.location, name)
    pretty = name
    idl_sym = f'WEBIDL_{name}'
    cpp_sym = f'T_mozilla::dom::{name}'
    js_sym = f'#{name}'

    emit_source(records, loc, 'idl', 'dictionary', pretty, idl_sym)
    emit_target(records, loc, 'idl', pretty, idl_sym)

    slots = []
    append_slot(slots, 'class', 'cpp', None, cpp_sym)
    append_slot(slots, 'class', 'js', None, js_sym)

    supers = []
    if hasattr(target, 'parent') and target.parent:
        handle_super(records, supers, target.parent)

    fields = []
    for member in target.members:
        if isinstance(member, WebIDL.IDLArgument):
            handle_dictionary_field(records, name, cpp_sym,
                                    fields, member)
        else:
            print(f'warning: WebIDL: Unknown member production: {member.__class__.__name__}',
                  file=sys.stderr)

    emit_structured(records, loc, 'class', pretty, idl_sym,
                    slots=slots, supers=supers,
                    fields=fields)


def handle_enum(records, target):
    '''Emit analysis record for IDLEnum.'''

    name = target.identifier.name
    loc = to_loc(target.identifier.location, name)
    pretty = name
    idl_sym = f'WEBIDL_{name}'
    cpp_sym = f'T_mozilla::dom::{name}'
    js_sym = f'#{name}'

    emit_source(records, loc, 'idl', 'enum', pretty, idl_sym)
    emit_target(records, loc, 'idl', pretty, idl_sym)

    slots = []
    append_slot(slots, 'const', 'cpp', None, cpp_sym)
    append_slot(slots, 'const', 'js', None, js_sym)

    emit_structured(records, loc, 'const', pretty, idl_sym,
                    slots=slots)


def handle_typedef(records, target):
    '''Emit analysis record for IDLTypedef.'''

    name = target.identifier.name
    loc = to_loc(target.identifier.location, name)
    pretty = name
    idl_sym = f'WEBIDL_{name}'

    emit_source(records, loc, 'idl', 'type', pretty, idl_sym)
    emit_target(records, loc, 'idl', pretty, idl_sym)

    handle_type(records, target.innerType)


def handle_callback(records, target):
    '''Emit analysis record for IDLCallback.'''

    name = target.identifier.name
    loc = to_loc(target.identifier.location, name)
    pretty = name
    idl_sym = f'WEBIDL_{name}'

    emit_source(records, loc, 'idl', 'callback', pretty, idl_sym)
    emit_target(records, loc, 'idl', pretty, idl_sym)

    handle_type(records, target._returnType)
    for arg in target._arguments:
        handle_argument(records, arg)


def handle_includes(records, target):
    '''Emit analysis record for IDLIncludesStatement.'''

    name = target.interface.identifier.name
    loc = to_loc(target.interface.identifier.location, name)
    pretty = name
    idl_sym = f'WEBIDL_{name}'

    emit_source(records, loc, 'type', 'class', pretty, idl_sym)
    emit_target(records, loc, 'use', pretty, idl_sym)

    name = target.mixin.identifier.name
    loc = to_loc(target.mixin.identifier.location, name)
    pretty = name
    idl_sym = f'WEBIDL_{name}'

    emit_source(records, loc, 'type', 'class', pretty, idl_sym)
    emit_target(records, loc, 'use', pretty, idl_sym)


def handle_external_interface(records, target):
    '''Emit analysis record for IDLExternalInterface.'''

    name = target.identifier.name
    loc = to_loc(target.identifier.location, name)
    pretty = name
    idl_sym = f'WEBIDL_{name}'

    emit_source(records, loc, 'type', 'class', pretty, idl_sym)
    emit_target(records, loc, 'use', pretty, idl_sym)


def preprocess(lines):
    '''Remove macros from the input.

    This expects the macro doesn't have conflicting then-clause vs else-clause.'''
    result = []
    for line in lines:
        if line.startswith('#'):
            result.append('\n')
        else:
            result.append(line)
    return result


def parse_files(index_root, files_root, analysis_root, cache_dir, bindings_local_path):
    '''Parse all WebIDL files and load corresponding C++ analysis files.'''

    parser = WebIDL.Parser(cache_dir)

    for local_path in sys.stdin:
        local_path = local_path.strip()

        if local_path.startswith('__GENERATED__/'):
            fname = os.path.join(index_root, 'objdir', local_path.replace('__GENERATED__/', ''))
        else:
            fname = os.path.join(files_root, local_path)

        lines = preprocess(open(fname).readlines())
        text = ''.join(lines)
        cpp_analysis_map[local_path] = read_cpp_analysis(analysis_root, local_path, bindings_local_path)

        try:
            parser.parse(text, local_path)
        except WebIDL.WebIDLError as e:
            print('WebIDL: Syntax error in IDL', fname, file=sys.stderr)
            raise e

    # NOTE: Do not call parser.finish() here because we need raw identifiers and
    #       raw productions, and we don't need auto-generated items.
    return parser._productions


def collect_mixin_consumers_map(productions):
    iface_map = {}
    for target in productions:
        if isinstance(target, WebIDL.IDLInterface):
            iface_name = target.identifier.name
            iface_map[iface_name] = target

    mixin_consumers_map = {}
    for target in productions:
        if isinstance(target, WebIDL.IDLIncludesStatement):
            iface_name = target.interface.identifier.name
            mixin = target.mixin.identifier.name

            if mixin not in mixin_consumers_map:
                mixin_consumers_map[mixin] = []

            if iface_name in iface_map:
                iface = iface_map.get(iface_name)
                mixin_consumers_map[mixin].append(iface)

    return mixin_consumers_map


def handle_productions(productions):
    '''Emit analysis records for all productions.'''

    mixin_consumers_map = collect_mixin_consumers_map(productions)

    for target in productions:
        if isinstance(target, WebIDL.IDLInterfaceOrNamespace):
            records = get_records(target)
            handle_interface_or_namespace(records, target)
        elif isinstance(target, WebIDL.IDLPartialInterfaceOrNamespace):
            records = get_records(target)
            handle_interface_or_namespace(records, target)
        elif isinstance(target, WebIDL.IDLInterfaceMixin):
            records = get_records(target)
            handle_interface_or_namespace(records, target, mixin_consumers_map)
        elif isinstance(target, WebIDL.IDLDictionary):
            records = get_records(target)
            handle_dictionary(records, target)
        elif isinstance(target, WebIDL.IDLPartialDictionary):
            records = get_records(target)
            handle_dictionary(records, target)
        elif isinstance(target, WebIDL.IDLEnum):
            records = get_records(target)
            handle_enum(records, target)
        elif isinstance(target, WebIDL.IDLTypedef):
            records = get_records(target)
            handle_typedef(records, target)
        elif isinstance(target, WebIDL.IDLCallback):
            records = get_records(target)
            handle_callback(records, target)
        elif isinstance(target, WebIDL.IDLIncludesStatement):
            records = get_records(target)
            handle_includes(records, target)
        elif isinstance(target, WebIDL.IDLExternalInterface):
            records = get_records(target)
            handle_external_interface(records, target);
        else:
            print(f'warning: WebIDL: Unknown top-level production: {target.__class__.__name__}',
                  file=sys.stderr)


def write_files(analysis_root):
    '''Write analysis records for each file.'''

    for local_path, records in analysis_map.items():
        analysis_path = os.path.join(analysis_root, local_path)
        print('WebIDL: Generating', analysis_path, file=sys.stderr)

        parent = os.path.dirname(analysis_path)
        os.makedirs(parent, exist_ok=True)

        with open(analysis_path, 'w') as fh:
            for r in records:
                print(json.dumps(r), file=fh)


index_root = sys.argv[1]
files_root = sys.argv[2]
analysis_root = sys.argv[3]
cache_dir = sys.argv[4]
bindings_local_path = sys.argv[5]
if bindings_local_path == 'null':
    bindings_local_path = None

productions = parse_files(index_root, files_root, analysis_root, cache_dir, bindings_local_path)
handle_productions(productions)
write_files(analysis_root)

```

## scripts/webtest.sh
```
#!/usr/bin/env bash

set -e

FILTER=$1

cargo install geckodriver

if ! [ -d mozsearch-firefox ]; then
    curl -L -o mozsearch-firefox.tar.bz2 "https://download.mozilla.org/?product=firefox-latest&os=linux64"
    tar xf mozsearch-firefox.tar.bz2
    mv firefox mozsearch-firefox
fi

stop_geckodriver() {
    PID=$(pgrep geckodriver)
    if [ "x${PID}" != "x" ]; then
        echo "Stopping geckodriver: PID=${PID}"
        kill $PID
    fi
}

set +e

stop_geckodriver

echo "Starting geckodriver"
geckodriver -b /vagrant/mozsearch-firefox/firefox >/dev/null 2>&1 &

echo "Running tests"
./tools/target/release/searchfox-tool "webtest ${FILTER}"

stop_geckodriver

```

## scripts/generate-other-resources-list.py
```
#!/usr/bin/env python3

import json
import sys

analysis_files_path = sys.argv[1]
url_map_path = sys.argv[2]
other_resources_path = sys.argv[3]

with open(analysis_files_path, "r") as f:
    analysis_files = set(f.read().split("\n"))

with open(url_map_path, "r") as f:
    aliases_map = json.load(f)

other_resources = set()

for (sym, aliases) in aliases_map.items():
    for item in aliases:
        path = item["pretty"].replace("file ", "")

        if path in analysis_files:
            continue

        other_resources.add(path)

with open(other_resources_path, "w") as f:
    for path in other_resources:
        print(path, file=f)

```

## scripts/indexer-logs-analyze.sh
```
#!/usr/bin/env bash

SCRIPT_DIR=$(dirname $0)

GREP=grep
if [ $(uname) == "Darwin" ]; then
    GREP=ggrep
    if ! which ${GREP} > /dev/null; then
        echo "Please install GNU grep with 'brew install grep'"
        exit 1
    fi
fi

PARSE_EXPR='parse "* + /home/ubuntu/mozsearch/scripts/* *" '
PARSE_EXPR+=' as time, script, args'
PARSE_EXPR+=' | parseDate(time) as time'
PARSE_EXPR+=' | split(args) on " "'
PARSE_EXPR+=' | split(args[0]) on "/" as args0'
# scripts where the 3rd argument is the tree name and we can use it as-is
PARSE_EXPR+=' | if(script == "find-repo-files.py" or script == "build.sh" or script=="output.sh", args[2], "") as tree'
# scripts where the 2nd argument is the tree name and we can use it as-is
PARSE_EXPR+=' | if(script == "js-analyze.sh" or script == "java-analyze.sh" or'
PARSE_EXPR+=' script == "scip-analyze.sh" or script == "idl-analyze.sh" or'
PARSE_EXPR+=' script == "ipdl-analyze.sh" or script == "crossref.sh" or'
PARSE_EXPR+=' script == "build-codesearch.py" or script == "check-index.sh" or'
PARSE_EXPR+=' script == "compress-outputs.sh" or script == "check-index.sh" or'
PARSE_EXPR+=' script == "html-analyze.sh" or script == "css-analyze.sh", args[1], tree) as tree'
# scripts where the 1st argument has a path segment which is the tree name we
# can use.  We split the first argument above to be `args0` for this.
PARSE_EXPR+=' | if(script == "process-chrome-map.py" or script == "replace-aliases.sh", args0[2], tree) as tree'

# - Grep the log in Perl mode looking for the pattern where a "date" invocation
#   is followed by a script invocation from mozsearch/scripts.
#   - We use `-P` to get fancy Perl mode
#   - We use `-a` to force ASCII mode so it doesn't decide it's a binary file.
#   - We use `-z` so that grep sees a single giant line, which combined with
#     `-o` only outputs what matched.  We use a look-behind assertion so that
#     we can match on the `+ date` line but not include it in the output.
#   - We use `-h` to suppress the filename
# - We use `paste` to join these consecutive lines.
# - We use `tr -d '\0'` to eat a leading nul that ends up in there at the start
#   of the lines.
#
# The net output looks like:
# Sat Oct  2 04:41:33 UTC 2021 + /home/ubuntu/mozsearch/scripts/find-repo-files.py /home/ubuntu/config /mnt/index-scratch/config.json nss
# Sat Oct  2 04:41:35 UTC 2021 + /home/ubuntu/mozsearch/scripts/build.sh /home/ubuntu/config /mnt/index-scratch/config.json nss
# Sat Oct  2 04:41:35 UTC 2021 + /home/ubuntu/mozsearch/scripts/indexer-setup.py
${GREP} -Pazoh "(?<=\n\+ date\n)[^\n]+\n\+ /home/ubuntu/mozsearch/scripts/[^\n]+\n" index-* \
  | paste -d" " - - \
  | tr -d '\0' \
  | agrind --output json "* | ${PARSE_EXPR}" \
  | ${SCRIPT_DIR}/indexer-logs-print.py

```

## scripts/staticprefs-analyze.sh
```
#!/usr/bin/env bash

set -x # Show commands
set -eu # Errors/undefined vars are fatal
set -o pipefail # Check all commands in a pipeline

$MOZSEARCH_PATH/scripts/staticprefs-analyze.py \
    $INDEX_ROOT/staticprefs-files \
    $STATICPREFS_BINDINGS_LOCAL_PATH \
    $FILES_ROOT $INDEX_ROOT/analysis

```

## scripts/idl-analyze.sh
```
#!/usr/bin/env bash

set -x # Show commands
set -eu # Errors/undefined vars are fatal
set -o pipefail # Check all commands in a pipeline

if [ $# -ne 2 ]
then
    echo "Usage: idl-analyze.sh config-file.json tree_name"
    exit 1
fi

CONFIG_FILE=$(realpath $1)
TREE_NAME=$2

if [[ $(cat $INDEX_ROOT/idl-files $INDEX_ROOT/webidl-files | wc -l) -eq 0 ]]; then
    # If there's no IDL files, bail out so as to avoid unnecessary file downloads
    exit 0
fi

if [ -f "${FILES_ROOT}/xpcom/idl-parser/xpidl/xpidl.py" -a \
     -f "${FILES_ROOT}/dom/bindings/parser/WebIDL.py" ]; then
    # The tree we're processing has IDL parsers, so let's use that
    TREE_PYMODULES="/tmp/pymodules-${TREE_NAME}"
    PULL_FROM_MC=0
else
    # The tree we're processing doesn't have IDL parsers, so we'll pull m-c's copy with wget
    TREE_PYMODULES="/tmp/pymodules"
    PULL_FROM_MC=1
fi

# Delete the temp dir if IDL parsers are older than a day (in minutes to avoid
# quantization weirdness).  We'll also try and delete the dir if the file just
# doesn't exist, which also means if the directory doesn't exist.  (We could
# have instead done `-mmin +1440` for affirmative confirmation it's old, but
# since our next check is just for the existence of the directory, this is least
# likely to result in weirdness.)
if [ ! "$(find $TREE_PYMODULES/xpidl.py -mmin -1440)" ]; then
    rm -rf $TREE_PYMODULES
fi

# download/copy as needed
if [ ! -d "${TREE_PYMODULES}" ]; then
    mkdir "${TREE_PYMODULES}"
    pushd "${TREE_PYMODULES}"
    if [ $PULL_FROM_MC -eq 0 ]; then
        cp "${FILES_ROOT}/xpcom/idl-parser/xpidl/xpidl.py" ./
        cp "${FILES_ROOT}/dom/bindings/parser/WebIDL.py" ./
    else
        wget "https://hg.mozilla.org/mozilla-central/raw-file/tip/xpcom/idl-parser/xpidl/xpidl.py"
        wget "https://hg.mozilla.org/mozilla-central/raw-file/tip/dom/bindings/parser/WebIDL.py"
    fi
    mkdir ply
    pushd ply
    for PLYFILE in __init__.py lex.py yacc.py; do
        if [ $PULL_FROM_MC -eq 0 ]; then
            cp "${FILES_ROOT}/other-licenses/ply/ply/${PLYFILE}" ./ || cp "${FILES_ROOT}/third_party/python/ply/ply/${PLYFILE}" ./
        else
            wget "https://hg.mozilla.org/mozilla-central/raw-file/tip/third_party/python/ply/ply/${PLYFILE}"
        fi
    done
    popd
    popd
fi

export PYTHONPATH="${TREE_PYMODULES}"

cat $INDEX_ROOT/idl-files | \
    parallel $MOZSEARCH_PATH/scripts/idl-analyze.py \
    $INDEX_ROOT $FILES_ROOT/{} ">" $INDEX_ROOT/analysis/{}

cat $INDEX_ROOT/webidl-files | \
    $MOZSEARCH_PATH/scripts/webidl-analyze.py \
    $INDEX_ROOT $FILES_ROOT $INDEX_ROOT/analysis /tmp \
    $WEBIDL_BINDINGS_LOCAL_PATH
echo $?

```

## scripts/staticprefs-analyze.py
```
#!/usr/bin/env python3

import json
import os
import re
import sys


# imported from m-c/modules/libpref/init/generate_static_pref_list.py
def mk_id(name):
    "Replace '.' and '-' with '_', e.g. 'foo.bar-baz' becomes 'foo_bar_baz'."
    return name.replace('.', '_').replace('-', '_')


def read_cpp_analysis_one(path, cpp_symbols):
    '''Read given analysis file and collect C++ symbols for generated code.'''

    if not os.path.exists(path):
        print('no', path)
        return

    try:
        lines = open(path).readlines()
    except IOError as e:
        return

    for line in lines:
        try:
            j = json.loads(line.strip())
        except ValueError as e:
            print('Syntax error in JSON file', path, line.strip(), file=sys.stderr)
            raise e

        if 'target' not in j:
            continue

        if j['kind'] != 'def':
            continue

        sym = j['sym']
        m = re.match('.+StaticPrefs[0-9]+(.+)Ev$', sym)
        if m:
            id = m.group(1)
        else:
            m = re.match('.+JS5Prefs[0-9]+(.+)Ev$', sym)
            if m:
                id = f'javascript_options_{m.group(1)}'
            else:
                continue

        if id not in cpp_symbols:
            cpp_symbols[id] = set()
        cpp_symbols[id].add(sym)


def read_cpp_analysis(ns, yaml_path, bindings_local_path, analysis_root):
    '''Read analysis files for given StaticPrefs file and collect C++ symbols
    for generated code.'''

    header_name = f'StaticPrefList_{ns}.h'
    cpp_symbols = {}

    if bindings_local_path:
        # Override the paths for bindings for testing.
        p = os.path.join(analysis_root, bindings_local_path, header_name)
        read_cpp_analysis_one(p, cpp_symbols)
    else:
        generated_dir = os.path.join(analysis_root, '__GENERATED__')
        header_local_path = os.path.join(os.path.dirname(yaml_path), header_name)

        p = os.path.join(generated_dir, header_local_path)
        read_cpp_analysis_one(p, cpp_symbols)

        for name in os.listdir(generated_dir):
            if name.startswith('__'):
                p = os.path.join(generated_dir, name, header_local_path)
                read_cpp_analysis_one(p, cpp_symbols)

        if ns == 'javascript':
            # Load SpiderMonkey-specific binding.
            header_local_path = 'js/public/PrefsGenerated.h'

            p = os.path.join(generated_dir, header_local_path)
            read_cpp_analysis_one(p, cpp_symbols)

            for name in os.listdir(generated_dir):
                if name.startswith('__'):
                    p = os.path.join(generated_dir, name, header_local_path)
                    read_cpp_analysis_one(p, cpp_symbols)

    return cpp_symbols


pref_bindings_map = {}


def get_cpp_symbols(name, yaml_path, bindings_local_path, analysis_root):
    '''Get the corresponding binding C++ symbols for the preference name.
    Returns an empty list if no binding is found.'''
    ns = name.split('.')[0]

    if ns in pref_bindings_map:
        cpp_symbols = pref_bindings_map[ns]
    else:
        cpp_symbols = read_cpp_analysis(ns, yaml_path, bindings_local_path, analysis_root)
        pref_bindings_map[ns] = cpp_symbols

    id = mk_id(name)

    if id not in cpp_symbols:
        return []

    return cpp_symbols[id]


def to_loc_with(lineno, colno, name_len):
    return f'{lineno}:{colno}-{colno + name_len}'


def process_file(yaml_path, bindings_local_path, files_root, analysis_root):
    records = []

    with open(os.path.join(files_root, yaml_path), 'r') as f:
        lineno = 1
        for line in f:
            m = re.match('^(- +name: )(.+)', line.rstrip())
            if m:
                prefix = m.group(1)
                name = m.group(2)

                loc = to_loc_with(lineno, len(prefix), len(name))
                sym = f'PREFS_{mk_id(name)}'

                slots = []

                cpp_syms = get_cpp_symbols(name, yaml_path, bindings_local_path, analysis_root)
                for cpp_sym in cpp_syms:
                    slots.append({
                        'slotKind': 'getter',
                        'slotLang': 'cpp',
                        'ownerLang': 'prefs',
                        'sym': cpp_sym,
                    })

                records.append({
                    'loc': loc,
                    'source': 1,
                    'syntax': 'def',
                    'pretty': f'StaticPrefs {name}',
                    'sym': sym,
                })

                records.append({
                    'loc': loc,
                    'target': 1,
                    'kind': 'def',
                    'pretty': name,
                    'sym': sym,
                })

                records.append({
                    'loc': loc,
                    'structured': 1,
                    'pretty': name,
                    'sym': sym,
                    'kind': 'prefs',
                    'implKind': 'StaticPrefs',
                    'bindingSlots': slots,
                })

            lineno += 1

    analysis_path = os.path.join(analysis_root, yaml_path)
    print('StaticPrefs: Generating', analysis_path, file=sys.stderr)

    parent = os.path.dirname(analysis_path)
    os.makedirs(parent, exist_ok=True)

    with open(analysis_path, 'w') as f:
        for r in records:
            print(json.dumps(r), file=f)


files_path = sys.argv[1]

bindings_local_path = sys.argv[2]
if bindings_local_path == 'null':
    bindings_local_path = ''

files_root = sys.argv[3]
analysis_root = sys.argv[4]

with open(files_path, 'r') as f:
    for line in f:
        process_file(line.strip(), bindings_local_path,
                     files_root, analysis_root)

```

## scripts/weblog-elb-fetch.sh
```
#!/usr/bin/env bash

# Helper to download one of the following date ranges' worth of logs to the
# current directory, as given by the argument to give to this script:
# - "yesterday" (default): The logs from just yesterday.
# - "last-full-week": The logs from the last full Sunday-Saturday range.  This
#   is calculated by asking date for "last saturday" (which is never today, even
#   if today is saturday), and then


# Here is an example S3 URI from a log directory:
# s3://searchfox-web-logs/AWSLogs/653057761566/elasticloadbalancing/us-west-2/2023/01/18/

# AFAICT in order to download a specific set of files, we need to use recursive
# and exclude everything and then only include what we want.  The following
# works:

DATE_RANGE=${1:-yesterday}

if [[ "$DATE_RANGE" == "today" ]]; then
  S3_DATE_URI=$(date -u --date='0 days ago' +s3://searchfox-web-logs/AWSLogs/653057761566/elasticloadbalancing/us-west-2/%Y/%m/%d/)
  aws s3 cp ${S3_DATE_URI} . --recursive
elif [[ "$DATE_RANGE" == "yesterday" ]]; then
  S3_DATE_URI=$(date -u --date='1 days ago' +s3://searchfox-web-logs/AWSLogs/653057761566/elasticloadbalancing/us-west-2/%Y/%m/%d/)
  aws s3 cp ${S3_DATE_URI} . --recursive
elif [[ "$DATE_RANGE" == "last-week" ]]; then
  # latch the saturday to avoid inconsistency if run around the end of the day.
  LAST_SATURDAY=$(date -u --date='last saturday')
  for ((i=0; i<=6; i++)); do
    S3_DATE_URI=$(date -u --date="${LAST_SATURDAY} -${i} days" +s3://searchfox-web-logs/AWSLogs/653057761566/elasticloadbalancing/us-west-2/%Y/%m/%d/)
    aws s3 cp ${S3_DATE_URI} . --recursive
  done
else
  echo "Unrecognized date-range: ${DATE_RANGE}"
  exit 1
fi

gunzip *.gz

```

## scripts/find-objdir-files.sh
```
#!/usr/bin/env bash

set -x # Show commands
set -eu # Errors/undefined vars are fatal
set -o pipefail # Check all commands in a pipeline

# Some repositories don't have objdir.
mkdir -p ${INDEX_ROOT}/objdir

pushd ${INDEX_ROOT}/objdir
# All text-like files in objdir are going to be reflected to the output.
#
# NOTE: exclude some known binary filename extensions to reduce the cost of
#       the file type detection.
set +o pipefail # grep can fail
find . -type f -not -regex "\.(o|out|so|a|so\..*|scip)$" -exec file --mime {} + \
    | grep -v 'charset=binary' \
    | cut -d ":" -f 1 \
    | sed -e 's#^./#__GENERATED__/#' \
    > ${INDEX_ROOT}/objdir-files
set -o pipefail
find . -mindepth 1 -type d \
    | sed -e 's#^./#__GENERATED__/#' \
    > ${INDEX_ROOT}/objdir-dirs
popd

# This is shuffled for the benefit of the "parallel" invocation in output.sh so that
# file complexity is (more) randomly distributed.  crossref.rs also now ingests
# this file too, but it doesn't inherently need the shuffling.  When we stop using
# "parallel" for output.sh, we can remove the "shuf" invocation.
cat ${INDEX_ROOT}/repo-files ${INDEX_ROOT}/objdir-files | shuf > ${INDEX_ROOT}/all-files
# This is being created for crossref.rs right now and we're not shuffling because
# we don't need to.
cat ${INDEX_ROOT}/repo-dirs ${INDEX_ROOT}/objdir-dirs > ${INDEX_ROOT}/all-dirs
# We need __GENERATED__ to exist on its own too, but it won't from the above.
echo __GENERATED__ >> ${INDEX_ROOT}/all-dirs

```

## scripts/css-analyze.sh
```
#!/usr/bin/env bash

set -x # Show commands
set -eu # Errors/undefined vars are fatal
set -o pipefail # Check all commands in a pipeline

if [ $# -ne 2 ]
then
    echo "Usage: css-analyze.sh config-file.json tree_name"
    exit 1
fi

CONFIG_FILE=$(realpath $1)
TREE_NAME=$2

cat $INDEX_ROOT/css-files | \
    parallel $MOZSEARCH_PATH/tools/target/release/css-analyze \
    $FILES_ROOT {} ">" $INDEX_ROOT/analysis/{}
echo $?

```

## scripts/web-analyze/wasm-css-analyzer/src/lib.rs
```
use tools::css_analyzer;
use wasm_bindgen::prelude::*;

#[wasm_bindgen]
pub fn analyze_css_source(text: String, first_line: u32, callback: &js_sys::Function) {
    let mut callback = |s| {
        let this = JsValue::null();
        let arg = JsValue::from(s);
        let _ = callback.call1(&this, &arg);
    };
    css_analyzer::analyze_css("".to_string(), first_line, text, &mut callback);
}

```

## scripts/web-analyze/wasm-css-analyzer/Cargo.toml
```
[package]
name = "wasm-css-analyzer"
version = "1.0.0"
authors = ["arai <arai_a@mac.com>"]
edition = "2018"

[lib]
crate-type = ["cdylib"]

[dependencies]
tools = { path = "../../../tools" }
js-sys = "0.3.69"
wasm-bindgen = "0.2.78"
getrandom = { version = "0.2.15", features = ["js"] }

[profile.release]
lto = true
codegen-units = 1
opt-level = "z"

```

## scripts/replace-aliases.sh
```
#!/usr/bin/env bash

set -x # Show commands
set -eu # Errors/undefined vars are fatal
set -o pipefail # Check all commands in a pipeline

ANALYSIS_FILES_PATH=$1

DIAGS_DIR=$INDEX_ROOT/diags/replace-aliases
mkdir -p $DIAGS_DIR
# clean up the directory since in the VM this can persist.
rm -f $DIAGS_DIR/*

JOBLOG_PATH=${DIAGS_DIR}/replace-aliases.joblog
TMPDIR_PATH=${DIAGS_DIR}

parallel --jobs 8 --pipepart -a $ANALYSIS_FILES_PATH --files --joblog $JOBLOG_PATH --tmpdir $TMPDIR_PATH \
    --block -1 --halt 2 \
    "$MOZSEARCH_PATH/scripts/replace-aliases.py $INDEX_ROOT/analysis $INDEX_ROOT/aliases/url-map.json"

```

## scripts/crossref.sh
```
#!/usr/bin/env bash

set -x # Show commands
set -eu # Errors/undefined vars are fatal
set -o pipefail # Check all commands in a pipeline

CONFIG_FILE=$(realpath $1)
TREE_NAME=$2
ANALYSIS_FILES_PATH=$3
OTHER_RESOURCES_PATH=$4

echo Root is $INDEX_ROOT

$MOZSEARCH_PATH/tools/target/release/crossref $CONFIG_FILE $TREE_NAME $ANALYSIS_FILES_PATH $OTHER_RESOURCES_PATH

# Re-sort the identifiers file so that it's case-insensitive.  (It was written
# to disk from a case-sensitive BTreeMap.)
ID_FILE=$INDEX_ROOT/identifiers
LC_ALL=C sort -f $ID_FILE > ${TMPDIR:-/tmp}/ids
mv ${TMPDIR:-/tmp}/ids $ID_FILE

```

## scripts/js-analyze.js
```
let nextSymId = 0;
let localFile, sourcePath, fileIndex, mozSearchRoot;

// The parsing mode we're currently using.
let gParsedAs = "script";
// Filename for logError to use heuristics to downgrade errors/warnings.
let gFilename = "";
// Was there an `#include` present which should downgrade errors/warnings?
let gIncludeUsed = false;
// Was the first character of the file `{`?
let gCouldBeJson = false;
// Attribute name for logError to use heuristics to downgrade errors.
let gAttrName = "";

const ERROR_INTERVENTIONS = [
  {
    includes: "expected expression, got '<': ",
    severity: "INFO",
    prepend: "React detected: ",
  },
  {
    includes: "import assertions are not currently supported: ",
    severity: "INFO",
    prepend: "Not yet supported: "
  },
  {
    includes: "illegal character",
    severity: "INFO",
    prepend: "Illegal characters are probably intentional: "
  },
  {
    includes: "invalid escape sequence:",
    severity: "INFO",
    prepend: "Invalid escapes are probably intentional: "
  },
  {
    includes: "missing ; after for-loop condition: ",
    severity: "INFO",
    prepend: "Wacky test idiom?: "
  },
  {
    includes: "expected expression, got '%'",
    severity: "INFO",
    prepend: "Probable WPT interpolation mechanism: "
  },
  // This happened on `import("./basic.css", { assert: { type: "css" } })` in
  // a WPT only in esr91 where it seems like dynamic import is hard-coded to
  // not know about the second optional args right now.  However, we do seem to
  // be implementing the assertions, so this could go away.
  //
  // That said, this type of problem in the JS code is not something searchfox
  // can do anything about, especially if our parser is mad, so this is
  // reasonable to downgrade in general.
  {
    includes: "missing ) after argument list",
    severity: "INFO",
    prepend: "(unsupported) import assertions can parse this way: "
  },
  // Another new variation on import assertions for CSS module assertions
  //
  // web-platform/tests/html/semantics/scripting-1/the-script-element/css-module-assertions/resources/integrity-matches.js:1
  // because SyntaxError: unexpected token: 'assert'
  {
    includes: "unexpected token: 'assert'",
    severity: "INFO",
    prepend: "(unsupported) import assertions can also parse this way: "
  },
  // This warning started appearing after https://bugzilla.mozilla.org/show_bug.cgi?id=1845085
  // which imported some new web-platform tests with a new import syntax that looks
  // like `import "./hello.js#3" with { type: "js" };`. It appears that this syntax is not
  // yet supported by our parser but may be supported in the future.
  {
    includes: "unexpected token: keyword 'with'",
    severity: "INFO",
    prepend: "(unsupported) import attributes can parse this way: "
  },
  {
    includes: "import attributes are not currently supported",
    severity: "INFO",
    prepend: "(unsupported) import attributes can also parse this way: "
  },
  {
    includes: "redeclaration of import",
    severity: "INFO",
    prepend: "Known buggy code pattern is not a problem: "
  },
  // Bug 1858251 landed https://github.com/web-platform-tests/wpt/pull/42467
  // which tests syntax changes from https://github.com/tc39/proposal-source-phase-imports
  // which SpiderMonkey doesn't understand yet.
  {
    includes: "after import clause",
    severity: "INFO",
    prepend: "(unsupported) source phase imports can parse this way: "
  }
];

// Note that once we can process .eslintignore most of these can go away because
// the heroic work of people like :Standard8 making eslint work means that we
// don't need hacky heuristics like this.
//
// Note: ALL INCLUDES MUST BE LOWERCASED because that's what we match against.
const FILENAME_INTERVENTIONS = [
  {
    // "dromaeo" is from:
    // https://searchfox.org/mozilla-central/source/testing/talos/talos/tests/dromaeo/test-tail.js
    //
    // dom/media/test/test_imagecapture.html had syntax error which is fixed in trunk.
    //
    // "JSTests" is wubkat.
    includes_list: ["error", "fixture", "bad", "syntax", "invalid", "dromaeo", "/jstests/",
                    "test_bug531176.html", "dom/media/test/test_imagecapture.html"],
    severity: "INFO",
    // JS engines love to have test cases that intentionally have syntax errors
    // in them.  To this end, we downgrade any such file to an info.  This
    // undoubtedly will catch some false positives but the warning mechanism
    // is about systemic issues in analysis, so we'd expect to have reports from
    // files that don't get caught by this too in that case.
    prepend: "It may intentionally be illegal JS: ",
  },
  {
    includes_list: ["parser/htmlparser/tests"],
    severity: "INFO",
    prepend: "It may be testing script tag handling: ",
  },
  {
    // Session Store has some JSON files with .js extensions.  .eslintignore
    // does already know about them, but until my work on file ingestion lands
    // we lack an easy way to filter the JS ingestion set.
    //
    // https://bugzilla.mozilla.org/show_bug.cgi?id=1792369 was filed to track
    // fixing that and we can remove this once it's fixed.
    //
    // Argh, there's actually the following too:
    // https://searchfox.org/mozilla-central/source/testing/talos/talos/startup_test/sessionrestore/profile-manywindows/sessionstore.js
    //
    // Okay, also adding "json" for
    // `dom/tests/mochitest/ajax/jquery/test/data/json_obj.js` which reports a
    // `SyntaxError: unexpected token: ':'` which could jointly be considered
    // but there may be other variations.
    //
    // Note: I've now added the gCouldBeJson mechanism which could perhaps moot
    // the need for this intervention, but I'm out of time for the day and don't
    // want to experiment with this at the risk of re-introducing warnings.
    includes_list: ["sessionstore", "json"], // be resistant to directory hierarchy changes
    severity: "INFO",
    prepend: "Could be a JSON file based on the name: ",
  },
  // mozbuild has some weird JS looking files that are not JS:
  // https://searchfox.org/mozilla-central/search?q=python%2Fmozbuild%2Fmozbuild%2Ftest%2Fbackend%2Fdata%2Fbuild%2Fbar.js&path=
  {
    includes_list: ["mozbuild"],
    severity: "INFO",
    prepend: "mozbuild has weird files: ",
  },
  {
    // "devtools/client/shared/vendor/jszip.js" is a UMD that makes things
    // angry and it's believable we could have a bunch of these.
    //
    // also: third_party/libwebrtc/tools/grit/grit/testdata/test_js.js
    includes_list: ["vendor", "third_party"],
    severity: "INFO",
    prepend: "Vendored files can be weird: ",
  },
  {
    // pref files can be weird / annoying, ex:
    // https://searchfox.org/mozilla-central/source/testing/condprofile/condprof/tests/profile/user.js
    // there's also things like `channel-prefs.js`, so I've decided not to
    // include a slash before either of these.
    //
    // Also explicitly excluding the libpref tree for
    // `modules/libpref/test/unit/data/testParser.js`.
    //
    // Also mozprofile has `prefs_with_comments.js` and could gain others.
    //
    // https://searchfox.org/l10n/source/tn/mail/all-l10n.js is an l10n example
    // of a pref file; there are others like `firefox-l10n.js`.
    includes_list: ["user.js", "prefs.js", "firefox.js", "/libpref/", "/mozprofile/", "-l10n.js"],
    severity: "INFO",
    prepend: "Prefs files can be weird: ",
  },
  {
    // `js/src/devtools/rootAnalysis/build.js` is a new thing but it was also
    // a case where a mozconfig had a .js syntax.
    includes_list: ["/rootanalysis/"], // lowercased to match the subject
    severity: "INFO",
    prepend: "rootAnalysis does some weird custom stuff: "
  },
  {
    // There are ton of things that are clearly templating under
    // toolkit/components/uniffi-bindgen-gecko-js/src/templates
    includes_list: ["template"],
    severity: "INFO",
    prepend: "May be templated JS: "
  },
  {
    // there are a bunch of things that make us sad under tools/lint/test/files/
    includes_list: ["lint"],
    severity: "INFO",
    prepend: "May be a linting test case: ",
  },
  {
    // testing/mochitest/MochiKit/Controls.js:578 is missing a close paren
    includes_list: ['/mochikit/'], // lowecased to match the subject
    severity: "INFO",
    prepend: "Legacy weird MochiKit stuff: ",
  },
  // testing/web-platform/meta/screen-wake-lock/wakelock-insecure-context.any.js
  // is an example of a file where a typo left off the ".ini" suffix.  There's
  // never going to be any actual JS under the meta dir.
  {
    includes_list: ["testing/web-platform/meta/"],
    severity: "INFO",
    prepend: "Someone forgot to add an .ini suffix: ",
  },
  {
    // .sub.js is an explicit WPT (idiom?) thing but there are permutations so
    // we need to just detect on `.sub.`, like `.sub.window.js`, and
    // `.sub.h2.any.js`.

    // `testing/web-platform/tests/cors/support.js` is also using the
    // replacement mechanism but doesn't follow the idiom.
    includes_list: [".sub.", "/support.js"],
    severity: "INFO",
    prepend: "Substitution JS files are usually not legal JS on their own: ",
  },
  // testing/web-platform/tests/html/semantics/scripting-1/the-script-element/import-assertions/dynamic-import-with-assertion-argument.any.js
  // is an example where we get a `missing ) after argument list` instead of the
  // explicit lack of support error.
  // There are also some other cases under "/json-module/" where "json" seems to
  // save us.
  {
    includes_list: ["/import-assertions/"],
    severity: "INFO",
    prepend: "Import assertions not yet supported and may parse weird: ",
  },
  {
    // There's a bunch of syntax errors in suite code; this should ideally be
    // handled via a repo settings.  I had made this specific to comm-central
    // at first but we have ESR versions we index too, so this is now more
    // general.
    includes_list: ["/suite/"],
    severity: "INFO",
    prepend: "Unmaintained code: "
  },
  {
    // https://searchfox.org/mozilla-vpn-client/source/glean/org/mozilla/Glean/glean.js
    // is apparently a QML js file that uses a weird ".import" and ".pragma"
    // syntax that's not legit JS, obviously.
    includes_list: ["glean.js"],
    severity: "INFO",
    prepend: "May be weird QML file: "
  },
  {
    includes_list: ["/puppeteer/"],
    severity: "INFO",
    prepend: "Puppeteer has weird JS in old m-c trees: "
  },
  {
    // dom/base/crashtests/1822717-module.js is an example.
    includes_list: ["/crashtests/"],
    severity: "INFO",
    prepend: "Crashtest may intentionally contain syntax errors: "
  },
  {
    // mozilla-central proper provides the coverage we need, whereas we have an
    // ever-growing list of ESR JS code that never gets updated.  These are
    // being added for config3 which is home to our oldest ESR code.
    includes_list: ["/mozilla-esr", "/comm-esr"],
    severity: "INFO",
    prepend: "ESR failsafe: "
  },
  {
    // We still have a lot of wubkat warnings.  The is a bulk silencing, but
    // patches would be accepted to eliminate this in conjunction with the
    // addition of more specific interventions.
    //
    // I'm not adding more interventions for this right now due to:
    // - time limitations
    // - the potential for .eslintignore hook-up to perhaps moot all of these
    //   interventions
    // - our plan to replace this file with scip-typescript; our reason for
    //   logging any warnings here is to make sure we don't have coverage gaps,
    //   but when we move to scip-typescript, the quality assurance comes from
    //   scip-typescript itself, not us.
    includes_list: ["/wubkat/"],
    severity: "INFO",
    prepend: "Wubkat failsafe: "
  },
  {
    includes_list: ["devtools/client/debugger/test/mochitest/examples/inline-preview.js",
                    "devtools/client/debugger/test/mochitest/examples/preview.js"],
    severity: "INFO",
    prepend: "It may contain experimental syntax: ",
  },
];

function logError(msg)
{
  // We log "errors" as warnings so the searchfox warning script will report it.
  let severity = "WARN";

  // But we also have some heuristics defined above that let us downgrade
  // expected problems to INFO.  Ideally these would be logged as diagnostic
  // records as proposed at https://bugzilla.mozilla.org/show_bug.cgi?id=1789515
  // but our expected migration to scip-typescript means it's probably not worth
  // it at this time, or at least not until we have the rest of the
  // diagnostic analysis record mechanism implemented.
  for (const intervention of ERROR_INTERVENTIONS) {
    if (msg.includes(intervention.includes)) {
      severity = intervention.severity;
      msg = "Downgrading warning to info because: " + intervention.prepend + msg;
      break;
    }
  }

  outer: for (const intervention of FILENAME_INTERVENTIONS) {
    let file_lower = gFilename?.toLowerCase();
    for (const include_entry of intervention.includes_list) {
      if (file_lower?.includes(include_entry)) {
        severity = intervention.severity;
        msg = "Downgrading warning to info because: " + intervention.prepend + msg;
        break outer;
      }
    }
  }

  if (gAttrName && (gFilename.includes("/test/") || gFilename.includes("/tests/"))) {
    severity = "INFO";
    msg = "Downgrading warning to info because attributes can sometimes contain syntax error in tests";
  }

  // https://searchfox.org/mozilla-central/source/browser/components/enterprisepolicies/schemas/schema.jsm
  // is an example of a file that does `const schema =` and then the next line
  // is an include and since we don't actually include things, things can break.
  // An enhancement would be accepted to try and do better, but this can't be a
  // supported feature at this time without a maintainer for it.
  if (severity === "WARN" && gIncludeUsed) {
    severity = "INFO";
    msg = `Downgrading warning to info because #include was used: ${msg}`;
  }
  if (severity === "WARN" && gCouldBeJson && msg.includes("SyntaxError: unexpected token")) {
    severity = "INFO";
    msg = `Downgrading warning to info because file could be JSON because it starts with '{': ${msg}`;
  }

  // This means we may end up needing to add a bunch of tree-specific
  // exclusions, which is probably fine.
  printErr(`${severity} when parsing as '${gParsedAs}': ${msg}\n`);
}

function SymbolTable()
{
  this.table = new Map();
}

SymbolTable.prototype = {
  put(name, symbol) {
    this.table.set(name, symbol);
  },

  get(name) {
    return this.table.get(name);
  },
};

SymbolTable.Symbol = function(name, loc)
{
  this.name = name;
  this.loc = loc;
  this.id = fileIndex + "-" + nextSymId++;
  this.uses = [];
  this.skip = false;
}

SymbolTable.Symbol.prototype = {
  use(loc) {
    this.uses.push(loc);
  },
};

function isSameLocation(loc1, loc2) {
  return loc1.start.line == loc2.start.line &&
    loc1.start.column == loc2.start.column &&
    loc1.end.line == loc2.end.line &&
    loc1.end.column == loc2.end.column;
}

function posBefore(pos1, pos2) {
  return pos1.line < pos2.line ||
         (pos1.line == pos2.line && pos1.column < pos2.column);
}

function locBefore(loc1, loc2) {
  return posBefore(loc1.start, loc2.start);
}

function locstr(loc)
{
  // mozsearch token columns are 0-based but SpiderMonkey's are now 1-based since
  // bug 1862692.
  return `${loc.start.line}:${loc.start.column - 1}`;
}

function locstr2(loc, str)
{
  // mozsearch token columns are 0-based but SpiderMonkey's are now 1-based since
  // bug 1862692.
  return `${loc.start.line}:${loc.start.column - 1}-${loc.start.column - 1 + str.length}`;
}

function locstrFull(startPos, endPos)
{
  // mozsearch token columns are 0-based but SpiderMonkey's are now 1-based since
  // bug 1862692.
  return `${startPos.line}:${startPos.column - 1}-${endPos.line}:${endPos.column - 1}`;
}

/**
 * Given an ESTree node, return true if it's potentially something that should
 * generate a nestingRange.  For our purposes, this means something that has
 * curly braces and is likely to span more than a single line of text.
 *
 * In the future this method might need to return the appropriate Location to
 * use rather than a boolean.  Right now the caller is expected to use the `loc`
 * of the provided node if we return true.
 */
function isNestingNode(node) {
  if (!node || !node.type) {
    return false
  }

  switch (node.type) {
    case "BlockStatement":
    case "FunctionExpression":
    case "ObjectExpression":
    case "ObjectPattern":
      return true;
    default:
      return false;
  }
}

function nameValid(name)
{
  if (!name) {
    return false;
  }
  for (var i = 0; i < name.length; i++) {
    var c = name.charCodeAt(i);
    switch (c) {
      case 0:  // '\0'
      case 10: // '\n'
      case 13: // '\r'
      case 32: // ' '
      case 34: // '"'
      case 92: // '\\'
        return false;
    }

    // If we have a Unicode surrogate character, make sure
    // it is a part of a valid surrogate pair, otherwise return false.

    if (c < 0xD800) {
      // Optimize common case
      continue;
    }
    if (c <= 0xDBFF && i + 1 < name.length) {
      // c is a high surrogate, check to make sure next char is a low surrogate
      var d = name.charCodeAt(i + 1);
      if (d >= 0xDC00 && d <= 0xDFFF) {
        // valid; skip over the pair and continue
        i++;
        continue;
      }
    }
    // fail on any surrogate characters that weren't part of a pair
    if (c <= 0xDFFF) {
      return false;
    }
  }
  return true;
}

function memberPropLoc(expr)
{
  // XXX this seems sketchy in terms of seeming like it thinks it is performing
  // a copy followed a mutation but that's not what is happening.  However, this
  // code is from the initial landing of searchfox so I'm not touching it right
  // now.
  let idLoc = expr.loc;
  idLoc.start.line = idLoc.end.line;
  // (we do not change the 1-base column to a 0-based column here; that will
  // happen in locstr2)
  idLoc.start.column = idLoc.end.column - expr.property.name.length;
  return idLoc;
}

function atEscape(text) {
  return text.replace(/[^A-Za-z0-9_/]/g, matched => "@" + matched.charCodeAt(0).toString(16).toUpperCase().padStart(2, "0"));
}

/**
 * Stateful singleton that assumes this script is run once per file.  General
 * structure is a imperative, recursive traversal of the
 * available-in-its-entirety JS AST.  There isn't really any streaming
 * processing and everything is kept on the stack.
 *
 * XBL is a special-case via `XBLParser`.  It is dealing with single atomic
 * chunks of JS that exist in namespace
 */
let Analyzer = {
  /**
   * The symbol table for the current scope.  When `enter` is invoked, the
   * current `symbols` table is pushed onto `symbolTableStack` and a new
   * SymbolTable is created and assigned to `symbols`.  When `exit` is invoked,
   * the current `symbols` table is discarded and replaced by popping
   * `symbolTableStack`.
   */
  symbols: new SymbolTable(),
  /**
   * Stack of `SymbolTable` instances corresponding to scopes that are reachable
   * from the current scope.  Does not include the immediate scope which is
   * found in `symbols`.
   */
  symbolTableStack: [],

  /**
   * Tracks the name of the current variable declaration so that qualified names
   * can be inferred.  When nesting occurs, the previous value is saved off on
   * the stack while call to recursive AST traversal occurs, and is restored on
   * the way out.  No attempt is currently made to infer deeply nested names,
   * just a single level, so this works as long as that assumption is okay.
   * (Note however that `contextStack` does track this nesting.)
   *
   * Specialization occurs for cases like "prototype".
   */
  nameForThis: null,
  /**
   * Tracks explicit ES "class" names.  As with `nameForThis`, nesting happens
   * on the stack so that context isn't lost, but those names are ignored for
   * symbol naming purposes.  (Note however that `contextStack` does track this
   * nesting.)
   */
  className: null,
  /**
   * Used to derive the "context" property for target records.  Whenever
   * `symbolTableStack`, `nameForThis`, or `className` are modified, the name
   * (possibly falsey) that is being used for the thing is pushed.  When
   * traversing an ObjectExpression or ObjectPattern, the key is also pushed.
   * (Object "dictionaries" like `{ a: { b: 1 } }` create a name hierarchy for
   * "a.b" but do not create lexical scopes on their own.)
   */
  contextStack: [],

  // Program lines.  Initialized by parse.  Used for getting back to program
  // source given a SourceLocation/Position.  For JS files, this should be
  // populated once.  For XUL/XBL files that invoke parse() multiple times with
  // a new, non-consecutive `line` each time, the missing lines are padded out
  // with empty strings.
  _lines: [],

  resetState() {
    this.symbols = new SymbolTable();
    this.symbolTableStack = [];
    this.nameForThis = null;
    this.className = null;
    this.contextStack = [];
    this._lines = [];
  },

  /**
   * Given a position, find the first instance of the given string starting
   * after the (exclusive, end) position.
   */
  findStrAfterPosition(str, pos) {
    // (lines are 1-based)
    let lineText = this._lines[pos.line - 1];
    if (!lineText) {
      return null;
    }
    // indexOf uses a 0-based position whereas column is 1-based but also
    // intended to be exclusive, so we subtract 1 off.
    let idx = lineText.indexOf(str, pos.column - 1);
    if (idx === -1) {
      return null;
    }
    return {
      line: pos.line,
      column: idx
    };
  },

  /**
   * If you've got some kind of outerNode like a ClassStatement where the left
   * brace comes after a node like its "id" node, use this.  The outerNode's
   * position gives the end Location and the first { found after the idNode
   * gives the start.  (Note that the end location is still chosen to be after
   * the right brace for consistency with BlockStatements.)
   */
  deriveLocationFromOuterNodeAndIdNode(outerNode, idNode) {
    let start = this.findStrAfterPosition('{', idNode.loc.end);
    if (!start) {
      return null;
    }

    return {
      start,
      end: outerNode.loc.end
    };
  },

  /**
   * Enter a new lexical scope, pushing both a new SymbolTable() to track
   * symbols defined in this scope, as well as pushing onto the contextStack
   * for "context" attribute generation purposes.
   */
  enter(name) {
    this.symbolTableStack.push(this.symbols);
    this.symbols = new SymbolTable();

    this.contextStack.push(name);
  },

  exit() {
    let old = this.symbols;
    this.symbols = this.symbolTableStack.pop();
    this.contextStack.pop();
    return old;
  },


  isToplevel() {
    return this.symbolTableStack.length == 0;
  },

  /**
   * Syntactic sugar helper to enter(name) the (potentially falsey) named
   * lexical scope, invoke the provided helper, then exit() the scope off the
   * scope/context stack.
   */
  scoped(name, f) {
    this.enter(name);
    f();
    this.exit();
  },

  get context() {
    return this.contextStack.filter(e => !!e).join(".");
  },

  dummyProgram(prog, args) {
    let stmt = prog.body[0];
    let expr = stmt.expression;

    for (let {name, skip} of args) {
      let sym = new SymbolTable.Symbol(name, null);
      sym.skip = true;
      this.symbols.put(name, sym);
    }

    if (expr.body.type == "BlockStatement") {
      this.statement(expr.body);
    } else {
      this.expression(expr.body);
    }
  },

  parse(text, filename, line, target, attrName="") {
    gAttrName = attrName;

    let ast;
    try {
      gParsedAs = target;
      try {
        ast = Reflect.parse(text, { loc: true, source: filename, line, target: gParsedAs });
      } catch (ex) {
        // If we were trying to parse something as script and it had an import,
        // attempt to re-parse it as a module.
        if ((ex.message.includes("import declarations may only appear") ||
             ex.message.includes("export declarations may only appear") ||
             // await is valid at the top-level in modules, so re-parse as a
             // module in this case too
             ex.message.includes("await is only valid in") ||
             ex.message.includes("import.meta may only appear in a module") ||
             text.includes("await import")) &&
            gParsedAs === "script") {
          gParsedAs = "module";
          ast = Reflect.parse(text, { loc: true, source: filename, line, target: gParsedAs });
        } else {
          // just re-throw because it didn't seem to be an import error.
          throw ex;
        }
      }

      let parsedLines = text.split('\n');

      if (line === 1) {
        this._lines = parsedLines;
      } else {
        // In the case of XUL/XBL, we are given random (processed) excerpts of
        // JS code with `line` representing the first line in the XML file where
        // the JS was sourced from.
        //
        // As such, we need to grow the array and insert the parsed lines so
        // that when we lookup the source JS from the AST the lines line up.
        let linesToInsert = line - this._lines.length - 1;
        while (linesToInsert-- > 0) {
          this._lines.push('');
        }
        this._lines.push(...parsedLines);
      }

    } catch (e) {
      const maybeAttr = attrName ? ` ${attrName.toLowerCase()} attribute` : '';
      logError(`Unable to parse JS file ${filename}:${line}${maybeAttr} because ${e}: ${e.fileName}:${e.lineNumber}`);
      return null;
    }
    return ast;
  },

  program(prog) {
    for (let stmt of prog.body) {
      this.statement(stmt);
    }
  },

  // maybeNesting allows passing a SourceLocation directly or a Node.  The node
  // is tested via a call to `isNestingNode` to determine whether it's an
  // appropriate type for its `loc` to be used.  This allows callers to pass
  // nodes without first checking their type.
  source(loc, name, syntax, pretty, sym, no_crossref, maybeNesting) {
    let locProp;
    if (typeof(loc) == "object" && "start" in loc) {
      locProp = locstr2(loc, name);
    } else {
      locProp = loc;
    }
    let obj = {loc: locProp, source: 1, syntax, pretty, sym};
    if (no_crossref) {
      obj.no_crossref = 1;
    }
    if (maybeNesting) {
      let nestLoc;
      if (maybeNesting.start) {
        nestLoc = maybeNesting;
      } else if (isNestingNode(maybeNesting)) {
        nestLoc = maybeNesting.loc;
      }
      if (nestLoc) {
        // substract 1 off the end column so that it points at a
        // closing brace rather than just beyond the closing brace.  This is desired for
        // the nestingRange where the goal is to reference the opening and closing
        // brace tokens directly.
        let adjustedEnd = { line: nestLoc.end.line, column: nestLoc.end.column };
        adjustedEnd.column--;
        // Handle the case where we wrap to a previous line as well, ensuring we
        // don't wrap backwards past the start position.
        while (adjustedEnd.column < 0 && posBefore(nestLoc.start, adjustedEnd)) {
          adjustedEnd.line--;
          // SM columns are now 1-based and locstrFull handles that, so we don't
          // subtract 1 off the length here.
          adjustedEnd.column = this._lines[adjustedEnd.line - 1].length;
        }
        obj.nestingRange = locstrFull(nestLoc.start, adjustedEnd);
      }
    }
    print(JSON.stringify(obj));
  },

  target(loc, name, kind, pretty, sym) {
    let locProp;
    if (typeof(loc) == "object" && "start" in loc) {
      locProp = locstr2(loc, name);
    } else {
      locProp = loc;
    }
    print(JSON.stringify({loc: locProp, target: 1, kind, pretty, sym,
                          context: this.context}));
  },

  defProp(name, loc, extra, extraPretty, maybeNesting) {
    if (!nameValid(name)) {
      return;
    }
    this.source(loc, name, "def,prop", `property ${name}`, `#${name}`, false,
                maybeNesting);
    this.target(loc, name, "def", name, `#${name}`);
    if (extra) {
      this.source(loc, name, "def,prop", `property ${extraPretty}`, extra,
                  false, maybeNesting);
      this.target(loc, name, "def", extraPretty, extra);
    }
  },

  useProp(name, loc, extra, extraPretty) {
    if (!nameValid(name)) {
      return;
    }
    this.source(loc, name, "use,prop", `property ${name}`, `#${name}`, false);
    this.target(loc, name, "use", name, `#${name}`);
    if (extra) {
      this.source(loc, name, "use,prop", `property ${extraPretty}`, extra,
                  false);
      this.target(loc, name, "use", extraPretty, extra);
    }
  },

  assignProp(name, loc, extra, extraPretty, maybeNesting) {
    if (!nameValid(name)) {
      return;
    }
    this.source(loc, name, "use,prop", `property ${name}`, `#${name}`, false,
                maybeNesting);
    this.target(loc, name, "assign", name, `#${name}`);
    if (extra) {
      this.source(loc, name, "use,prop", `property ${extraPretty}`, extra,
                  false, maybeNesting);
      this.target(loc, name, "assign", extraPretty, extra);
    }
  },

  defVar(name, loc, maybeNesting) {
    if (!nameValid(name)) {
      return;
    }
    if (this.isToplevel()) {
      this.defProp(name, loc, undefined, undefined, maybeNesting);
      return;
    }
    let sym = new SymbolTable.Symbol(name, loc);
    this.symbols.put(name, sym);

    this.source(loc, name, "deflocal,variable", `variable ${name}`, sym.id, true,
                maybeNesting);
  },

  findSymbol(name) {
    let sym = this.symbols.get(name);
    if (!sym) {
      for (let i = this.symbolTableStack.length - 1; i >= 0; i--) {
        sym = this.symbolTableStack[i].get(name);
        if (sym) {
          break;
        }
      }
    }
    return sym;
  },

  useVar(name, loc) {
    if (!nameValid(name)) {
      return;
    }
    let sym = this.findSymbol(name);
    if (!sym) {
      this.useProp(name, loc);
    } else if (!sym.skip) {
      this.source(loc, name, "uselocal,variable", `variable ${name}`, sym.id, true);
    }
  },

  assignVar(name, loc) {
    if (!nameValid(name)) {
      return;
    }
    let sym = this.findSymbol(name);
    if (!sym) {
      this.assignProp(name, loc);
    } else if (!sym.skip) {
      this.source(loc, name, "uselocal,variable", `variable ${name}`, sym.id, true);
    }
  },

  functionDecl(f) {
    for (let i = 0; i < f.params.length; i++) {
      this.pattern(f.params[i]);
      this.maybeExpression(f.defaults[i]);
    }
    if (f.rest) {
      this.defVar(f.rest.name, f.rest.loc);
    }
    if (f.body.type == "BlockStatement") {
      this.statement(f.body);
    } else {
      this.expression(f.body);
    }
  },

  statement(stmt) {
    switch (stmt.type) {
    case "EmptyStatement":
    case "BreakStatement":
    case "ContinueStatement":
    case "DebuggerStatement":
      break;

    case "BlockStatement":
      this.scoped(null, () => {
        for (let stmt2 of stmt.body) {
          this.statement(stmt2);
        }
      });
      break;

    case "ExpressionStatement":
      this.expression(stmt.expression);
      break;

    case "IfStatement":
      this.expression(stmt.test);
      this.statement(stmt.consequent);
      this.maybeStatement(stmt.alternate);
      break;

    case "LabeledStatement":
      this.statement(stmt.body);
      break;

    case "WithStatement":
      this.expression(stmt.object);
      this.statement(stmt.body);
      break;

    case "SwitchStatement":
      this.expression(stmt.discriminant);
      for (let scase of stmt.cases) {
        this.switchCase(scase);
      }
      break;

    case "ReturnStatement":
      this.maybeExpression(stmt.argument);
      break;

    case "ThrowStatement":
      this.expression(stmt.argument);
      break;

    case "TryStatement":
      this.statement(stmt.block);
      if (stmt.handler) {
        this.catchClause(stmt.handler);
      }
      this.maybeStatement(stmt.finalizer);
      break;

    case "WhileStatement":
      this.expression(stmt.test);
      this.statement(stmt.body);
      break;

    case "DoWhileStatement":
      this.statement(stmt.body);
      this.expression(stmt.test);
      break;

    case "ForStatement":
      this.scoped(null, () => {
        if (stmt.init && stmt.init.type == "VariableDeclaration") {
          this.variableDeclaration(stmt.init);
        } else if (stmt.init) {
          this.expression(stmt.init);
        }
        this.maybeExpression(stmt.test);
        this.maybeExpression(stmt.update);
        this.statement(stmt.body);
      });
      break;

    case "ForInStatement":
    case "ForOfStatement":
      this.scoped(null, () => {
        if (stmt.left && stmt.left.type == "VariableDeclaration") {
          this.variableDeclaration(stmt.left);
        } else {
          this.expression(stmt.left);
        }
        this.expression(stmt.right);
        this.statement(stmt.body);
      });
      break;

    case "LetStatement":
      this.scoped(null, () => {
        for (let decl of stmt.head) {
          this.variableDeclarator(decl);
        }
        this.statement(stmt.body);
      });
      break;

    case "FunctionDeclaration":
      this.defVar(stmt.id.name, stmt.loc, stmt.body);
      this.scoped(stmt.id.name, () => {
        this.functionDecl(stmt);
      });
      break;

    case "VariableDeclaration":
      this.variableDeclaration(stmt);
      break;

    //
    case "ClassStatement":
      this.defVar(stmt.id.name, stmt.id.loc,
                  this.deriveLocationFromOuterNodeAndIdNode(stmt, stmt.id));
      this.scoped(stmt.id.name, () => {
        let oldClass = this.className;
        this.className = stmt.id.name;
        if (stmt.superClass) {
          this.expression(stmt.superClass);
        }
        for (let stmt2 of stmt.body) {
          this.statement(stmt2);
        }
        this.className = oldClass;
      });
      break;

    case "ClassMethod": {
      let name = null;
      if (stmt.name.type == "Identifier") {
        name = stmt.name.name;
        this.defProp(
          stmt.name.name, stmt.name.loc,
          `${this.className}#${name}`, `${this.className}.${name}`,
          stmt.body);
      }

      this.scoped(name, () => {
        if (stmt.body.type == "FunctionExpression") {
          // Don't want to find the name twice.
          this.functionDecl(stmt.body);
        } else {
          this.expression(stmt.body);
        }
      });
      break;
    }

    // Class fields: https://github.com/tc39/proposal-class-fields
    // These are defined to have Object.defineProperty semantics.  The spec also
    // introduces private fields and these are partially supported, but
    // bug 1559269 disabled TokenStream support for them, so we don't support
    // them for now.
    case "ClassField": {
      let name = null;
      // name could be a computed name!
      if (stmt.name.type == "Identifier") {
        name = stmt.name.name;
        this.defProp(
          stmt.name.name, stmt.name.loc,
          `${this.className}#${name}`, `${this.className}.${name}`);
      }
      this.contextStack.push(name);
      if (stmt.init) {
        this.expression(stmt.init);
      }
      this.contextStack.pop();
      break;
    }

    case "StaticClassBlock": {
      this.statement(stmt.body);
      break;
    }

    case "ImportDeclaration": {
      for (const spec of stmt.specifiers) {
        if (spec.type === "ImportSpecifier" ||
            spec.type === "ImportNamespaceSpecifier") {
          this.pattern(spec.name);

          if (spec.type === "ImportSpecifier" &&
              !isSameLocation(spec.id.loc, spec.name.loc)) {
            this.expression(spec.id);
          }
        }
      }

      if (stmt.moduleRequest && stmt.moduleRequest.source &&
          stmt.moduleRequest.source.type === "Literal") {
        this.maybeLinkifyModuleSpecifier(stmt.moduleRequest.source);
      }
      break;
    }

    case "ExportDeclaration": {
      if (stmt.declaration) {
        if (stmt.declaration.type === "FunctionDeclaration") {
          if (stmt.declaration.id) {
            this.statement(stmt.declaration);
          }
        }
        else if (stmt.declaration.type === "VariableDeclaration" ||
                 stmt.declaration.type === "ClassStatement") {
          this.statement(stmt.declaration);
        } else {
          this.expression(stmt.declaration);
        }
      }

      if (stmt.specifiers) {
        for (const spec of stmt.specifiers) {
          if (spec.type === "ExportSpecifier" ||
              spec.type === "ExportNamespaceSpecifier") {
            if (spec.name.type !== "Literal") {
              this.pattern(spec.name);
            }

            if (spec.type === "ExportSpecifier" &&
                !isSameLocation(spec.id.loc, spec.name.loc)) {
              this.expression(spec.id);
            }
          }
        }
      }

      if (stmt.moduleRequest && stmt.moduleRequest.source &&
          stmt.moduleRequest.source.type === "Literal") {
        this.maybeLinkifyModuleSpecifier(stmt.moduleRequest.source, true);
      }
      break;
    }

    default:
      throw "Unexpected statement: " + stmt.type + " " + JSON.stringify(stmt);
      break;
    }
  },

  variableDeclaration(decl) {
    for (let d of decl.declarations) {
      this.variableDeclarator(d);
    }
  },

  variableDeclarator(decl) {
    this.pattern(decl.id);

    let oldNameForThis = this.nameForThis;
    if (decl.id.type == "Identifier" && decl.init) {
      if (decl.init.type == "ObjectExpression") {
        this.nameForThis = decl.id.name;
      } else {
        // Handle Object.freeze({...})
      }
    }
    this.contextStack.push(this.nameForThis);
    this.maybeExpression(decl.init);
    this.contextStack.pop();
    this.nameForThis = oldNameForThis;
  },

  maybeStatement(stmt) {
    if (stmt) {
      this.statement(stmt);
    }
  },

  maybeExpression(expr) {
    if (expr) {
      this.expression(expr);
    }
  },

  switchCase(scase) {
    if (scase.test) {
      this.expression(scase.test);
    }
    for (let stmt of scase.consequent) {
      this.statement(stmt);
    }
  },

  catchClause(clause) {
    if (clause.param) {
      this.pattern(clause.param);
    }
    if (clause.guard) {
      this.expression(clause.guard);
    }
    this.statement(clause.body);
  },

  expression(expr) {
    if (!expr) print(Error().stack);

    switch (expr.type) {
    case "Identifier":
      this.useVar(expr.name, expr.loc);
      break;

    case "Literal":
      this.maybeLinkifyLiteral(expr);
      break;

    case "Super":
      break;

    case "TemplateLiteral":
      for (let elt of expr.elements) {
        this.expression(elt);
      }
      break;

    case "TaggedTemplate":
      // Do something eventually!
      break;

    case "ThisExpression":
      // Do something eventually!
      break;

    case "ArrayExpression":
    case "ArrayPattern":
      for (let elt of expr.elements) {
        this.maybeExpression(elt);
      }
      break;

    case "ObjectExpression":
    case "ObjectPattern":
      for (let prop of expr.properties) {
        if (prop.type === "SpreadExpression") {
          this.expression(prop.expression);
          continue;
        }

        let name;

        if (prop.key) {
          let loc;
          if (prop.key.type == "Identifier") {
            name = prop.key.name;
            loc = prop.key.loc;
          } else if (prop.key.type == "Literal" && typeof(prop.key.value) == "string") {
            name = prop.key.value;
            loc = prop.key.loc;
            loc.start.column++;
          }
          let extra = null;
          let extraPretty = null;
          if (this.nameForThis) {
            extra = `${this.nameForThis}#${name}`;
            extraPretty = `${this.nameForThis}.${name}`;
          }
          if (name) {
            this.defProp(name, prop.key.loc, extra, extraPretty, prop.value);
          }
        }

        this.contextStack.push(name);
        if (prop.value) {
          this.expression(prop.value);
        }
        this.contextStack.pop();
      }
      break;

    case "FunctionExpression":
    case "ArrowFunctionExpression":
      // In theory this could declare a variable that can be used in
      // the function. But most of the time, it appears on class
      // methods that don't actually define such a variable. This is
      // probably a SpiderMonkey bug. We just don't do anything here
      // to be correct in the common case.
      //let name = expr.id ? expr.id.name : "";
      let name = null;
      this.scoped(name, () => {
        if (this.className && name == this.className) {
          // SPIDERMONKEY HACK: Fixes a bug where constructors get the
          // name of their class instead of "constructor".
          name = "constructor";
        }

        if (expr.type == "FunctionExpression" && name) {
          this.defVar(name, expr.loc);
        }

        this.functionDecl(expr);
      });
      break;

    case "SequenceExpression":
      for (let elt of expr.expressions) {
        this.expression(elt);
      }
      break;

    case "UnaryExpression":
    case "UpdateExpression":
      this.expression(expr.argument);
      break;

    case "AssignmentExpression":
      if (expr.left.type == "Identifier") {
        this.assignVar(expr.left.name, expr.left.loc);
      } else if (expr.left.type == "MemberExpression" && !expr.left.computed) {
        this.expression(expr.left.object);

        let extra = null;
        let extraPretty = null;
        if (expr.left.object.type == "ThisExpression" && this.nameForThis) {
          extra = `${this.nameForThis}#${expr.left.property.name}`;
          extraPretty = `${this.nameForThis}.${expr.left.property.name}`;
        } else if (expr.left.object.type == "Identifier") {
          extra = `${expr.left.object.name}#${expr.left.property.name}`;
          extraPretty = `${expr.left.object.name}.${expr.left.property.name}`;
        }
        this.assignProp(expr.left.property.name, memberPropLoc(expr.left), extra, extraPretty,
                        expr.right.loc);
      } else {
        this.expression(expr.left);
      }

      let oldNameForThis = this.nameForThis;
      if (expr.left.type == "MemberExpression" &&
          !expr.left.computed)
      {
        if (expr.left.property.name == "prototype" &&
            expr.left.object.type == "Identifier")
        {
          this.nameForThis = expr.left.object.name;
        }
        if (expr.left.object.type == "ThisExpression") {
          this.nameForThis = expr.left.property.name;
        }
      }
      this.contextStack.push(this.nameForThis);
      this.expression(expr.right);
      this.contextStack.pop();
      this.nameForThis = oldNameForThis;
      break;

    case "BinaryExpression":
    case "LogicalExpression":
      this.expression(expr.left);
      this.expression(expr.right);
      break;

    case "ConditionalExpression":
      this.expression(expr.test);
      this.expression(expr.consequent);
      this.expression(expr.alternate);
      break;

    case "NewExpression":
    case "CallExpression":
    case "OptionalCallExpression":
      this.expression(expr.callee);
      for (let arg of expr.arguments) {
        this.expression(arg);
      }
      break;

    case "MemberExpression":
    case "OptionalMemberExpression":
      this.expression(expr.object);
      if (expr.computed) {
        this.expression(expr.property);
      } else {
        let extra = null;
        let extraPretty = null;
        if (expr.object.type == "ThisExpression" && this.nameForThis) {
          extra = `${this.nameForThis}#${expr.property.name}`;
          extraPretty = `${this.nameForThis}.${expr.property.name}`;
        } else if (expr.object.type == "Identifier") {
          extra = `${expr.object.name}#${expr.property.name}`;
          extraPretty = `${expr.object.name}.${expr.property.name}`;
        }

        this.useProp(expr.property.name, memberPropLoc(expr), extra, extraPretty);
      }
      break;

    case "YieldExpression":
      this.maybeExpression(expr.argument);
      break;

    case "SpreadExpression":
      this.expression(expr.expression);
      break;

    case "ComprehensionExpression":
    case "GeneratorExpression":
      this.scoped(null, () => {
        let before = locBefore(expr.body.loc, expr.blocks[0].loc);
        if (before) {
          this.expression(expr.body);
        }
        for (let block of expr.blocks) {
          this.comprehensionBlock(block);
        }
        this.maybeExpression(expr.filter);
        if (!before) {
          this.expression(expr.body);
        }
      });
      break;

    case "ClassExpression":
      this.scoped(null, () => {
        if (expr.superClass) {
          this.expression(expr.superClass);
        }
        for (let stmt2 of expr.body) {
          this.statement(stmt2);
        }
      });
      break;

    case "OptionalExpression":
    case "DeleteOptionalExpression":
      // a?.b is an optional expression that is equivalent to a && a.b.
      // expr.expression is an OptionalMemberExpression or OptionalCallExpression
      this.expression(expr.expression);
      break;

    case "MetaProperty": // Not sure what this is!

    case "CallImport":
      if (expr.arguments && expr.arguments.length > 0 &&
          expr.arguments[0].type === "Literal") {
        this.maybeLinkifyModuleSpecifier(expr.arguments[0]);
      }
      break;

    default:
      printErr(Error().stack);
      throw `Invalid expression ${expr.type}: ${JSON.stringify(expr)}`;
      break;
    }
  },

  comprehensionBlock(block) {
    switch (block.type) {
    case "ComprehensionBlock":
      this.pattern(block.left);
      this.expression(block.right);
      break;

    case "ComprehensionIf":
      this.expression(block.test);
      break;
    }
  },

  pattern(pat) {
    if (!pat) {
      print(Error().stack);
    }

    switch (pat.type) {
    case "Identifier":
      this.defVar(pat.name, pat.loc);
      break;

    case "ObjectPattern":
      for (let prop of pat.properties) {
        if (prop.type == "Property") {
          this.pattern(prop.value);
        } else if (prop.type == "SpreadExpression") {
          this.pattern(prop.expression);
        } else {
          throw `Unexpected prop ${JSON.stringify(prop)} in ObjectPattern`;
        }
      }
      break;

    case "ArrayPattern":
      for (let e of pat.elements) {
        if (e) {
          this.pattern(e);
        }
      }
      break;

    case "SpreadExpression":
      this.pattern(pat.expression);
      break;

    case "AssignmentExpression":
      this.pattern(pat.left);
      this.expression(pat.right);
      break;

    default:
      throw `Unexpected pattern: ${pat.type} ${JSON.stringify(pat)}`;
      break;
    }
  },

  maybeLinkifyLiteral(expr) {
    if (typeof expr.value !== "string") {
      return false;
    }

    const isMozSrc = expr.value.startsWith("moz-src:///");

    if (!expr.value.startsWith("chrome://") &&
        !expr.value.startsWith("resource://") &&
        !isMozSrc) {
      return false;
    }

    const name = "\"" + expr.value + "\"";
    const loc = expr.loc;
    const url = expr.value;
    const sym = isMozSrc ? ("FILE_" + atEscape(url.slice(11))) : ("URL_" + atEscape(url));
    this.source(loc, name, "file,use", "file " + url, sym);
    this.target(loc, name, "use", url, sym);

    return true;
  },

  maybeLinkifyModuleSpecifier(expr) {
    if (typeof expr.value !== "string") {
      return false;
    }

    if (this.maybeLinkifyLiteral(expr)) {
      return true;
    }

    if (!expr.value.startsWith(".")) {
      return false;
    }

    // Relative path import.
    // This is going to be replaced by replace-aliases.py based on
    // the URL of the current file.
    const name = "\"" + expr.value + "\"";
    const loc = expr.loc;
    const relpath = expr.value;
    const sym = "RELPATH";
    this.source(loc, name, "file,use", relpath, sym);
    this.target(loc, name, "use", relpath, sym);

    return true;
  }
};

function printFileTarget(path) {
  print(JSON.stringify({
    loc: "00001:0",
    target: 1,
    kind: "def",
    pretty: "file " + path,
    sym: "FILE_" + atEscape(path),
  }));
}

// Helper for preprocessor directives so that JS assignments like `#error =`
// won't match.  All of this is obviously optimized for clarity/not messing up
// regexps, as we could combine most of the preproccesing checks into very few
// super-regexps.
function startsWithNoEquals(subjectString, checkString) {
  if (!subjectString.startsWith(checkString)) {
    return false;
  }
  if (subjectString.substring(checkString.length).trimStart()[0] === "=") {
    return false;
  }
  return true;
}

function preprocess(filename, comment)
{
  // Set the filename so that logError can downgrade any errors/warnings to INFO
  // if the filename has the word "error" in it.
  gFilename = filename;
  gIncludeUsed = false;
  gCouldBeJson = false;

  let text;
  try {
    text = snarf(filename);

    // There are a few `.js` files in the tree that use `#` as a comment for a
    // preprocessed file for the MPL and this is not helpful.  One is also a
    // mozconfig.  Just no-op the file.
    // https://searchfox.org/mozilla-central/search?q=path%3A.js%20%23%20This%20Source%20Code%20Form%20is%20subject%20to%20the%20terms%20of%20the%20Mozilla%20Public&path=
    // okay, also l10n (which is also getting a file constraint"):
    // https://searchfox.org/l10n/source/tn/mail/all-l10n.js
    if (text.startsWith("# This Source Code Form is subject to the terms of the Mozilla Public") ||
        text.startsWith("# ***** BEGIN LICENSE BLOCK *****")) {
      text = "";
    }
  } catch (e) {
    text = "";
  }

  if (text.startsWith("{")) {
    gCouldBeJson = true;
  }

  let substitution = false;
  let lines = text.split("\n");
  let preprocessedLines = [];
  let branches = [true];
  for (let i = 0; i < lines.length; i++) {
    let line = lines[i];
    if (substitution) {
      line = line.replace(/@(\w+)@/, "''");
    }
    let tline = line.trim();
    if (startsWithNoEquals(tline, "#ifdef ") || startsWithNoEquals(tline, "#ifndef ") || startsWithNoEquals(tline, "#if ")) {
      preprocessedLines.push(comment(tline));
      branches.push(branches[branches.length-1]);
    } else if (tline.startsWith("#else") ||
               startsWithNoEquals(tline, "#elif ") ||
               startsWithNoEquals(tline, "#elifdef ") ||
               startsWithNoEquals(tline, "#elifndef ")) {
      preprocessedLines.push(comment(tline));
      branches.pop();
      branches.push(false);
    } else if (tline.startsWith("#endif")) {
      preprocessedLines.push(comment(tline));
      branches.pop();
    } else if (!branches[branches.length-1]) {
      preprocessedLines.push(comment(tline));
    } else if (startsWithNoEquals(tline, "#include ") || startsWithNoEquals(tline, "#includesubst ")) {
      // Mark that we used an include so we know this file may experience parse
      // errors which should be downgraded to INFO from WARN.
      gIncludeUsed = true;

      /*
      let match = tline.match(/#include "?([A-Za-z0-9_.-]+)"?/);
      if (!match) {
        throw new Error(`Invalid include directive: ${filename}:${i+1}`);
      }
      let incfile = match[1];
      preprocessedLines.push(`PREPROCESSOR_INCLUDE("${incfile}");`);
      */
      preprocessedLines.push(comment(tline));
    } else if (tline.startsWith("#filter substitution")) {
      preprocessedLines.push(comment(tline));
      substitution = true;
      // require whitespace after the filter to avoid catching variable names
      // like `#filterLogins`.
    } else if (startsWithNoEquals(tline, "#filter ") || startsWithNoEquals(tline, "#unfilter ")) {
      preprocessedLines.push(comment(tline));
    } else if (startsWithNoEquals(tline, "#expand ")) {
      preprocessedLines.push(line.substring(String("#expand ").length));
    } else if (startsWithNoEquals(tline, "#literal ")) {
        preprocessedLines.push(line.substring(String("#literal ").length));
    } else if (startsWithNoEquals(tline, "#define ") ||
               startsWithNoEquals(tline, "#undef ") ||
               startsWithNoEquals(tline, "#error ")) {
      preprocessedLines.push(comment(tline));
    } else {
      preprocessedLines.push(line);
    }
  }

  return preprocessedLines.join("\n");
}

function analyzeJS(filename)
{
  let text = preprocess(filename, line => "// " + line);

  let target = filename.endsWith(".mjs") ? "module" : "script";

  let ast = Analyzer.parse(text, filename, 1, target);
  if (ast) {
    try {
      Analyzer.program(ast);
    } catch (ex) {
      logError(`In ${filename}, got: ${ex}`);
    }
  }
}

function replaceEntities(text)
{
  var table = {
    "&amp;&amp;": "&&        ",
    "&amp;": "&    ",
    "&lt;": "<   ",
    "&gt;": ">   ",
  };

  for (let ent in table) {
    let re = RegExp(ent, "gi");
    text = text.replace(re, table[ent]);
  }

  return text;
}

// XXX SpiderMonkey now uses 1-based column numbers since bug 1862692.  The SAX
// parser is definitely 0-based but I think we just create JS strings that get
// parsed into the JS universe so it doesn't matter that this parser is using
// 0-based columns.  But maybe I'm wrong?  I'm doing a quick fix.  Also, we
// don't really have XUL files anymore...
class BaseParser {
  constructor(filename, parser) {
    this.filename = filename;
    this.stack = [];
    this.curAttrs = {};
    this.parser = parser;
    this.eventListeners = [];

    for (let prop of ["onopentag", "onclosetag", "onattribute"]) {
      parser[prop] = this[prop].bind(this);
    }
  }

  onopentag(tag) {
    tag.line = this.parser.line;
    tag.column = this.parser.column;
    tag.attrs = this.curAttrs;
    this.curAttrs = {};
    this.stack.push(tag);
  }

  onclosetag(tagName) {
    let tag = this.stack[this.stack.length - 1];

    this.ontag(tagName, tag);

    this.stack.pop();
  }

  ontag(tagName, tag) {
  }

  onattribute(attr) {
    this.curAttrs[attr.name] = attr;
  }

  handleAttributes(tag) {
    for (let prop in tag.attrs) {
      if (prop.startsWith("ON")) {
        this.handleEventListener(tag, prop);
        continue;
      }
      if (prop == "STYLE") {
        this.handleStyleProp(tag, prop);
        continue;
      }

      let text = tag.attrs[prop].value;
      if (text.startsWith("chrome://") || text.startsWith("resource://")) {
        this.handleURLAttribute(tag, prop);
      }
    }
  }

  handleEventListener(tag, prop) {
    let text = tag.attrs[prop].value;
    let line = tag.attrs[prop].valueLine;
    let column = tag.attrs[prop].valueColumn;

    let spaces = " ".repeat(column);
    text = `(function (val) {\n${spaces}${text}\n})`;

    let ast = Analyzer.parse(text, this.filename, line, "script", prop);
    if (ast) {
      this.eventListeners.push(ast);
    }
  }

  processEventListeners() {
    for (let ast of this.eventListeners) {
      Analyzer.dummyProgram(ast, [{name: "event", skip: true}]);
    }
  }

  handleURLAttribute(tag, prop) {
    let url = tag.attrs[prop].value;
    let line = tag.attrs[prop].valueLine;
    let column = tag.attrs[prop].valueColumn;

    const locStr = `${line + 1}:${column}-${column + url.length}`;
    const sym = "URL_" + atEscape(url);
    Analyzer.source(locStr, url, "file,use", "file " + url, sym);
    Analyzer.target(locStr, url, "use", url, sym);
  }

  getScriptTarget(tag) {
    let type;
    if ("TYPE" in tag.attrs) {
      type = tag.attrs.TYPE.value;
    } else if ("LANGUAGE" in tag.attrs) {
      type = "text/" + tag.attrs.LANGUAGE.value;
    } else {
      return "script";
    }
    if (type === "module") {
      return "module";
    }
    const jsMIMETypes = [
      "text/javascript",
      "text/ecmascript",
      "application/javascript",
      "application/ecmascript",
      "application/x-javascript",
      "application/x-ecmascript",
      "text/javascript1.0",
      "text/javascript1.1",
      "text/javascript1.2",
      "text/javascript1.3",
      "text/javascript1.4",
      "text/javascript1.5",
      "text/jscript",
      "text/livescript",
      "text/x-ecmascript",
      "text/x-javascript",
    ];
    if (jsMIMETypes.includes(type.toLowerCase())) {
      return "script";
    }
    return "";
  }

  handleScript(text, tag) {
    let target = this.getScriptTarget(tag);

    if (target !== "script" && target !== "module") {
      return;
    }

    let {line, column} = tag;

    let spaces = " ".repeat(column);
    text = spaces + text;

    let ast = Analyzer.parse(text, this.filename, line + 1, target);
    if (ast) {
      Analyzer.program(ast);
    }
  }

  handleStyle(text, tag) {
    let {line, column} = tag;

    let spaces = " ".repeat(column);
    text = spaces + text;

    const analyzer = new CSSAnalyzer({ line: line + 1 });
    analyzer.parse(text);
  }

  handleStyleProp(tag, prop) {
    let text = tag.attrs[prop].value;
    let line = tag.attrs[prop].valueLine;
    let column = tag.attrs[prop].valueColumn;

    let spaces = " ".repeat(column);
    text = spaces + text;

    const analyzer = new CSSAnalyzer({ line: line + 1 });
    analyzer.parse(text);
  }
}

class XMLParser extends BaseParser {
  constructor(filename, parser) {
    super(filename, parser)
    this.curText = "";
    for (let prop of ["ontext", "oncdata"]) {
      parser[prop] = this[prop].bind(this);
    }
  }

  onopentag(tag) {
    super.onopentag(tag);
    this.curText = "";
  }

  ontext(text) {
    this.curText += text;
  }

  oncdata(text) {
    this.curText += replaceEntities(text);
  }
}

class XBLParser extends XMLParser {
  ontag(tagName, tag) {
    switch (tagName) {
    case "FIELD":
      this.onfield(tag);
      break;
    case "PROPERTY":
      this.onproperty(tag);
      break;
    case "GETTER":
      this.ongetter(tag);
      break;
    case "SETTER":
      this.onsetter(tag);
      break;
    case "METHOD":
      this.onmethod(tag);
      break;
    case "PARAMETER":
      this.onparameter(tag);
      break;
    case "BODY":
      this.onbody(tag);
      break;
    case "CONSTRUCTOR":
    case "DESTRUCTOR":
      this.onstructor(tag);
      break;
    case "HANDLER":
      this.onhandler(tag);
      break;
    }
  }

  onfield(tag) {
    if (!tag.attrs.NAME) {
      return;
    }

    let line = tag.attrs.NAME.valueLine;
    let column = tag.attrs.NAME.valueColumn;
    let name = tag.attrs.NAME.value;

    let locStr = `${line + 1}:${column}-${column + name.length}`;
    Analyzer.source(locStr, name, "def,prop", `property ${name}`, `#${name}`,
                    false);
    Analyzer.target(locStr, name, "def", name, `#${name}`);

    let spaces = " ".repeat(tag.column);
    let text = spaces + this.curText;

    let ast = Analyzer.parse(text, this.filename, tag.line + 1, "script");
    if (ast) {
      Analyzer.program(ast);
    }
  }

  onproperty(tag) {
    let name = null;
    if (tag.attrs.NAME) {
      let line = tag.attrs.NAME.valueLine;
      let column = tag.attrs.NAME.valueColumn;
      name = tag.attrs.NAME.value;

      let locStr = `${line + 1}:${column}-${column + name.length}`;
      Analyzer.source(locStr, name, "def,prop", `property ${name}`, `#${name}`,
                      false);
      Analyzer.target(locStr, name, "def", name, `#${name}`);
    }

    let line, column;
    for (let prop in tag.attrs) {
      if (prop != "ONGET" && prop != "ONSET") {
        continue;
      }

      let text = tag.attrs[prop].value;
      line = tag.attrs[prop].valueLine;
      column = tag.attrs[prop].valueColumn;

      let spaces = " ".repeat(column);
      text = `(function (val) {\n${spaces}${text}\n})`;

      let ast = Analyzer.parse(text, this.filename, line, "script", prop);
      if (ast) {
        Analyzer.scoped(name, () => Analyzer.dummyProgram(ast, [{name: "val", skip: true}]));
      }
    }

    for (let prop in tag) {
      if (prop != "getter" && prop != "setter") {
        continue;
      }

      let text = tag[prop].text;
      line = tag[prop].line;
      column = tag[prop].column;

      let spaces = " ".repeat(column);
      text = `(function (val) {\n${spaces}${text}\n})`;

      let ast = Analyzer.parse(text, this.filename, line, "script", prop);
      if (ast) {
        Analyzer.scoped(name, () => Analyzer.dummyProgram(ast, [{name: "val", skip: true}]));
      }
    }
  }

  ongetter(tag) {
    tag.text = this.curText;
    let parentTag = this.stack[this.stack.length - 2];
    if (parentTag) {
      parentTag.getter = tag;
    }
  }

  onsetter(tag) {
    tag.text = this.curText;
    let parentTag = this.stack[this.stack.length - 2];
    if (parentTag) {
      parentTag.setter = tag;
    }
  }

  onparameter(tag) {
    let parentTag = this.stack[this.stack.length - 2];
    if (parentTag) {
      if (!parentTag.params) {
        parentTag.params = [];
      }
      parentTag.params.push(tag);
    }
  }

  onbody(tag) {
    tag.text = this.curText;
    let parentTag = this.stack[this.stack.length - 2];
    if (parentTag) {
      parentTag.body = tag;
    }
  }

  onstructor(tag) {
    let text = this.curText;
    let {line, column} = tag;

    let spaces = " ".repeat(column);
    text = `(function () {\n${spaces}${text}\n})`;

    let ast = Analyzer.parse(text, this.filename, line, "script");
    if (ast) {
      Analyzer.scoped(null, () => Analyzer.dummyProgram(ast, []));
    }
  }

  onhandler(tag) {
    let text = this.curText;
    let {line, column} = tag;

    let spaces = " ".repeat(column);
    text = `(function () {\n${spaces}${text}\n})`;

    let ast = Analyzer.parse(text, this.filename, line, "script");
    if (ast) {
      Analyzer.scoped(null, () => Analyzer.dummyProgram(ast, []));
    }
  }

  onmethod(tag) {
    if (!tag.attrs.NAME) {
      return;
    }

    let line = tag.attrs.NAME.valueLine;
    let column = tag.attrs.NAME.valueColumn;
    let name = tag.attrs.NAME.value;

    let locStr = `${line + 1}:${column}-${column + name.length}`;
    Analyzer.source(locStr, name, "def,prop", `property ${name}`, `#${name}`,
                    false);
    Analyzer.target(locStr, name, "def", name, `#${name}`);

    Analyzer.enter(name);

    let params = tag.params || [];
    for (let p of params) {
      let text = p.attrs.NAME.value;
      line = p.attrs.NAME.valueLine;
      column = p.attrs.NAME.valueColumn;

      Analyzer.defVar(text, {start: {line: line + 1, column}});
    }

    if (tag.body) {
      let text = tag.body.text;
      line = tag.body.line;
      column = tag.body.column;

      params = params.map(p => p.attrs.NAME.value);
      let paramsText = params.join(", ");

      let spaces = " ".repeat(column);
      text = `(function (${paramsText}) {\n${spaces}${text}\n})`;

      let ast = Analyzer.parse(text, this.filename, line, "script");
      if (ast) {
        Analyzer.dummyProgram(ast, []);
      }
    }

    Analyzer.exit();
  }
}

// Prepare the `sax` global variable.
// This function shouldn't be called multiple times.
//
// js-analyze.js is executed for single file, and the code path
// "analyzeFile -> analyze* -> loadSax" is taken at most once.
function ensureSax()
{
  if ("sax" in globalThis) {
    return;
  }
  load(mozSearchRoot + "/sax/sax.js");
}

function analyzeXBL(filename)
{
  let text = preprocess(filename, line => `<!--${line}-->`);

  ensureSax();
  let parser = sax.parser(false, {trim: false, normalize: false, xmlns: true, position: true});

  new XBLParser(filename, parser);

  parser.write(text);
  parser.close();
}

class XULParser extends XMLParser {
  ontag(tagName, tag) {
    switch (tagName) {
    case "SCRIPT":
      this.handleScript(this.curText, tag);
      break;
    }

    this.handleAttributes(tag);
  }
}

class HTMLParser extends BaseParser {
  constructor(filename, parser) {
    super(filename, parser);

    this.inStyle = false;
    this.currentStyle = "";
    for (let prop of ["onscript", "ontext"]) {
      parser[prop] = this[prop].bind(this);
    }
  }

  onopentag(tag) {
    super.onopentag(tag);

    if (tag.local.toUpperCase() === "STYLE") {
      this.inStyle = true;
      this.currentStyle = "";
    }
  }

  ontext(text) {
    if (this.inStyle) {
      this.currentStyle += text;
    }
  }

  ontag(tagName, tag) {
    switch (tagName) {
    case "SCRIPT":
      this.handleScript(this.currentScript, tag);
      break;
    case "STYLE":
      this.inStyle = false;
      this.handleStyle(this.currentStyle, tag);
      break;
    }

    this.handleAttributes(tag);
  }

  onscript(script) {
    this.currentScript = replaceEntities(script);
  }
}

function analyzeXUL(filename)
{
  let text = preprocess(filename, line => `<!--${line}-->`);

  if (filename.endsWith(".inc")) {
    text = "<root>" + text + "</root>";
  }

  ensureSax();
  let parser = sax.parser(false, {trim: false, normalize: false, xmlns: true, position: true, noscript: true});

  let parser2 = new XULParser(filename, parser);

  parser.write(text);
  parser.close();

  parser2.processEventListeners();
}

function analyzeHTML(filename)
{
  let text = preprocess(filename, line => `<!--${line}-->`);

  if (filename.endsWith(".inc")) {
    text = "<root>" + text + "</root>";
  }

  ensureSax();
  let parser = sax.parser(false, {trim: false, normalize: false, xmlns: true, position: true, noscript: false});

  let parser2 = new HTMLParser(filename, parser);

  parser.write(text);
  parser.close();

  parser2.processEventListeners();
}

class CSSAnalyzer {
  static analyze_css_source = null;

  static ensureCSSAnalyzer() {
    if (CSSAnalyzer.analyze_css_source) {
      return;
    }

    const wasmPath = mozSearchRoot + "/scripts/web-analyze/wasm-css-analyzer/out";
    const wasmBinary = createMappedArrayBuffer(wasmPath + "/wasm_css_analyzer.wasm");

    // getrandom crate requires WebCrypto API.
    const MyCrypto = {
      getRandomValues(array) {
        let i = 0, length = array.length;
        while (i < length) {
          array[i++] = Math.random() * 256;
        }
        return array;
      }
    };

    // The binding JS requires TextEncoder and TextDecoder.
    class MyTextEncoder {
      encode(text) {
        // This is called only when the text is non-ASCII.

        let units = [], index = 0, length = text.length,
            n, trail, b1, b2, b3, b4;

        const NonBMPMin = 0x10000,
              NonBMPMax = 0x10FFFF,
              LeadSurrogateMin = 0xD800,
              LeadSurrogateMax = 0xDBFF,
              TrailSurrogateMin = 0xDC00,
              TrailSurrogateMax = 0xDFFF;

        while (index < length) {
          n = text.charCodeAt(index++);

          if (n <= 0x7F) {
            units.push(n);
            continue;
          }

          if (n >= LeadSurrogateMin && n <= LeadSurrogateMax) {
            trail = text.charCodeAt(index++);
            n = (n << 10) + trail +
              (NonBMPMin - (LeadSurrogateMin << 10) - TrailSurrogateMin);
          }

          if (n > NonBMPMax) {
            units.push(0x3F);
          } else if (n >= 0x010000) {
            b4 = n & 0x3F;
            n >>= 6;
            b3 = n & 0x3F;
            n >>= 6;
            b2 = n & 0x3F;
            n >>= 6;
            b1 = n & 0x3F;
            units.push(b1 | 0b1111_0000);
            units.push(b2 | 0b1000_0000);
            units.push(b3 | 0b1000_0000);
            units.push(b4 | 0b1000_0000);
          } else if (n >= 0x0800) {
            b3 = n & 0x3F;
            n >>= 6;
            b2 = n & 0x3F;
            n >>= 6;
            b1 = n & 0x3F;
            units.push(b1 | 0b1110_0000);
            units.push(b2 | 0b1000_0000);
            units.push(b3 | 0b1000_0000);
          } else {
            b2 = n & 0x3F;
            n >>= 6;
            b1 = n & 0x3F;
            units.push(b1 | 0b1100_0000);
            units.push(b2 | 0b1000_0000);
          }
        }

        return new Uint8Array(units);
      }
    }

    class MyTextDecoder {
      decode(buffer) {
        // This is used for all string received from wasm.
        // This is called only with complete data.

        if (buffer === undefined) {
          return "";
        }

        // Converted from DecodeOneUtf8CodePointInline in m-c/mfbt/Utf8.h,
        // with substituting bad code units with "?".

        let chars = [], index = 0, length = buffer.length,
            n, remaining, min, actual, i, unit;

        next: while (index < length) {
          n = buffer[index++];

          if ((n & 0b1000_0000) == 0b0000_0000) {
            chars.push(String.fromCodePoint(n));
            continue;
          }

          // |n| determines the number of trailing code units in the code point
          // and the bits of |n| that contribute to the code point's value.
          if ((n & 0b1110_0000) == 0b1100_0000) {
            remaining = 1;
            min = 0x80;
            n &= 0b0001_1111;
          } else if ((n & 0b1111_0000) == 0b1110_0000) {
            remaining = 2;
            min = 0x800;
            n &= 0b0000_1111;
          } else if ((n & 0b1111_1000) == 0b1111_0000) {
            remaining = 3;
            min = 0x10000;
            n &= 0b0000_0111;
          } else {
            chars.push("?");
            continue;
          }

          // If the code point would require more code units than remain, the encoding
          // is invalid.
          actual = length - i;
          if (actual < remaining) {
            chars.push("?");
            continue;
          }

          for (i = 0; i < remaining; i++) {
            unit = buffer[index++];

            // Every non-leading code unit in properly encoded UTF-8 has its high
            // bit set and the next-highest bit unset.
            if (!((unit & 0b1100_0000) == 0b1000_0000)) {
              index -= i + 1;
              chars.push("?");
              continue next;
            }

            // The code point being encoded is the concatenation of all the
            // unconstrained bits.
            n = (n << 6) | (unit & 0b0011_1111);
          }

          // UTF-16 surrogates and values outside the Unicode range are invalid.
          if (n > 0x10FFFF || (0xD800 <= n && n <= 0xDFFF)) {
            index -= remaining;
            chars.push("?");
            continue;
          }

          // Overlong code points are also invalid.
          if (n < min) {
            index -= remaining;
            chars.push("?");
            continue;
          }

          chars.push(String.fromCodePoint(n));
        }

        return chars.join("");
      }
    }

    globalThis.crypto = MyCrypto;
    globalThis.TextEncoder = MyTextEncoder;
    globalThis.TextDecoder = MyTextDecoder;
    load(wasmPath + "/wasm_css_analyzer.js");
    globalThis.initSync(wasmBinary);

    CSSAnalyzer.analyze_css_source = globalThis.analyze_css_source;
  }

  constructor({ line = 1 } = {}) {
    CSSAnalyzer.ensureCSSAnalyzer();

    this.startLine = line;
  }

  parse(text) {
    try {
      CSSAnalyzer.analyze_css_source(text, this.startLine, function(s) {
        print(s);
      });
    } catch (e) {
      if (e && e.message && e.message.includes("index out of bounds")) {
        // Deeply nested rules can hit stack overflow.
        return;
      }
      throw e;
    }
  }
}

function analyzeFile(filename)
{
  if (filename.endsWith(".xml")) {
    analyzeXBL(filename);
  } else if (filename.endsWith(".xul") || filename.endsWith(".inc") || filename.endsWith(".xhtml")) {
    analyzeXUL(filename);
  } else if (filename.endsWith(".html")) {
    analyzeHTML(filename);
  } else {
    analyzeJS(filename);
  }
}

function resetState() {
  gParsedAs = "script";
  gFilename = "";
  gIncludeUsed = false;
  gCouldBeJson = false;
  gAttrName = "";
  nextSymId = 0;
  Analyzer.resetState();
}


function decodeUTF8(s) {
  if (s.match(/[^\x00-\x7F]/)) {
    return decodeURIComponent(s.replace(/./g, m => {
      return "%" + m.charCodeAt(0).toString(16).padStart(2, '0');
    }));
  }

  return s;
}

mozSearchRoot = scriptArgs[0];
const localRoot = scriptArgs[1];
const analysisRoot = scriptArgs[2];

while (true) {
  const line = readline();
  if (!line) {
    break;
  }

  resetState();

  const m = line.match(/^([^ ]+) (.+)$/);
  if (!m) {
    continue;
  }
  fileIndex = m[1];

  // readline() returns raw byte sequence.
  // The filename is UTF-8 encoded in the js-files file.
  const sourcePath = decodeUTF8(m[2]);

  localFile = localRoot + "/" + sourcePath;
  const analysisFile = analysisRoot + "/" + sourcePath;

  const origOut = os.file.redirect(analysisFile);

  printFileTarget(sourcePath);

  analyzeFile(localFile);

  os.file.close(os.file.redirect(origOut));
}

```

## scripts/nginx-setup.py
```
#!/usr/bin/env python3

# Create our nginx configuration.
#
# The general scheme is `TREE/SERVICE/...` where SERVICE is "source" or
# "raw-analysis" for files available on disk and various dynamic requests that
# get proxied to per-tree local servers running on localhost.
#
# We have a docroot at /home/ubuntu/docroot that provides a place to decide what
# gets exposed in the root of the origin.  It also is used for the "source"
# mapping with symlinks helping map into /home/ubuntu/index/TREE/file (for
# rendered source files) and /home/ubuntu/index/TREE/dir (for rendered directory
# listings), but that could just as easily be accomplished with slightly fancier
# location directives.

from __future__ import absolute_import
from __future__ import print_function
import sys
import json
import os.path
import subprocess

# The config file at the root of the WORKING directory; all paths should be
# absolute paths.
config_fname = sys.argv[1]
# doc_root will usually be /home/ubuntu/docroot and will hold files like:
# - status.txt: A file written by the web-servers that the web-server triggering
#   process polls in order to know when the web-server is up and the load
#   balancers can be redirected at.
doc_root = sys.argv[2]
# although these arguments are optional, web-server-setup.sh explicitly passes
# empty values when omitted by wrapping them in quotes.
use_hsts = sys.argv[3] == 'hsts'
nginx_cache_dir = sys.argv[4] # empty string if not specified, which is falsey.

print('# use_hsts =', sys.argv[3])

mozsearch_path = os.path.realpath(os.path.join(os.path.dirname(os.path.realpath(sys.argv[0])), '..'))

config = json.load(open(config_fname))

# Keep this list in sync with the FormatAs::Binary list in languages.rs
binary_types = {
  'ogg opus': 'audio/ogg',
  'wav': 'audio/wav',
  'mp3': 'audio/mpeg',
  'png': 'image/png',
  'gif': 'image/gif',
  'jpg jpeg': 'image/jpeg',
  'bmp': 'image/bmp',
  'ico': 'image/vnd.microsoft.icon',
  'ogv': 'video/ogg',
  'mp4': 'video/mpeg',
  'webm': 'video/webm',
  'webp': 'image/webp',
  'ttf xpi bcmap icns sqlite jar woff class m4s mgif otf': 'application/x-unknown',
}

binary_types_str = " ".join((mime + " " + exts + ";") for (exts, mime) in binary_types.items())

def location(route, directives):
    print(f'  location {route} {{')

    # Use HSTS in release - ELB sets http_x_forwarded_proto, so this
    # won't match in dev builds.  This needs to be included in all
    # locations, instead of in the server block, since add_header
    # won't be inherited if a location sets any headers of its own.
    if use_hsts:
        print('    add_header Strict-Transport-Security "max-age=63072000; includeSubDomains; preload" always;')

    for directive in directives:
        print(f'    {directive}')
        if nginx_cache_dir and 'proxy_pass' in directive:
            print('    proxy_cache sfox;')
            print('    add_header X-Cache-Status $upstream_cache_status;')
    print('  }')
    print()

if nginx_cache_dir:
    # Proxy Cache Settings.
    #
    # These are enabled on a per-location basis
    #
    # - levels=1:2 - 2 levels of directories is a ward against file system
    #   slowness with tons of files in a directory.  May not actually be
    #   necessary.
    # - keys_zone=sfox:10m - 10 megs of keys at 8,000 keys per meg is 80,000
    #   keys or 80,000 cache things.  This was a default recommendation that's
    #   expected to be sufficient.  The "sfox" is the name of the cache to be
    #   used with `proxy_cache`.
    # - max_size=20g - 20 gigs of cached data, max.  This is a somewhat
    #   arbitrary decision based on the mozilla-releases.json using 223G of 296G
    #   right now, leaving 59G free.
    # - use_temp_path=off - Disables the file being written to disk in one
    #   location and then moved/copied to its final destination.  Recommended.
    # - inactive=7d - Keep the data basically forever until LRU evicted because
    #   the cache has filled up.  The machine should be reaped after 2 days in
    #   normal successful operation, so anything above that is really just a
    #   convenience for analysis purposes.
    print('proxy_cache_path', nginx_cache_dir, 'levels=1:2 keys_zone=sfox:10m max_size=20g use_temp_path=off inactive=7d;')
    # If a 2nd identical request comes in while we're still asking the server
    # for an answer, block the 2nd and serve it the result of the 1st.  This is
    # massively desired for cases where anxious users hit the refresh button on
    # a slow-to-load page.
    print('proxy_cache_lock on;')
    # And those worst-cases can be very bad, so choose much longer lock timeouts
    # than 5s.  Note that proxy_read_timeout is still 60s so if the server
    # buffers the whole time, things will still break.
    print('proxy_cache_lock_age 3m;')
    print('proxy_cache_lock_timeout 3m;')
    print('proxy_read_timeout 3m;')
    print('proxy_cache_valid 200 120d;')
    # XXX cache despite the server saying otherwise
    print('proxy_ignore_headers X-Accel-Expires Expires Cache-Control Set-Cookie;')
    print('')

print('''# we are in the "http" context here.
log_format custom_cache_log '[$time_local] [Cache:$upstream_cache_status] [$request_time] [$host] [Remote_Addr: $remote_addr] - $remote_user - $server_name to: $upstream_addr: "$request" $status $body_bytes_sent "$http_referer" "$http_user_agent" ' ;

map $status $expires {
  default 2m;
  "301" 1m;
}

server {
  listen 80 default_server;

  access_log /var/log/nginx/searchfox.log custom_cache_log ;

  # Redirect HTTP to HTTPS in release
  if ($http_x_forwarded_proto = "http") {
    return 301 https://$host$request_uri;
  }

  sendfile off;

  expires $expires;
  etag on;
''')

# root means "/static" will be appended to the root, versus alias which doesn't.
location('/static', [f'root {mozsearch_path};'])
location('= /robots.txt', [
    f'root {mozsearch_path}/static;',
    'try_files $uri =404;',
    'add_header Cache-Control "public";',
    'expires 1d;',
])

# TODO: it's possible some of the `try_files` machinations and symlinks
# we're using could better cleaned up by use of "alias".  The exception is
# "source" where the "try_files" is definitely absolutely necessary.
for repo in config['trees']:
    tree_config = config['trees'][repo]
    index_path = tree_config['index_path']
    head_rev = None
    if 'git_path' in config['trees'][repo]:
        try:
            head_rev = subprocess.check_output(['git', '--git-dir', config['trees'][repo]['git_path'] + '/.git', 'rev-parse', 'HEAD'], text=True).strip()
        except subprocess.CalledProcessError:
            # If this fails just leave head_rev as None and skip the optimization
            pass

    # we use alias because the we don't want the "/{repo}" portion.
    location(f'/{repo}/static/', [f'alias {mozsearch_path}/static/;'])

    location(f'/{repo}/pages/', [f'alias {index_path}/pages/;'])

    location(f'/{repo}/source', [
        f'root {doc_root};',
        'try_files /file/$uri /dir/$uri/index.html =404;',
        f'types {{ {binary_types_str} }}',
        'default_type text/html;',
        'add_header Cache-Control "must-revalidate";',
        'gzip_static always;',
        'gunzip on;',
    ])

    location(f'/{repo}/raw-analysis', [
        f'root {doc_root};',
        'try_files /raw-analysis/$uri =404;',
        'types { }',
        # I tried serving this as application/x-ndjson but then something weird
        # happened content-encoding-wise.  The received response was content
        # encoded but the response headers didn't express it, so Firefox didn't
        # decode the result.
        'default_type text/plain;',
        'add_header Cache-Control "must-revalidate";',
        'gzip_static always;',
        'gunzip on;',
    ])

    location(f'/{repo}/file-lists', [
        f'root {doc_root};',
        'try_files /file-lists/$uri =404;',
        'types { }',
        'default_type text/plain;',
        'add_header Cache-Control "must-revalidate";',
    ])

    # Optimization to handle the head revision by serving the file directly instead of going through
    # the rust web-server. This is worth it because when HEAD-rev permalinks are generated they are
    # often hit multiple times while they are still the HEAD revision.
    if head_rev is not None:
        location(f'~^/{repo}/rev/{head_rev}/(?<head_path>.+)$', [
            f'root {doc_root}/file/{repo}/source;',
            'try_files /$head_path =404;',
            f'types {{ {binary_types_str} }}',
            'default_type text/html;',
            'add_header Cache-Control "must-revalidate";',
            'gzip_static always;',
            'gunzip on;',
        ])

    # Handled by router/router.py
    location(f'/{repo}/search', ['proxy_pass http://localhost:8000;'])
    location(f'/{repo}/sorch', ['proxy_pass http://localhost:8000;'])
    location(f'/{repo}/define', ['proxy_pass http://localhost:8000;'])

    # Handled by Rust `web-server.rs`.
    location(f'/{repo}/diff', ['proxy_pass http://localhost:8001;'])
    location(f'/{repo}/commit', ['proxy_pass http://localhost:8001;'])
    location(f'/{repo}/rev', ['proxy_pass http://localhost:8001;'])
    location(f'/{repo}/hgrev', ['proxy_pass http://localhost:8001;'])
    location(f'/{repo}/complete', ['proxy_pass http://localhost:8001;'])
    location(f'/{repo}/commit-info', ['proxy_pass http://localhost:8001;'])

    # Handled by Rust `pipeline-server.rs`
    location(f'/{repo}/query', ['proxy_pass http://localhost:8002;'])


location('= /', [
    f'root {doc_root};',
    'try_files $uri/help.html =404;',
    'add_header Cache-Control "must-revalidate";',
])

location('= /index.html', [
    f'root {doc_root};',
    'try_files /help.html =404;',
    'add_header Cache-Control "must-revalidate";',
])

location('= /status.txt', [
    f'root {doc_root};',
    'try_files $uri =404;',
    'add_header Cache-Control "must-revalidate";',
])

location('= /tree-list.js', [
    f'root {doc_root};',
    'try_files $uri =404;',
    'add_header Cache-Control "must-revalidate";',
])

if config.get("allow_webtest"):
    location(f'/tests/webtest', [
        f'root {mozsearch_path};',
        'add_header Cache-Control "no-cache";',
    ])

print('}')

```

## scripts/ipdl-analyze.sh
```
#!/usr/bin/env bash

set -x # Show commands
set -eu # Errors/undefined vars are fatal
set -o pipefail # Check all commands in a pipeline

if [ $# -ne 2 ]
then
    echo "Usage: ipdl-analyze.sh config-file.json tree_name"
    exit 1
fi

CONFIG_FILE=$(realpath $1)
TREE_NAME=$2

# Note that we use "realpath" on FILES_ROOT because the IPDL parser likes to
# canonicalize things.  Because this is just a prefix that gets stripped off,
# all that matters is consistency and so pre-normalizing is fine.
pushd $FILES_ROOT
cat $INDEX_ROOT/ipdl-files | \
    xargs $MOZSEARCH_PATH/tools/target/release/ipdl-analyze $(cat $INDEX_ROOT/ipdl-includes) \
          -f $INDEX_ROOT/repo-files \
          -o $INDEX_ROOT/objdir-files \
          -b $(realpath $FILES_ROOT) \
          -a $INDEX_ROOT/analysis
popd

```

## scripts/lib.py
```
from __future__ import absolute_import
from __future__ import print_function
import sys
import subprocess

def run(cmd, **extra):
    p = subprocess.Popen(cmd, stdout=subprocess.PIPE, stderr=subprocess.PIPE, **extra)
    (stdout, stderr) = p.communicate()

    if p.returncode:
        print('Command failed', cmd, file=sys.stderr)
        print('Return code', p.returncode, file=sys.stderr)
        print(stdout.decode(), file=sys.stderr)
        print('---', file=sys.stderr)
        print(stderr.decode(), file=sys.stderr)
        sys.exit(p.returncode)

    return stdout

def run_showing_output(cmd, output_filter=None, **extra):
    print('running', repr(cmd))
    p = subprocess.Popen(cmd, stdout=subprocess.PIPE, stderr=subprocess.PIPE, **extra)
    (stdout, stderr) = p.communicate()

    if p.returncode:
        print('Command failed', cmd, file=sys.stderr)
        print('Return code', p.returncode, file=sys.stderr)
        print(stdout, file=sys.stderr)
        print('---', file=sys.stderr)
        print(stderr, file=sys.stderr)
        sys.exit(p.returncode)
    else:
        stdout = stdout.decode()
        stderr = stderr.decode()

        if output_filter is not None:
            stdout = output_filter(stdout)
            stderr = output_filter(stderr)

        print('--- stdout')
        print(stdout)
        print('--- stderr')
        print(stderr)
        print('--- (end output)')

    return stdout

```

## scripts/scip-analyze.sh
```
#!/usr/bin/env bash

# This script locates all .scip files in the OBJDIR and then generates analysis
# data from the scip indices.
#
# This data-flow path is evolved from the WIP rust SCIP analysis functionality,
# and changes are still in flight.

set -x # Show commands
set -eu # Errors/undefined vars are fatal
set -o pipefail # Check all commands in a pipeline

if [ $# -lt 2 ]
then
    echo "Usage: scip-analyze.sh config-file.json tree_name"
    exit 1
fi

CONFIG_FILE=$(realpath $1)
TREE_NAME=$2

# Extract the list of scip_subtrees for our tree of interest.  We extract each
# full POJO object rather than its keys in this pass because we explicitly allow
# for the empty string as a value but the `read` command does not like that.
# This also arguably allows for some extra flexibility when processing inside
# the loop, which is part of why we moved to this.  (Another option, previously
# used is that we could avoid passing `-r` which would leave the strings quoted,
# but then re-interpreting them inside the loop is a hassle and the cleanest
# option was invoking jq again with `-r`, but then we might as well be picking
# out of the object.)
#
# Note that another option would be to just have the scip-indexer directly
# access the information from the config file.  We're not doing that in order to
# faciliate use-cases like mozilla-central's per-platform in
# process-tc-artifacts.sh.  Note that for these per-platform cases we expect
# those scripts to just directly invoke `scip-indexer` themselves.
SCIP_SUBTREE_INFOS=$(jq -Mc ".trees[\"${TREE_NAME}\"].scip_subtrees | to_entries? | .[]?" ${CONFIG_FILE})

# Note: This structuring avoids use of a pipe and sub-shells which allows us to
# mutate global variables if we want.
if [[ $SCIP_SUBTREE_INFOS ]]; then
  while read -r subtree_obj; do
    scip_tree_name=$(jq -Mr '.key' <<< "$subtree_obj")
    scip_index_path=$(jq -Mr '.value.scip_index_path' <<< "$subtree_obj")
    subtree_root=$(jq -Mr '.value.subtree_root' <<< "$subtree_obj")
    $MOZSEARCH_PATH/tools/target/release/scip-indexer \
      "$CONFIG_FILE" \
      "$TREE_NAME" \
      --subtree-name "${scip_tree_name}" \
      --subtree-root "${subtree_root}" \
      "${scip_index_path}"
  done <<< "$SCIP_SUBTREE_INFOS"
fi

```

## scripts/idl-analyze.py
```
#!/usr/bin/env python3
#
# ## Overview / Purpose ##
#
# Currently, this file is responsible for using the downloaded python XPIDL
# parser to produce analysis records for XPIDL files (with an `idl` extension).
# The XPIDL file is parsed and used to cross-reference the C++ analysis records
# produced from the C++ analysis pass of the generated XPIDL C++ bindings in
# order to establish the relationship between the IDL file and the bindingss.
#
# This file is also aware of the JS bindings but because of the limitations of
# our JS language analyzers at this time, the JS bindings will generally result
# in an immense number of false positives.  Specifically, for an XPIDL method
# "foo", we will generate a symbol `#foo` which will match every JS method or
# variable named "foo".
#
# It is our hope that in the future this script can be superseded by having the
# in-tree XPIDL binding generator (and source of the `xpidl` import) directly
# generate analysis records with meta-data providing support for a new semantic
# linker step proposed at https://bugzilla.mozilla.org/show_bug.cgi?id=1727789
# to allow for a more
#
# ## XPIDL symbols versus C++, JS Symbols and router.py ##
#
# Before the "structured" branch landed, our IDL files (XPIDL, IPDL) never had
# symbols that corresponded directly to the IDL file itself.  Instead, the IDL
# analyzers would try and find the relevant C++ symbols and guess the relevant
# JS symbols and then reference them in the file.  This was necessary because
# searchfox never had any concept of relationship with symbols, it only had the
# ability to associate symbols with a token, grouping by the "pretty" identifier
# associated with the symbols.
#
# The "structured" functionality has now enabled us to convey the explicit
# relationships between symbols.  This comment is being written as part of an
# effort to transition the context-menu's "source" records from pre-conflating
# the C++ and JS symbols to instead using explicit `XPIDL_foo` symbols which
# have explicit relationship to the binding symbols.
#
# However, we can only modernize the "source" records right now because the UI
# breakdown is:
# - "source" records power the context menu in source listings.
# - "target" records power the crossref database which router.py displays.
#
# The new `pipeline-server.rs` and its "query" endpoint have been intentionally
# designed to be able to understand and handle following these relationship
# edges.
#
# `router.py` has also been augmented to:
# - Process the "slotOwner" "meta" field by exposing its def(s) as "IDL".  This
#   means that when looking at a C++ method/getter/setter, the IDL definition
#   will be exposed as "IDL".  Note that we will not traverse the IDL def, so
#   in order to see the JS method/getter/setter, the user will need to switch
# - Process the "bindingSlots"
# traverse the "slotOwner"
# and "bindingSlots" "meta" fields to maintain its behavior prior to this change
# but no attempt is made to enhance the "search" endpoint to opt out of this
# behavior or

from __future__ import absolute_import
from __future__ import print_function
import sys
import xpidl
import os.path
import json
import re
import sys

def find_line_column(text, token, pos):
    while text[pos : pos + len(token)] != token:
        if text[pos] == '\n':
            return (0, 0)

        pos = text.find(' ', pos) + 1

    line = 0
    while pos > linebreaks[line]:
        line += 1

    line += 1

    col = 0
    pos -= 1
    while pos > 0 and text[pos] != '\n':
        col += 1
        pos -= 1

    return (line, col)

def parse_mangled(mangled):
    def parse_inner(idents, inner):
        if inner[0] == 'E':
            return True

        m = re.match(r'([0-9]+)([a-zA-Z0-9_]+)', inner)
        if not m:
            return False

        length = int(m.group(1))
        idents.append(m.group(2)[:length])

        return parse_inner(idents, m.group(2)[length:])

    if mangled[:3] != '_ZN':
        return

    idents = []
    if not parse_inner(idents, mangled[3:]):
        return

    return idents

def read_cpp_analysis(fname):
    base = os.path.basename(fname)
    (idlName, suffix) = os.path.splitext(base)
    headerName = idlName + '.h'
    p = os.path.join(indexRoot, 'analysis', '__GENERATED__', 'dist', 'include', headerName)
    try:
        lines = open(p).readlines()
    except IOError as e:
        return None
    methods = {}
    enums = []
    for line in lines:
        try:
            j = json.loads(line.strip())
        except ValueError as e:
            print('Syntax error in JSON file', p, line.strip(), file=sys.stderr)
            raise e
        # Inline method definitions and pure virtual method declarations
        # will both be reported as definitions by the C++ indexer without a
        # declaration, so we need to accept both decls and defs.
        if 'target' in j and j['kind'] in ('decl', 'def'):
            if j['sym'].startswith('_Z'):
                idents = parse_mangled(j['sym'])
                if idents and len(idents) == 2:
                    methods.setdefault(idents[0], {})[idents[1]] = j['sym']
            elif j['sym'].startswith('E_'):
                enums.append(j['sym'])
    return (methods, enums)

def find_enum(enums, name):
    for e in enums:
        if e.endswith(name):
            return e

def cpp_method_name(m):
    '''Return the C++ pretty name for this method per binaryname or capitalization.'''
    if m.binaryname:
        return m.binaryname
    return m.name[0].capitalize() + m.name[1:]

def cpp_getter_name(attr):
    '''Return the C++ pretty name for this getter per binaryname or capitalization.'''
    if attr.binaryname:
        return 'Get' + attr.binaryname
    return 'Get' + attr.name[0].capitalize() + attr.name[1:]

def cpp_setter_name(attr):
    '''Return the C++ pretty name for this setter per binaryname or capitalization.'''
    if attr.binaryname:
        return 'Set' + attr.binaryname
    return 'Set' + attr.name[0].capitalize() + attr.name[1:]

def emit_record(o, variations=None):
    '''Emit a single record or a number of optional variations of a record.

    If no variations are provided, the base object is emitted as JSON.

    If a variations array is provided, any non-None entries are applied to the
    base dictionary and emitted as JSON.
    '''
    if variations is None:
        print(json.dumps(o))
        return

    for mods in variations:
        if mods is None:
            continue
        cur = o.copy()
        cur.update(mods)
        print(json.dumps(cur))

def handle_interface(methods, enums, iface):
    '''Derives analysis records for each provided interface.

    '''
    (lineno, colno) = find_line_column(text, iface.name, iface.location._lexpos)
    iface_cpp_sym = 'T_' + iface.name
    iface_idl_sym = f'XPIDL_{ iface.name }'

    iface_loc = '%d:%d-%d' % (lineno, colno, colno + len(iface.name))

    # structured record will be emitted after processing all the members, but
    # we build up any complex sub-structures here.
    iface_slots = [
        {
            'slotKind': 'class',
            'slotLang': 'cpp',
            'ownerLang': 'idl',
            'sym': iface_cpp_sym,
        }
    ]

    # source
    emit_record({
        'loc': iface_loc,
        'source': 1,
        'syntax': 'idl',
        'pretty': f'IDL class {iface.name}',
        'sym': iface_idl_sym,
    })

    # target
    emit_record({
        'loc': iface_loc,
        'target': 1,
        'kind': 'idl',
        'pretty': iface.name,
        'sym': iface_idl_sym,
    })

    if iface.attributes.scriptable:
        iface_js_sym = f'#{ iface.name }'

        iface_slots.append({
            'slotKind': 'interface_name',
            'slotLang': 'js',
            'ownerLang': 'idl',
            'sym': iface_js_sym,
        })

    iface_supers = []

    if iface.base:
        (lineno, colno) = find_line_column(text, iface.base, iface.location._lexpos)
        base_idl_sym =f'XPIDL_{ iface.base }'

        base_loc = '%d:%d-%d' % (lineno, colno, colno + len(iface.base))

        iface_supers.append({
            'sym': base_idl_sym,
        })

        # Base source
        emit_record({
            'loc': base_loc,
            'source': 1,
            'syntax': 'idl',
            'pretty': f'IDL class {iface.base}',
            'sym': base_idl_sym,
        })

        # Base target
        emit_record({
            'loc': base_loc,
            'target': 1,
            'syntax': 'idl',
            'pretty': iface.base,
            'sym': base_idl_sym,
        })

    iface_methods = []
    iface_fields = []

    #print p.name
    #print 'BASE', p.base
    for m in iface.members:
        name = getattr(m, 'name', '')
        (lineno, colno) = find_line_column(text, name, m.location._lexpos)

        # Want to deal with attributes like 00, as well as ConstMember

        if isinstance(m, xpidl.Method):
            method_pretty = f'{iface.name}::{m.name}'
            method_cpp_sym = methods.get(cpp_method_name(m))
            method_idl_sym = f'XPIDL_{iface.name}_{m.name}'

            method_loc = '%d:%d-%d' % (lineno, colno, colno + len(m.name))

            # target
            emit_record({
                'loc': method_loc,
                'target': 1,
                'kind': 'idl',
                'pretty': method_pretty,
                'sym': method_idl_sym,
            })

            # Source
            emit_record({
                'loc': method_loc,
                'source': 1,
                'syntax': 'idl',
                'pretty': f'IDL method {method_pretty}',
                'sym': method_idl_sym,
            })

            method_slots = []
            if method_cpp_sym:
                method_slots.append({
                    'slotKind': 'method',
                    'slotLang': 'cpp',
                    'ownerLang': 'idl',
                    'sym': method_cpp_sym,
                })


            if not m.noscript:
                method_js_sym = f'#{m.name}'
                method_slots.append({
                    'slotKind': 'method',
                    'slotLang': 'js',
                    'ownerLang': 'idl',
                    'sym': method_js_sym,
                })

            # structured
            emit_record({
                'loc': method_loc,
                'structured': 1,
                'pretty': method_pretty,
                'sym': method_idl_sym,
                'kind': 'method',
                'implKind': 'idl',
                'bindingSlots': method_slots,
            })

            iface_methods.append({
                'pretty': method_pretty,
                'sym': method_idl_sym,
                'props': [],
            })

        elif isinstance(m, xpidl.Attribute):
            attr_pretty = f'{iface.name}::{m.name}'
            attr_idl_sym = f'XPIDL_{iface.name}_{m.name}'
            getter_cpp_sym = methods.get(cpp_getter_name(m))

            attr_loc = '%d:%d-%d' % (lineno, colno, colno + len(m.name))

            # target
            emit_record({
                'loc': attr_loc,
                'target': 1,
                'kind': 'idl',
                'pretty': attr_pretty,
                'sym': attr_idl_sym,
            })

            # source
            emit_record({
                'loc': attr_loc,
                'source': 1,
                'syntax': 'idl',
                'pretty': 'IDL attribute %s' % attr_pretty,
                'sym': attr_idl_sym,
            })

            attr_slots = []
            if getter_cpp_sym:
                attr_slots.append({
                    'slotKind': 'getter',
                    'slotLang': 'cpp',
                    'ownerLang': 'idl',
                    'sym': getter_cpp_sym,
                })

            if not m.readonly:
                setter_cpp_sym = methods.get(cpp_setter_name(m))

                if setter_cpp_sym:
                    attr_slots.append({
                        'slotKind': 'setter',
                        'slotLang': 'cpp',
                        'ownerLang': 'idl',
                        'sym': setter_cpp_sym,
                    })

            if not m.noscript:
                attr_js_sym = f'#{m.name}'
                attr_slots.append({
                    'slotKind': 'attribute',
                    'slotLang': 'js',
                    'ownerLang': 'idl',
                    'sym': attr_js_sym,
                })

            emit_record({
                'loc': attr_loc,
                'structured': 1,
                'pretty': attr_pretty,
                'sym': attr_idl_sym,
                'kind': 'field',
                'implKind': 'idl',
                'bindingSlots': attr_slots,
            })

            iface_fields.append({
                'pretty': attr_pretty,
                'sym': attr_idl_sym,
                'props': [],
            })


        elif isinstance(m, xpidl.ConstMember):
            const_pretty = f'{iface.name}::{m.name}'
            const_idl_sym = f'XPIDL_{iface.name}_{m.name}'
            const_cpp_sym = find_enum(enums, m.name)

            const_loc = '%d:%d-%d' % (lineno, colno, colno + len(m.name))

            # target
            emit_record({
                'loc': const_loc,
                'target': 1,
                'kind': 'idl',
                'pretty': const_pretty,
                'sym': const_idl_sym,
            })

            # source
            emit_record({
                'loc': const_loc,
                'source': 1,
                'syntax': 'idl',
                'pretty': 'IDL constant %s' % const_pretty,
                'sym': const_idl_sym,
            })

            const_slots = [
                {
                    'slotKind': 'const',
                    'slotLang': 'cpp',
                    'ownerLang': 'idl',
                    'sym': const_cpp_sym,
                }
            ]

            # there's no such thing as noscript for a const, so we just go based
            # on whether the interface is itself scriptable.
            # XXX uh, should we have been checking that above too?
            if iface.attributes.scriptable:
                const_js_sym = f'#{m.name}'
                const_slots.append({
                    'slotKind': 'const',
                    'slotLang': 'js',
                    'ownerLang': 'idl',
                    'sym': const_js_sym,
                })

            emit_record({
                'loc': const_loc,
                'structured': 1,
                'pretty': const_pretty,
                'sym': const_idl_sym,
                'kind': 'enum',
                'implKind': 'idl',
                'bindingSlots': const_slots,
            })

            iface_fields.append({
                'pretty': const_pretty,
                'sym': const_idl_sym,
                'props': [],
            })


    emit_record({
        'loc': '%d:%d-%d' % (lineno, colno, colno + len(iface.name)),
        'structured': 1,
        'pretty': iface.name,
        'sym': iface_idl_sym,
        'kind': 'class',
        'implKind': 'idl',
        'bindingSlots': iface_slots,
        'supers': iface_supers,
        'methods': iface_methods,
        'fields': iface_fields
    })

indexRoot = sys.argv[1]
fname = sys.argv[2]

text = open(fname).read()
analysis = read_cpp_analysis(fname)

linebreaks = []
lines = text.split('\n')
cur = 0
for l in lines:
    cur += len(l) + 1
    linebreaks.append(cur)

if analysis:
    (methods, enums) = analysis
    p = xpidl.IDLParser()

    try:
        r = p.parse(text, filename=fname)
    except xpidl.IDLError as e:
        print('Syntax error in IDL', fname, file=sys.stderr)
        raise e
        sys.exit(1)
    print('XPIDL: Parsed', fname, 'into', len(r.productions), 'productions', file=sys.stderr)
    for p in r.productions:
        if isinstance(p, xpidl.Interface):
            handle_interface(methods.get(p.name, {}), enums, p)
else:
    print('XPIDL: No C++ analysis data found for', fname, file=sys.stderr)

```

## scripts/ld-wrapper
```
#!/usr/bin/env python3

import subprocess
import sys
import os

# libtool doesn't understand "-Xclang" syntax and it passes some parameters
# from CC/CXX set by indexer-setup.py into the linker command unexpectedly,
# which causes ld command to fail. Filter them out here.
args = []
for a in sys.argv[1:]:
    if a == '-load':
        continue
    if 'libclang-index-plugin.so' in a:
        continue
    args.append(a)

ld = os.environ.get("WRAPPED_LD", "/usr/bin/ld")

p = subprocess.run([ld] + args)
sys.exit(p.returncode)

```

## scripts/process-chrome-map.py
```
#!/usr/bin/env python3

import glob
import json
import re
import os
import sys


def process_chrome_map(url_map, chrome_map_path, topsrcdir):
    if not os.path.exists(chrome_map_path):
        return

    with open(chrome_map_path, "r") as f:
        url_prefixes, overrides, install_info, buildconfig = json.load(f)

    # See m-c/python/mozbuild/mozbuild/codecoverage/lcov_rewriter.py.
    if "resource:///" not in url_prefixes:
        url_prefixes["resource:///"] = ["dist/bin/browser"]
    if "resource://gre/" not in url_prefixes:
        url_prefixes["resource://gre/"] = ["dist/bin"]

    reverse_prefixes = {}
    for from_prefix, to_prefixes in url_prefixes.items():
        for to_prefix in to_prefixes:
            reverse_prefixes[to_prefix] = from_prefix


    def map_path(path):
        """Returns all mapped URLs for given path or URL."""
        for from_prefix, to_prefix in reverse_prefixes.items():
            if path.startswith(from_prefix):
                mapped = to_prefix + path[len(from_prefix) + 1:]
                yield mapped

                yield from map_path(mapped)


    def get_overrides(url):
        """Returns all overridden URLs for given URL."""
        for to_name, from_name in overrides.items():
            if from_name == url:
                yield to_name

                yield from get_overrides(to_name)


    def add_entries(url_map, src, obj):
        urls = list(map_path(obj))
        if len(urls) == 0:
            return

        overridden_urls = []
        for url in urls:
            overridden_urls += get_overrides(url)
        urls += overridden_urls

        for url in urls:
            if url not in url_map:
                url_map[url] = []
            if src not in url_map[url]:
                url_map[url].append(src)


    for obj, item in install_info.items():
        src = item[0]

        if "*" in src:
            # The source path is written with glob.
            # Handle all matching files.
            for src_path in glob.glob(src, root_dir=topsrcdir):
                obj_path = os.path.join(obj, os.path.basename(src_path))
                add_entries(url_map, src_path, obj_path)
        else:
            add_entries(url_map, src, obj)


topsrcdir = sys.argv[1]
url_map_path = sys.argv[2]
chrome_map_paths = sys.argv[3:]

url_map = {}
for path in chrome_map_paths:
    process_chrome_map(url_map, path, topsrcdir)


def atEscape(text):
  return re.sub("[^A-Za-z0-9_/]", lambda m: "@" + "{:02X}".format(ord(m.group(0))), text)


alias_map = {}

for url, paths in url_map.items():
    aliases = []
    for path in paths:
        aliases.append({
            "pretty": "file " + path,
            "sym": "FILE_" + atEscape(path),
        })

    sym = "URL_" + atEscape(url)

    alias_map[sym] = aliases

with open(url_map_path, "w") as f:
    json.dump(alias_map, f)

```

## scripts/indexer-setup.py
```
#!/usr/bin/env python3

from __future__ import absolute_import
from __future__ import print_function
import sys
import os
import os.path

mozSearchRoot = os.environ['MOZSEARCH_PATH']
indexRoot = os.environ['INDEX_ROOT']
treeRoot = os.environ['FILES_ROOT']
objdir = os.environ['OBJDIR']

plugin_folder = os.path.join(mozSearchRoot, 'clang-plugin')

flags = [
    '-load', os.path.join(plugin_folder, 'libclang-index-plugin.so'),
    '-add-plugin', 'mozsearch-index',
    '-plugin-arg-mozsearch-index', treeRoot,
    '-plugin-arg-mozsearch-index', os.path.join(indexRoot, 'analysis'),
    '-plugin-arg-mozsearch-index', objdir,
    '-fparse-all-comments',
]
flags_str = " ".join([ '-Xclang {}'.format(flag) for flag in flags ])

# See the comment in ld-wrapper for more details.
if len(sys.argv) >= 2:
    if sys.argv[1] == '--use-ld-wrapper':
        flags_str += ' --ld-path={}'.format(os.path.join(mozSearchRoot, 'scripts', 'ld-wrapper'))

env = {
    'CC': "clang %s" % flags_str,
    'CXX': "clang++ %s" % flags_str,
}

for (k, v) in env.items():
    print('export {}="{}"'.format(k, v))

```

## scripts/update-fonts.sh
```
#!/usr/bin/env bash

set -x # Show commands
set -eu # Errors/undefined vars are fatal
set -o pipefail # Check all commands in a pipeline

# Usage:
#
# Run this script from the root of your searchfox directory, outside of your VM,
# so that we can open the fontello.com website in your browser to let you start
# from our existing configuration and you can pick new icons.
#
# We'll wait for you to hit enter, indicating you're done editing the config,
# then we'll download the new font you configured.  You should commit that and
# probably then everything is great.

# # Font Updating
#
# This script is an attempt to somewhat automate regeneration of our CSS fonts
# through "fontello.com".  This is just the continuation of decisions made
# during DXR to use this mechanism, and I am opting for minimal change.  One
# related decision, however, is that I am replacing the inline SVGs we use for
# the navigation panel to instead use the font.  I am presuming the coice for
# the inline SVGs was made because we didn't have an easy/documented way to
# add more icons to the CSS fonts.
#
# I'm completely okay with us changing how we do this in the future.
#
# ## Archaeology
#
# It seems that our CSS fonts were inherited from an early version of DXR where
# we used "fontello.com" to build CSS fonts.  As of now, when trying to update
# the fonts, we had fonts that included ["ok", "down-dir", "up-dir", "tree-2"]
# but where we only seem to use "down-dir" and we animate it rotating.  Through
# eyeballing it seems that these are from the "Font Awesome" font.
#
# ## The Fontello API / This script
#
# Documentation is at https://github.com/fontello/fontello#developers-api which
# explains the session mechanism.
#
# This script is derived from the Makefile cited in the docs above,
# https://gist.github.com/puzrin/5537065.  We're not using a Makefile because
# we've largely standardized on bash for our glue logic.

FONTELLO_ROOT=https://fontello.com

# POST our configuration to receive a session id that's good for 24 hours and
# will be written to `.fontello-sid`.  We only need to do this if we don't have
# a sufficiently recent SID.  We use `-mmin -1440` to express less than 24
# hours because `-mtime` quantizes to days, and we use less than and negation
# because we want to create the file if it doesn't exist in addition to it being
# too old.
if [ ! "$(find .fontello-sid -mmin -1440)" ]; then
    curl --silent --show-error --fail --output .fontello-sid \
        --form "config=@scripts/fontello-config.json" \
        ${FONTELLO_ROOT}
fi

FONTELLO_SID=$(cat .fontello-sid)

# Open the browser!
x-www-browser ${FONTELLO_ROOT}/${FONTELLO_SID}

read -p "Press enter when you're done updating the fontello config, or ctrl-c to bail."

rm -rf .fontello.src .fontello.zip

curl --silent --show-error --fail --output .fontello.zip \
	${FONTELLO_ROOT}/${FONTELLO_SID}/get

unzip .fontello.zip -d .fontello.src

FONT_ROOT=$(ls -d .fontello.src/fontello-*)
FONT_DIR=${FONT_ROOT}/font
CSS_DIR=${FONT_ROOT}/css
FONT_NAME=icons
SF_FONT=static/font
SF_CSS=static/css

# The config may have changed, update it.
mv ${FONT_ROOT}/config.json scripts/fontello-config.json

mkdir -p ${SF_FONT}
mv ${FONT_ROOT}/LICENSE.txt ${SF_FONT}/LICENSE.txt
mv ${FONT_DIR}/${FONT_NAME}.eot ${SF_FONT}/icons.eot
mv ${FONT_DIR}/${FONT_NAME}.svg ${SF_FONT}/icons.svg
mv ${FONT_DIR}/${FONT_NAME}.ttf ${SF_FONT}/icons.ttf
mv ${FONT_DIR}/${FONT_NAME}.woff ${SF_FONT}/icons.woff
mv ${FONT_DIR}/${FONT_NAME}.woff2 ${SF_FONT}/icons.woff2
mv ${CSS_DIR}/icons.css ${SF_CSS}/font-icons.css

rm -rf .fontello.zip .fontello.src
# we intentionally leave the .fontello-sid around for now to avoid generating
# unnecessary server load.

```

## scripts/mkindex.sh
```
#!/usr/bin/env bash

set -x # Show commands
set -eu # Errors/undefined vars are fatal
set -o pipefail # Check all commands in a pipeline

if [ $# != 3 ]
then
    echo "usage: $0 <config-repo-path> <config-file> <tree-name>"
    exit 1
fi

CONFIG_REPO=$1
CONFIG_FILE=$2
TREE_NAME=$3

# This script also depends on a `handle_tree_error` function having been defined
# by `load-vars.sh` which will log a warning/error and either return a non-zero
# (failure) exit code if the `on_error` tree mode was "halt" or a zero (success)
# exit code if the mode was "continue".
#
# We annotate a bunch of analysis-related steps with the error handler here
# because they seem potentially prone to breakage due to changes in the tree but
# where the breakage is recoverable in the sense that if we keep going, nothing
# else is likely to break.  It's possible that some of these steps failing will
# in fact turn out to be fatal later on, but we can address that as situations
# arise.

export PYTHONPATH=$MOZSEARCH_PATH/scripts

# activate the venv we created for livegrep so we have access to the grpc
# dependencies.
LIVEGREP_VENV=$HOME/livegrep-venv
source $LIVEGREP_VENV/bin/activate

# This was previously "full" but "1" is much more readable.  Obviously change
# this back if we end up missing things.
#
# Also note that if "parallel" is used, it's necessary to add an arg pair of
# "--env RUST_BACKTRACE" unless using "env_parallel".
export RUST_BACKTRACE=1

date

$MOZSEARCH_PATH/scripts/find-repo-files.py $CONFIG_REPO $CONFIG_FILE $TREE_NAME
$MOZSEARCH_PATH/scripts/mkdirs.sh

date

$MOZSEARCH_PATH/scripts/build.sh $CONFIG_REPO $CONFIG_FILE $TREE_NAME

date

export RUST_LOG=info

date

# Transform any .scip files the config `scip_subtrees` setting tells us about.
# This does not generate any .scip files; instead it is assumed that they would
# have been generated in the config's `build` script.  It's also assumed that
# for complicated situations like mozilla-central where merge-analyses may be
# required, that the scripts will handle calling `scip-analyze.sh` or
# `scip-indexer` directly and will not list them in `scip_subtrees`.
$MOZSEARCH_PATH/scripts/scip-analyze.sh \
  "$CONFIG_FILE" \
  "$TREE_NAME" || handle_tree_error "scip-analyze.sh"

date

$MOZSEARCH_PATH/scripts/find-objdir-files.sh
$MOZSEARCH_PATH/scripts/objdir-mkdirs.sh

date

URL_MAP_PATH=$INDEX_ROOT/aliases/url-map.json
DOC_TREES_MAP=$INDEX_ROOT/doc-trees.json

$MOZSEARCH_PATH/scripts/process-chrome-map.py $GIT_ROOT $URL_MAP_PATH $INDEX_ROOT/*.chrome-map.json || handle_tree_error "process-chrome-map.py"

date

$MOZSEARCH_PATH/scripts/js-analyze.sh $CONFIG_FILE $TREE_NAME || handle_tree_error "js-analyze.sh"

date

$MOZSEARCH_PATH/scripts/html-analyze.sh $CONFIG_FILE $TREE_NAME || handle_tree_error "html-analyze.sh"

date

$MOZSEARCH_PATH/scripts/css-analyze.sh $CONFIG_FILE $TREE_NAME || handle_tree_error "css-analyze.sh"

date

$MOZSEARCH_PATH/scripts/idl-analyze.sh $CONFIG_FILE $TREE_NAME || handle_tree_error "idl-analyze.sh"

date

$MOZSEARCH_PATH/scripts/staticprefs-analyze.sh $CONFIG_FILE $TREE_NAME || handle_tree_error "idl-analyze.sh"

date

$MOZSEARCH_PATH/scripts/ipdl-analyze.sh $CONFIG_FILE $TREE_NAME || handle_tree_error "ipdl-analyze.sh"

date

ANALYSIS_FILES_PATH=$INDEX_ROOT/all-analysis-files

$MOZSEARCH_PATH/scripts/generate-analsysis-files-list.sh $ANALYSIS_FILES_PATH || handle_tree_error "generate-analsysis-files-list.sh"

date

OTHER_RESOURCES_PATH=$INDEX_ROOT/other-resource-files

$MOZSEARCH_PATH/scripts/generate-other-resources-list.py $ANALYSIS_FILES_PATH $URL_MAP_PATH $OTHER_RESOURCES_PATH || handle_tree_error "generate-other-resources-list.py"

date

$MOZSEARCH_PATH/scripts/replace-aliases.sh $ANALYSIS_FILES_PATH || handle_tree_error "replace-aliases.sh"

date

# crossref failures always need to be fatal because their outputs are required.
$MOZSEARCH_PATH/scripts/crossref.sh $CONFIG_FILE $TREE_NAME $ANALYSIS_FILES_PATH $OTHER_RESOURCES_PATH

date

$MOZSEARCH_PATH/scripts/output.sh $CONFIG_REPO $CONFIG_FILE $TREE_NAME $URL_MAP_PATH $DOC_TREES_MAP || handle_tree_error "output.sh"

date

$MOZSEARCH_PATH/scripts/build-codesearch.py $CONFIG_FILE $TREE_NAME || handle_tree_error "build-codesearch.py"

date

# This depends on INDEX_ROOT already being available.  The script doesn't
# actually care about CONFIG_FILE or TREE_NAME, but it's helpful to
# `indexer-logs-analyze.sh`.
$MOZSEARCH_PATH/scripts/compress-outputs.sh $CONFIG_FILE $TREE_NAME || handle_tree_error "compress-outputs.sh"

date

# Check the resulting index for correctness, but there's no webserver so the
# 4th argument needs to be empty.  We now also need the livegrep server to be
# available, so start that first.
$MOZSEARCH_PATH/router/codesearch.py $CONFIG_FILE start $TREE_NAME
date
$MOZSEARCH_PATH/scripts/check-index.sh $CONFIG_FILE $TREE_NAME "filesystem" ""

# And we want to stop it after.  It's possible if we errored above that it will
# still be hanging around, but codesearch.py always stops an existing server
# first, so we're not really concerned about this affecting a re-run of the
# indexing process.
$MOZSEARCH_PATH/router/codesearch.py $CONFIG_FILE stop $TREE_NAME

date

```

## scripts/mkdirs.sh
```
#!/usr/bin/env bash

set -x # Show commands
set -eu # Errors/undefined vars are fatal
set -o pipefail # Check all commands in a pipeline

# Remove the analysis dir and any platform variations.
rm -rf $INDEX_ROOT/analysis*
# Remove any objdir variants; in general we expect the "objdir" itself to
# potentially have been created during the "setup" step, or at least that's the
# case for m-c.
rm -rf $INDEX_ROOT/objdir-*
rm -rf $INDEX_ROOT/generated-*
rm -rf $INDEX_ROOT/file
rm -rf $INDEX_ROOT/dir
rm -rf $INDEX_ROOT/description
rm -rf $INDEX_ROOT/templates
rm -rf $INDEX_ROOT/aliases

mkdir -p $INDEX_ROOT/analysis
mkdir -p $INDEX_ROOT/file
mkdir -p $INDEX_ROOT/dir
mkdir -p $INDEX_ROOT/description
mkdir -p $INDEX_ROOT/aliases

mkdir -p $INDEX_ROOT/analysis/__GENERATED__

set +x # This part is annoyingly verbose, silence commands and use manual echo statement
cat $INDEX_ROOT/repo-dirs | while IFS= read dir
do
    echo "Making {file,dir,analysis,description} dirs for $dir"
    mkdir -p "$INDEX_ROOT/file/$dir"
    mkdir -p "$INDEX_ROOT/dir/$dir"
    mkdir -p "$INDEX_ROOT/analysis/$dir"
    mkdir -p "$INDEX_ROOT/description/$dir"
done
set -x
mkdir -p $INDEX_ROOT/templates

```

## scripts/indexer-logs-fetch.sh
```
#!/usr/bin/env bash

if [ $(uname) == "Darwin" ]; then
    YESTERDAY=$(date -Idate -v -1d)
else
    YESTERDAY=$(date -Idate --date='1 days ago')
fi

# AFAICT in order to download a specific set of files, we need to use recursive
# and exclude everything and then only include what we want.  The following
# works:
aws s3 cp s3://indexer-logs/ . --recursive --exclude '*' --include "index-${YESTERDAY}*.gz"
# grep -z doesn't work on gzipped things, so de-gzip them
gunzip *.gz

# having 2 indexer1 jobs is bad, delete the UTC22 one
rm index-*T0*_release1_config1

```

## scripts/weblog-analyze.sh
```
#!/usr/bin/env bash

# Arguments: <log path>

# This script uses https://github.com/rcoh/angle-grinder in order to process our
# searchfox logs from /var/log/nginx/searchfox.log
#
# Install/update angle-grinder via `cargo install ag`

# We don't want to show commands, so no: set -x
set -eu # Errors/undefined vars are fatal
# We intentionally induce a pipe error in our use of head, so no: set -o pipefail

# Core parsing
PARSE_EXPR='parse "[*] [Cache:*] [*] [*] [Remote_Addr: *]'
PARSE_EXPR+=' - * - * to: *: \"GET /*/* HTTP/1.1\" * * \"*\" \"*\"" '
PARSE_EXPR+='as time_local, cache_status, request_time, host, remote_addr,'
PARSE_EXPR+=' remote_user, server_name, upstream_addr, repo, repo_path, status,'
PARSE_EXPR+=' body_bytes_sent, referrer, user_agent'
# But we would like to be able to extract the action from the repo_path which
# could look like "rev/MOREPATH" or "search?query".
PARSE_EXPR+=' | substring(repo_path, 0, 6) as maybe_search'
PARSE_EXPR+=' | maybe_search == "search" as is_search'

if [[ ${2:-} ]]; then
  MAYBE_REPO_FILTER="where repo == \"$2\" | "
  MAYBE_REPO_LABEL=" for $2"
else
  MAYBE_REPO_FILTER=""
  MAYBE_REPO_LABEL=""
fi

CACHE_CHECK='where cache_status != "-"'
MISS_CHECK='where cache_status == "MISS"'

GET_ACTION='where !is_search | parse "*/*" from repo_path as action, path'
ONLY_SEARCH='where is_search'

STATS='count, p50(request_time), p66(request_time), p75(request_time), p90(request_time), p95(request_time), p99(request_time)'

# This includes 2 lines of headers.
SLOW_COUNT=12

## Output dynamic request latencies

echo "### Dynamic Non-Search Request Latencies${MAYBE_REPO_LABEL}"
echo ''
echo '```'
cat $1 | agrind "* | ${PARSE_EXPR} |${MAYBE_REPO_FILTER} ${CACHE_CHECK} | ${GET_ACTION} | ${STATS} by action, cache_status"
echo '```'

echo ''
echo "### Dynamic Search Request Latencies${MAYBE_REPO_LABEL}"
echo ''
echo '```'
cat $1 | agrind "* | ${PARSE_EXPR} |${MAYBE_REPO_FILTER} ${CACHE_CHECK} | ${ONLY_SEARCH} | ${STATS} by cache_status"
echo '```'


echo ''
echo "### Slowest Searches${MAYBE_REPO_LABEL}"
echo ''
echo '```'
# agrind supports a limit operator but it appears there's a buggy optimization
# which ends up performing the limit prior to the sort happening.  This doesn't
# appear to be a string/numeric mismatch as things still happen if I try and
# force a coercion.
#
# So we just pipe the output through head.  Actually, we pipe it through tac
# first so that agrind doesn't emit an error about the closed pipe.
cat $1 | agrind "* | ${PARSE_EXPR} |${MAYBE_REPO_FILTER} ${MISS_CHECK} | ${ONLY_SEARCH} | sort by request_time desc | fields + request_time, repo_path" | tac | tail -n${SLOW_COUNT} | tac
echo '```'


echo ''
echo "### Slowest Rev Requests${MAYBE_REPO_LABEL}"
echo ''
echo '```'
# agrind supports a limit operator but it appears there's a buggy optimization
# which ends up performing the limit prior to the sort happening.  This doesn't
# appear to be a string/numeric mismatch as things still happen if I try and
# force a coercion.
#
# So we just pipe the output through head.  Actually, we pipe it through tac
# first so that agrind doesn't emit an error about the closed pipe.
cat $1 | agrind "* | ${PARSE_EXPR} |${MAYBE_REPO_FILTER} ${MISS_CHECK} | ${GET_ACTION} | sort by request_time desc | fields + request_time, path, action" | tac | tail -n${SLOW_COUNT} | tac
echo '```'

```

## scripts/objdir-mkdirs.sh
```
#!/usr/bin/env bash

set -x # Show commands
set -eu # Errors/undefined vars are fatal
set -o pipefail # Check all commands in a pipeline

cat $INDEX_ROOT/objdir-dirs | while IFS= read dir
do
  mkdir -p "$INDEX_ROOT/file/$dir"
  mkdir -p "$INDEX_ROOT/dir/$dir"
  mkdir -p "$INDEX_ROOT/description/$dir"
done

```

## scripts/output.sh
```
#!/usr/bin/env bash

set -x # Show commands
set -eu # Errors/undefined vars are fatal
set -o pipefail # Check all commands in a pipeline

if [ $# -ne 5 ]
then
    echo "Usage: output.sh config_repo config-file.json tree_name url-map.json"
    exit 1
fi

CONFIG_REPO=$(realpath $1)
CONFIG_FILE=$(realpath $2)
TREE_NAME=$3
URL_MAP_PATH=$4
DOC_TREES_PATH=$5

# let's put the "parallel" output in a new `diags` directory, as we're still
# seeing really poor output-file performance in bug 1567724.
DIAGS_DIR=$INDEX_ROOT/diags/output
mkdir -p $DIAGS_DIR
# clean up the directory since in the VM this can persist.
rm -f $DIAGS_DIR/*

JOBLOG_PATH=${DIAGS_DIR}/output.joblog
# let's put all the temp files in our diagnostic dir too.
TMPDIR_PATH=${DIAGS_DIR}

# parallel args:
# --jobs 8: Limits us to 8 jobs to avoid creating an OOM nightmare; this is
#   consistent with our vagrant-on-linux setting.  The immediate motivation is
#   that under docker all of the system's cores will be exposed, which is
#   good for non-memory-intensive things.  But for mozilla-central, each
#   output-file instance ends up using ~2GiB of RAM individually and so a memory
#   budget of ~16GiB is reasonable.  Otherwise on my 28-core machines the 64GiB
#   of RAM gets eaten up and every other process terminated.
#   TODO: Overhaul output-file to have better memory usage characteristics by
#   using forking or just being parallel itself or something.
# --pipepart, -a: Pass the filenames to each job on the job's stdin by chopping
#   up the file passed via `-a`.  Compare with `--pipe` which instead divvies
#   inside the parallel perl process and can in theory be a bottleneck.
# --files: Place .par files in the ${TMPDIR_PATH} above which is now not
#   actually a temporary directory but instead a path we save so that we can see
#   what the output of the run was.
# --joblog: Emit a joblog that can be used to `--resume` the previous job and
#   also provides us with general performance runtime info
# --tmpdir: We specify the location of the .par files via this.
# --block: by passing `-1` we indicate each job should get 1 block of data, with
#   the size of the block basically being (1/nproc * file size).  A value of
#   `-2` would give each job a block half the size and result in twice as many
#   jobs (and therefore twice as much overhead).  The general trade-off reason
#   you might do this is that parallel can detect when a process terminates but
#   not when it's idle.  So to load balance, you potentially would want more
#   jobs, but we're looking at a startup cost of ~15 seconds per process, and
#   we can process about 2000 lines of source per 0.1 second with all 4 cores
#   active, so that suggests we give up about 300kloc's worth of rendering for
#   additional job, which potentially covers a lot of slop.  Also, there's a
#   chance that as some output-file jobs complete earlier, the other jobs may
#   then accelerate as there is reduced contention for (SSD) I/O and spare RAM
#   may increase to allow for writes to be buffered without needing to flush,
#   etc.
# --env RUST_BACKTRACE: propagate the RUST_BACKTRACE environment variable.
# "2>&1": If we don't do this, `--files` seems to just eat the stderr output
#   which is obviously suboptimal.
parallel --jobs 8 --pipepart -a $INDEX_ROOT/all-files --files --joblog $JOBLOG_PATH --tmpdir $TMPDIR_PATH \
    --block -1 --halt 2 --env RUST_BACKTRACE \
    "$MOZSEARCH_PATH/tools/target/release/output-file $CONFIG_FILE $TREE_NAME $URL_MAP_PATH $DOC_TREES_PATH - 2>&1"

TOOL_CMD="search-files --limit=0 --include-dirs --group-by=directory | batch-render dir"
SEARCHFOX_SERVER=${CONFIG_FILE} \
    SEARCHFOX_TREE=${TREE_NAME} \
    $MOZSEARCH_PATH/tools/target/release/searchfox-tool "$TOOL_CMD"

TOOL_CMD="render search-template"
SEARCHFOX_SERVER=${CONFIG_FILE} \
    SEARCHFOX_TREE=${TREE_NAME} \
    $MOZSEARCH_PATH/tools/target/release/searchfox-tool "$TOOL_CMD"

TOOL_CMD="render help"
SEARCHFOX_SERVER=${CONFIG_FILE} \
    SEARCHFOX_TREE=${TREE_NAME} \
    $MOZSEARCH_PATH/tools/target/release/searchfox-tool "$TOOL_CMD"

TOOL_CMD="render settings"
SEARCHFOX_SERVER=${CONFIG_FILE} \
    SEARCHFOX_TREE=${TREE_NAME} \
    $MOZSEARCH_PATH/tools/target/release/searchfox-tool "$TOOL_CMD"

```

## scripts/rust-analyze.sh
```
#!/usr/bin/env bash

# This file has been forked to scip-analyze.sh
#
# This script:
# 1. Locates the rust save-analysis directories under the provided root.
# 2. Invokes rust-indexer with those analysis directories and provides a number
#    of path prefixes to help map file paths to searchfox's special
#    __GENERATED__ prefix.

set -x # Show commands
set -eu # Errors/undefined vars are fatal
set -o pipefail # Check all commands in a pipeline

if [ $# -lt 5 ]
then
    echo "Usage: rust-analyze.sh config-file.json tree_name rust_analysis_in generated_src sf_analysis_out"
    exit 1
fi

CONFIG_FILE=$(realpath $1)
TREE_NAME=$2
# This is where we find the save-analysis files.  For mozilla-central builds where
# we have multiple platform-specific objdirs that are processed in parallel,
# we expect this to be objdir-$PLATFORM.  For self-built single-platform cases,
# this will be the objdir.
RUST_ANALYSIS_IN=$3
# This is where we find the source code corresponding to __GENERATED__ files.
# This is the objdir in self-built single-platform cases and generated-$PLATFORM
# in multi-platform cases at the current time.
GENERATED_SRC=$4
# This is where we write the resulting searchfox analysis files.  We expect
# this to be a platform-specific directory like analysis-$PLATFORM in
# multi-platform cases (which will be processed by merge-analyses) and analysis
# in single-platform cases.
SF_ANALYSIS_OUT=$5

if [ -d "$RUST_ANALYSIS_IN" ]; then
  INPUTS="$(find $RUST_ANALYSIS_IN -type f -name rust.scip)"
  SCIP_FLAGS="--scip --scip-prefix $RUST_ANALYSIS_IN"
  if [ "x$INPUTS" = "x" ]; then
    INPUTS="$(find $RUST_ANALYSIS_IN -type d -name save-analysis)"
    SCIP_FLAGS=""
    # Rust stdlib files use `analysis` directories instead of `save-analysis`, so
    # even though they live under the same root, it needs a separate find pass
    # because the above will not have found them.
    #
    # Note that we also only expect a rustlib in gecko indexing jobs.
    if [ -d "$RUST_ANALYSIS_IN/rustlib" ]; then
      INPUTS="$INPUTS $(find $RUST_ANALYSIS_IN/rustlib -type d -name analysis)"
    fi
  fi

  if [ "x$INPUTS" = "x" ]; then
    exit 0 # Nothing to analyze really
  fi

  $MOZSEARCH_PATH/tools/target/release/rust-indexer \
    "$FILES_ROOT" \
    "$SF_ANALYSIS_OUT" \
    "$GENERATED_SRC" \
    $SCIP_FLAGS \
    $INPUTS
fi

```

## scripts/load-vars.sh
```
# This file is intentionally not executable, because it should always be sourced
# into a pre-existing shell. MOZSEARCH_PATH should be defined prior to sourcing.
# Arguments are the config.json in the index and the tree for which variables
# are desired

if [ -z $MOZSEARCH_PATH ]
then
    echo "Error: load-vars.sh is being sourced without MOZSEARCH_PATH defined"
    return # leave load-vars.sh without aborting the calling script
fi

export CONFIG_FILE=$1
export TREE_NAME=$2

export INDEX_ROOT=$(jq -r ".trees[\"${TREE_NAME}\"].index_path" ${CONFIG_FILE})
export FILES_ROOT=$(jq -r ".trees[\"${TREE_NAME}\"].files_path" ${CONFIG_FILE})
export OBJDIR=$(jq -r ".trees[\"${TREE_NAME}\"].objdir_path" ${CONFIG_FILE})
# This is usually the same as FILES_ROOT, but its presence does serve as a marker
# that we have revision control data.  There are cases like the "tests" repo
# where we don't have revision-control data.  Accordingly, it usually makes more
# sense to use $FILES_ROOT.
export GIT_ROOT=$(jq -r ".trees[\"${TREE_NAME}\"].git_path" ${CONFIG_FILE})
export BLAME_ROOT=$(jq -r ".trees[\"${TREE_NAME}\"].git_blame_path" ${CONFIG_FILE})
export HISTORY_ROOT=$(jq -r ".trees[\"${TREE_NAME}\"].history_path" ${CONFIG_FILE})
export TREE_ON_ERROR=$(jq -r ".trees[\"${TREE_NAME}\"].on_error" ${CONFIG_FILE})
export TREE_CACHING=$(jq -r ".trees[\"${TREE_NAME}\"].cache" ${CONFIG_FILE})
export WEBIDL_BINDINGS_LOCAL_PATH=$(jq -r ".trees[\"${TREE_NAME}\"].webidl_binding_local_path" ${CONFIG_FILE})
export STATICPREFS_BINDINGS_LOCAL_PATH=$(jq -r ".trees[\"${TREE_NAME}\"].staticprefs_binding_local_path" ${CONFIG_FILE})

handle_tree_error() {
    local msg=$1
    echo "warning: Tree '$TREE_NAME' error: $msg"
    if [[ $TREE_ON_ERROR == "continue" ]]; then
        return 0
    fi
    return 1
}
export -f handle_tree_error

# We expect the "cache" key to be one of ["everything", "codesearch", "nothing"]
cache_when_everything() {
    local relpath=$1
    if [[ $TREE_CACHING == "everything" ]]; then
        vmtouch -t $INDEX_ROOT/$relpath
    fi
    return 0
}
export -f cache_when_everything

cache_when_codesearch() {
    local relpath=$1
    if [[ $TREE_CACHING != "nothing" ]]; then
        vmtouch -t $INDEX_ROOT/$relpath
    fi
    return 0
}
export -f cache_when_codesearch

```

## scripts/generate-analsysis-files-list.sh
```
#!/usr/bin/env bash

set -x # Show commands
set -eu # Errors/undefined vars are fatal
set -o pipefail # Check all commands in a pipeline

ANALYSIS_FILES_PATH=$1

# Find the files to replace aliases.
#
# NOTE: analysis-files.list is not useful for this purpose; it only exists for
#       gecko trees and only reflects the analysis files present after running
#       process-tc-artifacts.sh for all platforms, not new analysis files
#       generated by mkindex.sh. (We leave it around on disk to help debug
#       anything that went wrong.)
cd $INDEX_ROOT/analysis
find . -type f | cut -c 3- > $ANALYSIS_FILES_PATH
cd -

```

## scripts/weblog-elb-analyze.sh
```
#!/usr/bin/env bash

# Arguments: <log path>

# This script uses https://github.com/rcoh/angle-grinder in order to process our
# searchfox logs from /var/log/nginx/searchfox.log
#
# Install/update angle-grinder via `cargo install ag`

# We don't want to show commands, so no: set -x
set -eu # Errors/undefined vars are fatal
# We intentionally induce a pipe error in our use of head, so no: set -o pipefail

# Core parsing
# But we would like to be able to extract the action from the repo_path which
# could look like "rev/MOREPATH" or "search?query".
#PARSE_EXPR+=' | substring(repo_path, 0, 6) as maybe_search'
#PARSE_EXPR+=' | maybe_search == "search" as is_search'

PARSE_EXPR='parse "* * * *:* *:* * * * * * * * \"*\" \"*\" * * * \"*\" \"*\" \"*\"'
PARSE_EXPR+=' * * \"*\" \"*\" \"*\" \"*\" \"*\" \"*\" \"*\"" '
PARSE_EXPR+='as type, time, elb, client_ip, client_port, target_ip, target_port,'
PARSE_EXPR+=' request_processing_secs, target_processing_secs, response_processing_secs,'
PARSE_EXPR+=' elb_status_code, target_status_code, received_bytes, sent_bytes,'
PARSE_EXPR+=' request, user_agent,'
PARSE_EXPR+=' ssl_cipher, ssl_protocol, target_group_arn,'
PARSE_EXPR+=' trace_id, domain_name, chosen_cert_arn, matched_rule_priority,'
PARSE_EXPR+=' request_creation_time, actions_executed, redirect_url, error_reason,'
PARSE_EXPR+=' target_list, target_status_code_list, classification, classification_reason'
PARSE_EXPR+=' | parse "* *://*:*/* *" from request as req_method, req_scheme, req_host, req_port, req_path, req_protocol nodrop'

# bot-wise, ignore:
# - search engine bots that helpfully have "bot" or "Bot" in the name.
# - bots that self-identify by fetching robots.txt and I manually added checks
#   for them trying to do clever stateful IP stuff.
# - uh, we're also getting like 8500 cert-manager checks a day?  That seems high?
NOT_BOT_CHECK='where !contains(user_agent, "bot") | where !contains(user_agent, "cert-manager")'
NOT_BOT_CHECK+=' | where !contains(user_agent, "Bot") | where !contains(user_agent, "LinkChecker")'
NOT_BOT_CHECK+=' | where user_agent != "The Knowledge AI" | where !contains(user_agent, "Riddler")'
NOT_BOT_CHECK+=' | where !contains(user_agent, "search") | where !contains(user_agent, "spider")'
# Something using a focus user agent keeps asking for amazon and wikipedia icons
FOCUS_CHECK='where contains(user_agent, "Firefox%20Focus")'
NOT_FOCUS_CHECK='where !contains(user_agent, "Firefox%20Focus")'

# Requests that aren't against our host (ex: random IP) or are a POST are sketchy.
# We just ignore these specific requests but I guess we could try and statefully
# ignore the IP.
SKETCHY_CHECK='where req_host == "searchfox.org" && req_method == "GET"'

# ## Parse the Path and Validate Tree Names ##
PARSE_PATH='parse regex "(?P<sf_tree>[^/]+)/(?P<sf_endpoint>[^/\?]+)(?P<_ign_path>/?(?P<sf_path>[^\?]*))(?P<_ign_query>\??(?P<sf_query>.*))" from req_path nodrop'
# This successfully parses a number of things where the tree is not actually a
# real tree and instead an attempt for an attacker to find a vulnerable wordpress
# instance, so let's filter down trees to those we know exist or might exist in
# the future.  (Like, matching "mozilla-" or "comm-" is a sufficient constraint
# for mozilla-central and beta and release and our ESRs).
PARSE_PATH+=' | where isEmpty(sf_tree) || (contains(sf_tree, "mozilla-") || contains(sf_tree, "comm-")'
PARSE_PATH+=' || sf_tree == "wubkat" || sf_tree == "kaios" || sf_tree == "glean"'
PARSE_PATH+=' || sf_tree == "nss" || sf_tree == "whatwg-html" || sf_tree == "ecma262"'
PARSE_PATH+=' || sf_tree == "l10n" || sf_tree == "llvm" || sf_tree == "rust"'
PARSE_PATH+=' || sf_tree == "mingw" || sf_tree == "mingw_moz"'
PARSE_PATH+=')'
# We're also getting a bunch of gibberish endpoints, so let's constrain those too.
PARSE_PATH+=' | where isEmpty(sf_endpoint)'
PARSE_PATH+=' || sf_endpoint == "commit" || sf_endpoint == "commit-info"'
PARSE_PATH+=' || sf_endpoint == "complete" || sf_endpoint == "define"'
PARSE_PATH+=' || sf_endpoint == "diff" || sf_endpoint == "file-lists"'
PARSE_PATH+=' || sf_endpoint == "hgrev" || sf_endpoint == "pages"'
PARSE_PATH+=' || sf_endpoint == "query" || sf_endpoint == "raw-analysis"'
PARSE_PATH+=' || sf_endpoint == "rev" || sf_endpoint == "search"'
PARSE_PATH+=' || sf_endpoint == "sorch" || sf_endpoint == "source"'
PARSE_PATH+=' || sf_endpoint == "static"'


# ## Categorize ##
#
# Attempt to categorize the request; we start from the sf_endpoint but do some
# overrides when that's not enough.

# favicon is favicon
CATEGORIZE='if(req_path == "favicon.ico", "favicon", sf_endpoint) as category'
# apple-touch-icon stuff is favicon
CATEGORIZE+=' | if(isEmpty(sf_tree) && contains(req_path, "apple-touch-icon"), "favicon", category) as category'
# just loading the root page should be specially noted
CATEGORIZE+=' | if(isEmpty(req_path), "root", category) as category'
# Let's also capture people having fashioned "mozilla-central" or "mozilla-central/"
# URLs that don't work.
CATEGORIZE+=' | if(req_path == "mozilla-central" || req_path == "mozilla-central/", "bare-tree", category) as category'
# Let's introduce a synthetic category for "search.js" as a representative single
# static resource so that we can reason about the implication of cache validation
# without having to know how many different static resources we have in that set.
#
# Right now our validity is 2 minutes.
CATEGORIZE+=' | if(sf_path == "js/search.js", "static1", category) as category'
# lastly, let's discard entries that have a null category.  In general these are
# sketchy requests that are looking for vulnerable server.
CATEGORIZE+=' | where !isEmpty(category)'

IP_GROUPING='count as req_count by sf_tree, category, client_ip'
IP_GROUPING+=' | sum(req_count) as total_reqs, count as ips_1rq'
IP_GROUPING+=', count(req_count >= 2) as ips_2rq'
IP_GROUPING+=', sum(if(req_count >= 2, req_count, 0)) as reqs_2rq'
IP_GROUPING+=', count(req_count >= 4) as ips_4rq'
IP_GROUPING+=', sum(if(req_count >= 4, req_count, 0)) as reqs_4rq'
IP_GROUPING+=', count(req_count >= 8) as ips_8rq'
IP_GROUPING+=', sum(if(req_count >= 8, req_count, 0)) as reqs_8rq'
IP_GROUPING+=', count(req_count >= 16) as ips_16rq'
IP_GROUPING+=', sum(if(req_count >= 16, req_count, 0)) as reqs_16rq'
IP_GROUPING+=', count(req_count >= 32) as ips_32rq'
IP_GROUPING+=', sum(if(req_count >= 32, req_count, 0)) as reqs_32rq'
IP_GROUPING+=', count(req_count >= 64) as ips_64rq'
IP_GROUPING+=', sum(if(req_count >= 64, req_count, 0)) as reqs_64rq'
IP_GROUPING+=', count(req_count >= 128) as ips_128rq'
IP_GROUPING+=', sum(if(req_count >= 128, req_count, 0)) as reqs_128rq'
IP_GROUPING+=', count(req_count >= 256) as ips_256rq'
IP_GROUPING+=', sum(if(req_count >= 256, req_count, 0)) as reqs_256rq'
IP_GROUPING+=', count(req_count >= 512) as ips_512rq'
IP_GROUPING+=', sum(if(req_count >= 512, req_count, 0)) as reqs_512rq'
IP_GROUPING+=', count(req_count >= 1024) as ips_1024rq'
IP_GROUPING+=', sum(if(req_count >= 1024, req_count, 0)) as reqs_1024rq'
IP_GROUPING+=', count(req_count >= 2048) as ips_2048rq'
IP_GROUPING+=', sum(if(req_count >= 2048, req_count, 0)) as reqs_2048rq'
IP_GROUPING+=' by sf_tree, category | sort by sf_tree, category'

# From stackoverflow answer https://stackoverflow.com/a/34282594/17236969 by
# "peak" and edited by "TWiStErRob", a JQ pipeline to convert our JSON
# to a CSV rep that we can upload to Google sheets.  Note that this does depend
# on having a version of angle-grinder with https://github.com/rcoh/angle-grinder/pull/177
# or successor in it in order to have the keys ordered as desired, although this
# conversion is clever enough that we could also just insert a synthetic first
# row.
JQ_CSV_SCRIPT='(.[0] | keys_unsorted) as $firstkeys'
JQ_CSV_SCRIPT+=' | (map(keys) | add | unique) as $allkeys'
JQ_CSV_SCRIPT+=' | ($firstkeys + ($allkeys - $firstkeys)) as $cols'
JQ_CSV_SCRIPT+=' | ($cols, (.[] as $row | $cols | map($row[.])))'
JQ_CSV_SCRIPT+=' | @csv'

AGRIND_CORE="* | ${PARSE_EXPR} | ${NOT_BOT_CHECK} | ${NOT_FOCUS_CHECK} | ${SKETCHY_CHECK} | ${PARSE_PATH} | ${CATEGORIZE}"
#echo agrind "${AGRIND_CMD} | fields req_host, req_method"
if [[ ${1:-} == "export" ]]; then
  cat *.log | agrind -o json "${AGRIND_CORE} | ${IP_GROUPING}" | jq -r "${JQ_CSV_SCRIPT}"
else
  cat *.log | agrind  "${AGRIND_CORE} | ${IP_GROUPING}"

#cat *.log | agrind  "${AGRIND_CORE} | where sf_endpoint == \"static\""
#cat *.log | agrind "${AGRIND_CORE} | where sf_tree == \"mozilla-central\" && category == \"search\" | count by sf_tree, category, client_ip"

#cat *.log | agrind "${AGRIND_CORE} | where category == \"favicon\" | count by client_ip"
#cat *.log | agrind "${AGRIND_CORE} | fields category, req_path, sf_tree, sf_endpoint"
#cat *.log | agrind "${AGRIND_CORE} | where req_path == \"robots.txt\" | fields user_agent"

#cat *.log | agrind "* | ${PARSE_EXPR} | fields request"
#cat *.log | agrind "* | ${PARSE_EXPR} | ${NOT_BOT_CHECK} | ${NOT_FOCUS_CHECK} | count by client_ip, user_agent"
#cat *.log | agrind "* | ${PARSE_EXPR} | ${NOT_BOT_CHECK} | ${FOCUS_CHECK} | count"
#

fi

```

## scripts/replace-aliases.py
```
#!/usr/bin/env python3

import json
import glob
import os
import re
import sys

analysis_dir = sys.argv[1]
alias_map_path = sys.argv[2]


def at_escape(text):
  return re.sub("[^A-Za-z0-9_/]", lambda m: "@" + "{:02X}".format(ord(m.group(0))), text)


def at_unescape(text):
  return re.sub("@[0-9A-F][0-9A-F]", lambda m: chr(int(m.group(0)[1:], 16)), text)


def resolve_relpath(url, relpath):
    if url.startswith("chrome://"):
        prefix = "chrome:/"
        path = url[8:]
    elif url.startswith("resource://"):
        prefix = "resource:/"
        path = url[10:]

    parent = os.path.dirname(path)
    resolved = os.path.normpath(os.path.join(parent, relpath))

    return prefix + resolved


def replace_aliases(path, alias_map, reverse_map):
    """Replace URL records and RELPATH records with FILE records, based on
    the mapping extracted from *.chrome-map.json files.

    Note that the analysis record transformations done below are only safe for
    URL records which are currently never referenced for contextsym purposes
    or by structured records. Any more involved transformations should likely
    happen in rust code where we have the analysis.rs types available and can
    easily add helper transforms."""

    has_alias = False
    lines = []

    fullpath = os.path.join(analysis_dir, path)

    with open(fullpath, "r") as f:
        for line in f:
            line = line.rstrip()

            # Filter out definitely non-target lines.
            if "URL_" not in line and "RELPATH" not in line:
                lines.append(line)
                continue

            datum = json.loads(line)
            sym = datum["sym"]

            # Actually test if the symbol is the target.
            if not sym.startswith("URL_") and sym != "RELPATH":
                lines.append(line)
                continue

            handled_syms = set()

            has_alias = True


            def handle_url_sym(sym):
                if sym not in alias_map:
                    return

                for alias in alias_map[sym]:
                    datum["sym"] = alias["sym"]
                    datum["pretty"] = alias["pretty"]

                    # NOTE: A file can have multiple URLs, and resolving a
                    #       relative path from them can result in the same URL.
                    if datum["sym"] in handled_syms:
                        continue
                    handled_syms.add(datum["sym"])

                    lines.append(json.dumps(datum))


            if sym == "RELPATH":
                # This is special record for relative path import.
                # Resolve it based on the URLs for the current file,
                # and then map it to the corresponding files.
                relpath = datum["pretty"]

                if path not in reverse_map:
                    continue

                for url in reverse_map[path]:
                    new_url = resolve_relpath(url, relpath)
                    if new_url is None:
                        continue
                    new_sym = "URL_" + at_escape(new_url)
                    handle_url_sym(new_sym)
            else:
                handle_url_sym(sym)

    if not has_alias:
        return

    with open(fullpath, "w") as f:
        for line in lines:
            print(line, file=f)


with open(alias_map_path, "r") as f:
    alias_map = json.load(f)

reverse_map = {}
for sym, aliases in alias_map.items():
    for item in aliases:
        path = item["pretty"].replace("file ", "")
        url = at_unescape(sym.replace("URL_", ""))

        if path not in reverse_map:
            reverse_map[path] = []

        reverse_map[path].append(url)

for path in sys.stdin:
    path = path.rstrip()
    if not path:
        continue

    replace_aliases(path, alias_map, reverse_map)

```

## scripts/html-analyze.sh
```
#!/usr/bin/env bash

set -x # Show commands
set -eu # Errors/undefined vars are fatal
set -o pipefail # Check all commands in a pipeline

if [ $# -ne 2 ]
then
    echo "Usage: html-analyze.sh config-file.json tree_name"
    exit 1
fi

CONFIG_FILE=$(realpath $1)
TREE_NAME=$2

# Required by std::wcsrtombs, used in os.file.redirect.
export LC_CTYPE=C.UTF-8

# Add line number for the file list with `nl`, which is used as a global
# fileIndex and used for local variable symbols.
#
# See the comment in js-analyze.sh for more details.
cat $INDEX_ROOT/html-files | nl -w1 -s " " | \
    parallel --jobs 8 --pipe --halt 2 \
    js -f $MOZSEARCH_PATH/scripts/js-analyze.js -- \
    $MOZSEARCH_PATH $FILES_ROOT $INDEX_ROOT/analysis
echo $?

```

## scripts/check-index.sh
```
#!/usr/bin/env bash

set -x # Show commands
set -eu # Errors/undefined vars are fatal
set -o pipefail # Check all commands in a pipeline

if [[ $# -ne 4 ]]
then
    echo "usage: $0 <config-file> <tree-name> <do-local-check> <server-url>"
    echo ""
    echo "Pass empty strings for do-local-check or server-url to not perform"
    echo "those checks."
    exit 1
fi

CONFIG_FILE=$(realpath $1)
TREE_NAME=$2
CHECK_DISK=$3
CHECK_SERVER_URL=$4

# CARGO_TEST_EXTRA_ARGS exists so you can evaluate something like the following
# before runing `make build-test-repo` and have hacky `println!` output from
# your tests show up.
#
# ```
# export CARGO_TEST_EXTRA_ARGS="-- --nocapture"
# ```
#
# CARGO_TEST_LOG can also be used in order to get logging to happen in realtime
# to stdout.  This probably depends on the above too.
#
# XXX enabling the extra log output is currently using try_from_default_env
# which we're always setting below, but we should be only conditionally setting
# RUST_LOG.
#
# ```
# export CARGO_TEST_LOG=tools=trace
# ```

if [[ -d $CONFIG_REPO/$TREE_NAME/checks ]]
then
  # change into the test dir in order to ensure there's no confusion about
  # whether our config.toml should be used.
  pushd ${MOZSEARCH_PATH}/tools
  if [[ $CHECK_DISK ]]; then
    RUST_LOG=${CARGO_TEST_LOG:-} RUST_BACKTRACE=1 \
      SEARCHFOX_SERVER=${CONFIG_FILE} \
      SEARCHFOX_TREE=${TREE_NAME} \
      CHECK_ROOT=${CONFIG_REPO}/${TREE_NAME}/checks \
      cargo test --release test_check_glob ${CARGO_TEST_EXTRA_ARGS:-}
  fi
  if [[ $CHECK_SERVER_URL ]]; then
    RUST_LOG=${CARGO_TEST_LOG:-} RUST_BACKTRACE=1 \
      SEARCHFOX_SERVER="$CHECK_SERVER_URL" \
      SEARCHFOX_TREE=${TREE_NAME} \
      CHECK_ROOT=${CONFIG_REPO}/${TREE_NAME}/checks \
      cargo test --release test_check_glob ${CARGO_TEST_EXTRA_ARGS:-}
  fi
  popd
  #$CONFIG_REPO/$TREE_NAME/check "$MOZSEARCH_PATH/scripts/check-helper.sh" "$CHECK_DISK" "$CHECK_SERVER_URL"
fi

```

## scripts/fontello-config.json
```
{
  "name": "icons",
  "css_prefix_text": "icon-",
  "css_use_suffix": false,
  "hinting": true,
  "units_per_em": 1000,
  "ascent": 850,
  "glyphs": [
    {
      "uid": "c8585e1e5b0467f28b70bce765d5840c",
      "css": "docs",
      "code": 61637,
      "src": "fontawesome"
    },
    {
      "uid": "e99461abfef3923546da8d745372c995",
      "css": "cog",
      "code": 59392,
      "src": "fontawesome"
    },
    {
      "uid": "2d6150442079cbda7df64522dc24f482",
      "css": "down-dir",
      "code": 59393,
      "src": "fontawesome"
    },
    {
      "uid": "fb1c799ffe5bf8fb7f8bcb647c8fe9e6",
      "css": "right-dir",
      "code": 59394,
      "src": "fontawesome"
    },
    {
      "uid": "9dc654095085167524602c9acc0c5570",
      "css": "left-dir",
      "code": 59395,
      "src": "fontawesome"
    },
    {
      "uid": "12f4ece88e46abd864e40b35e05b11cd",
      "css": "ok",
      "code": 59396,
      "src": "fontawesome"
    },
    {
      "uid": "9dd9e835aebe1060ba7190ad2b2ed951",
      "css": "search",
      "code": 59397,
      "src": "fontawesome"
    },
    {
      "uid": "895405dfac8a3b7b2f23b183c6608ee6",
      "css": "export",
      "code": 59398,
      "src": "fontawesome"
    },
    {
      "uid": "f9cbf7508cd04145ade2800169959eef",
      "css": "font",
      "code": 59399,
      "src": "fontawesome"
    },
    {
      "uid": "9396b2d8849e0213a0f11c5fd7fcc522",
      "css": "tasks",
      "code": 61614,
      "src": "fontawesome"
    },
    {
      "uid": "5381e124761bc5dbcf32ec5dd86f91d6",
      "css": "brush",
      "code": 59400,
      "src": "typicons"
    },
    {
      "uid": "13b9eebfea581ad8e756ee7a18a7cba8",
      "css": "export-alt",
      "code": 61773,
      "src": "fontawesome"
    }
  ]
}

```

## static/robots.txt
```
# Disallow everything
User-Agent: *
Disallow: /

# ... except the nginx-served pregenerated content for the repos
# that Mozilla is the canonical owner for. For things like whatwg-html
# we don't want random Google searches sending random people to
# searchfox because it is unlikely to be the best destination for them.
# Also we don't allow crawls on mozilla-beta and other release repos
# because it's better if people get directed to mozilla-central instead.
Allow: /mozilla-central/source/
Allow: /comm-central/source/
Allow: /mozilla-mobile/source/
# There's a copy of mozilla-central inside comm-central, let's skip that
# since we have a top-level mozilla-central instead.
Disallow: /comm-central/source/mozilla/

# Also allow indexing the top-level help page, so as to improve
# findability when people search in search engines.
Allow: /index.html

```

## static/font/icons.woff2
```
wOF2     ï¿½      ï¿½  ï¿½                       T` ï¿½"	ï¿½
ï¿½Dï¿½G 6$4 ï¿½)ï¿½ï¿½Bï¿½l@ ï¿½[ï¿½ï¿½ï¿½nï¿½ï¿½5Å˜ï¿½ï¿½ï¿½ï¿½BXSï¿½Vï¿½ï¿½uï¿½ï¿½Û¾GÚŠï¿½ï¿½vï¿½,ï¿½2_ï¿½L!ï¿½,<ï¿½ï¿½ï¿½ï¿½ï¿½GfH&ï¿½ï¿½ï¿½eÖ²ï¿½ï¿½PEwï¿½e&Mï¿½2\fï¿½ï¿½mï¿½fï¿½L7cadc7ï¿½ic?ï¿½pï¿½ï¿½ï¿½ï¿½9ï¿½ï¿½ï¿½"Wï¿½ï¿½kï¿½ï¿½Eä…‡ï¿½ï¿½ ï¿½ï¿½ï¿½ï¿½<4aï¿½-ï¿½_ï¿½ï¿½éª›ï¿½ï¿½_ï¿½ï¿½p6ï¿½ï¿½%k	bï¿½ï¿½ï¿½{ï¿½m*1ï¿½{ï¿½ï¿½ï¿½ï¿½ï¿½  ï¿½Zï¿½Ò¾ï¿½3}ï¿½(ï¿½!	ï¿½ï¿½ï¿½ï¿½ï¿½Ù™ï¿½ï¿½Ew@YBï¿½ï¿½ï¿½bï¿½ï¿½ï¿½ Ù­Hb!ï¿½b=kï¿½exCuï¿½ï¿½ï¿½ï¿½ï¿½ï¿½3zï¿½xï¿½ï¿½S#ï¿½7ï¿½ !)&P{ï¿½9C|mï¿½ï¿½e @ï¿½ï¿½RÖ–ï¿½ï¿½sï¿½B?ï¿½ï¿½Îªï¿½ï¿½3XPï¿½ÏOtï¿½ï¿½İ¢@a5ï¿½ï¿½;ï¿½ï¿½wï¿½_gï¿½'}ï¿½c\ï¿½_ç³€ï¿½ï¿½ï¿½Yï¿½ï¿½mï¿½,/ï¿½ï¿½ï¿½H.Bb8!\ï¿½ï¿½ï¿½Yï¿½ï¿½ï¿½ï¿½?}Yï¿½ï¿½!eï¿½ï¿½^2ï¿½ï¿½ï¿½ï¿½QG: Ê˜/ï¿½ kï¿½Bï¿½Eï¿½Mï¿½Cï¿½Kï¿½Gï¿½ï¿½f>ï¿½2ï¿½ï¿½ï¿½ï¿½ï¿½1#ï¿½O60ÊŸ/ï¿½ï¿½oï¿½.p6ï¿½Mï¿½ï¿½?ï¿½Oqï¿½}ï¿½4ï¿½ï¿½tg_PQï¿½ï¿½ï¿½ï¿½ï¿½\ï¿½9ï¿½!ï¿½uï¿½#ï¿½;Ç€ï¿½dYÔ«hHï¿½S/Yï¿½C
(9Çªï¿½ï¿½ï¿½6ï¿½7Bï¿½Qï¿½ssĞ«ï¿½ï¿½HMï¿½ï¿½Gqï¿½Ç—ï¿½ï¿½O	ï¿½ï¿½]ï¿½p"}ï¿½>ï¿½ï¿½ï¿½ï¿½ï¿½3ELï¿½ï¿½ï¿½è«“Vï¿½ï¿½&}ï¿½uï¿½ï¿½ï¿½wï¿½<:lï¿½cï¿½ï¿½Æ‰ï¿½ï¿½LW	ï¿½ï¿½ï¿½^M*ï¿½/-ï¿½lï¿½Iï¿½ï¿½ï¿½ï¿½,ï¿½Û®'ï¿½2Tï¿½_ï¿½=2 iï¿½ï¿½oTï¿½N%8ï¿½ï¿½ï¿½)ï¿½ï¿½ï¿½Uï¿½ï¿½_ï¿½Ì‰sï¿½ï¿½2cï¿½Cï¿½xï¿½ï¿½5Zï¿½BÆ‚ï¿½ï¿½
g	ï¿½#ï¿½Gï¿½!%ï¿½dm(ï¿½ï¿½ï¿½D	à¼º.î¼•×‹|lï¿½[|ï¿½tÉ¿okï¿½ï¿½ï¿½ï¿½ï¿½W)ï¿½\Ù·ï¿½ï¿½MHï¿½'ï¿½ï¿½Jï¿½x<.ï¿½ï¿½ï¿½ï¿½ï¿½cï¿½xTï¿½ï¿½XPï¿½ï¿½E
^;@ï¿½zï¿½sï¿½ï¿½ÈŒ@ï¿½ï¿½.ï¿½ï¿½GÖ³Bï¿½ï¿½ï¿½ï¿½Ç«>ï¿½@ï¿½"rï¿½æ–¬ï¿½Yï¿½]]ï¿½Vï¿½	ï¿½kï¿½ï¿½ï¿½&iiï¿½ï¿½ï¿½ï¿½&?ï¿½Ø¸ï¿½ï¿½' ï¿½ï¿½!Î¹yï¿½?cSï¿½gï¿½;ï¿½vï¿½rPï¿½ï¿½!s!ï¿½ï¿½\Dï¿½á“±#ï¿½
ï¿½Fï¿½Oan~ ï¿½)ï¿½ï¿½ï¿½?#ï¿½]]%ï¿½Yï¿½ï¿½Vï¿½ï¿½ï¿½ï¿½ï¿½TModï¿½ï¿½ï¿½ï¿½Eï¿½ï¿½ï¿½×…xï¿½ï¿½`ï¿½BTï¿½oEï¿½2=R#yï¿½`7*bï¿½Icï¿½7dEï¿½Teï¿½ï¿½nï¿½ï¿½ï¿½ï¿½m{ï¿½ï¿½ï¿½#ï¿½ï¿½ï¿½ï¿½rï¿½ï¿½ï¿½inPï¿½0ï¿½ï¿½ï¿½ï¿½ï¿½+ï¿½!Lï¿½ï¿½ZzßŠï¿½TQï¿½oï¿½eLï¿½#1ï¿½ï¿½Qï¿½ß’ï¿½eï¿½[:ï¿½c
_Rkï¿½Xf'oï¿½ï¿½@ï¿½ï¿½A&ï¿½7"PBï¿½eï¿½@9ï¿½PATï¿½ï¿½ï¿½$ï¿½iï¿½ï¿½ï¿½+ï¿½<}ï¿½Tï¿½sN99ï¿½ï¿½ï¿½ï¿½vï¿½ï¿½kï¿½ï¿½YKï¿½dï¿½ï¿½ï¿½ï¿½ï¿½Q?Ñ°ï¿½Xï¿½ï¿½uhXï¿½ï¿½hØˆï¿½MhØŒï¿½-hØŠVï¿½	ï¿½Iï¿½ï¿½U?SRdï¿½wgï¿½ï¿½)tï¿½k\
ï¿½ï¿½ï¿½ï¿½tCd*Sï¿½@Ë†.Ô‡NTNpï¿½ï¿½ï¿½ï¿½È¡ï¿½=ï¿½ï¿½vï¿½ï¿½<Nï¿½M@ï¿½W6C'ï¿½Aï¿½ï¿½ï¿½ï¿½ï¿½ï¿½0ï¿½3#ï¿½ï¿½ï¿½ï¿½/ ï¿½ï¿½ï¿½Fï¿½ï¿½e1ï¿½ï¿½ dGï¿½ï¿½ï¿½`wï¿½#
ï¿½9ï¿½-ï¿½ï¿½wï¿½aT%ï¿½ï¿½ï¿½ï¿½æ”²ï¿½ï¿½ï¿½ï¿½>Ißœ7ï¿½c?ï¿½ï¿½|ï¿½j`ï¿½ï¿½ï¿½#&ï¿½9ï¿½ï¿½kTvï¿½ï¿½{ï¿½ï¿½";ï¿½ï¿½b4IV%8ï¿½ï¿½ï¿½ï¿½ï¿½É´ï¿½ï¿½ï¿½ï¿½Øºï¿½ï¿½&ï¿½oX\ï¿½Vï¿½ï¿½ï¿½ï¿½:M'9;Qï¿½ï¿½ï¿½.jï¿½ï¿½ï¿½0ï¿½ï¿½eimÂ¬ï¿½]*/ï¿½ï¿½O/*ï¿½3'ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½1KZ)ï¿½*ï¿½ï¿½ï¿½+ï¿½(Xï¿½ï¿½ï¿½Tï¿½ï¿½Hï¿½XKï¿½>Ñ”X`L[ï¿½6ï¿½ï¿½ï¿½Æ¿ï¿½x"!ILï¿½ï¿½(Kï¿½lï¿½ï¿½$Jï¿½ï¿½LjdGï¿½ï¿½ï¿½%ï¿½ï¿½M`ï¿½ï¿½Ç¸ï¿½ï¿½-Qï¿½Ğ­)ï¿½<ï¿½ï¿½LÖ¸ï¿½ï¿½ï¿½Qï¿½6Y[ï¿½iihWï¿½+ï¿½Pï¿½4sï¿½ï¿½dÇšï¿½Ò¥ï¿½cï¿½Cï¿½ï¿½lï¿½]ï¿½2ï¿½5ï¿½#9ï¿½ï¿½{ï¿½hï¿½7ï¿½ï¿½ï¿½Fï¿½ï¿½cC*ï¿½&ï¿½0M7)ï¿½ï¿½ï¿½ï¿½7Oï¿½Bï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½CqÄ“ï¿½bjï¿½\ï¿½ï¿½tG7ï¿½7ï¿½doa#ï¿½ï¿½ï¿½Jï¿½ï¿½ï¿½ï¿½ï¿½ï¿½Eï¿½|Bï¿½ï¿½-Ilï¿½ï¿½'ï¿½ï¿½ï¿½"ï¿½xUï¿½0ï¿½ï¿½Ñ§ï¿½ï¿½42ï¿½êª¢ï¿½!1ï¿½sï¿½ï¿½w=ï¿½3xï¿½ï¿½ï¿½SDï¿½i2ï¿½'g8ï¿½ï¿½"xï¿½hï¿½ï¿½ï¿½	ï¿½ï¿½xAï¿½ï¿½ï¿½EMï¿½R^ï¿½ï¿½ï¿½ï¿½UMÕµï¿½p/ï¿½ï¿½ï¿½dï¿½ï¿½<Tï¿½)<@ï¿½x
ï¿½Dpï¿½ï¿½ï¿½	ï¿½ï¿½%ï¿½Ë"ï¿½ï¿½ï¿½ï¿½ï¿½8ï¿½5M`=P74ï¿½ï¿½DeL{Î–-ï¿½tï¿½ï¿½ï¿½)uSâ¦…ï¿½Ä­_WÈŒï¿½Tï¿½;Tï¿½n*ï¿½ï¿½>ï¿½ï¿½ï¿½qï¿½Awï¿½CLï¿½#+Sï¿½'ï¿½Lï¿½Â§ï¿½>ï¿½'ï¿½>ï¿½/,/ï¿½ï¿½+*ï¿½5ï¿½Mï¿½}ï¿½ï¿½ï¿½Nï¿½ï¿½ï¿½ï¿½T`ï¿½Jï¿½ï¿½#[ï¿½upï¿½ï¿½8ï¿½Oï¿½7ï¿½ï¿½>zovï¿½ï¿½ï¿½ï¿½ï¿½#rï¿½1Xå• ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½^ï¿½ï¿½aQï¿½LNN<ï¿½ï¿½vï¿½Kï¿½ï¿½Ü¥ï¿½ï¿½:iï¿½eï¿½ï¿½×²,ï¿½ï¿½ï¿½lï¿½v_*ï¿½FY\ï¿½ï¿½wzï¿½ï¿½ï¿½ï¿½p{<ï¿½ï¿½[6ï¿½ï¿½yï¿½ï¿½\#ï¿½ï¿½ï¿½8ï¿½ajï¿½Aï¿½ï¿½ï¿½+ï¿½ï¿½ï¿½{|:ï¿½ï¿½vyQï¿½ï¿½9ï¿½ï¿½Fï¿½/ï¿½Kv$Mï¿½#ï¿½ï¿½ï¿½ï¿½Fï¿½Plg_ï¿½pï¿½ï¿½q{0[aï¿½ï¿½zo6ï¿½ï¿½PBSï¿½bï¿½Zï¿½ï¿½ï¿½ï¿½ï¿½ï¿½<ï¿½ï¿½ï¿½Y}^ï¿½/gï¿½ï¿½|Oï¿½ï¿½Hï¿½ï¿½Lï¿½
"ï¿½ï¿½ï¿½ï¿½ï¿½@A:Uï¿½ï¿½}ï¿½{zï¿½hï¿½~eï¿½{ï¿½2ï¿½ï¿½]U2.ï¿½ï¿½wï¿½ï¿½,NÆ’4ï¿½jqï¿½ï¿½ï¿½Gy{ï¿½ÆCï¿½.ï¿½ï¿½l|ï¿½Ãˆdï¿½ï¿½mï¿½ï¿½7ï¿½ï¿½É…9ï¿½gï¿½&QUaï¿½ï¿½%ï¿½i	ï¿½9l	8ï¿½iï¿½H-+~ï¿½@ï¿½ï¿½ï¿½kï¿½ï¿½pï¿½"ï¿½ï¿½ï¿½va^ï¿½ï¿½jyï¿½ï¿½Nï¿½ï¿½ï¿½Iï¿½ï¿½ï¿½(ï¿½^ï¿½ï¿½\ï¿½Sï¿½Bï¿½İ¸ï¿½ï¿½	Tï¿½_{ï¿½n<Wï¿½ï¿½ï¿½_YYï¿½=ï¿½ï¿½ï¿½Yï¿½ï¿½{QKï¿½5ï¿½ï¿½ï¿½n_Õ“â” bï¿½ï¿½ï¿½`vFï¿½Nï¿½ï¿½ï¿½ï¿½ï¿½ï¿½cï¿½Üƒ"ï¿½uï¿½sï¿½ï¿½Rï¿½ï¿½(& OlWBï¿½ï¿½xï¿½M/_5ï¿½\Ğ±ï¿½ï¿½ï¿½ï¿½VG5IC;ï¿½ï¿½Ô¦ Ñ«zï¿½ï¿½FjVkyĞ¾(Dï¿½ï¿½ï¿½ï¿½Iï¿½td\ï¿½ÒOï¿½aX%R*Zï¿½ï¿½ï¿½ï¿½ï¿½$ï¿½ï¿½Iqï¿½eQSï¿½dï¿½1ï¿½oOï¿½Tï¿½ï¿½!-{Û‹$2K23ï¿½&ï¿½Tï¿½ï¿½=ï¿½+ï¿½&Ql%eï¿½ï¿½7ï¿½ï¿½ï¿½Yï¿½Yï¿½Pï¿½ï¿½ï¿½ï¿½ï¿½sï¿½n9ï¿½ï¿½(>iï¿½Nï¿½ï¿½ï¿½nï¿½ï¿½fDï¿½ï¿½^ï¿½ï¿½Oï¿½ï¿½ï¿½Ùï¿½ï¿½I"ï¿½ï¿½iï¿½ï¿½ï¿½ï¿½<oIï¿½Dï¿½ï¿½Eï¿½j1qï¿½ï¿½Û‡A"ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½[Æ¾ï¿½>?ï¿½Ø±ï¿½ï¿½k5ï¿½Oï¿½?ï¿½4ï¿½33ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½xFï¿½ï¿½ï¿½t&>ï¿½ï¿½ï¿½ï¿½eï¿½Lbk[ï¿½ï¿½M2ï¿½ï¿½9ï¿½ï¿½p"ï¿½ï¿½YBÊŒFï¿½2ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½kjï¿½7!ï¿½ï¿½ï¿½ï¿½sï¿½ï¿½G?J5Jï¿½0zï¿½%Uï¿½ï¿½<ï¿½ï¿½ï¿½ß¾i{ÑŒï¿½QCï¿½m{Å‰ï¿½ï¿½Û³ï¿½7fYï¿½ï¿½ï¿½ï¿½*4wH>ï¿½$ï¿½;ß®Yï¿½ï¿½ï¿½İ¶ï¿½Aï¿½9nï¿½sï¿½]ï¿½_ï¿½ï¿½ï¿½ï¿½2~ï¿½Oï¿½ZÔ‰3Tİ‹ï¿½ï¿½\NU/ï¿½_zGï¿½Bï¿½ï¿½ï¿½ï¿½ï¿½#/Dï¿½ï¿½@ï¿½	ï¿½Pï¿½fkï¿½ï¿½(J\MÂ¬2=lï¿½ï¿½*ï¿½ï¿½ï¿½ï¿½^ï¿½0ï¿½ï¿½ï¿½mï¿½ï¿½Yï¿½1cï¿½ï¿½r5ï¿½ï¿½ï¿½u,ï¿½Lï¿½RVï¿½ï¿½ï¿½Òºï¿½rï¿½ï¿½4_ï¿½ï¿½<ï¿½Wï¿½Nv[ï¿½ï¿½>Vï¿½ï¿½ï¿½KÓ³)Qï¿½ï¿½@T5l^:mï¿½U*Cï¿½ï¿½ï¿½bwï¿½}ï¿½Sï¿½v.4>ï¿½%ï¿½ï¿½ï¿½ï¿½ï¿½x![p"&ï¿½ï¿½Hï¿½Å†de`1gï¿½ï¿½)ï¿½x y)ï¿½hu<ï¿½Iï¿½Uï¿½ï¿½Æ»ï¿½oï¿½-~kHï¿½a'ï¿½ï¿½ï¿½ï¿½ï¿½Iï¿½ï¿½ï¿½ï¿½Srï¿½ï¿½ï¿½İ»ï¿½4ï¿½ï¿½ï¿½qï¿½Ğ¡/ï¿½/ï¿½w_6;Oï¿½×‘ï¿½/ï¿½ï¿½_ï¿½ï¿½~}ï¿½ï¿½Qï¿½:ï¿½ï¿½iï¿½Xï¿½ï¿½ï¿½&3Ê ï¿½ZEï¿½ï¿½^wï¿½ï¿½.Iï¿½ï¿½+ï¿½ï¿½ufï¿½kï¿½Sï¿½ï¿½5uï¿½ï¿½ï¿½ï¿½ ï¿½P	ï¿½@ï¿½ï¿½<×‰h5P-mï¿½ï¿½`ï¿½ï¿½Kï¿½ï¿½Ì‘)ï¿½ï¿½ï¿½ï¿½\ï¿½kï¿½ï¿½#Xcï¿½81ï¿½7 ï¿½Aï¿½ï¿½A+ï¿½ï¿½ï¿½^ï¿½tï¿½ï¿½ï¿½mï¿½ï¿½É‚|Ï•ï¿½ï¿½Dï¿½ ï¿½ï¿½nï¿½ï¿½S?Jï¿½ï¿½P\qï¿½qï¿½AAe9aJï¿½ï¿½_ï¿½ï¿½ï¿½Pï¿½Rï¿½ï¿½ï¿½Zï¿½ï¿½hQï¿½ï¿½ï¿½ï¿½-Ä°ï¿½h-Dï¿½4Zï¿½ï¿½ï¿½Â‚|ï¿½ï¿½ï¿½Ò»ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½Oï¿½'jk<ï¿½ï¿½}ï¿½ï¿½ï¿½ï¿½O=ï¿½ï¿½Pï¿½ï¿½ï¿½ï¿½ï¿½Ê¶t01Pï¿½ï¿½ï¿½á¹¤tkï¿½~ï¿½ï¿½VÚªï¿½(ï¿½ï¿½yï¿½9eï¿½kĞšó€¾†F/?ï¿½ï¿½uï¿½ï¿½4ï¿½3ï¿½shï¿½&]ï¿½É¤ï¿½^ï¿½ï¿½Ş>tptï¿½ï¿½ï¿½ï¿½ï¿½/ï¿½ï¿½::Úšï¿½++ï¿½/(Gï¿½+ï¿½ï¿½uzï¿½4(Nï¿½ï¿½ï¿½Ö”Õ´7ï¿½wï¿½rï¿½ï¿½Hï¿½ï¿½ï¿½BtP(sï¿½ï¿½Dï¿½"ï¿½00aï¿½^ï¿½Vï¿½^Oï¿½{ï¿½_ï¿½Cï¿½$ï¿½ï¿½ï¿½ï¿½ï¿½áï¿½tï¿½6766ï¿½ï¿½87ï¿½ï¿½ï¿½ï¿½uï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½>ï¿½Æ¡ï¿½bT ï¿½ï¿½X<cï¿½ï¿½ï¿½ï¿½ï¿½ï¿½bL""ï¿½ï¿½ï¿½qï¿½{ï¿½ï¿½Oï¿½ï¿½Jï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½Úšï¿½ï¿½x4pTKUï¿½%ï¿½ï¿½)ï¿½ï¿½ï¿½-Oï¿½ï¿½,Ëˆ1ï¿½Aï¿½\Ãˆ9ï¿½ï¿½W_ï¿½Zï¿½\E\ï¿½Rï¿½_ê±ï¿½ï¿½ï¿½ï¿½ï¿½Qï¿½ï¿½Zyï¿½!ï¿½ï¿½2ï¿½$ï¿½6Oï¿½ï¿½Hï¿½4XÆ¾aï¿½Sï¿½I2_3Qï¿½?ï¿½@ï¿½Hï¿½Mg2Xc7ï¿½ï¿½ï¿½]|xï¿½ï¿½sï¿½ï¿½]4ï¿½oï¿½ï¿½ï¿½5
ï¿½ï¿½%:%ï¿½}k,Kï¿½ï¿½Üšï¿½6.ï¿½ï¿½ï¿½ï¿½hï¿½ï¿½
_@kï¿½Pï¿½E\bR#-ï¿½iï¿½KBï¿½ï¿½Cï¿½wMï¿½99~Iï¿½Fï¿½Ú¯ï¿½gï¿½nI~xï¿½}bÑº,Zï¿½ï¿½Dï¿½ï¿½ï¿½ï¿½9C~ï¿½/ï¿½Ö‹BLd6ï¿½ï¿½ï¿½2ï¿½;Sï¿½~ï¿½z7ï¿½uwï¿½ï¿½ï¿½ï¿½|ï¿½ï¿½ï¿½ï¿½_ g]VÑï¿½ï¿½ï¿½fï¿½-pï¿½eYï¿½6Iï¿½ï¿½MIzHMï¿½$ï¿½qf'Ù¤ï¿½yï¿½ï¿½Y#ï¿½4wï¿½Í’ï¿½ï¿½E2Mr!ï¿½ï¿½ï¿½gï¿½vï¿½w2ï¿½dï¿½ï¿½ï¿½ï¿½!ò”œ¾ÈLM3ï¿½ï¿½ï¿½ï¿½2ï¿½1o9ï¿½ï¿½['é¦“	ï¿½;dg:ï¿½ï¿½9xé·¸kdï¿½ï¿½yï¿½ ï¿½ÍˆOï¿½dï¿½ï¿½$[ï¿½rï¿½ï¿½ï¿½t
.oï¿½ï¿½6B]"$)ï¿½^ï¿½,>&Jï¿½ï¿½ï¿½ï¿½_	%"Qï¿½I=ï¿½ï¿½ï¿½Uï¿½ï¿½[ï¿½ï¿½)ï¿½T+Oï¿½ï¿½Uwï¿½6Ú½_M5ï¿½37ï¿½ï¿½Zï¿½gï¿½IÌšï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½Sï¿½ï¿½ï¿½T9ï¿½ï¿½uï¿½;xï¿½ï¿½ï¿½ï¿½ï¿½ï¿½KDrï¿½ï¿½{ï¿½S/ï¿½Oï¿½oï¿½\ï¿½ï¿½iï¿½ï¿½ï¿½Oï¿½{2ï¿½ï¿½iï¿½ï¿½ï¿½ï¿½ï¿½1!)ï¿½o<9~ï¿½ï¿½oï¿½ï¿½|ï¿½ï¿½Qß†ß³sï¿½={ 
```

## static/font/icons.woff
```
wOFF     D      ï¿½                       GSUB  X   ;   T ï¿½%zOS/2  ï¿½   D   `>#Rï¿½cmap  ï¿½   ï¿½  "ï¿½+ï¿½;cvt   p          fpgm  |  ï¿½  b.ï¿½zgasp  	l         glyf  	t  ï¿½  
Dï¿½ï¿½ï¿½head      3   6$ï¿½ï¿½5hhea  T      $-Ohmtx  t   *   4)ï¿½ï¿½ï¿½loca  ï¿½      ï¿½Nmaxp  ï¿½        >ï¿½name  ï¿½  {  ï¿½ï¿½uï¿½post  X   m   ï¿½eï¿½5prep  ï¿½   z   ï¿½~ï¿½;ï¿½xï¿½c`d``ï¿½b0`ï¿½c`rqï¿½	aï¿½ï¿½I,ï¿½cï¿½b`aï¿½ ï¿½<2ï¿½1'3=ï¿½ï¿½ï¿½Ê±ï¿½i fï¿½ï¿½ &;H xï¿½c`aï¿½dï¿½ï¿½ï¿½ï¿½ï¿½ï¿½TÅ´ï¿½ï¿½ï¿½ï¿½B3>`0ddï¿½2ï¿½23`iï¿½)^0|ï¿½eï¿½ï¿½ï¿½Å¼ï¿½aPï¿½E ftï¿½xï¿½ï¿½ï¿½ï¿½0Dï¿½BHï¿½@+ï¿½R_TBtï¿½^ï¿½Ó© ï¿½ï¿½+ï¿½	ï¿½zï¿½>ï¿½ï¿½ï¿½ï¿½)2ï¿½7	ï¿½ï¿½ï¿½ï¿½ï¿½{ï¿½ï¿½Ï¬ï¿½'F::Ë·lï¿½r`ï¿½yDÒ¹G]ï¿½wz#ë§ï¿½nï¿½4ï¿½3k0ï¿½ï¿½ï¿½îŸ³2ï¿½ï¿½Rï¿½-ï¿½1ï¿½Iï¿½4Hs,ï¿½ï¿½X ï¿½[ï¿½.[CŞ°/ï¿½ ,ï¿½  xï¿½c`@    xï¿½ï¿½Wk[ï¿½ï¿½ï¿½ï¿½Bï¿½Íºï¿½(cQï¿½ï¿½$qb+ï¿½eQ%ï¿½ï¿½ï¿½ï¿½unï¿½Hï¿½Mï¿½^ï¿½ï¿½ï¿½5ï¿½_ï¿½?sVï¿½Oï¿½oï¿½iyï¿½ï¿½JÜ§ï¿½S>ï¿½3ï¿½Îœï¿½YHhIï¿½~ï¿½Rvï¿½ï¿½ï¿½.Uï¿½=ï¿½ï¿½Kï¿½ï¿½ï¿½Şï¿½ï¿½H?ï¿½Ó¢ï¿½Wï¿½nï¿½N"&ï¿½ï¿½ï¿½pDï¿½ï¿½Mr4ï¿½ï¿½aï¿½
Zï¿½Uï¿½IE-Gï¿½ï¿½ï¿½ï¿½2Iï¿½ï¿½ï¿½ï¿½Yï¿½P!ï¿½;ï¿½4ï¿½ ï¿½t@ï¿½ï¿½ï¿½P(ï¿½ï¿½\ï¿½ï¿½ï¿½hï¿½ï¿½ï¿½ï¿½%ï¿½ï¿½GUï¿½ï¿½5Eï¿½Ä£ï¿½`ï¿½4=ï¿½ë£• ï¿½	ï¿½Hï¿½ï¿½Gï¿½ï¿½ï¿½kï¿½\ï¿½Cï¿½ï¿½Qï¿½ï¿½ï¿½xï¿½ï¿½dwIï¿½ï¿½0ï¿½	6m0Úˆcï¿½Y6,ï¿½ï¿½ï¿½|$iï¿½ï¿½×™ï¿½y/ï¿½ï¿½ï¿½0ï¿½4Ó‹ï¿½H^ï¿½atï¿½ï¿½ï¿½ï¿½Mï¿½8v-ï¿½ï¿½$v#]&ï¿½1vï¿½tï¿½Ñ•nï¿½hAï¿½ï¿½ï¿½,ï¿½ï¿½xï¿½ï¿½ï¿½xqï¿½{ï¿½ï¿½Q~Ü¤ï¿½ï¿½ï¿½ï¿½ï¿½Hï¿½ï¿½TĞ‹hJï¿½4ï¿½|d [ï¿½&ULï¿½	9È¦ï¿½}É‹ï¿½kï¿½ï¿½_IØ§ï¿½Zï¿½ï¿½ï¿½!teï¿½ï¿½"ï¿½%=7İ#ï¿½cIï¿½ï¿½"ï¿½ï¿½ï¿½Ü”&Mi:x#Qï¿½iï¿½ï¿½Pï¿½
ï¿½ï¿½ï¿½
ï¿½ï¿½ï¿½ï¿½ï¿½Zkï¿½9-ï¿½ï¿½yï¿½Uï¿½ï¿½Oï¿½ï¿½$fJï¿½eï¿½ï¿½Ñ£sï¿½"ï¿½ï¿½ï¿½ï¿½pï¿½ë“…4kOq<ï¿½ï¿½ï¿½Dï¿½Cï¿½rRMï¿½ï¿½ï¿½	!ï¿½ï¿½È±ï¿½Hï¿½Jï¿½ï¿½ï¿½ï¿½3ï¿½ï¿½Uï¿½î—®ï¿½4ï¿½ï¿½CGsï¿½ï¿½Bï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½
ï¿½ï¿½ï¿½Vï¿½4ï¿½RÒ…ï¿½5>  ï¿½ï¿½bï¿½`òµˆƒLP$bĞ‡fZ9L$-"lMZï¿½İ½(+ï¿½ï¿½4wï¿½ï¿½Tï¿½İï¿½{ï¿½Nï¿½uï¿½Wï¿½ï¿½ï¿½ï¿½ï¿½Rp?Ê–ï¿½rRï¿½=ï¿½r(-?ï¿½ï¿½?ï¿½!gï¿½(6zQï¿½áƒ¿ï¿½ï¿½Ú…ï¿½ï¿½Â¶1vï¿½:oï¿½Mï¿½ï¿½t`ï¿½'ï¿½uF
3!ï¿½
ï¿½BAï¿½9ï¿½cï¿½Uï¿½"ï¿½p/ï¿½%ï¿½Ëï¿½Q~sï¿½pï¿½ï¿½ï¿½?ï¿½.9bQTï¿½ï¿½ï¿½ï¿½eï¿½9iï¿½<ï¿½Ñ§ï¿½ï¿½4Âµk^ï¿½.ï¿½ï¿½ay	ï¿½fï¿½5ï¿½Y>ï¿½ï¿½KWgeï¿½ï¿½uVaï¿½uï¿½Mï¿½ï¿½ï¿½ï¿½iï¿½ï¿½ï¿½ï¿½9ï¿½ï¿½Vï¿½ï¿½S%Aï¿½ï¿½lï¿½ï¿½ß–&ï¿½cï¿½+ï¿½Åï¿½bï¿½ï¿½ï¿½ï¿½dï¿½cï¿½(ï¿½ï¿½Ş™~Â©[Wï¿½ï¿½ï¿½ï¿½ï¿½áŸ„]Oï¿½?ï¿½
ï¿½ï¿½ï¿½
ï¿½X6ï¿½ï¿½Uï¿½ï¿½ï¿½ï¿½ï¿½ï¿½5ï¿½ï¿½ï¿½[ï¿½ï¿½ï¿½ï¿½cï¿½Ò²m
ï¿½ï¿½ï¿½ï¿½Kï¿½Dï¿½sï¿½ï¿½ï¿½ï¿½ï¿½ï¿½5ï¿½ï¿½ï¿½Gï¿½qï¿½ï¿½Uï¿½ï¿½3ï¿½ï¿½ï¿½Åï¿½ï¿½ï¿½ï¿½vï¿½ï¿½lï¿½rï¿½ï¿½3kYÙ©ï¿½ï¿½"{ï¿½Ü±ï¿½Å¹ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½qï¿½ï¿½_Õ‰k{ï¿½-</Vï¿½%ï¿½oï¿½ï¿½ï¿½n85ï¿½ï¿½&ï¿½Nï¿½ï¿½%ï¿½hï¿½ï¿½uï¿½İ¤ï¿½ï¿½FEaï¿½A)+Ù’n	ï¿½ï¿½á°£:ï¿½!>t]ï¿½Hï¿½S[Fï¿½oï¿½wï¿½ï¿½Eï¿½Jhï¿½Cï¿½fï¿½Oï¿½ï¿½`ï¿½RRï¿½ï¿½8ï¿½ï¿½Iï¿½lï¿½ï¿½ï¿½1[Rï¿½=es':*É²tï¿½Jï¿½ï¿½bï¿½;ï¿½ï¿½ï¿½2;ï¿½vBï¿½ï¿½ï¿½ï¿½pï¿½ï¿½ï¿½R)Hï¿½ï¿½xTï¿½\
R8ï¿½Nï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½mï¿½XAï¿½6ï¿½X3ï¿½Ñ‚ï¿½NQï¿½lOï¿½ï¿½ï¿½ e\ï¿½+ï¿½ï¿½D6ï¿½ï¿½Fï¿½wï¿½/uï¿½ï¿½ï¿½XHÌ–Wï¿½Xï¿½6ï¿½ï¿½ï¿½dï¿½fï¿½ï¿½ï¿½ï¿½Rï¿½ï¿½Iï¿½i{QKï¿½ï¿½ï¿½ï¿½ï¿½dï¿½ï¿½TPï¿½ï¿½ï¿½ï¿½ï¿½ï¿½.6ï¿½ï¿½U{ï¿½-ï¿½%ï¿½ï¿½1Kï¿½qï¿½ï¿½ï¿½yï¿½ï¿½qï¿½7ï¿½?Zï¿½mï¿½D=oï¿½lÇ­lï¿½YÆ½}ï¿½ï¿½ï¿½ï¿½;ï¿½êŸºï¿½I;Mï¿½ï¿½')ï¿½ï¿½tï¿½ï¿½6ï¿½18u&	mï¿½:vï¿½ï¿½eï¿½ï¿½Uï¿½hï¿½uï¿½Tï¿½ï¿½ï¿½pï¿½ï¿½ï¿½ï¿½:ï¿½ï¿½[3ï¿½ï¿½?ï¿½tï¿½ï¿½Uï¿½ï¿½ï¿½ï¿½ï¿½Bï¿½:V/ï¿½8ï¿½ï¿½ï¿½|ï¿½Gï¿½Uï¿½n{uï¿½ï¿½%ï¿½fï¿½ï¿½Aï¿½^{|ï¿½ï¿½ï¿½W[tï¿½ï¿½ï¿½3ï¿½8ï¿½Yï¿½ï¿½ï¿½ï¿½kzï¿½ï¿½bï¿½pï¿½m<ï¿½ï¿½hï¿½ï¿½ï¿½ï¿½ï¿½ï¿½oï¿½ï¿½ = ï¿½ï¿½ï¿½9ffï¿½ï¿½ï¿½cN`ï¿½9ï¿½3ï¿½ï¿½wï¿½ï¿½ï¿½ï¿½ï¿½0 ï¿½ï¿½ï¿½b}ï¿½Ø¹@vï¿½-ï¿½9ï¿½ï¿½fï¿½Aï¿½0Ï wï¿½gï¿½{ï¿½3HX'ï¿½ï¿½u2ï¿½gï¿½ï¿½ï¿½y`ï¿½ï¿½aï¿½9ï¿½gï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½.F?0v1ï¿½ï¿½ï¿½ï¿½ï¿½Ccï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½Çˆq{ï¿½ï¿½ï¿½ï¿½m~dï¿½+ï¿½sï¿½ï¿½ï¿½ï¿½ï¿½xksï¿½ï¿½,dï¿½ï¿½ï¿½ï¿½9ï¿½ï¿½ï¿½&ï¿½ï¿½ÒŒÌCyÇ¯,dï¿½ï¿½qNNï¿½ï¿½ï¿½Lï¿½ï¿½ï¿½Lï¿½ï¿½w&ï¿½ï¿½ŞŒï¿½ï¿½ï¿½ï¿½ï¿½Gï¿½ï¿½	ï¿½ï¿½	ï¿½ï¿½	ï¿½ï¿½ï¿½y5#Cï¿½ï¿½ï¿½Lï¿½ï¿½ï¿½Lï¿½vï¿½ZÈ„ï¿½ï¿½Lï¿½Tï¿½Î›/[ï¿½ï¿½ï¿½Rï¿½ï¿½&ï¿½ï¿½ï¿½ï¿½hï¿½ï¿½ï¿½W{ï¿½ï¿½Çºï¿½%ï¿½A     ï¿½ï¿½ xï¿½ï¿½V[oï¿½ï¿½3ï¿½;ï¿½jw).g)ï¿½ï¿½x/u3ï¿½$[ï¿½ï¿½iï¿½ï¿½Å”%Ë‚mï¿½ï¿½dï¿½e;Nï¿½m4ï¿½k$Qï¿½ï¿½m8Aï¿½1ï¿½Tï¿½mQï¿½Hï¿½Cï¿½d?Tï¿½yï¿½ï¿½Ğ¼Ô‰ï¿½ï¿½ï¿½ï¿½@Ú‡ï¿½Ã™=ï¿½g>ï¿½ï¿½ï¿½\ï¿½PB6+ï¿½ï¿½ï¿½dwc×™ï¿½ï¿½Nï¿½ï¿½ï¿½:ï¿½bï¿½,ï¿½ï¿½$ï¿½ï¿½,3ï¿½ï¿½eBï¿½,ï¿½`ï¿½HTZ&ï¿½^:ï¿½ï¿½Ã‡ï¿½ï¿½+ï¿½ï¿½ï¿½nï¿½"Eï¿½riDï¿½ï¿½;ï¿½Æ…+nB>=ï¿½~ï¿½ï¿½ï¿½Uï¿½<giï¿½s^mjï¿½ï¿½|Å«ï¿½jï¿½3Y.ï¿½ï¿½ï¿½ï¿½q(ï¿½>nï¿½Zr;`'@ï¿½gï¿½ï¿½ï¿½#?<qï¿½ï¿½ÓPï¿½)ï¿½Lï¿½S=Qï¿½ï¿½ï¿½U5`ï¿½ï¿½ifï¿½=BVÔ‚nï¿½gï¿½
ï¿½rN5ï¿½ì–®:ï¿½jï¿½ï¿½ï¿½`ï¿½ï¿½iï¿½ï¿½ï¿½YÖ¶ï¿½uXï¿½ï¿½ï¿½47wï¿½o'Â±3Yxï¿½zï¿½ÚŒï¿½:_ï¿½uï¿½5ï¿½ï¿½%+ï¿½ ï¿½Ûº=ï¿½ï¿½pgï¿½?Tï¿½qEï¿½ï¿½wTï¿½6ï¿½O(~hï¿½NLï¿½6ï¿½ï¿½~ï¿½dï¿½Kï¿½ï¿½ï¿½:H$ï¿½ï¿½ï¿½TIï¿½qï¿½ï¿½7ï¿½Â„/M1ï¿½[\ï¿½ï¿½ï¿½Oï¿½ï¿½ï¿½pï¿½ï¿½ï¿½\%ï¿½M ï¿½	;ï¿½ï¿½>ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½Sï¿½ï¿½ï¿½Jï¿½ï¿½ï¿½Mnï¿½mSï¿½ï¿½2	ï¿½ï¿½$ï¿½ï¿½ï¿½Ôï¿½>hï¿½kLatL$ï¿½ï¿½(İ¥ï¿½0ï¿½mDï¿½-<
tï¿½ß ï¿½ï¿½Hï¿½ï¿½ï¿½hfï¿½ï¿½ï¿½NHwï¿½Vï¿½ï¿½[ï¿½[Vï¿½ï¿½%ï¿½6_Xï¿½ï¿½hß…ï¿½ï¿½vï¿½"tssï¿½ï¿½Ô…~ï¿½"ï¿½ï¿½İ–ï¿½Jï¿½"ï¿½M29ï¿½ï¿½,ï¿½<#ï¿½"ï¿½ï¿½ÜŒï¿½VXï¿½xï¿½#& Ú›dï¿½ï¿½0ï¿½ï¿½ï¿½uï¿½+ï¿½kï¿½ï¿½ï¿½ï¿½ï¿½1ï¿½ï¿½ï¿½Wfnï¿½kï¿½ï¿½Å•W.ï¿½aï¿½ï¿½0ï¿½ï¿½tï¿½ï¿½ï¿½ï¿½ï¿½gï¿½Û½ï¿½ï¿½ï¿½'.ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½]gß™zï¿½ï¿½ï¿½0ï¿½ï¿½ï¿½hï¿½,ï¿½Fï¿½ï¿½qGTPï¿½ï¿½ï¿½0Ef7ï¿½&]Gj(zï¿½p@ï¿½,	9<J$ï¿½,ï¿½ï¿½>|hï¿½@6ï¿½lË–5ï¿½[ï¿½Lï¿½}ï¿½ï¿½Zï¿½$ï¿½ï¿½	6Æƒï¿½pSï¿½ï¿½sï¿½ï¿½ï¿½$Bï¿½ï¿½ï¿½=ï¿½4gï¿½ï¿½ w8,ï¿½	^+WPï¿½<ï¿½Hï¿½ï¿½ï¿½ï¿½É…ï¿½ï¿½?4ï¿½}ï¿½<ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½n[ï¿½ï¿½ï¿½ï¿½nq.ï¿½ï¿½ï¿½ï¿½ï¿½Cï¿½ï¿½ï¿½X2ï¿½rï¿½ï¿½ï¿½ï¿½hï¿½ï¿½ï¿½ï¿½Æšoï¿½0ï¿½pï¿½ï¿½%Y!ï¿½e[ï¿½mï¿½Ë†ï¿½MM}Íï¿½ï¿½ï¿½ï¿½ï¿½ï¿½Å€ï¿½zï¿½`olï¿½,Ä£*}!ï¿½IÆšï¿½hï¿½J;g}ï¿½9ï¿½ï¿½ï¿½ï¿½ï¿½6QMiï¿½"Aï¿½ï¿½=}ï¿½Xc`Ë¥ï¿½O
ï¿½Aï¿½,Qï¿½:gTï¿½_'>yG}W\Dï¿½ï¿½Ø{ï¿½7Vï¿½@ï¿½Dï¿½uï¿½Cï¿½ï¿½ï¿½ï¿½ï¿½|#ï¿½ï¿½ï¿½:ï¿½<ï¿½ï¿½ï¿½ï¿½Lï¿½ï¿½Gï¿½Pï¿½Tï¿½AmBï¿½Å ï¿½ï¿½bï¿½ï¿½ï¿½Mï¿½xï¿½eï¿½ï¿½Ï‡ï¿½ï¿½^nï¿½ï¿½ï¿½:	?vkï¿½ï¿½Ó¹Lï¿½ï¿½ï¿½bï¿½9ï¿½gï¿½G[ï¿½ï¿½ï¿½{ï¿½ï¿½}ï¿½ï¿½Yï¿½ï¿½Ú†j45ï¿½Bfï¿½ï¿½ï¿½0ï¿½Ï™ï¿½)ï¿½ï¿½v#_ï¿½c`'ï¿½dï¿½ï¿½^M*ï¿½Ñ¯ ï¿½ï¿½ï¿½#6ï¿½ï¿½Z_?ï¿½B?JEï¿½fKBï¿½tï¿½ï¿½	Ù¨ï¿½)eï¿½)ï¿½Wï¿½ï¿½ï¿½fï¿½[Gh0ÂŠ`ï¿½yï¿½ÍŸHï¿½<ï¿½,ï¿½!2ï¿½*ï¿½{ãŒ¶ï¿½	ï¿½eï¿½H<ï¿½y(9ï¿½ï¿½ï¿½hOO*ï¿½,ï¿½>Qï¿½Sï¿½aï¿½Ï¤Ghï¿½ï¿½ï¿½Zwï¿½Jï¿½Zï¿½Yï¿½X0ï¿½ï¿½Komï¿½yï¿½u'ï¿½y>"ï¿½ï¿½ï¿½w^Vï¿½Gp8ï¿½7b{Oï¿½Õ©]:8ï¿½ï¿½dbï¿½ï¿½ï¿½?~-ï¿½ï¿½;Zï¿½yï¿½ï¿½soï¿½Fï¿½ï¿½'ï¿½tu8ï¿½&ï¿½ï¿½ï¿½c{*ï¿½Yï¿½<\ï¿½ï¿½ï¿½Cï¿½ï¿½_ï¿½\ï¿½}ï¿½ï¿½ï¿½v}OBï¿½3Hï¿½ï¿½8Ylï¿½ï¿½ï¿½@ï¿½ï¿½ iï¿½nï¿½ï¿½Ä·ï¿½l`ï¿½ï¿½Dï¿½d}ï¿½hX15i	ï¿½ï¿½rï¿½.ï¿½z ï¿½ï¿½ï¿½ï¿½ï¿½-"<ï¿½ï¿½ï¿½F
ï¿½\6ï¿½ï¿½ ï¿½+fSï¿½Tï¿½r%ï¿½G9ï¿½(ï¿½k%Æ´ï¿½ï¿½ï¿½Èµï¿½ï¿½ï¿½ï¿½[ï¿½Î¶nï¿½ï¿½ï¿½ï¿½8ï¿½ï¿½1$ï¿½ï¿½ï¿½Iï¿½aJwï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½>Ş»ï¿½ï¿½ÒYï¿½8`ï¿½ï¿½jï¿½X7ï¿½)ï¿½Qï¿½ï¿½ 
ï¿½ryï¿½%}$Mï¿½ï¿½Lï¿½ï¿½ï¿½Æ‹ ï¿½4ï¿½ORï¿½ï¿½=Bï¿½Jl)ï¿½ï¿½@Sï¿½ï¿½ï¿½ë¸¬ï¿½6Gï¿½Ï‘Jï¿½ï¿½Ò¥.0 ï¿½ï¿½8ï¿½HPï¿½ï¿½<ï¿½ï¿½Ô½ï¿½ï¿½ï¿½ï¿½ï¿½Ñ‘ï¿½ï¿½mï¿½l:uÍ€ï¿½^L&ï¿½xKpï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ ï¿½ï¿½ï¿½ï¿½ï¿½eï¿½ï¿½ï¿½nï¿½ï¿½ï¿½9ï¿½SH}!ï¿½	Jï¿½ï¿½ï¿½Kï¿½ï¿½'8ï¿½a'ï¿½Ï¬ï¿½XÇ§\jï¿½ï¿½jï¿½ï¿½ï¿½ï¿½%ï¿½'H!ï¿½=Nï¿½ï¿½ï¿½*ï¿½o/ï¿½ï¿½pYTï¿½ï¿½lï¿½ï¿½ï¿½Wï¿½ï¿½ï¿½ï¿½=y"ì½›ï¿½ï¿½:H_[Ihï¿½~ï¿½ï¿½ï¿½Ñ‡ï¿½ï¿½GÊQï¿½ï¿½ï¿½*ï¿½ï¿½
ï¿½ï¿½ï¿½O&Uï¿½ï¿½$ï¿½ï¿½ï¿½$/ï¿½ï¿½ï¿½ï¿½ï¿½0/fkU/Rï¿½6î´“I5ï¿½ï¿½Lï¿½Ú¥ï¿½wï¿½ï¿½ï¿½Appï¿½ï¿½ï¿½	Gï¿½ï¿½1Uï¿½ï¿½hOï¿½gá Ÿï¿½ï¿½ï¿½0ï¿½ï¿½ï¿½ï¿½ï¿½Ô\ï¿½ï¿½,ÜŸCï¿½qï¿½ï¿½z{ï¿½ï¿½3ï¿½(ï¿½'ï¿½ï¿½ï¿½yï¿½ï¿½ï¿½ÓuLï¿½'wÒˆ?ï¿½ï¿½ï¿½ï¿½.xï¿½c`d`` ï¿½ï¿½B+ï¿½ï¿½mï¿½2ï¿½3ï¿½ ï¿½0<d?ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½9ï¿½ï¿½``ï¿½ \ï¿½
 xï¿½c`d``ï¿½ï¿½$_ï¿½ï¿½ï¿½ï¿½=ï¿½ï¿½
ï¿½ ï¿½ï¿½ï¿½ xï¿½c~ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½dï¿½ï¿½ï¿½ï¿½
ï¿½ï¿½ ï¿½ï¿½ï¿½3/ ï¿½@ï¿½ï¿½ï¿½ï¿½T- ï¿½       ï¿½ ï¿½4lï¿½Z.ï¿½$ï¿½"     k        B ï¿½   ï¿½    xï¿½uï¿½ï¿½Nï¿½@ï¿½ï¿½p3Btï¿½ï¿½7ï¿½ï¿½@L
ï¿½ï¿½aï¿½!ï¿½Â…	6ï¿½J)mIï¿½ï¿½@ï¿½ï¿½>ï¿½ï¿½ï¿½x:Lï¿½ï¿½ï¿½wï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½#×‘ï¿½TGï¿½ï¿½Cï¿½Uï¿½Oï¿½kï¿½ï¿½:ZxqÜ ï¿½;nï¿½oï¿½[ï¿½ï¿½;;ï¿½ï¿½9ï¿½ï¿½ï¿½ï¿½ê¸‚Kqï¿½Jï¿½ï¿½qï¿½|ç¸ï¿½uÜ ï¿½ì¸‰ï¿½xuï¿½Â½ï¿½ï¿½ï¿½Aï¿½qbd{Ü‘~ï¿½ï¿½ï¿½A*Zid2Ø™Dï¿½Bï¿½ï¿½Jï¿½&ï¿½2ï¿½jï¿½ï¿½*/fQï¿½ï¿½mï¿½nï¿½Hï¿½ï¿½eï¿½ï¿½Y=ï¿½ï¿½H&Zï¿½ï¿½}ï¿½ï¿½ï¿½+ï¿½6rï¿½zÉ­Vï¿½(4^bï¿½vï¿½íï¿½1ï¿½8@#Eï¿½mï¿½~}ï¿½ï¿½ï¿½3$3ï¿½Y)rï¿½ï¿½Ø±"ï¿½ï¿½ï¿½zÄµï¿½ï¿½ï¿½Fï¿½ï¿½ï¿½Bï¿½Vï¿½6R`ï¿½Xï¿½ÊŒï¿½ï¿½ï¿½ï¿½ï¿½93ï¿½uJ-ï¿½gï¿½ï¿½I|ï¿½xns{ï¿½ï¿½gï¿½{ï¿½ï¿½ï¿½NRNï¿½ï¿½ï¿½?ï¿½ß¹$ï¿½]ï¿½ï¿½tBï¿½ï¿½ï¿½ï¿½ï¿½;Dï¿½ï¿½?ï¿½ï¿½Yï¿½xï¿½ xï¿½mï¿½Qï¿½ Eï¿½yï¿½ï¿½u']-Mï¿½ï¿½htï¿½Fï¿½ï¿½ï¿½:ï¿½%Egï¿½ï¿½7@ï¿½ï¿½ï¿½ï¿½ï¿½=<ï¿½ï¿½aï¿½ï¿½ï¿½È³Kï¿½ï¿½Ç´H/ï¿½\ï¿½Wnï¿½ï¿½ï¿½W[sï¿½Xl~?Yï¿½ï¿½xoï¿½!G-ï¿½ï¿½ï¿½Vï¿½8Vï¿½1lï¿½ï¿½   xï¿½cï¿½ï¿½ï¿½p"(b##c_ï¿½Æï¿½Ø620hAh.z'7k'3ï¿½ï¿½FÆï¿½ï¿½ ~ï¿½ï¿½FDï¿½ï¿½%Rzï¿½:HhG#ï¿½CGrL62ï¿½iï¿½`ï¿½ßºï¿½ï¿½w#ï¿½ï¿½fï¿½6 ï¿½*  
```

## static/font/LICENSE.txt
```
Font license info


## Font Awesome

   Copyright (C) 2016 by Dave Gandy

   Author:    Dave Gandy
   License:   SIL ()
   Homepage:  http://fortawesome.github.com/Font-Awesome/


## Typicons

   (c) Stephen Hutchings 2012

   Author:    Stephen Hutchings
   License:   SIL (http://scripts.sil.org/OFL)
   Homepage:  http://typicons.com/

```

## static/font/icons.ttf
```
     ï¿½  pGSUB ï¿½%z   ï¿½   TOS/2>#Rï¿½  P   `cmapï¿½+ï¿½;  ï¿½  "cvt       ,   fpgmb.ï¿½z  <  gasp     $   glyfï¿½ï¿½ï¿½  ï¿½  
Dhead$ï¿½ï¿½5     6hhea-O  P   $hmtx)ï¿½ï¿½ï¿½  t   4locaï¿½N  ï¿½   maxp>ï¿½  ï¿½    nameï¿½uï¿½  ï¿½  ï¿½posteï¿½5  ï¿½   ï¿½prep~ï¿½;ï¿½   H   ï¿½    
 0 > DFLT latn                      liga                     9ï¿½   zï¿½   ï¿½zï¿½  ï¿½ 1                         PfEd ï¿½ï¿½ ï¿½MRï¿½j Zï¿½ ï¿½                         ,     ï¿½      |     ,  
  ï¿½  P   
   ï¿½ï¿½ï¿½ï¿½ï¿½Mï¿½ï¿½  ï¿½ ï¿½ï¿½ï¿½ï¿½Mï¿½ï¿½          
              	 
                                                                                                                                                                                                                                                                            ï¿½         ï¿½   ï¿½      ï¿½  ï¿½     ï¿½  ï¿½     ï¿½  ï¿½     ï¿½  ï¿½     ï¿½  ï¿½     ï¿½  ï¿½     ï¿½  ï¿½     ï¿½  ï¿½   	  ï¿½  ï¿½   
  ï¿½ï¿½  ï¿½ï¿½     ï¿½M  ï¿½M        ï¿½ï¿½Z  j E@BeYLA ;
 4(L ï¿½ ï¿½   ï¿½ ï¿½ ï¿½ v\[SQIH+*" +4&"26%'"/+"&5'&'"'&'&47>7&/.'546?67&'&47>3267676;2762;RxRVtVh
(PM	|OF(
f
h%PM	|OH	(
f
^;TTvTTx|2P<L
g	<@2| -P<L
g	;C2       <ï¿½  @  L  ï¿½   v5+"/&463!2;
ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½      A}  
ï¿½   v+"&54>A
ï¿½ï¿½
^ï¿½ï¿½ï¿½
      g|  @  L  ï¿½   v+"/&4?62e 	ï¿½

ï¿½Xï¿½ï¿½ï¿½        ï¿½ï¿½  @ L ï¿½  ï¿½   v+"'&4?6262ï¿½ï¿½ ,ï¿½ï¿½L,ï¿½n,Lï¿½ ,Lï¿½oL ï¿½ï¿½ï¿½jï¿½  ! 2@/ L ï¿½    i Y a Q#+4.>"/#".>ï¿½ï¿½Ğ’ï¿½Ğ’,:ï¿½d{Pï¿½h@<lï¿½ï¿½ï¿½l<Eï¿½ï¿½gï¿½ï¿½Ê˜ï¿½ï¿½ï¿½*ï¿½E>jï¿½ï¿½ï¿½n:Bfï¿½M{dï¿½   ï¿½ï¿½ï¿½R ) W E@B ï¿½ ï¿½ ~  i  g   W  _   OSQMK%%5)%3	+#!"&546732+"!26=47676#"'&=#""/.'4>73547632^Cï¿½0C^^Cï¿½+>%46$ï¿½%4
ï¿½ï¿½
Yï¿½@B	 $(@Lf;Yï¿½*ï¿½C^^Cï¿½B^4%ï¿½0%46$w	

ï¿½	kIMï¿½	<2N04.,(k	ï¿½
      ï¿½ï¿½ï¿½  P ï¿½@	>6!	LKï¿½
PX@*  ï¿½ ï¿½ ~ï¿½   W   _  OKï¿½PX@*  ï¿½ ï¿½~ï¿½   W   _  O@*  ï¿½ ï¿½ ~ï¿½   W   _  OYY@PPLKJI;:*#Q+327&7>73"&"#4?6?6?6'4&/7"&#"'ï¿½_L:0ï¿½5$ï¿½ï¿½HrR	0L	#ï¿½$*ï¿½I"ï¿½:  !ï¿½ -ï¿½ï¿½ï¿½,
Xï¿½ï¿½ï¿½+ï¿½4z!	


	Z68 ï¿½
 ï¿½ï¿½ï¿½ï¿½ï¿½4  ! ( - 0@--+($L ï¿½  Y a   Q!!,(+#"/&76767%632267'%67&'767ï¿½1ï¿½ï¿½ï¿½ï¿½iLKDpï¿½S+ï¿½Gd6M\
&7T#ï¿½ï¿½`ï¿½1?ï¿½ï¿½ï¿½
hï¿½L`V
aï¿½ï¿½ï¿½XF5xRï¿½(T0-&ï¿½ï¿½`ï¿½      ï¿½ï¿½ï¿½     + ; _@\,4
	 L  g  
	
g 	 	g  g   g   W   _  O:72/*(&&%+%!5!'!5!%35#!"&'546!2'!"&'5467!2#!"&'5463!2;fï¿½ï¿½ï¿½<ï¿½ï¿½eï¿½ï¿½ï¿½`ï¿½ï¿½`ï¿½ï¿½`ï¿½@Hï¿½Gï¿½Gï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½   ï¿½jï¿½R  " % 3 < p@m# 	 ' L   g  	 	g  g  

g  gW_ O44 4<4<;9650/.,)(%$"!	 +2!"&'5!"&'46?>;2633375##!546#'#ï¿½ ï¿½ï¿½ï¿½ï¿½ï¿½6ï¿½&!Gï¿½ï¿½ï¿½ï¿½ï¿½ï¿½mï¿½ï¿½ï¿½&ï¿½ï¿½| ï¿½Z ï¿½ w6ï¿½ ï¿½wï¿½}ï¿½Â°ï¿½ï¿½ï¿½ï¿½ï¿½6ï¿½Nï¿½ï¿½ ï¿½ï¿½    ï¿½ï¿½Y $ 4 3@0   ï¿½  ~    i Y _ O54$*+%764/&"3276'&7>332!"&5467!21ï¿½ï¿½,BlJ6]<^N3^Cï¿½ï¿½C^^CC^ï¿½ï¿½ï¿½Z"64F6#e}ï¿½CY	ï¿½ï¿½ï¿½B^`AB^`      kï¿½fï¿½_<ï¿½ ï¿½    ï¿½Wï¿½    ï¿½Wï¿½ï¿½ï¿½ï¿½jï¿½R             Rï¿½j  ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½                ï¿½  Y  ;  e  e  ï¿½  ï¿½ï¿½ï¿½ï¿½  ï¿½  ï¿½ï¿½ï¿½ï¿½  ï¿½  Y       ï¿½ ï¿½4lï¿½Z.ï¿½$ï¿½"     k        B ï¿½   ï¿½        ï¿½        5          5        :        A        F        K        V      
 + [        ï¿½  	   j ï¿½  	  
  	    	  
  	  
%  	  /  	  
E  	 
 VO  	  &ï¿½Copyright (C) 2023 by original authors @ fontello.comiconsRegulariconsiconsVersion 1.0iconsGenerated by svg2ttf from Fontello project.http://fontello.com C o p y r i g h t   ( C )   2 0 2 3   b y   o r i g i n a l   a u t h o r s   @   f o n t e l l o . c o m i c o n s R e g u l a r i c o n s i c o n s V e r s i o n   1 . 0 i c o n s G e n e r a t e d   b y   s v g 2 t t f   f r o m   F o n t e l l o   p r o j e c t . h t t p : / / f o n t e l l o . c o m           
                       	
 cogdown-dir	right-dirleft-diroksearchexportfontbrushtasksdocs
export-alt       ï¿½ï¿½                 ï¿½ , ï¿½ UXEY  Kï¿½ QKï¿½SZXï¿½4ï¿½(Y`f ï¿½UXï¿½%aï¿½  cc#b!!ï¿½ Yï¿½ C#Dï¿½  C`B-ï¿½,ï¿½ `f-ï¿½,#!#!-ï¿½, dï¿½ BCï¿½C ``Bï¿½CBï¿½%Cï¿½CTx ï¿½#ï¿½CCadï¿½Pxï¿½C`Bï¿½!e!ï¿½CCï¿½B ï¿½C#Bï¿½C`B#ï¿½ PXeYï¿½C`B-ï¿½,ï¿½+ï¿½CX#!#!ï¿½CC#ï¿½ PXeY d ï¿½ï¿½Pï¿½&Zï¿½(CEcEï¿½EX!ï¿½%YR[X!#!ï¿½X ï¿½PPX!ï¿½@Y ï¿½8PX!ï¿½8YY ï¿½CEcEadï¿½(PX!ï¿½CEcE ï¿½0PX!ï¿½0Y ï¿½ï¿½PX f ï¿½ï¿½a ï¿½
PX` ï¿½ PX!ï¿½
` ï¿½6PX!ï¿½6``YYYï¿½%ï¿½Ccï¿½ RXï¿½ Kï¿½
PX!ï¿½CKï¿½PX!ï¿½Kaï¿½ cï¿½Ccï¿½ bYYdaYï¿½+YY#ï¿½ PXeYY dï¿½C#BY-ï¿½, E ï¿½%ad ï¿½CPXï¿½#Bï¿½#B!!Yï¿½`-ï¿½,#!#!ï¿½+ dï¿½bB ï¿½#Bï¿½EXï¿½CEcï¿½Cï¿½ `Ecï¿½*! ï¿½C ï¿½ ï¿½ï¿½+ï¿½0%ï¿½&QX`PaRYX#Y!Y ï¿½@SXï¿½+!ï¿½@Y#ï¿½ PXeY-ï¿½,ï¿½	C+ï¿½  C`B-ï¿½,ï¿½	#B# ï¿½ #Baï¿½bfï¿½cï¿½`ï¿½*-ï¿½	,  E ï¿½Ccï¿½ b ï¿½ PXï¿½@`Yfï¿½c`Dï¿½`-ï¿½
,ï¿½	 CEB*!ï¿½  C`B-ï¿½,ï¿½ C#Dï¿½  C`B-ï¿½,  E ï¿½+#ï¿½ Cï¿½%` Eï¿½#a d ï¿½ PX!ï¿½ ï¿½0PXï¿½ ï¿½@YY#ï¿½ PXeYï¿½%#aDDï¿½`-ï¿½,  E ï¿½+#ï¿½ Cï¿½%` Eï¿½#a dï¿½$PXï¿½ ï¿½@Y#ï¿½ PXeYï¿½%#aDDï¿½`-ï¿½, ï¿½ #Bï¿½ EPX!#!Y*!-ï¿½,ï¿½Eï¿½daD-ï¿½,ï¿½`  ï¿½CJï¿½ PX ï¿½#BYï¿½CJï¿½ RX ï¿½#BY-ï¿½, ï¿½bfï¿½c ï¿½ cï¿½#aï¿½C` ï¿½` ï¿½#B#-ï¿½,KTXï¿½dDY$ï¿½e#x-ï¿½,KQXKSXï¿½dDY!Y$ï¿½e#x-ï¿½,ï¿½ CUXï¿½Cï¿½aBï¿½+Yï¿½ Cï¿½%Bï¿½%Bï¿½%Bï¿½# ï¿½%PXï¿½ C`ï¿½%Bï¿½ï¿½ ï¿½#aï¿½*!#ï¿½a ï¿½#aï¿½*!ï¿½ C`ï¿½%Bï¿½%aï¿½*!Yï¿½CGï¿½CG`ï¿½b ï¿½ PXï¿½@`Yfï¿½c ï¿½Ccï¿½ b ï¿½ PXï¿½@`Yfï¿½c`ï¿½  #Dï¿½Cï¿½ >ï¿½C`B-ï¿½, ï¿½ ETXï¿½#B Eï¿½#Bï¿½#ï¿½ `B `ï¿½   BBBï¿½` ï¿½#Bï¿½aï¿½+ï¿½ï¿½+"Y-ï¿½,ï¿½ +-ï¿½,ï¿½+-ï¿½,ï¿½+-ï¿½,ï¿½+-ï¿½,ï¿½+-ï¿½,ï¿½+-ï¿½,ï¿½+-ï¿½,ï¿½+-ï¿½,ï¿½+-ï¿½,ï¿½	+-ï¿½+,# ï¿½bfï¿½cï¿½`KTX# .ï¿½]!!Y-ï¿½,,# ï¿½bfï¿½cï¿½`KTX# .ï¿½q!!Y-ï¿½-,# ï¿½bfï¿½cï¿½&`KTX# .ï¿½r!!Y-ï¿½ , ï¿½+ï¿½ ETXï¿½#B Eï¿½#Bï¿½#ï¿½ `B `ï¿½aï¿½  BBï¿½`ï¿½+ï¿½ï¿½+"Y-ï¿½!,ï¿½  +-ï¿½",ï¿½ +-ï¿½#,ï¿½ +-ï¿½$,ï¿½ +-ï¿½%,ï¿½ +-ï¿½&,ï¿½ +-ï¿½',ï¿½ +-ï¿½(,ï¿½ +-ï¿½),ï¿½ +-ï¿½*,ï¿½	 +-ï¿½., <ï¿½`-ï¿½/, `ï¿½` C#ï¿½`Cï¿½%aï¿½`ï¿½.*!-ï¿½0,ï¿½/+ï¿½/*-ï¿½1,  G  ï¿½Ccï¿½ b ï¿½ PXï¿½@`Yfï¿½c`#a8# ï¿½UX G  ï¿½Ccï¿½ b ï¿½ PXï¿½@`Yfï¿½c`#a8!Y-ï¿½2, ï¿½ ETXï¿½EBï¿½ï¿½1*ï¿½EX0Y"Y-ï¿½3, ï¿½+ï¿½ ETXï¿½EBï¿½ï¿½1*ï¿½EX0Y"Y-ï¿½4, 5ï¿½`-ï¿½5, ï¿½EBï¿½Ecï¿½ b ï¿½ PXï¿½@`Yfï¿½cï¿½+ï¿½Ccï¿½ b ï¿½ PXï¿½@`Yfï¿½cï¿½+ï¿½ ï¿½     D>#8ï¿½4*!-ï¿½6, < G ï¿½Ccï¿½ b ï¿½ PXï¿½@`Yfï¿½c`ï¿½ Ca8-ï¿½7,.<-ï¿½8, < G ï¿½Ccï¿½ b ï¿½ PXï¿½@`Yfï¿½c`ï¿½ Caï¿½Cc8-ï¿½9,ï¿½ % . Gï¿½ #Bï¿½%Iï¿½ï¿½G#G#a Xb!Yï¿½#Bï¿½8*-ï¿½:,ï¿½ ï¿½#Bï¿½%ï¿½%G#G#aï¿½ Bï¿½C+eï¿½.#  <ï¿½8-ï¿½;,ï¿½ ï¿½#Bï¿½%ï¿½% .G#G#a ï¿½#Bï¿½ Bï¿½C+ ï¿½`PX ï¿½@QXï¿½  ï¿½&YBB# ï¿½
C ï¿½#G#G#a#F`ï¿½Cï¿½b ï¿½ PXï¿½@`Yfï¿½c` ï¿½+ ï¿½ï¿½a ï¿½C`d#ï¿½CadPXï¿½Caï¿½C`Yï¿½%ï¿½b ï¿½ PXï¿½@`Yfï¿½ca#  ï¿½&#Fa8#ï¿½
CFï¿½%ï¿½
CG#G#a` ï¿½Cï¿½b ï¿½ PXï¿½@`Yfï¿½c`# ï¿½+#ï¿½C`ï¿½+ï¿½%aï¿½%ï¿½b ï¿½ PXï¿½@`Yfï¿½cï¿½&a ï¿½%`d#ï¿½%`dPX!#!Y#  ï¿½&#Fa8Y-ï¿½<,ï¿½ ï¿½#B   ï¿½& .G#G#a#<8-ï¿½=,ï¿½ ï¿½#B ï¿½
#B   F#Gï¿½+#a8-ï¿½>,ï¿½ ï¿½#Bï¿½%ï¿½%G#G#aï¿½ TX. <#!ï¿½%ï¿½%G#G#a ï¿½%ï¿½%G#G#aï¿½%ï¿½%Iï¿½%aï¿½  cc# Xb!Ycï¿½ b ï¿½ PXï¿½@`Yfï¿½c`#.#  <ï¿½8#!Y-ï¿½?,ï¿½ ï¿½#B ï¿½
C .G#G#a `ï¿½ `fï¿½b ï¿½ PXï¿½@`Yfï¿½c#  <ï¿½8-ï¿½@,# .Fï¿½%Fï¿½CXPRYX <Y.ï¿½0+-ï¿½A,# .Fï¿½%Fï¿½CXRPYX <Y.ï¿½0+-ï¿½B,# .Fï¿½%Fï¿½CXPRYX <Y# .Fï¿½%Fï¿½CXRPYX <Y.ï¿½0+-ï¿½C,ï¿½:+# .Fï¿½%Fï¿½CXPRYX <Y.ï¿½0+-ï¿½D,ï¿½;+ï¿½  <ï¿½#Bï¿½8# .Fï¿½%Fï¿½CXPRYX <Y.ï¿½0+ï¿½C.ï¿½0+-ï¿½E,ï¿½ ï¿½%ï¿½&   F#Gaï¿½#B.G#G#aï¿½C+# < .#8ï¿½0+-ï¿½F,ï¿½
%Bï¿½ ï¿½%ï¿½% .G#G#a ï¿½#Bï¿½ Bï¿½C+ ï¿½`PX ï¿½@QXï¿½  ï¿½&YBB# Gï¿½Cï¿½b ï¿½ PXï¿½@`Yfï¿½c` ï¿½+ ï¿½ï¿½a ï¿½C`d#ï¿½CadPXï¿½Caï¿½C`Yï¿½%ï¿½b ï¿½ PXï¿½@`Yfï¿½caï¿½%Fa8# <#8!  F#Gï¿½+#a8!Yï¿½0+-ï¿½G,ï¿½ :+.ï¿½0+-ï¿½H,ï¿½ ;+!#  <ï¿½#B#8ï¿½0+ï¿½C.ï¿½0+-ï¿½I,ï¿½  Gï¿½ #Bï¿½ .ï¿½6*-ï¿½J,ï¿½  Gï¿½ #Bï¿½ .ï¿½6*-ï¿½K,ï¿½ ï¿½7*-ï¿½L,ï¿½9*-ï¿½M,ï¿½ E# . Fï¿½#a8ï¿½0+-ï¿½N,ï¿½
#Bï¿½M+-ï¿½O,ï¿½  F+-ï¿½P,ï¿½ F+-ï¿½Q,ï¿½ F+-ï¿½R,ï¿½F+-ï¿½S,ï¿½  G+-ï¿½T,ï¿½ G+-ï¿½U,ï¿½ G+-ï¿½V,ï¿½G+-ï¿½W,ï¿½   C+-ï¿½X,ï¿½  C+-ï¿½Y,ï¿½  C+-ï¿½Z,ï¿½ C+-ï¿½[,ï¿½  C+-ï¿½\,ï¿½ C+-ï¿½],ï¿½ C+-ï¿½^,ï¿½C+-ï¿½_,ï¿½  E+-ï¿½`,ï¿½ E+-ï¿½a,ï¿½ E+-ï¿½b,ï¿½E+-ï¿½c,ï¿½  H+-ï¿½d,ï¿½ H+-ï¿½e,ï¿½ H+-ï¿½f,ï¿½H+-ï¿½g,ï¿½   D+-ï¿½h,ï¿½  D+-ï¿½i,ï¿½  D+-ï¿½j,ï¿½ D+-ï¿½k,ï¿½  D+-ï¿½l,ï¿½ D+-ï¿½m,ï¿½ D+-ï¿½n,ï¿½D+-ï¿½o,ï¿½ <+.ï¿½0+-ï¿½p,ï¿½ <+ï¿½@+-ï¿½q,ï¿½ <+ï¿½A+-ï¿½r,ï¿½ ï¿½ <+ï¿½B+-ï¿½s,ï¿½<+ï¿½@+-ï¿½t,ï¿½<+ï¿½A+-ï¿½u,ï¿½ ï¿½<+ï¿½B+-ï¿½v,ï¿½ =+.ï¿½0+-ï¿½w,ï¿½ =+ï¿½@+-ï¿½x,ï¿½ =+ï¿½A+-ï¿½y,ï¿½ =+ï¿½B+-ï¿½z,ï¿½=+ï¿½@+-ï¿½{,ï¿½=+ï¿½A+-ï¿½|,ï¿½=+ï¿½B+-ï¿½},ï¿½ >+.ï¿½0+-ï¿½~,ï¿½ >+ï¿½@+-ï¿½,ï¿½ >+ï¿½A+-ï¿½ï¿½,ï¿½ >+ï¿½B+-ï¿½ï¿½,ï¿½>+ï¿½@+-ï¿½ï¿½,ï¿½>+ï¿½A+-ï¿½ï¿½,ï¿½>+ï¿½B+-ï¿½ï¿½,ï¿½ ?+.ï¿½0+-ï¿½ï¿½,ï¿½ ?+ï¿½@+-ï¿½ï¿½,ï¿½ ?+ï¿½A+-ï¿½ï¿½,ï¿½ ?+ï¿½B+-ï¿½ï¿½,ï¿½?+ï¿½@+-ï¿½ï¿½,ï¿½?+ï¿½A+-ï¿½ï¿½,ï¿½?+ï¿½B+-ï¿½ï¿½,ï¿½ EPXï¿½ï¿½EX#!!YYB+ï¿½eï¿½$Pxï¿½EX0Y- Kï¿½ ï¿½RXï¿½ï¿½Yï¿½ï¿½  cpï¿½ Bï¿½  *ï¿½ Bï¿½ 
*ï¿½ Bï¿½ 
*ï¿½ Bï¿½   *ï¿½ Bï¿½   *ï¿½   Dï¿½$ï¿½QXï¿½@ï¿½Xï¿½  dDï¿½(ï¿½QXï¿½ ï¿½Xï¿½   DYï¿½'ï¿½QXï¿½ï¿½ @ï¿½cTXï¿½   DYYYYYï¿½ *ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ Dï¿½d DD
```

## static/font/icons.eot
```
ï¿½!  ï¿½                 ï¿½    LP                       ï¿½fï¿½k                  
 i c o n s    R e g u l a r    V e r s i o n   1 . 0   
 i c o n s          ï¿½  pGSUB ï¿½%z   ï¿½   TOS/2>#Rï¿½  P   `cmapï¿½+ï¿½;  ï¿½  "cvt       ,   fpgmb.ï¿½z  <  gasp     $   glyfï¿½ï¿½ï¿½  ï¿½  
Dhead$ï¿½ï¿½5     6hhea-O  P   $hmtx)ï¿½ï¿½ï¿½  t   4locaï¿½N  ï¿½   maxp>ï¿½  ï¿½    nameï¿½uï¿½  ï¿½  ï¿½posteï¿½5  ï¿½   ï¿½prep~ï¿½;ï¿½   H   ï¿½    
 0 > DFLT latn                      liga                     9ï¿½   zï¿½   ï¿½zï¿½  ï¿½ 1                         PfEd ï¿½ï¿½ ï¿½MRï¿½j Zï¿½ ï¿½                         ,     ï¿½      |     ,  
  ï¿½  P   
   ï¿½ï¿½ï¿½ï¿½ï¿½Mï¿½ï¿½  ï¿½ ï¿½ï¿½ï¿½ï¿½Mï¿½ï¿½          
              	 
                                                                                                                                                                                                                                                                            ï¿½         ï¿½   ï¿½      ï¿½  ï¿½     ï¿½  ï¿½     ï¿½  ï¿½     ï¿½  ï¿½     ï¿½  ï¿½     ï¿½  ï¿½     ï¿½  ï¿½     ï¿½  ï¿½   	  ï¿½  ï¿½   
  ï¿½ï¿½  ï¿½ï¿½     ï¿½M  ï¿½M        ï¿½ï¿½Z  j E@BeYLA ;
 4(L ï¿½ ï¿½   ï¿½ ï¿½ ï¿½ v\[SQIH+*" +4&"26%'"/+"&5'&'"'&'&47>7&/.'546?67&'&47>3267676;2762;RxRVtVh
(PM	|OF(
f
h%PM	|OH	(
f
^;TTvTTx|2P<L
g	<@2| -P<L
g	;C2       <ï¿½  @  L  ï¿½   v5+"/&463!2;
ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½      A}  
ï¿½   v+"&54>A
ï¿½ï¿½
^ï¿½ï¿½ï¿½
      g|  @  L  ï¿½   v+"/&4?62e 	ï¿½

ï¿½Xï¿½ï¿½ï¿½        ï¿½ï¿½  @ L ï¿½  ï¿½   v+"'&4?6262ï¿½ï¿½ ,ï¿½ï¿½L,ï¿½n,Lï¿½ ,Lï¿½oL ï¿½ï¿½ï¿½jï¿½  ! 2@/ L ï¿½    i Y a Q#+4.>"/#".>ï¿½ï¿½Ğ’ï¿½Ğ’,:ï¿½d{Pï¿½h@<lï¿½ï¿½ï¿½l<Eï¿½ï¿½gï¿½ï¿½Ê˜ï¿½ï¿½ï¿½*ï¿½E>jï¿½ï¿½ï¿½n:Bfï¿½M{dï¿½   ï¿½ï¿½ï¿½R ) W E@B ï¿½ ï¿½ ~  i  g   W  _   OSQMK%%5)%3	+#!"&546732+"!26=47676#"'&=#""/.'4>73547632^Cï¿½0C^^Cï¿½+>%46$ï¿½%4
ï¿½ï¿½
Yï¿½@B	 $(@Lf;Yï¿½*ï¿½C^^Cï¿½B^4%ï¿½0%46$w	

ï¿½	kIMï¿½	<2N04.,(k	ï¿½
      ï¿½ï¿½ï¿½  P ï¿½@	>6!	LKï¿½
PX@*  ï¿½ ï¿½ ~ï¿½   W   _  OKï¿½PX@*  ï¿½ ï¿½~ï¿½   W   _  O@*  ï¿½ ï¿½ ~ï¿½   W   _  OYY@PPLKJI;:*#Q+327&7>73"&"#4?6?6?6'4&/7"&#"'ï¿½_L:0ï¿½5$ï¿½ï¿½HrR	0L	#ï¿½$*ï¿½I"ï¿½:  !ï¿½ -ï¿½ï¿½ï¿½,
Xï¿½ï¿½ï¿½+ï¿½4z!	


	Z68 ï¿½
 ï¿½ï¿½ï¿½ï¿½ï¿½4  ! ( - 0@--+($L ï¿½  Y a   Q!!,(+#"/&76767%632267'%67&'767ï¿½1ï¿½ï¿½ï¿½ï¿½iLKDpï¿½S+ï¿½Gd6M\
&7T#ï¿½ï¿½`ï¿½1?ï¿½ï¿½ï¿½
hï¿½L`V
aï¿½ï¿½ï¿½XF5xRï¿½(T0-&ï¿½ï¿½`ï¿½      ï¿½ï¿½ï¿½     + ; _@\,4
	 L  g  
	
g 	 	g  g   g   W   _  O:72/*(&&%+%!5!'!5!%35#!"&'546!2'!"&'5467!2#!"&'5463!2;fï¿½ï¿½ï¿½<ï¿½ï¿½eï¿½ï¿½ï¿½`ï¿½ï¿½`ï¿½ï¿½`ï¿½@Hï¿½Gï¿½Gï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½   ï¿½jï¿½R  " % 3 < p@m# 	 ' L   g  	 	g  g  

g  gW_ O44 4<4<;9650/.,)(%$"!	 +2!"&'5!"&'46?>;2633375##!546#'#ï¿½ ï¿½ï¿½ï¿½ï¿½ï¿½6ï¿½&!Gï¿½ï¿½ï¿½ï¿½ï¿½ï¿½mï¿½ï¿½ï¿½&ï¿½ï¿½| ï¿½Z ï¿½ w6ï¿½ ï¿½wï¿½}ï¿½Â°ï¿½ï¿½ï¿½ï¿½ï¿½6ï¿½Nï¿½ï¿½ ï¿½ï¿½    ï¿½ï¿½Y $ 4 3@0   ï¿½  ~    i Y _ O54$*+%764/&"3276'&7>332!"&5467!21ï¿½ï¿½,BlJ6]<^N3^Cï¿½ï¿½C^^CC^ï¿½ï¿½ï¿½Z"64F6#e}ï¿½CY	ï¿½ï¿½ï¿½B^`AB^`      kï¿½fï¿½_<ï¿½ ï¿½    ï¿½Wï¿½    ï¿½Wï¿½ï¿½ï¿½ï¿½jï¿½R             Rï¿½j  ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½                ï¿½  Y  ;  e  e  ï¿½  ï¿½ï¿½ï¿½ï¿½  ï¿½  ï¿½ï¿½ï¿½ï¿½  ï¿½  Y       ï¿½ ï¿½4lï¿½Z.ï¿½$ï¿½"     k        B ï¿½   ï¿½        ï¿½        5          5        :        A        F        K        V      
 + [        ï¿½  	   j ï¿½  	  
  	    	  
  	  
%  	  /  	  
E  	 
 VO  	  &ï¿½Copyright (C) 2023 by original authors @ fontello.comiconsRegulariconsiconsVersion 1.0iconsGenerated by svg2ttf from Fontello project.http://fontello.com C o p y r i g h t   ( C )   2 0 2 3   b y   o r i g i n a l   a u t h o r s   @   f o n t e l l o . c o m i c o n s R e g u l a r i c o n s i c o n s V e r s i o n   1 . 0 i c o n s G e n e r a t e d   b y   s v g 2 t t f   f r o m   F o n t e l l o   p r o j e c t . h t t p : / / f o n t e l l o . c o m           
                       	
 cogdown-dir	right-dirleft-diroksearchexportfontbrushtasksdocs
export-alt       ï¿½ï¿½                 ï¿½ , ï¿½ UXEY  Kï¿½ QKï¿½SZXï¿½4ï¿½(Y`f ï¿½UXï¿½%aï¿½  cc#b!!ï¿½ Yï¿½ C#Dï¿½  C`B-ï¿½,ï¿½ `f-ï¿½,#!#!-ï¿½, dï¿½ BCï¿½C ``Bï¿½CBï¿½%Cï¿½CTx ï¿½#ï¿½CCadï¿½Pxï¿½C`Bï¿½!e!ï¿½CCï¿½B ï¿½C#Bï¿½C`B#ï¿½ PXeYï¿½C`B-ï¿½,ï¿½+ï¿½CX#!#!ï¿½CC#ï¿½ PXeY d ï¿½ï¿½Pï¿½&Zï¿½(CEcEï¿½EX!ï¿½%YR[X!#!ï¿½X ï¿½PPX!ï¿½@Y ï¿½8PX!ï¿½8YY ï¿½CEcEadï¿½(PX!ï¿½CEcE ï¿½0PX!ï¿½0Y ï¿½ï¿½PX f ï¿½ï¿½a ï¿½
PX` ï¿½ PX!ï¿½
` ï¿½6PX!ï¿½6``YYYï¿½%ï¿½Ccï¿½ RXï¿½ Kï¿½
PX!ï¿½CKï¿½PX!ï¿½Kaï¿½ cï¿½Ccï¿½ bYYdaYï¿½+YY#ï¿½ PXeYY dï¿½C#BY-ï¿½, E ï¿½%ad ï¿½CPXï¿½#Bï¿½#B!!Yï¿½`-ï¿½,#!#!ï¿½+ dï¿½bB ï¿½#Bï¿½EXï¿½CEcï¿½Cï¿½ `Ecï¿½*! ï¿½C ï¿½ ï¿½ï¿½+ï¿½0%ï¿½&QX`PaRYX#Y!Y ï¿½@SXï¿½+!ï¿½@Y#ï¿½ PXeY-ï¿½,ï¿½	C+ï¿½  C`B-ï¿½,ï¿½	#B# ï¿½ #Baï¿½bfï¿½cï¿½`ï¿½*-ï¿½	,  E ï¿½Ccï¿½ b ï¿½ PXï¿½@`Yfï¿½c`Dï¿½`-ï¿½
,ï¿½	 CEB*!ï¿½  C`B-ï¿½,ï¿½ C#Dï¿½  C`B-ï¿½,  E ï¿½+#ï¿½ Cï¿½%` Eï¿½#a d ï¿½ PX!ï¿½ ï¿½0PXï¿½ ï¿½@YY#ï¿½ PXeYï¿½%#aDDï¿½`-ï¿½,  E ï¿½+#ï¿½ Cï¿½%` Eï¿½#a dï¿½$PXï¿½ ï¿½@Y#ï¿½ PXeYï¿½%#aDDï¿½`-ï¿½, ï¿½ #Bï¿½ EPX!#!Y*!-ï¿½,ï¿½Eï¿½daD-ï¿½,ï¿½`  ï¿½CJï¿½ PX ï¿½#BYï¿½CJï¿½ RX ï¿½#BY-ï¿½, ï¿½bfï¿½c ï¿½ cï¿½#aï¿½C` ï¿½` ï¿½#B#-ï¿½,KTXï¿½dDY$ï¿½e#x-ï¿½,KQXKSXï¿½dDY!Y$ï¿½e#x-ï¿½,ï¿½ CUXï¿½Cï¿½aBï¿½+Yï¿½ Cï¿½%Bï¿½%Bï¿½%Bï¿½# ï¿½%PXï¿½ C`ï¿½%Bï¿½ï¿½ ï¿½#aï¿½*!#ï¿½a ï¿½#aï¿½*!ï¿½ C`ï¿½%Bï¿½%aï¿½*!Yï¿½CGï¿½CG`ï¿½b ï¿½ PXï¿½@`Yfï¿½c ï¿½Ccï¿½ b ï¿½ PXï¿½@`Yfï¿½c`ï¿½  #Dï¿½Cï¿½ >ï¿½C`B-ï¿½, ï¿½ ETXï¿½#B Eï¿½#Bï¿½#ï¿½ `B `ï¿½   BBBï¿½` ï¿½#Bï¿½aï¿½+ï¿½ï¿½+"Y-ï¿½,ï¿½ +-ï¿½,ï¿½+-ï¿½,ï¿½+-ï¿½,ï¿½+-ï¿½,ï¿½+-ï¿½,ï¿½+-ï¿½,ï¿½+-ï¿½,ï¿½+-ï¿½,ï¿½+-ï¿½,ï¿½	+-ï¿½+,# ï¿½bfï¿½cï¿½`KTX# .ï¿½]!!Y-ï¿½,,# ï¿½bfï¿½cï¿½`KTX# .ï¿½q!!Y-ï¿½-,# ï¿½bfï¿½cï¿½&`KTX# .ï¿½r!!Y-ï¿½ , ï¿½+ï¿½ ETXï¿½#B Eï¿½#Bï¿½#ï¿½ `B `ï¿½aï¿½  BBï¿½`ï¿½+ï¿½ï¿½+"Y-ï¿½!,ï¿½  +-ï¿½",ï¿½ +-ï¿½#,ï¿½ +-ï¿½$,ï¿½ +-ï¿½%,ï¿½ +-ï¿½&,ï¿½ +-ï¿½',ï¿½ +-ï¿½(,ï¿½ +-ï¿½),ï¿½ +-ï¿½*,ï¿½	 +-ï¿½., <ï¿½`-ï¿½/, `ï¿½` C#ï¿½`Cï¿½%aï¿½`ï¿½.*!-ï¿½0,ï¿½/+ï¿½/*-ï¿½1,  G  ï¿½Ccï¿½ b ï¿½ PXï¿½@`Yfï¿½c`#a8# ï¿½UX G  ï¿½Ccï¿½ b ï¿½ PXï¿½@`Yfï¿½c`#a8!Y-ï¿½2, ï¿½ ETXï¿½EBï¿½ï¿½1*ï¿½EX0Y"Y-ï¿½3, ï¿½+ï¿½ ETXï¿½EBï¿½ï¿½1*ï¿½EX0Y"Y-ï¿½4, 5ï¿½`-ï¿½5, ï¿½EBï¿½Ecï¿½ b ï¿½ PXï¿½@`Yfï¿½cï¿½+ï¿½Ccï¿½ b ï¿½ PXï¿½@`Yfï¿½cï¿½+ï¿½ ï¿½     D>#8ï¿½4*!-ï¿½6, < G ï¿½Ccï¿½ b ï¿½ PXï¿½@`Yfï¿½c`ï¿½ Ca8-ï¿½7,.<-ï¿½8, < G ï¿½Ccï¿½ b ï¿½ PXï¿½@`Yfï¿½c`ï¿½ Caï¿½Cc8-ï¿½9,ï¿½ % . Gï¿½ #Bï¿½%Iï¿½ï¿½G#G#a Xb!Yï¿½#Bï¿½8*-ï¿½:,ï¿½ ï¿½#Bï¿½%ï¿½%G#G#aï¿½ Bï¿½C+eï¿½.#  <ï¿½8-ï¿½;,ï¿½ ï¿½#Bï¿½%ï¿½% .G#G#a ï¿½#Bï¿½ Bï¿½C+ ï¿½`PX ï¿½@QXï¿½  ï¿½&YBB# ï¿½
C ï¿½#G#G#a#F`ï¿½Cï¿½b ï¿½ PXï¿½@`Yfï¿½c` ï¿½+ ï¿½ï¿½a ï¿½C`d#ï¿½CadPXï¿½Caï¿½C`Yï¿½%ï¿½b ï¿½ PXï¿½@`Yfï¿½ca#  ï¿½&#Fa8#ï¿½
CFï¿½%ï¿½
CG#G#a` ï¿½Cï¿½b ï¿½ PXï¿½@`Yfï¿½c`# ï¿½+#ï¿½C`ï¿½+ï¿½%aï¿½%ï¿½b ï¿½ PXï¿½@`Yfï¿½cï¿½&a ï¿½%`d#ï¿½%`dPX!#!Y#  ï¿½&#Fa8Y-ï¿½<,ï¿½ ï¿½#B   ï¿½& .G#G#a#<8-ï¿½=,ï¿½ ï¿½#B ï¿½
#B   F#Gï¿½+#a8-ï¿½>,ï¿½ ï¿½#Bï¿½%ï¿½%G#G#aï¿½ TX. <#!ï¿½%ï¿½%G#G#a ï¿½%ï¿½%G#G#aï¿½%ï¿½%Iï¿½%aï¿½  cc# Xb!Ycï¿½ b ï¿½ PXï¿½@`Yfï¿½c`#.#  <ï¿½8#!Y-ï¿½?,ï¿½ ï¿½#B ï¿½
C .G#G#a `ï¿½ `fï¿½b ï¿½ PXï¿½@`Yfï¿½c#  <ï¿½8-ï¿½@,# .Fï¿½%Fï¿½CXPRYX <Y.ï¿½0+-ï¿½A,# .Fï¿½%Fï¿½CXRPYX <Y.ï¿½0+-ï¿½B,# .Fï¿½%Fï¿½CXPRYX <Y# .Fï¿½%Fï¿½CXRPYX <Y.ï¿½0+-ï¿½C,ï¿½:+# .Fï¿½%Fï¿½CXPRYX <Y.ï¿½0+-ï¿½D,ï¿½;+ï¿½  <ï¿½#Bï¿½8# .Fï¿½%Fï¿½CXPRYX <Y.ï¿½0+ï¿½C.ï¿½0+-ï¿½E,ï¿½ ï¿½%ï¿½&   F#Gaï¿½#B.G#G#aï¿½C+# < .#8ï¿½0+-ï¿½F,ï¿½
%Bï¿½ ï¿½%ï¿½% .G#G#a ï¿½#Bï¿½ Bï¿½C+ ï¿½`PX ï¿½@QXï¿½  ï¿½&YBB# Gï¿½Cï¿½b ï¿½ PXï¿½@`Yfï¿½c` ï¿½+ ï¿½ï¿½a ï¿½C`d#ï¿½CadPXï¿½Caï¿½C`Yï¿½%ï¿½b ï¿½ PXï¿½@`Yfï¿½caï¿½%Fa8# <#8!  F#Gï¿½+#a8!Yï¿½0+-ï¿½G,ï¿½ :+.ï¿½0+-ï¿½H,ï¿½ ;+!#  <ï¿½#B#8ï¿½0+ï¿½C.ï¿½0+-ï¿½I,ï¿½  Gï¿½ #Bï¿½ .ï¿½6*-ï¿½J,ï¿½  Gï¿½ #Bï¿½ .ï¿½6*-ï¿½K,ï¿½ ï¿½7*-ï¿½L,ï¿½9*-ï¿½M,ï¿½ E# . Fï¿½#a8ï¿½0+-ï¿½N,ï¿½
#Bï¿½M+-ï¿½O,ï¿½  F+-ï¿½P,ï¿½ F+-ï¿½Q,ï¿½ F+-ï¿½R,ï¿½F+-ï¿½S,ï¿½  G+-ï¿½T,ï¿½ G+-ï¿½U,ï¿½ G+-ï¿½V,ï¿½G+-ï¿½W,ï¿½   C+-ï¿½X,ï¿½  C+-ï¿½Y,ï¿½  C+-ï¿½Z,ï¿½ C+-ï¿½[,ï¿½  C+-ï¿½\,ï¿½ C+-ï¿½],ï¿½ C+-ï¿½^,ï¿½C+-ï¿½_,ï¿½  E+-ï¿½`,ï¿½ E+-ï¿½a,ï¿½ E+-ï¿½b,ï¿½E+-ï¿½c,ï¿½  H+-ï¿½d,ï¿½ H+-ï¿½e,ï¿½ H+-ï¿½f,ï¿½H+-ï¿½g,ï¿½   D+-ï¿½h,ï¿½  D+-ï¿½i,ï¿½  D+-ï¿½j,ï¿½ D+-ï¿½k,ï¿½  D+-ï¿½l,ï¿½ D+-ï¿½m,ï¿½ D+-ï¿½n,ï¿½D+-ï¿½o,ï¿½ <+.ï¿½0+-ï¿½p,ï¿½ <+ï¿½@+-ï¿½q,ï¿½ <+ï¿½A+-ï¿½r,ï¿½ ï¿½ <+ï¿½B+-ï¿½s,ï¿½<+ï¿½@+-ï¿½t,ï¿½<+ï¿½A+-ï¿½u,ï¿½ ï¿½<+ï¿½B+-ï¿½v,ï¿½ =+.ï¿½0+-ï¿½w,ï¿½ =+ï¿½@+-ï¿½x,ï¿½ =+ï¿½A+-ï¿½y,ï¿½ =+ï¿½B+-ï¿½z,ï¿½=+ï¿½@+-ï¿½{,ï¿½=+ï¿½A+-ï¿½|,ï¿½=+ï¿½B+-ï¿½},ï¿½ >+.ï¿½0+-ï¿½~,ï¿½ >+ï¿½@+-ï¿½,ï¿½ >+ï¿½A+-ï¿½ï¿½,ï¿½ >+ï¿½B+-ï¿½ï¿½,ï¿½>+ï¿½@+-ï¿½ï¿½,ï¿½>+ï¿½A+-ï¿½ï¿½,ï¿½>+ï¿½B+-ï¿½ï¿½,ï¿½ ?+.ï¿½0+-ï¿½ï¿½,ï¿½ ?+ï¿½@+-ï¿½ï¿½,ï¿½ ?+ï¿½A+-ï¿½ï¿½,ï¿½ ?+ï¿½B+-ï¿½ï¿½,ï¿½?+ï¿½@+-ï¿½ï¿½,ï¿½?+ï¿½A+-ï¿½ï¿½,ï¿½?+ï¿½B+-ï¿½ï¿½,ï¿½ EPXï¿½ï¿½EX#!!YYB+ï¿½eï¿½$Pxï¿½EX0Y- Kï¿½ ï¿½RXï¿½ï¿½Yï¿½ï¿½  cpï¿½ Bï¿½  *ï¿½ Bï¿½ 
*ï¿½ Bï¿½ 
*ï¿½ Bï¿½   *ï¿½ Bï¿½   *ï¿½   Dï¿½$ï¿½QXï¿½@ï¿½Xï¿½  dDï¿½(ï¿½QXï¿½ ï¿½Xï¿½   DYï¿½'ï¿½QXï¿½ï¿½ @ï¿½cTXï¿½   DYYYYYï¿½ *ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ Dï¿½d DD
```

## static/js/blame.js
```
/**
 * The BlamePopup is used for showing coverage and "annotate" (the less
 * judgemental term for "blame").  It previously was "annotate" specific.  Out
 * of an abundance of laziness and to minimize diff size, the existing
 * terminology is being left intact for now.
 */
var BlamePopup = new (class BlamePopup {
  constructor() {
    this.popup = document.createElement("div");
    this.popup.id = "blame-popup";
    this.popup.className = "blame-popup";
    this.popup.style.display = "none";
    document.body.appendChild(this.popup);

    // The .blame-strip element for which blame is currently being displayed.
    this._blameElement = null;
    this._expansionIndex = null;

    // The previous blame element for which we have already shown the popup.
    //
    // This is the current owner of the popup element.
    this.popupOwner = null;

    // A very simply MRU cache of size 1.  We don't issue an XHR if we already
    // have the data.  This is important for the case where the user is moving
    // their mouse along the same contiguous run of blame data.  In that case,
    // the `blameElement` changes, but the `revs` stays the same.
    this.prevRevs = null;
    this.prevJson = null;

    this.coverageDetailsShown = true;
    this.hideCoverageStripDetails();
  }

  showCoverageStripDetails() {
    if (this.coverageDetailsShown) {
      return;
    }
    this.coverageDetailsShown = true;
    document.documentElement.classList.add("coverage-details-shown");
  }

  hideCoverageStripDetails() {
    if (!this.coverageDetailsShown) {
      return;
    }
    this.coverageDetailsShown = false;
    document.documentElement.classList.remove("coverage-details-shown");
  }

  detachFromCurrentOwner() {
    if (!this.popupOwner) {
      return;
    }
    this.popupOwner.parentNode.removeAttribute("aria-owns");
    this.popupOwner.setAttribute("aria-expanded", "false");
    this.popupOwner = null;
  }

  // Hides the popup if open.
  hide() {
    this.detachFromCurrentOwner();
    this.popup.style.display = "none";

    this.hideCoverageStripDetails();
  }

  // Asynchronously initiates lookup and display of the blame data for the current
  // `blameElement`. The popup is added as a child of the blameElt in the DOM.
  async update() {
    // If there's no current element, just bail.
    if (!this.blameElement) {
      this.hide();
      return;
    }

    // Latch the current element in case by the time our fetch comes back it's
    // no longer the current one.
    const elt = this.blameElement;
    let content;

    let top;
    let left;

    const isExpansion = typeof elt.dataset.expansions !== 'undefined' && elt.dataset.expansions !== null;
    const isAnnotate = !!elt.dataset.blame;
    if (isExpansion) {
      content = await this.generateExpansionContent(elt);
      let rect = elt.getBoundingClientRect();
      top = rect.bottom + window.scrollY;
      left = rect.left + window.scrollX;
    } else {
      // The coverage and annotate strips are adjacent and it would be bad UX for
      // hovering over the coverage strip to occlude the annotate strip, so we
      // adjust the coverage elements to use the annotate element for positioning.
      let hoverRightOfElt;

      if (isAnnotate) {
        content = await this.generateAnnotateContent(elt);
        hoverRightOfElt = elt;
      } else {
        content = await this.generateCoverageContent(elt);
        // This obviously assumes the known hard-coded DOM from `format.rs`.
        hoverRightOfElt = elt.parentElement.nextElementSibling?.firstElementChild;
      }

      if (!hoverRightOfElt) {
        return;
      }

      let rect = hoverRightOfElt.getBoundingClientRect();
      top = rect.top + window.scrollY;
      left = rect.right + window.scrollX;
    }

    // If no content was returned or the blame element has changed, bail.
    if (!content || this.blameElement != elt) {
      return;
    }

    this.detachFromCurrentOwner();
    this.popup.style.display = "";
    // This also works, but transform doesn't even require layout.
    // this.popup.style.left = left + "px";
    // this.popup.style.top = top + "px";
    this.popup.style.transform = `translatey(${top}px) translatex(${left}px)`;
    this.popup.innerHTML = content;
    this.popupOwner = this.blameElement;
    // We set aria-owns on the parent role=cell instead of the button.
    this.popupOwner.parentNode.setAttribute("aria-owns", "blame-popup");
    this.popupOwner.setAttribute("aria-expanded", "true");

    // Adjust transform to ensure the popup doesn't go outside the window.
    let popupBox = this.popup.getBoundingClientRect();
    if (popupBox.bottom > window.innerHeight) {
      top -= popupBox.bottom - window.innerHeight;
      this.popup.style.transform = `translatey(${top}px) translatex(${left}px)`;
    }

    if (!isExpansion) {
      if (isAnnotate) {
        this.hideCoverageStripDetails();
      } else {
        this.showCoverageStripDetails();
      }
    }
  }

  async generateExpansionContent(elt) {
    const expansions = JSON.parse(elt.dataset.expansions);
    const sym = this.expansionIndex[0];
    const platform = this.expansionIndex[1];
    const jumpref = this.expansionIndex[2];
    const expansion = expansions[sym][platform];
    const onlyOneExpansion = Object.keys(expansions).length == 1 && Object.keys(expansions[sym]).length == 1;

    if (jumpref && jumpref.jumps.def) {
      const tree = document.getElementById("data").getAttribute("data-tree");
      const jumpFileName = jumpref.jumps.def.slice(jumpref.jumps.def.lastIndexOf(',') + 1)
      if (onlyOneExpansion) {
        return `Expansion of <span class="symbol" data-symbols="${sym}">${jumpref.pretty}</span>:<br><code>${expansion}</code>`;
      } else {
        return `Expansion of <span class="symbol" data-symbols="${sym}">${jumpref.pretty}</span> on ${platform}:<br><code>${expansion}</code>`;
      }
    } else {
      if (onlyOneExpansion) {
        return `Expansion:<br><code>${expansion}</code>`;
      } else {
        return `Expansion on ${platform}:<br><code>${expansion}</code>`;
      }
    }
  }

  async generateCoverageContent(elt) {
    let content;

    if (elt.classList.contains("cov-no-data")) {
      content = `<div>There is no coverage data for this file.</div>`;
    } else if (elt.classList.contains("cov-unknown")) {
      content = `<div>There was coverage data for this file but not for this line.</div>`;
    } else if (elt.classList.contains("cov-interpolated")) {
      content =
        `<div>This line wasn't instrumented for coverage, but we ` +
        `interpolated coverage for this line to make it visually less ` +
        `distracting.</div>`;
    } else if (elt.classList.contains("cov-uncovered")) {
      content = `<div>This line wasn't instrumented for coverage.</div>`;
    } else {
      const hitCount = parseInt(elt.dataset.coverage, 10);
      content =
        `<div>This line was hit ${hitCount} times per coverage ` +
        `instrumentation.<div>`;
    }

    return content;
  }

  async generateAnnotateContent(elt) {
    const blame = elt.dataset.blame;
    const [revs, filespecs, linenos] = blame.split("#");

    const data = document.getElementById("data");
    const path = data.getAttribute("data-path");
    const tree = data.getAttribute("data-tree");

    if (this.prevRevs != revs) {
      let response = await fetch(`/${tree}/commit-info/${revs}`);
      this.prevJson = await response.json();
      this.prevRevs = revs;
    }

    // If the request was too slow, we may no longer want to display blame for
    // this element, bail.
    if (this.blameElement != elt) {
      return;
    }

    let json = this.prevJson;

    let content = "";
    let revList = revs.split(",");
    let filespecList = filespecs.split(",");
    let linenoList = linenos.split(",");

    // The last entry in the list (if it's not empty) is the real one we want
    // to show. The entries before that were "ignored", so we put them in a
    // hidden box that the user can expand.
    let ignored = [];
    for (let i = 0; i < revList.length; i++) {
      // An empty final entry is used to indicate that all the entries we
      // provided were "ignored" (but we didn't provide more because we hit the
      // max limit).
      if (!revList[i]) {
        break;
      }

      let rendered = "";
      let revPath = filespecList[i] == "%" ? path : filespecList[i];
      rendered += `<div class="blame-entry">`;
      rendered += json[i].header;

      let diffLink = `/${tree}/diff/${revList[i]}/${revPath}#${linenoList[i]}`;
      rendered += `<br>Show <a href="${encodeURI(diffLink)}">annotated diff</a>`;

      if (json[i].fulldiff) {
        rendered += ` or <a href="${encodeURI(json[i].fulldiff)}">full diff</a>`;
      }

      if (json[i].parent) {
        let parentLink = `/${tree}/rev/${json[i].parent}/${revPath}#${linenoList[i]}`;
        rendered += `<br><a href="${encodeURI(parentLink)}" class="deemphasize">Show latest version without this line</a>`;
      }

      let revLink = `/${tree}/rev/${revList[i]}/${revPath}#${linenoList[i]}`;
      rendered += `<br><a href="${encodeURI(revLink)}" class="deemphasize">Show earliest version with this line</a>`;
      rendered += "</div>";

      if (i < revList.length - 1) {
        ignored.push(rendered);
      } else {
        content += rendered;
      }
    }

    if (ignored.length) {
      content += `<br><details><summary>${
        ignored.length
      } ignored changesets</summary>${ignored.join("")}</details>`;
    }

    return content;
  }

  get blameElement() {
    return this._blameElement;
  }

  set blameElement(newElement) {
    if (this.blameElement == newElement) {
      return;
    }
    this._blameElement = newElement;
    this.update();
  }

  get expansionIndex() {
    return this._expansionIndex;
  }

  set expansionIndex(value) {
    if (this.expansionIndex == value) {
      return;
    }
    this._expansionIndex = value;
    this.update();
  }
})();

var BlameStripHoverHandler = new (class BlameStripHoverHandler {
  constructor() {
    // The .blame-strip element the mouse is hovering over.  Clicking on the
    // element will null this out as a means of letting the user get rid of the
    // popup if they didn't actually want to see the pop-up.
    this.mouseElement = null;
    // Set to true if the user clicks on the blame strip. This will keep the
    // keep the popup visible until the user clicks elsewhere.
    this.keepVisible = false;

    for (let element of document.querySelectorAll(".blame-strip")) {
      element.addEventListener("mouseenter", this);
      element.addEventListener("mouseleave", this);
      // Use passive listeners for touch, because these elements already
      // disable touch-panning via touch-action properties.
      element.addEventListener("touchstart", this, { passive: true });
      element.addEventListener("touchmove", this, { passive: true });
    }

    for (let element of document.querySelectorAll(".cov-strip")) {
      element.addEventListener("mouseenter", this);
      element.addEventListener("mouseleave", this);
      // Use passive listeners for touch, because these elements already
      // disable touch-panning via touch-action properties.
      element.addEventListener("touchstart", this, { passive: true });
      element.addEventListener("touchmove", this, { passive: true });
    }

    BlamePopup.popup.addEventListener("mouseenter", this);
    BlamePopup.popup.addEventListener("mouseleave", this);
    // Click listener needs to be capturing since whatever is being clicked on
    // (e.g. a code fragment that displays a context menu) may actually
    // consume the event.
    window.addEventListener("click", this, { capture: true });
    document
      .getElementById("scrolling")
      .addEventListener("scroll", this, { passive: true });
  }

  isStripElement(elem) {
    return elem.matches(".blame-strip") || elem.matches(".cov-strip");
  }

  handleEvent(event) {
    if (event.type == "touchstart" || event.type == "touchmove") {
      // For touch events, event.target is always the element at which the first touchstart landed. So
      // this condition filters for touch sequences where the touchstart started on a strip element.
      if (this.isStripElement(event.target) && event.touches.length == 1) {
        // Within those touch sequences, update the blame element to whatever is under the touch
        // point, or null if the touch point moves off the strip.
        let elementUnderTouch = document.elementFromPoint(
          event.touches[0].clientX,
          event.touches[0].clientY
        );
        BlamePopup.blameElement = this.isStripElement(elementUnderTouch)
          ? elementUnderTouch
          : null;
      }
      return;
    }

    // Suppress the blame hover popup if the context menu is visible.
    if (ContextMenu.active) {
      return;
    }

    if (this.keepVisible) {
      if (event.type == "mouseenter" || event.type == "mouseleave") {
        // Ignore mouseenter/mouseleave events if keepVisible is true
        return;
      }
    }

    let clickedOutsideBlameStrip =
      event.type == "click" && !this.isStripElement(event.target);
    if (clickedOutsideBlameStrip && !BlamePopup.blameElement) {
      // Don't care about clicks outside the blame strip if there's no popup showing.
      return;
    }
    if (clickedOutsideBlameStrip && BlamePopup.popup.contains(event.target)) {
      // Also don't care if the click landed on the blame popup itself (e.g. clicking
      // on a link or expanding the details box in the popup).
      return;
    }

    // Debounced pop-up closer..
    //
    // If the mouse leaves a blame-strip element and doesn't move onto another
    // one within 100ms, close the popup.  Also, if the user clicks on the
    // blame-strip element and doesn't move onto a new element, close the
    // pop-up.
    if (
      event.type == "mouseleave" ||
      event.type == "scroll" ||
      clickedOutsideBlameStrip
    ) {
      this.keepVisible = false;
      this.mouseElement = null;
      setTimeout(() => {
        if (this.mouseElement) {
          return;
        } // Mouse moved somewhere else inside the strip.
        BlamePopup.blameElement = null;
      }, 100);
    } else {
      // We run this code on either "mouseenter", or on a "click" event where
      // the click landed inside the blame strip. In the latter case we set
      // keepVisible to pin the blame popup open until another click dismisses
      // it, or a scroll event happens (since the popup doesn't move properly with
      // scrolling).
      const isClick = event.type == "click";
      // A click on the same exact element as the last click should toggle the
      // popup closed.  This is important for the screen-reader use-case where
      // inspecting blame details involves hitting enter on the role=button
      // blame cell to toggle the blame popup on and then off.
      //
      // We could check `keepVisible` here but that creates a weird situation
      // when activating via mouse where we get the sequence 1) hover triggers
      // popup, 2) click maintains popup and makes it keepVisible, and then 3)
      // second click hides popup.  By not checking, we get 1) hover triggers
      // popup, 2) click hides popup.
      if (isClick && this.mouseElement === event.target) {
        this.keepVisible = false;
        this.mouseElement = null;
        BlamePopup.blameElement = null;
        return;
      }
      this.keepVisible = isClick;
      this.mouseElement = event.target;
      if (this.mouseElement != BlamePopup.popup) {
        BlamePopup.blameElement = this.mouseElement;
      }
    }
  }
})();

```

## static/js/settings.js
```
/**
 * Compactly defined setting names and defaults; expanded and interpreted by the
 * `Settings` class as its construction time in conjunction with
 * `SETTINGS_VERSION`.
 *
 * Although we define a more complex conceptual model for how settings work in
 * `settings.liquid`, we have a simple nested model here that operates based
 * on convention / specific names and it's on the `settings.liquid` page to make
 * sure it names its input "id" attributes to match with the automatic mangling
 * we perform below.
 *
 * General definition rules:
 * - The top level defines settings groups, and these should be named using
 *   camelCase.
 * - Inside each of those group definitions, each camelCase key names a setting
 *   that will be exposed as `Settings.[groupName].[keyName]` on the Settings
 *   object.  For form binding purposes, the names will be normalized from
 *   camelCase "fooBarBaz" to dash-delimited "foo-bar-baz", and the group name
 *   will be separated from the key name by a double-dash, like
 *   "group-name--key-name".
 * - A key name of "enabled" is treated as a feature gate.  The value exposed to
 *   JS will be a simple boolean (ex: `Settings.fancyBar.enabled`), but the
 *   storage value will be one of "" (to use the default feature gate), "alpha",
 *   "beta", or "release".
 * - Setting keys will have a dictionary that may contain the following keys:
 *   - `default`: This identifies the default value and also serves to indicate
 *     the type of the value.  This is not relevant for "enabled" keys.
 *   - `quality`: For the case of the special "enabled" featured gates, this
 *     expresses the current quality of the feature for feature gate logic
 *     purposes.  Legal values are "alpha", "beta", and "release".
 *   - `dependencies`: For the case of the special "enabled" feature gates, this
 *     expresses the other features that must also be enabled for this feature
 *     to be enabled.  Only the camelCase groupName needs to be identified.  The
 *     dependency check mechanism will run in as `SETTING_DEFS` is processed in
 *     its natural order, so features that depend on other features should
 *     appear after them in the dictionaries and will therefore emergently
 *     support transitive dependency checks correctly.
 *   - `introducedIn`: Indicates the `SETTINGS_VERSION` number version in which
 *     the setting was introduced.  The intent is to enable the ability to
 *     badge the settings button to let users know there are new settings that
 *     they might want to check out and then to help highlight and/or provide a
 *     table of contents with direct links to point out the new settings on the
 *     settings page without requiring the user to manually skim the page and
 *     guess what might be new.  This is used in conjunction with the
 *     `userSawVersion` and `userAckedVersion` storage values.
 *     This field is required to ensure the SETTINGS_VERSION is bumped for each
 *     new entry.
 *
 * Bump SETTINGS_VERSION whenever you add a new entry here, with setting
 * `introducedIn` property to the new version's value! This ensure that
 * previously saved settings are migrated by #upgradeSettings().
 */
const SETTING_DEFS = {
  global: {
    defaultFeatureGate: {
      // On release channels stick to "release" as our default gate, but for
      // experimental channels let's turn everything on.  Users can set this
      // back to "release" if they want.
      default: (document.location.host === "searchfox.org") ? "release" : "alpha",
      introducedIn: 1,
    },
  },
  pageTitle: {
    lineSelection: {
      default: true,
      introducedIn: 1,
    },

    stickySymbol: {
      default: true,
      introducedIn: 1,
    },
  },
  contextMenu: {
    // There are a number of resources that may be gated behind Mozilla LDAP
    // access.  For discoverability reasons, we want to default this to true,
    // but for users without access, it should be possible to set this to false
    // so they don't have to see options they can't use.
    haveMozillaLdap: {
      default: true,
      introducedIn: 1,
    }
  },
  fancyBar: {
    enabled: {
      quality: "alpha",
      introducedIn: 1,
    },
  },
  semanticInfo: {
    enabled: {
      quality: "alpha",
      introducedIn: 3,
    },
  },
  diagramming: {
    enabled: {
      quality: "alpha",
      introducedIn: 2,
    },
  },
  expansions: {
    enabled: {
      quality: "alpha",
      introducedIn: 5,
    },
  },
  debug: {
    ui: {
      default: (document.location.host !== "searchfox.org"),
      introducedIn: 4,
    },
  },
};

const QUALITY_ORDERING = [
  "alpha",
  "beta",
  "release",
];

/**
 * Checks if the user's selected quality gate (falling back to their default)
 * is met/exceeded by the feature's current quality.
 */
function meetsQualityGate(userSpecific, userDefault, featureQuality) {
  let userCriteria = userSpecific || userDefault;
  let userIndex = QUALITY_ORDERING.indexOf(userCriteria);
  let featureIndex = QUALITY_ORDERING.indexOf(featureQuality);
  return featureIndex >= userIndex;
}

/**
 * This is just a specialized version of SETTING_DEFS where each top-level
 * group is conceptually associated with a widget which must usually must be
 * explicitly enabled by the user, but we do allow for a default which will be
 * populated on first load.  Currently there is no plan to build a widget
 * abstraction layer; the term exists to distinguish optional, loosely-coupled
 * features (widgets) from core features that will eventually be non-optional.
 * (Noting that there are still discussions to be had in this space, but the
 * intent is to set expectations appropriately.)
 *
 * The main differences from SETTINGS_DEFS above:
 * - `enabled` can be omitted; if it's omitted, a value of `{ default: false }`
 *   is assumed.  This is notably different from how feature `enabled` keys
 *   work.  You can express `dependencies` if you want, but in general it will
 *   probably be assumed that most widgets will depend on the "fancyBar" being
 *   enabled.
 */
const WIDGET_DEFS = {
  // Proposed in https://bugzilla.mozilla.org/show_bug.cgi?id=1808415 and with
  // its setting definitions stubbed here as an exercise for seeing how
  // widgets could be hooked up.  The widget doesn't actually exist yet.
  openInEditor: {
    enabled: {
      default: false,
      introducedIn: 1,
    },
    linkTemplate: {
      default: "editor://open/?file={{tree}}/{{path}}",
      introducedIn: 1,
    },
  }
};

/**
 * This version is stored in local storage as part of our persisted settings
 * structure.  In particular, we store it in the top level twice:
 * - `version`: The version of SETTING_DEFS that was last used to populate the
 *   `settings` field that contains the actual persisted settings values.
 * - `userSawVersion`: The `SETTINGS_VERSION` of the last time the user saw the
 *   settings page.  This would be used for quieting any badging that's
 *   notifying the user there are new settings.
 * - `userAckedVersion`: The `SETTINGS_VERSION` of the last time the user
 *   implicitly or explicitly confirmed they understand what the new settings
 *   are.  This differs from `userSawVersion` in that we might still highlight
 *   (and list in a TOC) new settings introduced between `userAckedVersion` and
 *   `version` even if we aren't displaying a badge.  In particular, it's our
 *   expectation that many users may find the (probably opt-in) badge
 *   distracting and that they likely would want to actually process the new
 *   settings later on.
 */
const SETTINGS_VERSION = 5;

/**
 * Convert a "camelCaseString" to "camel-case-string".
 */
function camelCaseToLowerCaseDash(ccStr) {
  return ccStr.replaceAll(/[A-Z]/g, x => `-${x.toLowerCase()}`);
}

/**
 * Convert a "group-name--key-name" string to a tuple of ["groupName", "keyName"].
 */
function convertDashedIdentifierToGroupAndName(idStr) {
  const pieces = idStr.split("--");
  return pieces.map(s => s.replace(/-[a-z]/g, x => x[1].toUpperCase()));
}

/**
 * Settings singleton which will block on loading LocalStorage data so that
 * setting data is available immediately after its initialization.  We can
 * potentially change this in the future.  Settings are exposed in a parallel
 * fashion to how they are defined in `SETTING_DEFS` and `WIDGET_DEFS`, so
 * the definition for `pageTitle.lineSelection` will be exposed at
 * `Settings.pageTitle.lineSelection`.
 *
 * Binding to forms on the `settings.html` page derived from the
 * `settings.liquid` template is handled via the `SettingsBinder` singleton
 * below.
 *
 * Storage is global for the entire origin and is not tree-specific at this
 * time.
 *
 * All settings storage happens in the single "settings" LocalStorage key,
 * stored as JSON and read only at page load time, currently.  In the future
 * we can have this listen for "storage" events to live-update, but the
 * complexity does not seem merited at this time.
 *
 * The choice of a single JSON key/value is made because:
 * - We want richer types and structures than just raw strings.
 * - There are advantages to the coherency of a single key/value, including
 *   self-descriptive versioning.
 * - We read all of the settings at page load time, and given this access
 *   pattern, and the fact that we don't/won't store much other data in
 *   LocalStorage, it's easier and faster for Gecko's LocalStorage (NG) if it's
 *   just giving us a single value.  (Actually, if we do start storing other
 *   data in LS, using a single key/value ends up even better.)
 *
 * The choice of LocalStorage is the most pragmatic choice for storing a small
 * amount of data in a way that can be used to impact UI to minimize flashes of
 * changed content.  Cookies are not appropriate because they would be sent to
 * the server (which we neither want nor need) and `document.cookie` is limited
 * to 7-days by anti-tracking policy.  Firefox's LocalStorage (NG) will preload
 * data when it knows the connection is going to happen, and will keep it around
 * in memory so that file I/O should be avoided for searchfox.  This differs
 * from options like IndexedDB and the Cache API.
 */
const Settings = new (class Settings {
  #canonicalData;

  constructor() {
    if (document.location.host !== "searchfox.org") {
      this.#verifyIntegrity();
    }

    // This will synchronously block if the browser has not been able to preload
    // the LocalStorage for the origin.  This is a hard-block and will not
    // spin an event loop.  This is the worst part of LocalStorage.
    this.#canonicalData = this.#loadCanonicalData();
    this.#applyAndTransformCanonicalDataToSelf();
  }

  #verifyIntegrity() {
    // In order to ensure the SETTINGS_VERSION is bumped for each new setting
    // item, ensure each setting item has valid `introducedIn` property.
    for (const defs of [SETTING_DEFS, WIDGET_DEFS]) {
      for (const [groupName, groupDefs] of Object.entries(defs)) {
        for (const [keyName, keyDef] of Object.entries(groupDefs)) {
          if (!("introducedIn" in keyDef)) {
            alert(`settings.js : ${groupName}.${keyName} has no introducedIn property. Please set it after bumping SETTINGS_VERSION.`);
          }

          if (keyDef.introducedIn > SETTINGS_VERSION) {
            alert(`settings.js : ${groupName}.${keyName}.introducedIn is greater than SETTINGS_VERSION. Please  bump SETTINGS_VERSION.`);
          }
        }
      }
    }
  }

  #mergeSettingsCanonicalData(settingsRoot, groupName, groupDefs, isWidget) {
    let groupVals = settingsRoot[groupName];
    if (!groupVals) {
      groupVals = settingsRoot[groupName] = {};
    }

    for (const [keyName, keyDef] of Object.entries(groupDefs)) {
      // There's currently no concept of migration, so we have nothing to do if
      // the key already exist in the values dictionary.
      if (keyName in groupVals) {
        continue;
      }

      if (keyName === "enabled") {
        // For widgets we propagate the default, but for features we use an
        // default
        if (isWidget) {
          groupVals[keyName] = ("default" in keyDef) ? keyDef.default : false;
        } else {
          groupVals[keyName] = "";
        }
      } else {
        groupVals[keyName] = keyDef.default;
      }
    }
  }

  #generateDefaultSettings(parseFailed) {
    const data = {
      version: SETTINGS_VERSION,
      // In order to help debug storage-related problems, we want to track when
      // the data structured was first created.  These values will never be
      // submitted anywhere, although we may ask the user to help
      created: {
        version: SETTINGS_VERSION,
        timestamp: Date.now(),
        // Did we have a local storage payload but we failed to parse it?
        parseFailed,
      },
      userSawVersion: SETTINGS_VERSION,
      userAckedVersion: SETTINGS_VERSION,
      settings: {},
    };

    // Currently, the upgrade logic just fills in values it doesn't already
    // know.
    return this.#upgradeSettings(data);
  }

  #upgradeSettings(data) {
    for (const [groupName, groupDefs] of Object.entries(SETTING_DEFS)) {
      this.#mergeSettingsCanonicalData(data.settings, groupName, groupDefs, false);
    }

    for (const [widgetName, widgetDefs] of Object.entries(WIDGET_DEFS)) {
      this.#mergeSettingsCanonicalData(data.settings, widgetName, widgetDefs, true);
    }
    return data;
  }

  /**
   * Load the settings data from LocalStorage, creating default values if there
   * was no existing data, and upgrading existing data if appropriate.  Data
   * will automatically be written to LocalStorage if any changes may have
   * happened.
   *
   * Note that the returned data has not had feature gates applied; those are
   * only interpreted
   */
  #loadCanonicalData() {
    const strData = window.localStorage.getItem("settings");
    let data = null;
    let parseFailed = false;
    if (strData !== null) {
      try {
        data = JSON.parse(strData);
      } catch(ex) {
        parseFailed = true;
      }
    }
    if (data === null) {
      data = this.#generateDefaultSettings(parseFailed);
      this.#saveCanonicalData(data);
    } else if (data.version < SETTINGS_VERSION) {
      data = this.#upgradeSettings(data);
      this.#saveCanonicalData(data);
    }
    return data;
  }

  #saveCanonicalData(explicitData) {
    if (explicitData) {
      this.#canonicalData = explicitData;
    }
    const strData = JSON.stringify(this.#canonicalData);
    window.localStorage.setItem("settings", strData);
  }

  /**
   * Applies the storage-representation data in `this.#canonicalData` to `this`,
   * applying feature-gating logic transformations in the process.  We freeze
   * the group value objects, replacing them each time this method is called.
   * No attempt is made to protect the group objects themselves because that
   * seems like a less likely problem.
   */
  #applyAndTransformCanonicalDataToSelf() {
    const settings = this.#canonicalData.settings;
    const userDefaultQualityGate = settings.global.defaultFeatureGate;

    for (const [groupName, groupValues] of Object.entries(settings)) {
      let transformed = {};
      const isWidget = groupName in WIDGET_DEFS;
      let groupDef;
      if (isWidget) {
        groupDef = WIDGET_DEFS[groupName];
      } else {
        groupDef = SETTING_DEFS[groupName];
      }

      for (const [keyName, keyValue] of Object.entries(groupValues)) {
        if (keyName === "enabled") {
          let enabled;
          if (isWidget) {
            enabled = keyValue;
          } else {
            enabled = meetsQualityGate(keyValue, userDefaultQualityGate, groupDef.enabled.quality);
          }
          let maybeDeps = groupDef[keyName]?.dependencies;
          if (maybeDeps) {
            for (const depGroupName of maybeDeps) {
              if (!this[depGroupName].enabled) {
                enabled = false;
              }
            }
          }
          transformed.enabled = enabled;
        } else {
          transformed[keyName] = keyValue;
        }
      }

      Object.freeze(transformed);
      this[groupName] = transformed;
    }
  }

  __hasUnseenSettings() {
    return this.#canonicalData.userSawVersion < SETTINGS_VERSION;
  }

  __markSettingsSeen() {
    this.#canonicalData.userSawVersion = SETTINGS_VERSION;
    this.#saveCanonicalData();
  }

  __hasUnacknowledgedSettings() {
    return this.#canonicalData.userAckedVersion < SETTINGS_VERSION;
  }

  __markSettingsAcknowledged() {
    this.#canonicalData.userAckedVersion = SETTINGS_VERSION;
    this.#saveCanonicalData();
  }

  __lookupSettingFromId(id) {
    const [groupName, keyName] = convertDashedIdentifierToGroupAndName(id);
    const isWidget = groupName in WIDGET_DEFS;
    let groupDef;
    if (isWidget) {
      groupDef = WIDGET_DEFS[groupName];
    } else {
      groupDef = SETTING_DEFS[groupName];
    }
    if (!groupDef) {
      return null;
    }
    const keyDef = groupDef[keyName];

    let type;
    let coerceFromString = x => x;
    if (keyName === "enable") {
      if (isWidget) {
        type = "feature-gate";
      } else {
        type = "boolean"
      }
    } else {
      type = typeof(keyDef);
    }

    if (type === "number") {
      coerceFromString = x => parseInt(x, 10)
    } else if (type === "boolean") {
      // assuming checkbox with default "on" value if checked.
      coerceFromString = x => x.length > 0;
    }
    // the identity transform is appropriate for "feature-gate" and everything
    // else right now.

    let rawValue = this.#canonicalData.settings[groupName][keyName];

    return {
      isWidget,
      groupName,
      groupDef,
      keyName,
      keyDef,
      type,
      rawValue,
      coerceFromString,
    };
  }

  __setValueFromIdSpace(id, value) {
    const { groupName, keyName } = this.__lookupSettingFromId(id);

    this.#canonicalData.settings[groupName][keyName] = value;
    this.#saveCanonicalData();
    this.#applyAndTransformCanonicalDataToSelf();
  }
})();

/**
 * Very basic routing / URL parsing / URL generating support.  This primarily
 * exists to try and support varying URLs based on user settings as it relates
 * to using the "search" endpoint versus the "query" endpoint and things like
 * that.  Searchfox's URL scheme is stable and so, apart from experimental
 * features that may not end up making it to "release", there is no reason to
 * favor using this class over inline URL generation.  (And in particular, if
 * you can generate a URL on the server as part of the HTML we serve, you
 * absolutely should.)
 */
const Router = new (class Router {
  constructor() {
    const pathParts = document.location.pathname.split("/");
    this.treeName = pathParts[1];
    this.endpoint = pathParts[2];
    if (this.endpoint === "pages") {
      this.page = pathParts.slice(3).join("/");
    } else if (this.endpoint === "rev") {
      this.rev = pathParts[3];
      this.sourcePath = pathParts.slice(4).join("/");
    } else if (this.endpoint === "source") {
      this.sourcePath = pathParts.slice(3).join("/");
    }
  }
})();

/**
 * Self-activating singleton which will automatically bind itself to the
 * `settings.html` page derived from the `settings.liquid` template.  In the
 * future we may also provide a means for adding interactive mechanisms to
 * opt-out of features without visiting the settings page, and that would also
 * want to operate through this class.
 */
const SettingsBinder = new (class SettingsBinder {
  constructor() {
    // If we are the settings page, bind form elements and do any template
    // expansions for the feature gate select payloads.  Our JS script is
    // currently loaded as part of the `scroll_footer.liquid` template which
    // comes after all content and so the DOM should therefore already exist.
    if (Router.page === "settings.html") {
      this.bindAndExpandTemplatedForms();
    }
  }

  bindAndExpandTemplatedForms() {
    // Make sure we only manipulate the content area and don't interfere with
    // the search UI.

    const root = document.querySelector("#content");

    // Let's add an idempotency guard that complains in order to help shine a
    // light on any logic problems which might otherwise result in weirdness.
    if ("boundSettings" in root) {
      console.error("Attempted to re-bind settings!");
      throw new Error("Redundant attempt to bind settings.");
    }
    root.boundSettings = true;

    const featureGateOptions = root.querySelector("#feature-gate-options");

    // Neutralize all form elements so nothing ever submits anywhere, as this is
    // all content-side.
    for (const form of root.querySelectorAll("form")) {
      form.addEventListener("submit", (evt) => { evt.preventDefault(); });
    }

    // Bind all form inputs
    for (const elem of root.querySelectorAll("input, select")) {
      const info = Settings.__lookupSettingFromId(elem.id);
      if (!info) {
        console.warn("Thought about binding to", elem, "with id", elem.id, "but could not.");
        continue;
      }
      if (elem.tagName === "INPUT") {
        if (elem.type === "checkbox") {
          elem.checked = Settings[info.groupName][info.keyName];
          elem.addEventListener("change", () => {
            Settings.__setValueFromIdSpace(elem.id, elem.checked);
          });
        } else if (elem.type === "text") {
          elem.value = Settings[info.groupName][info.keyName];
          elem.addEventListener("change", () => {
            Settings.__setValueFromIdSpace(elem.id, elem.value);
          });
        } else {
          console.warn("Don't know how to bind to", elem, "with type", elem.type);
        }
      } else if (elem.tagName === "SELECT") {
        // Enabling
        if (!info.isWidget && info.keyName === "enabled") {
          // To reduce maintenance if we change the feature gate payloads, we
          // just use our template clone on the inside, and it could also make
          // sense to may set/propagate any other attributes as appropriate.
          elem.appendChild(featureGateOptions.content.cloneNode(true));
        }
        elem.value = info.rawValue;
        elem.addEventListener("change", () => {
          Settings.__setValueFromIdSpace(elem.id, elem.value);
        });
      }
    }

    // Bind things that say what current qualities are:
    for (const elem of root.querySelectorAll('[id^="quality--"]')) {
      const useId = elem.id.substring("quality--".length);
      const info = Settings.__lookupSettingFromId(useId);
      elem.textContent = info.keyDef.quality;
    }
  }
})();

```

## static/js/search.js
```
var Dxr = new (class Dxr {
  constructor() {
    let constants = document.getElementById("data");

    // This will usually be "/"
    this.wwwRoot = constants.getAttribute("data-root");
    // This will look like "mozilla-central"
    this.tree = constants.getAttribute("data-tree");
    this.baseUrl = location.protocol + "//" + location.host;
    // This will end up "/TREE/static/icons/"
    this.icons = this.wwwRoot + `${this.tree}/static/icons/`;
    // This will usually be "/TREE/search"
    this.searchUrl = constants.getAttribute("data-search");
    this.timeouts = {
      search: 300,
      // We start the history timeout after the search updates (i.e., after
      // timeouts.search has elapsed).
      history: 2000 - 300,
    };
    this.searchBox = document.getElementById("search-box");

    // TODO(emilio): Maybe these should be their own web component or
    // something.
    this.fields = {
      query: document.getElementById("query"),
      path: document.getElementById("path"),
      caseSensitive: document.getElementById("case"),
      regexp: document.getElementById("regexp"),
    };
    this.bubbles = {
      query: document.getElementById("query-bubble"),
      path: document.getElementById("path-bubble"),
    };

    this.setupColSelector();

    this.startSearchTimer = null;
    // The timer to move to the next url.
    this.historyTimer = null;
    // The controller to allow aborting a fetch().
    this.fetchController = null;

    window.addEventListener("pageshow", () =>
      this.initFormFromLocalStorageOrUrl()
    );
    // FIXME: This reloads the page when you navigate to #lineno.
    window.addEventListener("popstate", () => window.location.reload(), {
      once: true,
    });

    // XXX hacky mechanism so that we only run this on pages with a "search"
    // header rather than a "query" header.  We should refactor this and the
    // general code listing generation so that we:
    // - pick search versus query based on Setting and can change by user
    //   choice even on any page.
    // - explicitly understand if it's operating in search or query mode.
    if (this.fields.path) {
      this.fields.query.addEventListener("input", () => this.startSearchSoon());
      this.fields.path.addEventListener("input", () => this.startSearchSoon());
      this.fields.regexp.addEventListener("change", () => this.startSearch());
      this.fields.caseSensitive.addEventListener("change", event => {
        window.localStorage.setItem("caseSensitive", event.target.checked);
        this.startSearch();
      });
      this.initFormFromLocalStorageOrUrl();
    }
  }

  cancel(cancelFetch = true) {
    if (this.startSearchTimer) {
      clearTimeout(this.startSearchTimer);
      this.startSearchTimer = null;
    }
    if (this.historyTimer) {
      clearTimeout(this.historyTimer);
      this.historyTimer = null;
    }
    if (cancelFetch && this.fetchController) {
      this.fetchController.abort();
      this.fetchController = null;
    }
  }

  startSearchSoon() {
    this.cancel(/* cancelFetch = */ false);
    this.startSearchTimer = setTimeout(() => {
      this.startSearchTimer = null;
      this.startSearch();
    }, this.timeouts.search);
  }

  async startSearch() {
    this.cancel();

    let query = this.fields.query.value;
    let path = this.fields.path.value.trim();

    if (!query.length && !path.length) {
      return this.hideBubbles();
    }

    if (query.length < 3 && path.length < 3) {
      return this.showBubble(
        "info",
        "Enter at least 3 characters to do a search.",
        query.length ? "query" : "path"
      );
    }

    this.hideBubbles();

    let url = this.constructURL();

    let controller = new AbortController();

    this.fetchController = controller;

    this.searchBox.classList.add("in-progress");
    let results;
    try {
      let response = await fetch(url.href, {
        headers: {
          Accept: "application/json",
        },
        signal: this.fetchController.signal,
      });
      results = await response.json();
      if (!response.ok) {
        return this.showBubble(results.error_level, results.error_html);
      }
    } catch (error) {
      if (controller.signal.aborted) {
        // This fetch was cancelled in order to do a new query, nothing to do
        // here.
        return;
      }
      return this.showBubble("error", "An error occurred. Please try again.");
    } finally {
      this.searchBox.classList.remove("in-progress");
    }

    populateResults(results, false, false);

    this.updateHistory(url);
  }

  constructURL() {
    let url = new URL(this.searchUrl, window.location);
    url.searchParams.set("q", this.fields.query.value);
    if (this.fields.path) {
      url.searchParams.set("path", this.fields.path.value.trim());
    }
    if (this.fields.caseSensitive) {
      url.searchParams.set("case", this.fields.caseSensitive.checked);
    }
    if (this.fields.regexp) {
      url.searchParams.set("regexp", this.fields.regexp.checked);
    }
    return url;
  }

  updateHistory(url) {
    this.historyTimer = setTimeout(() => {
      this.historyTimer = null;
      window.history.pushState({}, "", url.href);

      Panel.updateDebugSectionForLocation();
    }, this.timeouts.history);
  }

  initFormFromLocalStorageOrUrl() {
    // XXX similar to in the constructor, we're using the path field as an
    // indication of the mode we're in, but we should clean this up.
    if (!this.fields.path) {
      return;
    }

    let url = new URL(location.href);
    let params = url.searchParams;

    // If the `case` param is in the URL, use its boolean value, so that the
    // checkbox reflects what's in the URL. If the `case` param is not in the
    // URL and this is in fact a search URL, then the search is implicitly
    // case-insensitive, so ensure the checkbox reflects that. Don't update
    // the localStorage value in either of these cases, because the user
    // may just have received a link from somebody else and we don't want to
    // update the user's saved defaults. The saved defaults *only* get updated
    // when the user explicitly clicks on the checkbox and the change event
    // listener triggers.
    // Finally, if we're not in a search already, have the checkbox reflect
    // the saved default from localStorage.
    let caseSensitive = params.get("case");
    if (caseSensitive) {
      caseSensitive = caseSensitive === "true";
    } else if (params.get("q")) {
      caseSensitive = false;
    } else {
      caseSensitive = window.localStorage.getItem("caseSensitive") === "true";
    }
    this.fields.caseSensitive.checked = caseSensitive;

    this.fields.regexp.checked = params.get("regexp") === "true";

    let query = params.get("q");
    if (query) {
      this.fields.query.value = query;
    }

    let path = params.get("path");
    if (path) {
      this.fields.path.value = path;
    }
  }

  // Hang an advisory message off the search field.
  // @param {string} level - The seriousness: 'info', 'warning', or 'error'
  // @param {string} html - The HTML message to be displayed
  // @param {string} which - 'query' or 'path', defaults to the focused element
  // or 'query' otherwise.
  showBubble(level, html, which) {
    if (!which) {
      which = document.activeElement == this.fields.path ? "path" : "query";
    }
    let other = which == "path" ? "query" : "path";
    this.hideBubble(other);

    let bubble = this.bubbles[which];
    bubble.classList.remove("error");
    bubble.classList.remove("warning");
    bubble.classList.remove("info");
    bubble.classList.add(level);
    bubble.innerHTML = html;

    // TODO(emilio): Old code animated the bubble.
    bubble.style.display = "block";
  }

  hideBubble(which) {
    // TODO(emilio): Old code animated the bubble.
    this.bubbles[which].style.display = "none";
  }

  hideBubbles() {
    for (let kind in this.bubbles) {
      this.hideBubble(kind);
    }
  }

  setupColSelector() {
    this.colSelector = document.querySelector("#symbol-tree-table-col-selector");
    if (!this.colSelector) {
      return;
    }
    this.symbolTreeTableList = document.querySelector("#symbol-tree-table-list");
    if (!this.symbolTreeTableList) {
      return;
    }

    const defaultCols = {
      name: true,
      type: false,
      line: true,
    };

    this.cols = {};
    for (const [key, defaultValue] of Object.entries(defaultCols)) {
      const node = document.querySelector("#col-show-" + key);
      node.addEventListener("change", () => {
        this.onColChange();
      });
      this.cols[key] = {
        node: node,
        currentValue: defaultValue,
        defaultValue: defaultValue,
      };
    }

    this.parseColQuery();
  }

  parseColQuery() {
    if (!this.colSelector) {
      return;
    }

    let query = this.fields.query.value;

    for (const m of query.matchAll(/(show|hide)-cols:([a-z,]+)/g)) {
      const show = m[1] == "show";
      const cols = m[2].split(/,/);

      for (const col of cols) {
        this.cols[col].currentValue = show;
      }
    }

    this.updateColCheckbox();
  }

  updateColCheckbox() {
    for (const [key, obj] of Object.entries(this.cols)) {
      obj.node.checked = obj.currentValue;
    }
  }

  onColChange() {
    for (const [key, obj] of Object.entries(this.cols)) {
      obj.currentValue = obj.node.checked;

      if (!obj.defaultValue) {
        this.symbolTreeTableList.classList.toggle("show-" + key, obj.currentValue);
      } else {
        this.symbolTreeTableList.classList.toggle("hide-" + key, !obj.currentValue);
      }
    }

    this.updateColQuery();
  }

  getShowCols() {
    const showCols = [];
    for (const [key, obj] of Object.entries(this.cols)) {
      if (obj.currentValue != obj.defaultValue && !obj.defaultValue) {
        showCols.push(key);
      }
    }
    return showCols.join(",");
  }

  getHideCols() {
    const hideCols = [];
    for (const [key, obj] of Object.entries(this.cols)) {
      if (obj.currentValue != obj.defaultValue && obj.defaultValue) {
        hideCols.push(key);
      }
    }
    return hideCols.join(",");
  }

  updateColQuery() {
    const showCols = this.getShowCols();
    const hideCols = this.getHideCols();

    let query = this.fields.query.value;
    query = query.replace(/ +(show|hide)-cols:([a-z,]+)/g, "");

    if (showCols) {
      query += " show-cols:" + showCols;
    }
    if (hideCols) {
      query += " hide-cols:" + hideCols;
    }

    this.fields.query.value = query;
    let url = this.constructURL();
    this.updateHistory(url);
  }
})();

function hashString(string) {
  let hash = 0;
  if (string.length == 0) {
    return hash;
  }
  for (let i = 0; i < string.length; i++) {
    let char = string.charCodeAt(i);
    hash = (hash << 5) - hash + char;
    hash = hash & hash; // Convert to 32bit integer
  }
  return hash;
}

function classOfResult(pathkind, qkind, isContext) {
  var klass = pathkind + ":" + qkind;
  let cssClass = "EXPANDO" + hashString(klass);
  if (isContext) {
    cssClass += ` ${isContext}-context-line`;
  }
  return cssClass;
}

function onExpandoClick(event) {
  let target = event.target;
  let wasOpen = target.classList.contains("open");
  let elements = document.querySelectorAll(
    "." + target.getAttribute("data-klass")
  );
  for (let element of elements) {
    element.style.display = wasOpen ? "none" : "";
  }
  target.classList.toggle("open");
  target.innerHTML = wasOpen ? "&#9654;" : "&#9660;";
}

var populateEpoch = 0;
function populateResults(data, full, jumpToSingle) {
  populateEpoch++;

  // We use to delete these fields from the data, but that's not compatible of
  // the current 2-pass rendering approach, so now the logic below knows to skip
  // any key that starts with a "*".
  var title = data["*title*"];
  if (title) {
    document.title = title + " - mozsearch";

    // Tell the title to webtest.
    document.dispatchEvent(new Event("titlechanged"));
  }
  var timed_out = data["*timedout*"];

  let limits_hit = data["*limits*"] || [];

  window.scrollTo(0, 0);

  function makeURL(path) {
    return "/" + Dxr.tree + "/source/" + encodeURI(path);
  }

  function makeSearchUrl(q) {
    return `/${Dxr.tree}/search?q=${encodeURIComponent(q)}`;
  }

  function chooseIcon(path) {
    var suffix = path.lastIndexOf(".");
    if (suffix == -1) {
      return "unknown";
    }
    suffix = path.slice(suffix + 1);
    return (
      {
        cpp: "cpp",
        h: "h",
        c: "c",
        m: "mm",
        mm: "mm",
        js: "js",
        jsm: "js",
        mjs: "js",
        py: "py",
        ini: "conf",
        sh: "sh",
        txt: "txt",
        xml: "xml",
        xul: "ui",
        java: "java",
        in: "txt",
        html: "html",
        png: "image",
        gif: "image",
        svg: "svg",
        build: "build",
        json: "js",
        css: "css",
      }[suffix] || "unknown"
    );
  }

  function renderPath(pathkind, qkind, fileResult) {
    var klass = classOfResult(pathkind, qkind);

    var html = "";
    html += "<tr class='result-head " + klass + "'>";
    html +=
      "<td class='left-column'><div class='mimetype-icon-" +
      chooseIcon(fileResult.path) +
      " mimetype-floating-container'></div></td>";

    // This span exists for a11y reasons.  See bug 1558691 but the core idea is:
    // - It's fine (and good!) to upgrade this to an explicit h3 in the future.
    //   We should always favor semantic HTML over use of divs/spans.
    // - At the current moment we're just introducing the span because the
    //   styling fallout is potentially more than we want to get into.
    html += `<td><span role="heading" aria-level="3">`;

    var elts = fileResult.path.split("/");
    var pathSoFar = "";
    for (var i = 0; i < elts.length; i++) {
      if (i != 0) {
        html += "<span class='path-separator'>/</span>";
      }

      var elt = elts[i];
      pathSoFar += elt;
      html += "<a href='" + makeURL(pathSoFar) + "'>" + elt + "</a>";
      pathSoFar += "/";
    }

    html += "</span></td>";
    html += "</tr>";

    return html;
  }

  function escape(s) {
    return s.replace(/&/gm, "&amp;").replace(/</gm, "&lt;");
  }

  function renderSingleSearchResult(pathkind, qkind, file, line, isContext, hasContext) {
    var [start, end] = line.bounds || [0, 0];
    var before = line.line.slice(0, start);
    // Do not truncate off the leading whitespace if we're trying to present in context.
    if (!hasContext) {
      before = before.replace(/^\s+/, "");
    }
    var middle = line.line.slice(start, end);
    var after = line.line.slice(end).replace(/\s+$/, "");

    var klass = classOfResult(pathkind, qkind, isContext);
    var html = "";
    html += "<tr class='" + klass + "'>";
    html +=
      "<td class='left-column'><a href='" +
      makeURL(file.path) +
      "#" +
      line.lno +
      "'>" +
      line.lno +
      "</a></td>";
    html += "<td><a href='" + makeURL(file.path) + "#" + line.lno + "'>";

    html += "<code>";
    // in the context cases, we may only have an after.
    if (before) {
      html += escape(before);
    }
    if (middle) {
      html += "<b>" + escape(middle) + "</b>";
    }
    html += escape(after);
    html += "</code>";

    html += "</a>";

    if (line.context) {
      var inside = line.context;
      if (line.contextsym) {
        var url = `/${Dxr.tree}/search?q=symbol:${encodeURIComponent(
          line.contextsym
        )}&redirect=false`;
        inside = "<a href='" + url + "'>" + escape(line.context) + "</a>";
      }
      html +=
        " <span class='result-context'>// found in <code>" +
        inside +
        "</code></span>";
    }

    // Hacky attempt to provide a means of providing related searches.
    if (line.upsearch) {
      html += `<span class='result-upsearch'><a href="${makeSearchUrl(
        line.upsearch
      )}">Symbol Search This</a></span>`;
    }

    html += "</td>";
    html += "</tr>";

    return html;
  }

  // Accumulate the total number of results for our initial summary and to
  // support our "automatically go to a single result" UX optimization below.
  var count = 0;
  var tupledCounts = new Map();
  var pathkindCounts = new Map();
  for (var pathkind in data) {
    // Skip metadata fields.  The current idiom calls this method twice to
    // reduce initial above-the-folder rendering into place, which means that
    // we can't just delete these fields.
    if (pathkind.startsWith("*")) {
      continue;
    }
    let pathkindHits = 0;
    let pathkindFiles = 0;
    for (var qkind in data[pathkind]) {
      let qkindHitcount = 0;
      for (var k = 0; k < data[pathkind][qkind].length; k++) {
        var path = data[pathkind][qkind][k];
        // 0 lines implies this is a file, in which case just the bare file still
        // counts for our result count purposes and how router.py determined the
        // limits.
        count += (path.lines.length || 1);
        qkindHitcount += path.lines.length;
      }
      tupledCounts.set(`${pathkind}-${qkind}`, { hits: qkindHitcount, files: data[pathkind][qkind].length });
      pathkindHits += qkindHitcount;
      pathkindFiles += data[pathkind][qkind].length;
    }

    pathkindCounts.set(pathkind, { hits: pathkindHits, files: pathkindFiles });
  }

  // Accumulate the total number of files to support our "automatically go to a
  // single result" UX optimization below.
  var fileCount = 0;
  for (var pathkind in data) {
    if (pathkind.startsWith("*")) {
      continue;
    }
    for (var qkind in data[pathkind]) {
      fileCount += data[pathkind][qkind].length;
    }
  }

  // If there's only a single result, redirect ourselves there directly.
  if (jumpToSingle && fileCount == 1 && count <= 1) {
    var pathkind = Object.keys(data)[0];
    var qkind = Object.keys(data[pathkind])[0];
    var file = data[pathkind][qkind][0];
    var path = file.path;

    if (count == 1) {
      var line = file.lines[0];
      // If there was only a "file" response just act like we clicked on the
      // file def by using a line number of 1.
      var lno = line?.lno || 1;
      window.location = `/${Dxr.tree}/source/${path}#${lno}`;
    } else {
      window.location = `/${Dxr.tree}/source/${path}`;
    }
    return;
  }

  // If no data is returned, inform the user.
  let container = document.getElementById("content");
  // Clobber any additional classes that existed on the content container.
  container.setAttribute("class", "content");

  const items = [];

  const breadcrumbs = document.querySelector(".breadcrumbs");
  if (breadcrumbs) {
    // Preserve breadcrumbs if present.
    items.push(breadcrumbs);

    // Breadcrumbs is hidden in some page.
    breadcrumbs.style.display = "inline-block";

    // Remove path and symbols.
    //
    // NOTE: Search can be initiated from source or directory listing,
    //       where breadcrumbs has path for the file or directory.
    let foundSep = false;
    for (const node of [...breadcrumbs.childNodes]) {
      if (node instanceof HTMLElement) {
        if (node.classList.contains("path-separator")) {
          foundSep = true;
        }
      }
      if (foundSep) {
        breadcrumbs.removeChild(node);
      }
    }
  }
  const navigationPanel = document.querySelector("#panel");
  if (navigationPanel) {
    // Preserve navigation panel if present.
    items.push(navigationPanel);

    try {
      Panel.prepareForSearch();
    } catch {}
  }

  const header = document.createElement("div");
  header.classList.add("search-result-header");
  items.push(header);

  if (!fileCount) {
    const div = document.createElement("div");
    div.textContent = "No results for current query.";
    header.append(div);
  } else {
    if (count) {
      const div = document.createElement("div");
      div.textContent = `Number of results: ${count} (maximum is around 4000)`;
      header.append(div);
    }
  }

  if (limits_hit.length > 0) {
    const div = document.createElement("div");
    const b = document.createElement("b");
    b.textContent = "Warning";
    div.append(b);
    div.append(`: The following limits were hit in your search: ${limits_hit.join(", ")}`);
    header.append(div);
  }

  let timeoutWarning = null;
  if (timed_out) {
    const div = document.createElement("div");
    const b = document.createElement("b");
    b.textContent = "Warning";
    div.append(b);
    div.append(": Results may be incomplete due to server-side search timeout!");
    header.append(div);
  }

  if (fileCount) {
    var table = document.createElement("table");
    table.className = "results";
    items.push(table);

    var counter = 0;

    var pathkindNames = {
      // Previously we would not say normal, but we need a place to hang the
      // counts.
      normal: "Core code",
      test: "Test files",
      generated: "Generated code",
      thirdparty: "Third-party code",
    };

    var html = "";
    // Loop over normal/test/generated/thirdparty "pathkind"s
    for (var pathkind in data) {
      if (pathkind.startsWith("*")) {
        continue;
      }
      var pathkindName = pathkindNames[pathkind];
      if (pathkindName) {
        let pathkindCount = pathkindCounts.get(pathkind);
        if (pathkindCount) {
          if (pathkindCount.hits) {
            maybeCounts = ` (${pathkindCount.hits} lines across ${pathkindCount.files} files)`;
          } else {
            maybeCounts = ` (${pathkindCount.files} files)`;
          }
        }

        html += "<tr><td>&nbsp;</td></tr>";
        html +=
          "<tr><td class='section'>Â§</td><td><div class='result-pathkind'>" +
          pathkindName + maybeCounts +
          "</div></td></tr>";
      }

      // Loop over definition/declaration/use/etc. "qkind"s
      for (var qkind in data[pathkind]) {
        if (data[pathkind][qkind].length) {
          html += "<tr><td>&nbsp;</td></tr>";

          html += "<tr><td class='left-column'>";
          html +=
            "<div class='expando open' data-klass='" +
            classOfResult(pathkind, qkind) +
            "'>&#9660;</div>";
          html += "</td>";

          let maybeCounts = "";
          let qkindCount = tupledCounts.get(`${pathkind}-${qkind}`);
          if (qkindCount) {
            if (qkindCount.hits) {
              maybeCounts = ` (${qkindCount.hits} lines across ${qkindCount.files} files)`;
            } else {
              maybeCounts = ` (${qkindCount.files} files)`;
            }
          }
          html += "<td><h2 class='result-kind'>" + escape(qkind) + maybeCounts + "</h2></td></tr>";
        }

        // Loop over the files with hits.
        for (var i = 0; i < data[pathkind][qkind].length; i++) {
          var file = data[pathkind][qkind][i];

          if (counter > 100 && !full) {
            break;
          }

          html += renderPath(pathkind, qkind, file);

          file.lines.map(function (line) {
            counter++;
            if (counter > 100 && !full) {
              return;
            }

            let has_context = line.context_before || line.context_after;

            if (line.context_before) {
              let lineDelta = -line.context_before.length;
              for (const lineStr of line.context_before) {
                html += renderSingleSearchResult(
                  pathkind,
                  qkind,
                  file,
                  { lno: line.lno + lineDelta, line: lineStr },
                  "before",
                  true
                );
                lineDelta++;
              }
            }
            html += renderSingleSearchResult(pathkind, qkind, file, line, false, has_context);
            if (line.context_after) {
              let lineDelta = 1;
              for (const lineStr of line.context_after) {
                html += renderSingleSearchResult(
                  pathkind,
                  qkind,
                  file,
                  { lno: line.lno + lineDelta, line: lineStr },
                  "after",
                  true
                );
                lineDelta++;
              }
            }
          });
        }
      }
    }

    table.innerHTML = html;

    for (let element of table.querySelectorAll(".expando")) {
      element.addEventListener("click", onExpandoClick);
    }

    if (counter > 100 && !full) {
      var epoch = populateEpoch;
      setTimeout(function () {
        if (populateEpoch == epoch) {
          populateResults(data, true, false);
        }
      }, 750);
    }
  }

  container.replaceChildren(...items);

  Dxr.parseColQuery();
}

window.showSearchResults = function (results) {
  var jumpToSingle = window.location.search.indexOf("&redirect=false") == -1;
  populateResults(results, true, jumpToSingle);
};

/**
 * Adds a leading 0 to numbers less than 10 and greater that 0
 *
 * @param int number The number to test against
 *
 * return Either the original number or the number prefixed with 0
 */
function addLeadingZero(number) {
  return number <= 9 || number === 0 ? "0" + number : number;
}

/**
 * Converts string to new Date and returns a formatted date in the
 * format YYYY-MM-DD h:m
 * @param String dateString A date in string form.
 *
 */
function formatDate(dateString) {
  var fullDateTime = new Date(dateString),
    date =
      fullDateTime.getFullYear() +
      "-" +
      (fullDateTime.getMonth() + 1) +
      "-" +
      addLeadingZero(fullDateTime.getDate()),
    time =
      fullDateTime.getHours() + ":" + addLeadingZero(fullDateTime.getMinutes());

  return date + " " + time;
}

for (let element of document.querySelectorAll(".pretty-date")) {
  element.innerText = formatDate(element.getAttribute("data-datetime"));
}

```

## static/js/panel.js
```
var Panel = new (class Panel {
  constructor() {
    this.panel = document.getElementById("panel");
    // Avoid complaining if this page doesn't have a panel on it.
    if (!this.panel) {
      return;
    }
    this.toggleButton = document.getElementById("panel-toggle");
    this.icon = this.panel.querySelector(".navpanel-icon");
    this.settingsButton = document.getElementById("show-settings");
    this.content = document.getElementById("panel-content");
    this.accelEnabledCheckbox = document.getElementById("panel-accel-enable");

    this.permalinkNode = this.findItem("Permalink");
    this.unpermalinkNode = this.findItem("Remove the Permalink");

    this.selectedSymbol = null;

    this.markdown = {
      filename: {
        node: this.findItem("Filename Link"),
        isEnabled: () => {
          return true;
        },
        getText: url => {
          const filename = new URL(url).pathname.match(/\/([^\/]+)$/)[1];
          return `[${filename}](${url})`;
        },
      },
      symbol: {
        node: this.findItem("Symbol Link"),
        isEnabled: () => {
          return this.selectedSymbol;
        },
        getText: url => {
          return `[${this.selectedSymbol}](${url})`;
        },
      },
      block: {
        node: this.findItem("Code Block"),
        isEnabled: () => {
          return Highlighter?.selectedLines.size > 0;
        },
        getText: url => {
          const file = document.getElementById("file");
          const lang = file.getAttribute("data-markdown-slug") || "";
          return [url, "```" + lang, ...this.formatSelectedLines(), "```"].join(
            "\n"
          );
        },
      },
    };

    // We want the default event for clicking on the settings link to work, but
    // we want to stop its propagation so that it doesn't bubble up to the
    // toggle button handler that comes next.
    this.settingsButton.addEventListener("click", (evt) => { evt.stopPropagation(); });

    this.toggleButton.addEventListener("click", () => this.toggle());
    this.accelEnabledCheckbox.addEventListener("change", () => {
      localStorage.setItem("accel-enable", event.target.checked ? "1" : "0");
      this.updateAccelerators();
    });
    document.documentElement.addEventListener("keypress", event =>
      this.maybeHandleAccelerator(event)
    );

    for (let copy of this.panel.querySelectorAll("button.copy")) {
      copy.addEventListener("click", e => {
        e.preventDefault();

        if (copy.hasAttribute("data-copying")) {
          return;
        }

        this.copyText(copy, copy.parentNode.href);
      });
    }

    const updateUnpermalink = () => {
      if (/^\/[^\/]+\/rev\//.test(document.location.pathname)) {
        if (this.unpermalinkNode) {
           this.unpermalinkNode.classList.remove("disabled");
        }
      } else {
        if (this.unpermalinkNode) {
           this.unpermalinkNode.classList.add("disabled");
        }
      }
    };

    updateUnpermalink();

    if (this.permalinkNode) {
      this.permalinkNode.addEventListener("click", event => {
        if (
          event.defaultPrevented ||
          event.altKey ||
          event.ctrlKey ||
          event.metaKey ||
          event.shiftKey
        ) {
          return;
        }
        window.history.pushState(
          { permalink: event.target.href },
          window.title,
          event.target.href
        );
        event.preventDefault();

        updateUnpermalink();
      });
    }

    if (this.unpermalinkNode) {
      this.unpermalinkNode.addEventListener("click", event => {
        if (
          event.defaultPrevented ||
          event.altKey ||
          event.ctrlKey ||
          event.metaKey ||
          event.shiftKey
        ) {
          return;
        }
        window.history.pushState(
          {},
          window.title,
          event.target.href
        );
        event.preventDefault();

        updateUnpermalink();
      });
    }

    for (const [name, { node }] of Object.entries(this.markdown)) {
      if (!node) {
        continue;
      }
      node.addEventListener("click", event => {
        if (
          event.defaultPrevented ||
          event.altKey ||
          event.ctrlKey ||
          event.metaKey ||
          event.shiftKey
        ) {
          return;
        }

        this.copyMarkdown(name);

        event.preventDefault();
      });
    }

    // If the user toggles it in a different tab, update the checkbox/state here
    //
    // TODO(emilio): We should probably do the same for the case-sensitive
    // checkbox and such.
    window.addEventListener("storage", () => this.initFromLocalStorage());

    this.initFromLocalStorage();

    if (Settings.fancyBar.enabled) {
      this.addSymbolSection();
    }

    if (Settings.debug.ui) {
      this.addDebugSection();
    }
  }

  get acceleratorsEnabled() {
    return this.accelEnabledCheckbox.checked;
  }

  initFromLocalStorage() {
    let acceleratorsEnabled =
      !("accel-enable" in localStorage) ||
      localStorage.getItem("accel-enable") == "1";
    this.accelEnabledCheckbox.checked = acceleratorsEnabled;
    this.updateAccelerators();
  }

  updateAccelerators() {
    let enabled = this.acceleratorsEnabled;
    for (let accel of this.panel.querySelectorAll("span.accel")) {
      accel.style.display = enabled ? "" : "none";
    }
  }

  findItem(title) {
    return this.panel.querySelector(`.item[title="${title}"]`);
  }

  findAccel(key) {
    return this.panel.querySelector(`.item[data-accel="${key}"]`);
  }

  maybeHandleAccelerator(event) {
    if (!this.acceleratorsEnabled) {
      return;
    }
    if (event.altKey || event.ctrlKey || event.metaKey) {
      return;
    }
    var inputs = /input|select|textarea/i;
    if (inputs.test(event.target.nodeName)) {
      return;
    }
    let link = (() => {
      switch (event.key) {
        case "y":
        case "Y":
          return this.findAccel('Y');
        case "l":
        case "L":
          return this.findAccel('L');
        case "r":
        case "R":
          return this.findAccel('R');
        case "f":
        case "F":
          return this.findAccel('F');
        case "s":
        case "S":
          return this.findAccel('S');
        case "c":
        case "C":
          return this.findAccel('C');
      }
    })();

    if (link) {
      link.click();
      event.preventDefault();
    }
  }

  toggle() {
    let hidden = this.content.style.display != "none";
    this.content.style.display = hidden ? "none" : "";
    this.content.setAttribute("aria-hidden", hidden);
    this.content.setAttribute("aria-expanded", !hidden);
    this.icon.classList.toggle("expanded");
  }

  isExpanded() {
    return this.icon.classList.contains("expanded");
  }

  copyText(copy, text) {
    copy.setAttribute("data-copying", "true");
    navigator.clipboard
      .writeText(text)
      .then(function () {
        copy.classList.add("copied");
        setTimeout(function () {
          if (!copy.hasAttribute("data-copying")) {
            copy.classList.remove("copied");
          }
        }, 1000);
      })
      .finally(function () {
        copy.removeAttribute("data-copying");
      });
  }

  copyMarkdown(type) {
    const { node, getText } = this.markdown[type];
    if (!node || node.disabled) {
      return;
    }

    const copy = node.querySelector(".copy");
    let url = this.permalinkNode?.href || document.location.href;
    if (Settings.fancyBar.enabled) {
      url = this.reflectSelectedSymbolLineToURL(url);
    }
    const text = getText(url);

    this.copyText(copy, text);
  }

  formatSelectedLines() {
    const kPlaceholder = "...";
    const lines = [];
    let lastLine = -1;
    let commonWhitespacePrefix = null;

    function computeCommonWhitespacePrefix(lineText, existingPrefix) {
      function isWhitespace(character) {
        return character == " " || character == "\t";
      }

      if (!lineText.length) {
        // Empty lines don't contribute to the whitespace prefix.
        return existingPrefix;
      }

      // NOTE: existingPrefix === null means it's first call.
      //       existingPrefix === "" means there's no leading whitespace.
      let min = existingPrefix !== null
        ? Math.min(existingPrefix.length, lineText.length)
        : lineText.length;
      let count = 0;
      for (; count < min; ++count) {
        const inPrefix = existingPrefix
          ? existingPrefix[count] == lineText[count]
          : isWhitespace(lineText[count]);
        if (!inPrefix) {
          break;
        }
      }

      return lineText.substring(0, count);
    }

    const unsortedLines = [...Highlighter.selectedLines];
    if (Settings.fancyBar.enabled) {
      const extraLine = this.getLineNumberForSelectedSymbol();
      if (extraLine !== undefined && !unsortedLines.includes(extraLine)) {
        unsortedLines.push(extraLine);
      }
    }
    for (const line of unsortedLines.sort((a, b) => a - b)) {
      if (lastLine !== -1 && lastLine != line - 1) {
        lines.push(kPlaceholder);
      }

      const lineElem = document
        .getElementById(`line-${line}`)
        .querySelector(".source-line");
      const lineText = lineElem.textContent.replace(/\n/, "");
      commonWhitespacePrefix = computeCommonWhitespacePrefix(
        lineText,
        commonWhitespacePrefix
      );
      lines.push(lineText);
      lastLine = line;
    }

    if (commonWhitespacePrefix?.length) {
      for (let i = 0; i < lines.length; ++i) {
        if (lines[i] && lines[i] != kPlaceholder) {
          lines[i] = lines[i].substring(commonWhitespacePrefix.length);
        }
      }
    }

    return lines;
  }

  findSelectedSymbol() {
    let selectedSymbol = null;
    if (Settings.fancyBar.enabled) {
      if (ContextMenu?.selectedToken) {
        const symbols = ContextMenu.selectedToken.getAttribute("data-symbols").split(",");

        for (const sym of symbols) {
          const symInfo = SYM_INFO[sym];
          if (!symInfo || !symInfo.pretty) {
            continue;
          }

          return symInfo.pretty.replace(/[A-Za-z0-9]+ /, "");
        }
      }
    }

    return DocumentTitler?.selectedSymbol;
  }

  updateCopyState() {
    // If we're on a page without a panel, there's nothing to do.
    if (!this.panel) {
      return;
    }

    this.selectedSymbol = this.findSelectedSymbol();

    if (Settings.fancyBar.enabled) {
      if (this.copySymbolBox) {
        this.copySymbolBox.classList.toggle("disabled", !this.selectedSymbol);
      }
    }

    for (const [_, { node, isEnabled }] of Object.entries(this.markdown)) {
      if (!node) {
        continue;
      }
      if (isEnabled()) {
        node.disabled = false;
        node.removeAttribute("aria-disabled");
      } else {
        node.disabled = true;
        node.setAttribute("aria-disabled", "true");
      }
    }

    if (Settings.fancyBar.enabled) {
      this.updateSelectedSymbolView();
    }
  }

  // Add Symbol section with the symbol name and copy button.
  addSymbolSection() {
    const markdownHeader = [...this.content.querySelectorAll("h4")]
      .find(n => n.textContent == "Copy as Markdown");
    if (!markdownHeader) {
      return;
    }

    const h4 = document.createElement("h4");
    h4.textContent = "Symbol";

    markdownHeader.before(h4);

    const box = document.createElement("div");
    box.classList.add("selected-symbol-section");

    const symBox = document.createElement("div");
    symBox.classList.add("selected-symbol-box");
    this.selectedSymbolNS = document.createElement("div");
    this.selectedSymbolNS.classList.add("selected-symbol-ns");
    symBox.append(this.selectedSymbolNS);
    this.selectedSymbolLocal = document.createElement("div");
    this.selectedSymbolLocal.classList.add("selected-symbol-local");
    symBox.append(this.selectedSymbolLocal);
    box.append(symBox);

    this.copySymbolBox = document.createElement("div");
    this.copySymbolBox.classList.add("copy-box");
    const copyIndicator = document.createElement("span");
    copyIndicator.classList.add("icon", "copy", "indicator");
    const copyIcon = document.createElement("span");
    copyIcon.classList.add("icon-docs", "copy-icon");
    copyIndicator.append(copyIcon);
    const copyOk = document.createElement("span");
    copyOk.classList.add("icon-ok", "tick-icon");
    copyIndicator.append(copyOk);
    this.copySymbolBox.append(copyIndicator);
    box.append(this.copySymbolBox);

    copyIndicator.addEventListener("click", e => {
      e.preventDefault();

      if (!this.selectedSymbol) {
        return;
      }

      if (copyIndicator.hasAttribute("data-copying")) {
        return;
      }

      this.copyText(copyIndicator, this.selectedSymbol);
    });

    markdownHeader.before(box);
  }

  addDebugSection() {
    const items = [];

    const pageContent = document.getElementById("content");
    if (document.location.pathname.match(/^\/[^\/]+\/source\//) &&
        pageContent && pageContent.classList.contains("source-listing")) {
      const li = document.createElement("li");
      const link = document.createElement("a");
      link.classList.add("icon");
      link.classList.add("item");
      link.href = document.location.href.replace(/\/source\//, "/raw-analysis/");
      link.textContent = "Raw analysis records";
      li.append(link);
      items.push(li);
    }

    if (window.IS_DEBUG_LOGS_AVAILABLE) {
      const li = document.createElement("li");
      const link = document.createElement("a");
      link.classList.add("icon");
      link.classList.add("item");
      li.append(link);
      items.push(li);

      this.showHideLogsLink = link;
      this.updateDebugSectionForLocation();
    }

    this.resultsJSONBox = document.getElementById("query-debug-results-json");
    this.resultsJSONPre = document.getElementById("query-debug-results-json-pre");
    if (this.resultsJSONBox && this.resultsJSONPre) {
      const li = document.createElement("li");
      const button = document.createElement("button");
      button.classList.add("icon");
      button.classList.add("item");
      button.textContent = "Show results JSON";
      li.append(button);
      items.push(li);

      button.addEventListener("click", () => {
        if (this.resultsJSONBox.hasAttribute("aria-hidden")) {
          this.resultsJSONBox.removeAttribute("aria-hidden");
          this.resultsJSONPre.textContent = JSON.stringify(window.QUERY_RESULTS_JSON, undefined, 2);
          button.textContent = "Hide results JSON";
        } else {
          this.resultsJSONBox.setAttribute("aria-hidden", "true");
          this.resultsJSONPre.textContent = "";
          button.textContent = "Show results JSON";
        }
      });
    }

    if (items.length > 0) {
      const h4 = document.createElement("h4");
      h4.textContent = "Debug";
      this.content.append(h4);

      const ul = document.createElement("ul");
      ul.append(...items);
      this.content.append(ul);
    }
  }

  updateDebugSectionForLocation() {
    if (this.showHideLogsLink) {
      if (document.location.href.includes("&debug=true")) {
        this.showHideLogsLink.href = document.location.href.replace(/&debug=true/, "");
        this.showHideLogsLink.textContent = "Hide debug log";
      } else {
        this.showHideLogsLink.href = document.location.href + "&debug=true";
        this.showHideLogsLink.textContent = "Show debug log";
      }
    }
  }

  // Show the selected symbol's namespace prefix and the local name in the
  // Symbol section.
  updateSelectedSymbolView() {
    const sym = this.selectedSymbol || '(no symbol clicked)';
    const index = sym.lastIndexOf("::");
    let ns = "";
    let local = sym;
    if (index != -1) {
      ns = sym.slice(0, index + 2);
      local = sym.slice(index + 2);
    }
    if (this.selectedSymbolNS) {
      this.selectedSymbolNS.textContent = ns;
    }
    if (this.selectedSymbolLocal) {
      this.selectedSymbolLocal.textContent = local;
    }
  }

  // Reflect the line number of selected symbol, if any and if it's outside of
  // the selected lines.
  reflectSelectedSymbolLineToURL(spec) {
    const line = this.getLineNumberForSelectedSymbol();
    if (line === undefined) {
      return spec;
    }

    const url = new URL(spec);
    url.hash = Highlighter.toHash(line);
    return url.toString();
  }

  // Return the line number of the selected symbol if any.
  // Otherwise returns undefined.
  getLineNumberForSelectedSymbol() {
    if (!ContextMenu.selectedToken) {
      return undefined;
    }

    const containingLine = ContextMenu.selectedToken.closest(".source-line-with-number");
    if (!containingLine) {
      return undefined;
    }

    const lineNumberNode = containingLine.querySelector(".line-number");
    if (!lineNumberNode) {
      return undefined;
    }

    return parseInt(lineNumberNode.dataset.lineNumber, 10);
  }

  onSelectedLineChanged() {
    this.updateCopyState();
  }

  onSelectedSymbolChanged() {
    this.updateCopyState();
  }

  onSelectedTokenChanged() {
    this.updateCopyState();
  }

  // Returns true if the event is dispatched inside the navigation panel.
  isOnPanel(event) {
    return !!event.target.closest("#panel");
  }

  prepareForSearch() {
    // Remove any item not shared between search and other contexts.
    for (const node of [...this.content.childNodes]) {
      if (node instanceof HTMLElement) {
        if (node.classList.contains("panel-accel")) {
          continue;
        }
      }
      this.content.removeChild(node);
    }

    if (this.isExpanded()) {
      this.toggle();
    }
  }
})();

// Blurring magic based on the quite useful article by Antony Garand from
// Dec 7, 2020 at https://dev.to/antogarand/svg-metaballs-35pj that's about
// implementing metaballs in SVG with SVG filters.
//
// My initial desire for this implementation was to provide for a means of
// allowing clustering in the "neato" layout view where we fundamentally lose
// our clusters and having graphviz just draw a bounding box around things is
// insufficient.  Metaballs are a way to potentially handle that.  But
// especially with our changes to generate tables more often and neato's
// ability to handle tables, this has been less of a concern.
//
// I ended up doing the initial experimental hookup to instead try and visually
// express which nodes in the graph are "close" to the initial diagram request
// in terms of depth.  See the invocation of this method that follows it for
// more comments.
function blurrifyDiagram() {
  const diag = document.querySelector("svg");
  if (!diag) {
    return;
  }

  const createSVGElem = (name) => {
    return document.createElementNS("http://www.w3.org/2000/svg", name);
  };

  let trueDiag = diag.children[0];
  // disable the background white layer (for both modes)
  trueDiag.children[0].setAttribute("style", "display: none;")

  let blurDiag = trueDiag.cloneNode(true);
  blurDiag.setAttribute("class", "blurry depth-mode");

  // Put the filters in
  let filtRoot = createSVGElem("filter");
  filtRoot.setAttribute("id", "blur_bg");

  let filtBlurGrow = createSVGElem("feGaussianBlur");
  filtBlurGrow.setAttribute("in", "SourceGraphic");
  filtBlurGrow.setAttribute("result", "blur1");
  filtBlurGrow.setAttribute("stdDeviation", "15");
  filtRoot.appendChild(filtBlurGrow);

  let filtThresh = createSVGElem("feColorMatrix");
  filtThresh.setAttribute("in", "blur1");
  filtThresh.setAttribute("result", "matrix");
  filtThresh.setAttribute("type", "matrix");
  let transformMatrix = [
    [1, 0, 0, 0, 0],
    [0, 1, 0, 0, 0],
    [0, 0, 1, 0, 0],
    [0, 0, 0, 50, -15]
  ];
  filtThresh.setAttribute("values", transformMatrix.flat().join(" "));
  filtRoot.appendChild(filtThresh);

  let filtBlurProper = createSVGElem("feGaussianBlur");
  filtBlurProper.setAttribute("in", "matrix");
  filtBlurProper.setAttribute("result", "blur2");
  filtBlurProper.setAttribute("stdDeviation", "10");
  filtRoot.appendChild(filtBlurProper);

  diag.insertBefore(filtRoot, trueDiag);

  // Enable the filter on the blur diagram
  blurDiag.setAttribute("filter", "url(#blur_bg");

  // ### Make the normal diagram normal-ish
  trueDiag.classList.add("true-diag");

  diag.insertBefore(blurDiag, trueDiag);
}
// The initial attempt at blurring based on depth is not feeling particularly
// useful and has a wildly non-trivial performance cost, but I want to be able
// to activate it on the fly for A/B investigations, so I'm leaving it around
// so one can just invoke the command below.
//
// Hm, in fact, it seems like the performance impact is nightmarish on the
// context menu, at least when it's being shown or hidden; if you click
// elsewhere when it's already open, the performance is fine.  I wonder if the
// fact that we're cloning nodes that have identifiers which creates duplicate
// identifiers is creating a pathological situation?
//blurrifyDiagram();

// In order to provide more useful click/hover targets for diagram edges, we
// duplicate line body "path" element to create one with a wider stroke that is
// not visible.
function makeDiagramHoverEdges() {
  const diag = document.querySelector("svg");
  if (!diag) {
    return;
  }

  const edges = diag.querySelectorAll("g.edge > path");
  for (const path of edges) {
    const dupe = path.cloneNode(false);
    dupe.classList.add("clicktarget");
    // let's insert the clicktarget after the actual path so it is always what
    // the hit test finds.
    path.insertAdjacentElement("afterend", dupe);
  }
}
makeDiagramHoverEdges();

// Scroll the first root node of the diagram so that it's centered.  Through use
// of `?.` this won't freak out if there are no matches.  According to
// performance.now(), this takes 3ms on the 20,765 line indexedDB/ActorsParent.cpp
// right now.
//
// This file is loaded at the bottom of the HTML file so the DOM is available,
// although I'm not entirely sure this is wise versus hooking the load event.
//
// TODO: Be more wise.
document.querySelector(".diagram-depth-0 polygon")?.scrollIntoView({
  behavior: "instant",
  block: "center",
  inline: "center"
});

```

## static/js/code-highlighter.js
```
/* jshint devel:true, esnext: true */

/**
 * This file consists of four major pieces of functionality for a file view:
 * 0) Any combination of 1) and 2)
 * 1) Multi-select highlight lines with shift key and update window.location.hash
 * 2) Multi-select highlight lines with command/control key and update window.location.hash
 * 3) Highlight lines when page loads, if window.location.hash exists
 */

/**
 * Decides what the document title should be.
 */
var DocumentTitler = new (class DocumentTitler {
  constructor() {
    this.originalTitle = document.title;
    this.currentTitle = this.originalTitle;

    this.stickyTitle = null;
    this.selectionTitle = null;
    this.selectedSymbol = null;
  }

  updateTitle() {
    let bestTitle;

    // If we have a better title than the original title, use it, but making
    // sure to include the original filename because this is important for
    // finding the tab again in the awesomebar via its filename.
    if (Settings.pageTitle.lineSelection && this.selectionTitle) {
      bestTitle = `${this.selectionTitle} (${this.originalTitle})`;
    } else if (Settings.pageTitle.stickySymbol && this.stickyTitle) {
      bestTitle = `${this.stickyTitle} (${this.originalTitle})`;
    } else {
      bestTitle = this.originalTitle;
    }

    // Debounce setting the title out of a fear of slowing down scrolling but
    // not wanting to to do the legwork to figure out if this would pose a
    // problem.  I am assuming this shouldn't result in synchronous reflows.
    if (bestTitle === this.currentTitle) {
      return;
    }
    document.title = this.currentTitle = bestTitle;

    // Tell the title to webtest.
    document.dispatchEvent(new Event("titlechanged"));
  }

  /**
   * Recognizing that namespaces can be a little verbose and use up precious
   * space, flatten things that look like namespaces to be a single-character
   * delimited by a single colon.
   *
   *
   * Good example transforms:
   * - `Foo` => `Foo`
   * - `Foo::Bar` => `Foo::Bar`
   * - `mozilla::Foo::Bar` => `m:Foo::Bar`
   * - `mozilla::dom::quota::Foo::Bar` => `m:d:q:Foo::Bar`
   *
   * Sketchy example transforms:
   * - `mozilla::Foo` => `m:Foo`
   * - `mozilla::dom::Foo` => `m:d:Foo`
   *
   * The sketchy transforms are sketchy because we're assuming that lowercase is
   * indicative of a namespace.  Note that, similarly to the comments in
   * `_findBestPrettySymbolInSourceLineElem`, this heuristic should simply be
   * mooted by having the symbol dictionary provide a pre-computed concise
   * pretty name for a symbol.  That mechanism can leverage knowing what is and
   * isn't a namespace/class and also having global knowledge of the names that
   * are in the codebase so that additional tokens can be used in cases where
   * ambiguity exists, etc.
   */
  _shortenNamespaces(pretty) {
    const pieces = pretty.split("::");
    // Nothing to do if we don't have anything to split.
    if (pieces.length < 2) {
      return pretty;
    }

    // We want to use the last two components if they are both initial-caps,
    // but not for the "mozilla::Foo" case, or "mozilla::dom::Foo" case, where
    // we still want to collapse the lower-cased namespace.  In that case, we
    // collapse everything but the last piece.
    let splitPoint;
    // Our regexp assumes it's a namespace if it's:
    // - All ASCII lowercase.
    // - And therefore has no underscores (which helps avoid us getting tricked
    //   by nested functions named_like_this in some hypothetical world where
    //   we understand Python).
    if (/^[a-z]+$/.test(pieces[pieces.length - 2])) {
      splitPoint = -1;
    } else {
      splitPoint = -2;
    }

    const nsPieces = pieces.slice(0, splitPoint);
    const fullPieces = pieces.slice(splitPoint);

    const nsTransformed = nsPieces.map(piece => {
      return piece[0];
    });

    if (nsTransformed.length === 0) {
      return fullPieces.join("::");
    }
    return nsTransformed.join(":") + ":" + fullPieces.join("::");
  }

  /**
   * Given an element corresponding to a source line, figure out the most
   * appropriate pretty symbol in the line.  This is currently done using a
   * heuristic, but this should ideally be handled by either:
   * - Directly annotating the DOM with the symbol element that is inducing
   *   the nesting.
   * - Augmenting the SYM_INFO symbol dictionary to provide information about
   *   nesting in parallel to the "jumps".
   *
   * The current heuristic logic is:
   * - Pick the last observed symbol preceding a `(`.
   */
  _findBestPrettySymbolInSourceLineElem(elem) {
    let bestPretty = null,
      bestShortPretty = null;
    if (!elem) {
      return { long: bestPretty, short: bestShortPretty };
    }

    // format.rs maps "def", "decl", and "idl" syntax kinds to "syn_def" so this
    // covers all "kinds" of interest.
    const defs = elem.querySelectorAll(".syn_def");
    scan: for (const def of defs) {
      // Check if any of the preceding nodes had a "(" in them.  If they did,
      // this symbol is irrelevant and we should break out of the outer "scan"
      // loop.
      //
      // Okay, and now we're gaining one more hacky heuristic to deal with the
      // situation "class Foo : public DontCare, public AlsoDontCare {".  If we
      // see "class" and " : ", we also bail.  The complication here is that we
      // do absolutely want to pick "Bar" in "Foo::Bar()", so we can't just bail
      // when we see a colon anywhere.
      let sawClass = false;
      let sawColon = false;
      // Also check if there's tilde, to distinguish a destructor from its
      // class symbol.
      let sawTilde = false;
      for (
        let prevNode = def.previousSibling;
        prevNode;
        prevNode = prevNode.previousSibling
      ) {
        // A "(" always means stop immediately.
        if (prevNode.textContent.includes("(")) {
          break scan;
        }

        // The compound class check.
        if (prevNode.textContent.includes("class")) {
          sawClass = true;
        }
        if (prevNode.textContent.includes(" : ")) {
          sawColon = true;
        }
        if (prevNode.textContent.includes("~")) {
          sawTilde = true;
        }
        if (sawClass && sawColon) {
          break scan;
        }
      }

      // Extract the most appropriate pretty data from the symbols.
      // Specifically, we are looking for "pretty" text in the symbols that
      // contains the textContent from the semantic token.  We do this to
      // compensate for the implicitly invoked field constructors which
      // currently end up coalesced into the constructor's symbol/point.
      const visibleToken = def.textContent;
      const symbols = def.getAttribute("data-symbols").split(",");

      // Process all of the symbols, retaining the last one we see as the way
      // we sort the symbols currently means the most appropriate symbol may be
      // last.  The motivating scenario here is WorkerPrivate::MemoryReporter
      // that subclasses nsIMemoryReporter (and where "MemoryReporter" is also a
      // substring of "nsIMemoryReporter") and the MemoryReporter symbol is
      // currently deterministically last in the list.
      for (const sym of symbols) {
        const symInfo = window.SYM_INFO[sym];
        if (!symInfo) {
          continue;
        }
        if (!symInfo.pretty?.includes(visibleToken)) {
          continue;
        }
        if (sawTilde) {
          if (!symInfo.pretty?.includes("~")) {
            continue;
          }
        }
        bestPretty = symInfo.pretty;
      }
    }

    // The pretty will include a descriptor prefix like "function " which we
    // don't care about.
    if (bestPretty) {
      bestPretty = bestPretty.replace(/[A-Za-z0-9]+ /, "");

      // Shorten any namespaces.
      bestShortPretty = this._shortenNamespaces(bestPretty);
    }

    return { long: bestPretty, short: bestShortPretty };
  }

  /**
   * Called by `Sticky` when it updates the currently visible sticky lines.
   * This method attempts to extract the symbol on the line that would
   * correspond to whatever is opening a nesting block.
   */
  processStickyElems(stickyElems) {
    this.stickyTitle = null;
    if (stickyElems.length) {
      const useSticky = stickyElems[stickyElems.length - 1];
      const stickySourceLine = useSticky.querySelector(".source-line");
      this.stickyTitle =
        this._findBestPrettySymbolInSourceLineElem(stickySourceLine).short;
    }

    this.updateTitle();
  }

  /**
   * Called by Highlight when it updates the hash (which it does whenever the
   * hash changes, etc.)  This method finds either:
   *   * the symbol in the selected line
   *   * the symbol of the nesting block that encloses the line
   */
  processLineSelection(selectedLines) {
    this.selectionTitle = null;
    this.selectedSymbol = null;
    if (selectedLines.size) {
      const nestingContainer = this._findNestingContainerFor(selectedLines);
      const longestPretty = this._findBestPrettySymbolInContainer(selectedLines, nestingContainer);
      this.selectionTitle = longestPretty.short;
      this.selectedSymbol = longestPretty.long;
    }

    this.updateTitle();

    Panel.onSelectedSymbolChanged();
  }

  /**
   * Find the deepest container where there's only one container in the depth
   * in given lines.
   */
  _findNestingContainerFor(lines) {
    let maxDepth = 0;
    let lastContainer = null;
    let containersPerDepth = new Map();

    for (const line of [...lines].sort()) {
      const selectedLine = document.getElementById(`line-${line}`);
      const nestingContainer = selectedLine?.closest(".nesting-container");
      if (!nestingContainer) {
        continue;
      }

      if (nestingContainer === lastContainer) {
        continue;
      }

      lastContainer = nestingContainer;

      let depth = null;
      for (const className of nestingContainer.classList) {
        const m = className.match(/nesting-depth-(\d+)/);
        if (m) {
          depth = parseInt(m[1]);
          break;
        }
      }

      if (depth === null) {
        continue;
      }

      if (depth > maxDepth) {
        maxDepth = depth;
      }

      let containers;
      if (!containersPerDepth.has(depth)) {
        containers = new Set();
        containersPerDepth.set(depth, containers);
      } else {
        containers = containersPerDepth.get(depth);
      }
      containers.add(nestingContainer);
    }

    for (let depth = maxDepth; depth >= 0; depth--) {
      if (!containersPerDepth.has(depth)) {
        continue;
      }

      const containers = containersPerDepth.get(depth);
      if (containers.size == 1) {
        return [...containers][0];
      }
    }

    // There's no such container.
    // This can happen if multiple top-level functions are selected, or simply
    // there's no nesting container.
    return null;
  }

  /**
   * Given a set of lines and optional nesting container, figure out the most
   * appropriate pretty symbol in the lines.
   *
   * If nesting container is given, lines outside of the container is ignored.
   * If there's no appropriate pretty symbol in given lines, the nesting
   * container's symbol is used if any.
   */
  _findBestPrettySymbolInContainer(lines, maybeNestingContainer) {
    for (const line of [...lines].sort()) {
      const selectedLine = document.getElementById(`line-${line}`);
      if (maybeNestingContainer && !maybeNestingContainer.contains(selectedLine)) {
        continue;
      }
      const sourceLine = selectedLine.querySelector(".source-line");
      const bestPretty = this._findBestPrettySymbolInSourceLineElem(sourceLine);
      if (bestPretty.long) {
        return bestPretty;
      }
    }

    const maybeNestingLine = maybeNestingContainer?.querySelector(
      ".nesting-sticky-line"
    );
    const sourceLine = maybeNestingLine?.querySelector(".source-line");
    return this._findBestPrettySymbolInSourceLineElem(sourceLine);
  }
})();

var Sticky = new (class Sticky {
  constructor() {
    // List of already stuck elements.
    this.stuck = [];
    this.scroller = document.getElementById("scrolling");

    // Our logic can't work on our diff output because there will be line
    // number discontinuities and line numbers that are simply missing.
    let hasLineNumbers = !!document.querySelector(".line-number");
    let isDiffView = document
      .getElementById("content")
      .classList.contains("diff");
    if (hasLineNumbers && !isDiffView) {
      this.scroller.addEventListener("scroll", () => this.handleScroll(), {
        passive: true,
      });
    }
  }

  /**
   * Hacky but workable sticky detection logic.
   *
   * document.elementsFromPoint can give us the stack of all of the boxes that
   * occur at a given point in the viewport.  The naive assumption that we can
   * look at the stack of returned elements and see if there are two
   * "source-line-with-number" in the stack (one for the sticky bit, one for the
   * actual source it's occluding) turns out to run into problems when sticky
   * things start or stop stickying.  Also, you potentially have to probe twice
   * with a second offset to compensate for exclusive box boundary issues.
   *
   * So instead we can leverage some important facts:
   * - Our sticky lines line up perfectly.  They're always fully visible.
   *   (That said, given that fractional pixel sizes can happen with scaling and
   *   all that, it's likely smart to avoid doing exact math on that.)
   * - We've annotated every line with line numbers.  So any discontinuity greater
   *   than 1 is an indication of a stuck line.  Unfortunately, since we also
   *   expect that sometimes there will be consecutive stuck lines, we can't treat
   *   lack of discontinuity as proof that things aren't stuck.  However, we can
   *   leverage math by making sure that a line beyond our maximum nesting level's
   *   line number lines up.
   */
  handleScroll() {
    const MAX_NESTING = 10;
    const scrolling = document.getElementById("scrolling");
    const firstSourceY = scrolling.offsetTop;
    // The goal is to make sure we're in the area the source line numbers live.
    const lineForSizing = document.querySelector(".line-number");
    const lineRect = lineForSizing.getBoundingClientRect();
    const sourceLinesX = lineRect.left + 6;
    const lineHeight = lineRect.height;

    let firstStuck = null;
    let lastStuck = null;
    const jitter = 6;

    function extractLineNumberFromElem(elem) {
      if (!elem || !elem.classList.contains("line-number")) {
        return null;
      }
      let num = parseInt(elem.dataset.lineNumber, 10);
      if (isNaN(num) || num < 0) {
        return null;
      }
      return num;
    }

    /**
     * Walk at a fixed offset into the middle of what should be stuck line
     * numbers.
     *
     * If we don't find a line-number, then we expect that to be due to the
     * transition from stuck elements to partially-scrolled source lines.  It
     * means the current set of lines are all stuck.
     *
     * If we do find a line-number, then we look at the displacement vs. the
     * statically-positioned ancestor, to detect which ones are stuck.
     */
    function walkLinesAndGetLines() {
      let offset = 6;
      let sourceLines = [];

      // Find a line number that can't possibly be stuck.
      let sanityCheckLineNum = extractLineNumberFromElem(
        document.elementFromPoint(
          sourceLinesX,
          firstSourceY + offset + lineHeight * MAX_NESTING
        )
      );
      // if we didn't find a line, try again with a slight jitter because there
      // might have been a box boundary edge-case.
      if (!sanityCheckLineNum) {
        sanityCheckLineNum = extractLineNumberFromElem(
          document.elementFromPoint(
            sourceLinesX,
            jitter + firstSourceY + offset + lineHeight * MAX_NESTING
          )
        );
      }

      // If we couldn't find a sanity-checking line number, then either our logic
      // is broken or the file doesn't have enough lines to necessitate the sticky
      // logic.  Just bail.
      if (!sanityCheckLineNum) {
        return sourceLines;
      }

      for (let iLine = 0; iLine <= MAX_NESTING; iLine++) {
        let elem = document.elementFromPoint(
          sourceLinesX,
          firstSourceY + offset
        );
        if (!elem || !elem.classList.contains("line-number")) {
          break;
        }

        let parentElem = elem.parentElement;
        if (!parentElem.classList.contains("nesting-sticky-line")) {
          break;
        }

        // We're stuck if the sticky element is further down from its parent's
        // static position.
        if (parentElem.offsetTop <= parentElem.parentElement.offsetTop) {
          break;
        }

        // the line-number's parent is the source-line-with-number
        sourceLines.push(parentElem);
        offset += lineHeight;
      }

      return sourceLines;
    }

    let newlyStuckElements = walkLinesAndGetLines();

    let noLongerStuck = new Set(this.stuck);
    for (let elem of newlyStuckElements) {
      elem.classList.add("stuck");
      noLongerStuck.delete(elem);
    }
    let lastElem = null;
    if (newlyStuckElements.length) {
      lastElem = newlyStuckElements[newlyStuckElements.length - 1];
    }
    let prevLastElem = null;
    if (this.stuck.length) {
      prevLastElem = this.stuck[this.stuck.length - 1];
    }
    if (lastElem !== prevLastElem) {
      if (prevLastElem) {
        prevLastElem.classList.remove("last-stuck");
      }
      if (lastElem) {
        lastElem.classList.add("last-stuck");
      }
    }

    for (let elem of noLongerStuck) {
      elem.classList.remove("stuck");
    }

    this.stuck = newlyStuckElements;
    DocumentTitler.processStickyElems(this.stuck);
  }
})();

var Highlighter = new (class Highlighter {
  constructor() {
    for (let line of document.querySelectorAll(".line-number")) {
      line.addEventListener("click", event => this.onLineNumberClick(event));
    }
    this.lastSelectedLine = null;
    this.selectedLines = new Set();
    this.updateFromHash();
    window.addEventListener("hashchange", () => {
      this.updateFromHash();
    });
  }

  addSelectedLine(line) {
    document.getElementById("line-" + line).classList.add("highlighted");
    // NOTE: The order here is intentional so that we throw above if the line
    // is not in the document.
    this.selectedLines.add(line);
    this.lastSelectedLine = line;

    Panel.onSelectedLineChanged();
  }

  removeSelectedLine(line) {
    this.selectedLines.delete(line);
    if (this.lastSelectedLine == line) {
      this.lastSelectedLine = null;
    }
    document.getElementById("line-" + line).classList.remove("highlighted");

    Panel.onSelectedLineChanged();
  }

  toggleSelectedLine(line) {
    if (this.selectedLines.has(line)) {
      this.removeSelectedLine(line);
    } else {
      this.addSelectedLine(line);
    }
  }

  removeAllLines() {
    for (let line of [...this.selectedLines]) {
      this.removeSelectedLine(line);
    }
  }

  selectSingleLine(line) {
    this.removeAllLines();
    this.addSelectedLine(line);
  }

  onLineNumberClick(event) {
    if (!event.shiftKey && !event.ctrlKey) {
      // Hacky logic to jump if this is a stuck line number
      //
      // TODO(emilio): This should probably select the line as well, or something?
      let containingLine = event.target.closest(".source-line-with-number");
      if (containingLine && containingLine.classList.contains("stuck")) {
        let nestingContainer = containingLine.closest(".nesting-container");
        if (nestingContainer) {
          Sticky.scroller.scrollTop -=
            containingLine.offsetTop - nestingContainer.offsetTop;
        }
        return;
      }
    }

    let line = parseInt(event.target.dataset.lineNumber, 10);
    if (event.shiftKey) {
      // Range-select on shiftkey.
      //
      // TODO(emilio): This should maybe toggle instead of just adding to the
      // selection?
      if (!this.lastSelectedLine) {
        this.addSelectedLine(line);
      } else if (this.lastSelectedLine == line) {
        this.removeSelectedLine(line);
      } else if (this.lastSelectedLine < line) {
        for (let i = this.lastSelectedLine; i < line; ++i) {
          this.addSelectedLine(i + 1);
        }
      } else {
        for (let i = this.lastSelectedLine; i > line; --i) {
          this.addSelectedLine(i - 1);
        }
      }
    } else if (event.ctrlKey || event.metaKey) {
      // Toggle select on ctrl / meta.
      this.toggleSelectedLine(line);
    } else {
      this.selectSingleLine(line);
    }
    this.updateHash();
  }

  /**
   * Creates a synthetic anchor for all hash configurations, even ones that
   * highlight more than one line and therefore can't be understood by the
   * browser's native anchor-seeking like "#200-205" and "#200,205".
   *
   * Even if it seemed like a good idea to attempt to manually trigger this
   * scrolling on load and the "hashchange" event, Firefox notably will manually
   * seek to an anchor if you press the enter key in the location bar and have not
   * changed the hash.  This is a UX flow used by many developers, so it's
   * essential the synthetic anchor is in place.  For this reason, any
   * manipulation of history state via replaceState must call this method.
   *
   * This synthetic anchor also doubles as a means of creating sufficient padding
   * so that "position:sticky" stuck lines don't obscure the line we're seeking
   * to.  (That's what the "goto" class accomplishes.)  Please see mosearch.css
   * for some additional details and context here.
   */
  createSyntheticAnchor(id) {
    if (document.getElementById(id)) {
      return;
    }

    // XXX A bit hackish.
    let firstLineno = id.split(/[,-]/)[0];
    let anchor = document.createElement("div");
    anchor.id = id;
    anchor.className = "goto";

    let elt = document.getElementById("line-" + firstLineno);
    if (elt.classList.contains("nesting-sticky-line")) {
      // As an special-case, if this is a sticky line, we insert the anchor in
      // the container, so that it has its static position.
      elt.parentNode.insertBefore(anchor, elt);
    } else {
      elt.insertBefore(anchor, elt.firstChild);
    }
  }

  updateHash() {
    let hash = this.toHash();
    {
      let historyHash = hash ? "#" + hash : "";
      if (historyHash != window.location.hash) {
        if (historyHash) {
          window.history.replaceState(null, "", historyHash);
        } else {
          // If hash is empty, the 3rd parameter should be either
          // the filename or the pathname.
          window.history.replaceState(null, "", document.location.pathname);
        }
      }
    }
    if (hash) {
      this.createSyntheticAnchor(hash);
    }
    for (let link of document.querySelectorAll("a[data-update-link]")) {
      let extra = link.getAttribute("data-update-link").replace("{}", hash);
      link.href = link.getAttribute("data-link") + extra;
    }
    DocumentTitler.processLineSelection(this.selectedLines);
  }

  // Convert the list of selected lines into a hash string.
  // Passing extraLine makes that line also selected.
  toHash(extraLine = undefined) {
    if (extraLine === undefined && !this.selectedLines.size) {
      return "";
    }
    // Try to create ranges out of the lines.
    const unsortedLines = [...this.selectedLines];
    if (extraLine !== undefined && !unsortedLines.includes(extraLine)) {
      unsortedLines.push(extraLine);
    }
    let lines = unsortedLines.sort((a, b) => a - b);
    let ranges = [];
    let current = { start: lines[0], end: lines[0] };
    for (let i = 1; i < lines.length; ++i) {
      if (lines[i] == current.end + 1) {
        current.end += 1;
      } else {
        ranges.push(current);
        current = { start: lines[i], end: lines[i] };
      }
    }
    ranges.push(current);
    return ranges
      .map(range => {
        if (range.start == range.end) {
          return range.start + "";
        }
        return range.start + "-" + range.end;
      })
      .join(",");
  }

  updateFromHash() {
    this.removeAllLines();
    let hash = window.location.hash.substring(1);
    if (!hash) {
      return;
    }
    for (let chunk of hash.split(",")) {
      if (!chunk.length) {
        continue;
      }
      let range = chunk.split("-");
      if (range.length == 1) {
        let line = parseInt(range[0], 10);
        if (isNaN(line)) {
          continue;
        }
        try {
          this.addSelectedLine(line);
        } catch (ex) {
          // The line may not be in the document.
        }
      } else if (range.length == 2) {
        let first = parseInt(range[0], 10);
        let second = parseInt(range[1], 10);
        if (isNaN(first) || isNaN(second)) {
          continue;
        }
        // Allow inverted ranges in the url, in case they're manually written
        // or what not.
        let start = Math.min(first, second);
        let end = Math.max(first, second);
        for (let i = start; i <= end; ++i) {
          try {
            this.addSelectedLine(i);
          } catch (ex) {
            // The line may not be in the document.
          }
        }
      } else {
        // Something unknown, ignore.
      }
    }
    // We're done parsing the hash, now update so we use the sanitized version
    // if we have at least one line selected. Otherwise it could be an idref or
    // something of that sort.
    if (this.selectedLines.size) {
      this.updateHash();
    }
  }
})();

Panel.onSelectedLineChanged();

```

## static/js/context-menu.js
```
function atUnescape(text) {
  return text.replace(/@([0-9A-F][0-9A-F])/g, (_, s) => String.fromCharCode(parseInt(s, 16)));
}

class ContextMenuBase {
  constructor() {
    this.menu = null;
    this.columns = [];

    window.addEventListener("mousedown", event => this.hideOnMouseDown(event));
    window.addEventListener("pageshow", event => this.hideOnPageShow(event));
  }

  hideOnMouseDown(event) {
    this.hide();
  }

  hideOnPageShow(event) {
    this.hide();
  }

  hide() {
    if (this.menu) {
      this.menu.style.display = "none";
    }
  }

  createMenuItem(item) {
    let li = document.createElement("li");
    li.classList.add("contextmenu-row");
    li.setAttribute("role", "none");

    if (item.confidence) {
      li.classList.add(`confidence-${item.confidence}`);
    }

    let link = li.appendChild(document.createElement("a"));
    link.setAttribute("role", "menuitem");
    if (item.action) {
      link.addEventListener("click", (evt) => {
        evt.preventDefault();
        evt.stopPropagation();
        item.action();
      }, {
        // Debounce by only letting us hear one click.
        once: true
      });
      link.href = "#";
    } else if (item.href) {
      link.href = item.href;
    }

    link.classList.add("contextmenu-link");
    if (item.icon) {
      link.classList.add(`icon-${item.icon}`);
    }
    if (item.classNames) {
      for (const name of item.classNames) {
        link.classList.add(name);
      }
    }
    if (item.attrs) {
      for (const [name, value] of Object.entries(item.attrs)) {
        link.setAttribute(name, value);
      }
    }
    link.addEventListener("keydown", this.onKeyDown.bind(this));

    link.innerHTML = item.html;

    return li;
  }

  populateMenu(menu, menuItems) {
    const column = [];

    let suppression = new Set();
    menu.innerHTML = "";
    let lastSection = null;
    for (const item of menuItems) {
      // Avoid adding anything we've definitely added before.  This currently
      // can happen for hierarchical diagrams where we unify based on the
      // "pretty" and in particular for IDL interfaces/methods where the iface
      // pretties will be the same as the C++ impl pretty.
      let itemAsJson = JSON.stringify(item);
      if (suppression.has(itemAsJson)) {
        continue;
      }
      suppression.add(itemAsJson);

      const li = this.createMenuItem(item);
      if (lastSection === null) {
        lastSection = item.section;
      } else if (lastSection === item.section) {
        // nothing to do for the same section
        li.classList.add("contextmenu-same-section");
      } else {
        li.classList.add("contextmenu-new-section");
        lastSection = item.section;
      }

      menu.appendChild(li);

      column.push({
        link: li.firstChild,
      });
    }

    // Default behavior for single column and no groups.
    // See TreeSwitcherMenu#setupMenu for multi-column + groups.
    this.columns = [column];
  }

  getItemPos(target) {
    for (let col = 0; col < this.columns.length; col++) {
      const column = this.columns[col];
      for (let row = 0; row < column.length; row++) {
        if (column[row].link == target) {
          return { col, row };
        }
      }
    }
    return null;
  }

  focusItemAt(pos) {
    while (!this.columns[pos.col][pos.row].link) {
      pos.row++;
    }

    this.focusItem(this.columns[pos.col][pos.row].link);
  }

  focusItem(item) {
    item.focus();

    // Given focus needs user interaction, tell webtest separately.
    const event = new Event("focusmenuitem");
    event.targetItem = item;
    document.dispatchEvent(event);
  }

  onKeyDown(event) {
    switch (event.key) {
      case "Esc":
      case "Escape":
        this.hide();
        return;
    }

    const pos = this.getItemPos(event.target);
    if (!pos) {
      return;
    }

    switch (event.key) {
      case "ArrowUp":
      case "Up":
        pos.row--;
        if (pos.row >= 0 && !this.columns[pos.col][pos.row].link) {
          // Skip label.
          pos.row--;
        }
        if (pos.row < 0) {
          if (pos.col > 0) {
            pos.col--;
            pos.row = this.columns[pos.col].length - 1;
          } else {
            pos.row = 0;
          }
        }
        break;

      case "ArrowDown":
      case "Down":
        pos.row++;
        if (pos.row >= this.columns[pos.col].length) {
          if (pos.col < this.columns.length - 1) {
            pos.col++;
            pos.row = 0;
          } else {
            pos.row = this.columns[pos.col].length - 1;
          }
        }
        break;

      case "Home":
        pos.row = 0;
        pos.col = 0;
        break;

      case "End":
        pos.col = this.columns.length - 1;
        pos.row = this.columns[pos.col].length - 1;
        break;

      case "PageUp":
        pos.row = 0;
        break;

      case "PageDown":
        pos.row = this.columns[pos.col].length - 1;
        break;

      case "ArrowLeft":
      case "Left":
        pos.col--;
        if (pos.col < 0) {
          pos.col = 0;
        }
        if (pos.row >= this.columns[pos.col].length) {
          pos.row = this.columns[pos.col].length - 1;
        }
        break;

      case "ArrowRight":
      case "Right":
        pos.col++;
        if (pos.col >= this.columns.length) {
          pos.col = this.columns.length - 1;
        }
        if (pos.row >= this.columns[pos.col].length) {
          pos.row = this.columns[pos.col].length - 1;
        }
        break;
    }

    this.focusItemAt(pos);
  }
}

var ContextMenu = new (class ContextMenu extends ContextMenuBase {
  constructor() {
    super();
    this.menu = document.createElement("ul");
    this.menu.className = this.menu.id = "context-menu";
    this.menu.tabIndex = 0;
    this.menu.style.display = "none";
    this.menu.setAttribute("role", "menu");
    document.body.appendChild(this.menu);

    this.selectedToken = null;

    this.menu.addEventListener("mousedown", function (event) {
      // Prevent clicks on the menu to propagate
      // to the window, so that the menu is not
      // removed and links will be followed.
      event.stopPropagation();
    });

    window.addEventListener("click", event => this.tryShowOnClick(event));
  }

  fmt(s, data) {
    data = data
      .replace(/&/g, "&amp;")
      .replace(/</g, "&lt;")
      .replace(/>/g, "&gt;")
      .replace(/"/g, "&quot;")
      .replace(/'/g, "&#039;");
    return s.replace("_", data);
  }

  fmtLang(lang) {
      lang = lang.toUpperCase();
      if (lang === "CPP") {
        lang = "C++";
      }
      return lang;
  }

  generatePseudoFileSymInfo(sym) {
    let file = atUnescape(sym.replace(/^FILE_/, ""));
    return {
      sym: sym,
      pretty: file,
      jumps: {
        def: file + "#1",
      },
    };
  }

  sortBindingSlots(bindingSlots) {
    return bindingSlots.slice().sort((a, b) => {
      if (a.slotKind < b.slotKind) {
        return -1;
      }
      if (a.slotKind > b.slotKind) {
        return 1;
      }

      if (a?.implKind) {
        if (b?.implKind) {
          if (a.implKind < b.implKind) {
            return -1;
          }
          if (a.implKind > b.implKind) {
            return 1;
          }
        } else {
          return 1;
        }
      } else {
        if (b?.implKind) {
          return -1;
        }
      }

      return 0;
    });
  }

  tryShowOnClick(event) {
    if (Settings.fancyBar.enabled) {
      if (this.selectedToken) {
        if (!Panel?.isOnPanel?.(event)) {
          this.selectedToken.classList.remove("selected");
          this.selectedToken = null;
          Panel?.onSelectedTokenChanged?.();
        }
      }
    }

    // Don't display the context menu if there's a selection.
    // User could be trying to select something and the context menu will undo it.
    if (!window.getSelection().isCollapsed) {
      return;
    }

    // We expect to find symbols in:
    // - source listings ("code")
    // - diagrams ("svg")
    // - breadcrumbs ("breadcrumbs")
    if (!event.target.closest("code") &&
        !event.target.closest("svg") &&
        !event.target.closest(".breadcrumbs") &&
        !event.target.closest(".symbol-tree-table") &&
        !event.target.closest(".symbol")) {
      return;
    }

    // Tree switcher is inside breadcrumbs, but it has its own menu.
    if (event.target.closest("#tree-switcher") ||
        event.target.closest("#tree-switcher-menu")) {
      return;
    }

    let tree = document.getElementById("data").getAttribute("data-tree");

    // Figure out the source line this click was on, if it was on any line, so
    // that we can compare it against jump strings in order to avoid offering
    // the option to jump to the line the user literally just clicked on.
    let sourceLineClicked = null;
    {
      let sourceLineNode = event.target.closest("code");
      let lineNumberNode = sourceLineNode?.previousElementSibling;
      if (lineNumberNode && Router.sourcePath) {
        sourceLineClicked = `${Router.sourcePath}#${lineNumberNode.dataset.lineNumber}`;
      }
    }

    // jumps come first
    let jumpMenuItems = [];
    // then searches
    let searchMenuItems = [];
    // the the text search and sticky highlight option
    let stickyItems = [];
    // then these extra menu items which are for new/experimental features where
    // we don't want to mess with muscle memory at the top of the list.
    let extraMenuItems = [];

    let expansions = {};
    let onlyOneExpansion = true;
    const expansionToken = event.target.closest("[data-expansions]");
    if (Settings.expansions.enabled) {
      if (expansionToken) {
        expansions = JSON.parse(expansionToken.dataset.expansions);
        onlyOneExpansion = Object.keys(expansions).length == 1;
        if (onlyOneExpansion) {
          for (const key in expansions) {
            onlyOneExpansion = Object.keys(expansions[key]).length == 1;
          }
        }
      }
    }

    let symbolToken = event.target.closest("[data-symbols]");
    if (symbolToken) {
      if (Settings.fancyBar.enabled) {
        this.selectedToken = symbolToken;
        this.selectedToken.classList.add("selected");
        Panel?.onSelectedTokenChanged?.();
      }

      let symbols = symbolToken.getAttribute("data-symbols").split(",");
      let confidences = JSON.parse(symbolToken.getAttribute("data-confidences"));
      // if data-confidences is missing, assume everything is concrete
      if (!confidences) {
        confidences = Array(symbols.length);
        confidences.fill("concrete");
      }

      const seenSyms = new Set();
      // For debugging/investigation purposes, expose the symbols that got
      // clicked on on the window global.
      const exposeSymbolsForDebugging = window.CLICKED_SYMBOLS = [];

      // ## Diagram edge specialization
      if (symbolToken.id?.startsWith("Gide")) {
        // The "data-symbols" we have is of the form `A->B` where A is a comma
        // delimited list of the source symbols that were consolidated into a
        // single node, and the same deal with B.  This is exactly what was
        // declared to graphviz.  In acylic dot layouts, this edge will be
        // pointed downwards even if the arrowhead is visually pointing upwards
        // (ex: inheritance).
        const [srcSyms, targSyms] = symbolToken.getAttribute("data-symbols").split("->").map(x => x.split(","));

        // Just clear the normal symbol list as we don't actually want the
        // normal per-symbol behavior below.
        symbols = [];

        // We just want a pretty, so let's just use the first symbol of each.
        let srcSymInfo = SYM_INFO[srcSyms[0]];
        let targSymInfo = SYM_INFO[targSyms[0]];

        // Generate a "go to use"
        const edgeExtra = GRAPH_EXTRA[0].edges[symbolToken.id];
        if (edgeExtra.jump && targSymInfo) {
          jumpMenuItems.push({
            html: this.fmt("Go to use of <strong>_</strong>", targSymInfo.pretty),
            href: `/${tree}/source/${edgeExtra.jump}`,
            icon: "export-alt",
            section: "jumps",
          });
        }
      }

      // ## First pass: Process symbols and potentially filter out implicit constructors
      //
      // In the future we can potentially use this pass to do more clever things,
      // but right now the main interesting situation that can arise is that the
      // user is clicking on a constructor where we have both the constructor
      // symbol plus all of the implicit constructors that will be invoked as
      // part of the constructor and we are weirdly attributing to the constructor.
      //
      // We can detect this case because we can detect when the user is clicking
      // on a line that's already the target of a definition jump.  And then in
      // that case we can filter out all the symbols that aren't definition jumps.
      //
      // In general, we only expect to see multiple symbols here when the symbol
      // varies per platform or as a result of implicit constructors like this.
      // Our logic to remove implicit constructors here will not affect the
      // platform case because all symbols will have the line as a definition.
      // (For other platforms where the definition is on a different line, the
      // symbol won't be present here because it won't have been mered in by the
      // merge-analyses step.)
      let filteredSymTuples = [];
      let sawDef = false;
      symbols.forEach((sym, index) => {
        // Avoid processing the same symbol more than once.
        if (seenSyms.has(sym)) {
          return;
        }

        let symInfo = SYM_INFO[sym];

        if (!symInfo) {
          if (sym.startsWith("FILE_")) {
            symInfo = this.generatePseudoFileSymInfo(sym);
          }
        }

        // XXX Ignore no_crossref data that's currently not useful/used.
        if (!symInfo || !symInfo.sym || !symInfo.pretty) {
          return;
        }

        const confidence = confidences[index];

        // The symInfo is self-identifying via `pretty` and `sym` so we don't
        // need to try and include any extra context.
        exposeSymbolsForDebugging.push(symInfo);

        if (symInfo?.jumps?.idl === sourceLineClicked ||
            symInfo?.jumps?.def === sourceLineClicked ) {
          if (!sawDef) {
            // Transition to "kick out the implicit constructors" mode.
            sawDef = true;
            filteredSymTuples = [];
          }
          filteredSymTuples.push([sym, confidence, symInfo]);
        } else if (!sawDef) {
          filteredSymTuples.push([sym, confidence, symInfo]);
        }
      });

      for (const [sym, confidence, symInfo] of filteredSymTuples) {
        let diagrammableSyms = [];
        // We need structured data to do diagramming; no structured data means
        // no diagramming.  Currently we expect this to be the case for our
        // JS analysis, but when we're able to switch at least some of the JS
        // analysis to scip-typescript that will change.
        //
        // That said, we want to ignore IDL symbols in favor of their language
        // binding symbols.  The main rationale right now is that for XPIDL
        // attributes we can't do anything for the pretty of the attribute, so
        // having a menu entry for it, especially given that we'll also have an
        // entry for the C++ getter (and potentially setter), is not helpful.
        //
        // Also, we don't currently de-duplicate the diagram links, but it
        // would be appropriate to do so or otherwise address that the traverse
        // logic itself will follow binding slots.
        if (symInfo.meta && symInfo.meta.implKind !== "idl") {
          diagrammableSyms.push(symInfo);
        }

        // Define a helper we can also use for the binding slots below.
        const jumpify = (jumpref, pretty) => {
          if (!jumpref.jumps) {
            return;
          }
          if (jumpref.jumps.idl && jumpref.jumps.idl !== sourceLineClicked) {
            jumpMenuItems.push({
              html: this.fmt("Go to IDL definition of <strong>_</strong>", pretty),
              href: `/${tree}/source/${jumpref.jumps.idl}`,
              icon: "export-alt",
              section: "jumps",
              confidence,
            });
          }

          if (jumpref.jumps.def && jumpref.jumps.def !== sourceLineClicked) {
            jumpMenuItems.push({
              html: this.fmt("Go to definition of <strong>_</strong>", pretty),
              href: `/${tree}/source/${jumpref.jumps.def}`,
              icon: "export-alt",
              section: "jumps",
              confidence,
            });
          }

          if (jumpref.jumps.decl && jumpref.jumps.decl !== sourceLineClicked) {
            jumpMenuItems.push({
              html: this.fmt("Go to declaration of <strong>_</strong>", pretty),
              href: `/${tree}/source/${jumpref.jumps.decl}`,
              icon: "export",
              section: "jumps",
              confidence,
            });
          }

          for (const key in expansions) {
            if (key.startsWith(sym)) {
              for (const platform in expansions[key]) {
                const expansion = expansions[key][platform]
                let html;
                if (onlyOneExpansion) {
                  html = `Expansion: <code>${expansion}</code>`;
                } else {
                  html = `Expansion on ${platform}: <code>${expansion}</code>`;
                }
                jumpMenuItems.push({
                  html: html,
                  classNames: ["contextmenu-expansion-preview"],
                  action: () => {
                    this.hide();
                    BlamePopup.expansionIndex = [key, platform, jumpref];
                    BlamePopup.blameElement = expansionToken;
                    BlameStripHoverHandler.keepVisible = true;
                  },
                  confidence,
                });
              }
              delete expansions[key]
            }
          }
        }

        // Helper for cases like showing the recv def when the user is clicking
        // on a call to its send, but where we don't want to crowd the context
        // menu with the decl.
        const directDefJumpify = (jumpref, pretty) => {
          if (!jumpref.jumps) {
            return;
          }

          if (jumpref.jumps.def && jumpref.jumps.def !== sourceLineClicked) {
            jumpMenuItems.push({
              html: this.fmt("Go to definition of <strong>_</strong>", pretty),
              href: `/${tree}/source/${jumpref.jumps.def}`,
              icon: "export-alt",
              section: "jumps",
              confidence,
            });
          }
        }

        // If the symbol has <= 2 overrides (we depend on the logic in our
        // rust `determine_desired_extra_syms_from_jumpref` helper at jumpref
        // generation time, so you can't just change the number here and have
        // things work out well), then emit direct def jump options.
        //
        // This is motivated by XPIDL where we want to be able to jump directly
        // to the overrides of the use of an XPIDL method in C++ where we are
        // dealing with an interface pointer, as well as for the binding slots
        // for when we are dealing with an XPIDL IDL def symbol.  This is
        // factored out into a helper because those are different call-sites; we
        // don't do an open-ended graph traversal.
        const overrideJumpifyHelper = (jumpref) => {
          if (jumpref.meta?.overriddenBy?.length && jumpref.meta?.overriddenBy?.length <= 2) {
            for (const overSym of jumpref.meta.overriddenBy) {
              const overInfo = SYM_INFO[overSym];
              if (overInfo) {
                let overPretty;
                if (jumpref.meta.overriddenBy.length === 1) {
                  overPretty = `Sole Override ${overInfo.pretty}`;
                } else {
                  overPretty = `Override ${overInfo.pretty}`;
                }
                directDefJumpify(overInfo, overPretty)
              }
            }
          }
        }

        let searches = [];

        // If we have a slotOwner, it can help make our "go to def" description
        // more descriptive and identical to what would be generated for when
        // the bindingSlot that refers to us from our slotOwner would describe.
        if (symInfo.meta?.slotOwner) {
          let slotOwner = symInfo.meta.slotOwner;
          let ownerJumpref = SYM_INFO[slotOwner.sym];
          // XXX Ignore no_crossref data that's currently not useful/used.
          if (ownerJumpref && ownerJumpref.sym && ownerJumpref.pretty) {
            let implKind = ownerJumpref.meta.implKind || "impl";
            if (implKind === "idl") {
              implKind = "IDL";
              // Our owner being an IDL type does not change whether this is
              // something diagrammable.
            }

            let maybeLang = "";
            if (slotOwner.slotLang) {
              maybeLang = ` ${this.fmtLang(slotOwner.slotLang)}`;
            }

            const canonLabel = `${implKind}${maybeLang} ${slotOwner.slotKind} ${symInfo.pretty}`;
            jumpify(symInfo, canonLabel);
            searches.push([ canonLabel, sym ])
            jumpify(ownerJumpref, ownerJumpref.pretty);

            // If our current symbol is an IPC Send method, offer a direct jump to the Recv def
            if (slotOwner?.slotKind === "send" && ownerJumpref) {
              const bindingSlots = this.sortBindingSlots(ownerJumpref?.meta?.bindingSlots);

              for (const slot of bindingSlots) {
                if (slot.slotKind === "recv") {
                  let recvJumpref = SYM_INFO[slot.sym];
                  if (recvJumpref?.pretty) {
                    let maybeSlotImplKind = "";
                    if (slot?.implKind) {
                      maybeSlotImplKind = ` ${slot.implKind}`;
                    }
                    directDefJumpify(recvJumpref, `${implKind}${maybeLang} ${slot.slotKind}${maybeSlotImplKind} ${recvJumpref.pretty}`);
                  }
                }
              }
            }
          } else {
            jumpify(symInfo, symInfo.pretty);
            searches.push([ symInfo.pretty, sym ]);
          }
        } else {
          jumpify(symInfo, symInfo.pretty);
          searches.push([ symInfo.pretty, sym ]);
        }

        // Binding slots
        if (symInfo.meta?.bindingSlots) {
          let implKind = symInfo.meta.implKind || "impl";
          if (implKind === "idl") {
            implKind = "IDL";
          }

          const bindingSlots = this.sortBindingSlots(symInfo.meta.bindingSlots);

          let allSearchSyms = [];
          for (const slot of bindingSlots) {
            // XXX Ignore no_crossref data that's currently not useful/used.
            let slotJumpref = SYM_INFO[slot.sym];
            // (we do handle the pretty not existing below)
            if (!slotJumpref || !slotJumpref.sym) {
              continue;
            }

            let maybeLang = "";
            if (slot.slotLang) {
              const lang = slot.slotLang;
              // Previously this was === "cpp", but the reality is that our
              // concern is that our JS analysis is soupy.  Especially with the
              // new TS XPIDL magic, if we can switch to scip-typescript at
              // least for system JS, we can remove this constraint.
              if (lang !== "js") {
                diagrammableSyms.push(slotJumpref);
              }
              maybeLang = ` ${this.fmtLang(lang)}`;
            }

            // Favor the slot's pretty if available.
            const effectivePretty = slotJumpref?.pretty || symInfo.pretty;
            let maybeSlotImplKind = "";
            if (slot?.implKind) {
              maybeSlotImplKind = ` ${slot.implKind}`;
            }
            let slotPretty =
              `${implKind}${maybeLang} ${slot.slotKind}${maybeSlotImplKind} ${effectivePretty}`;
            searches.push([slotPretty, slot.sym]);
            allSearchSyms.push(slot.sym);

            if (slotJumpref) {
              jumpify(slotJumpref, slotPretty);
              // For XPIDL, we also want to do the same overriddenBy check here
              // that we do below so that if we're browsing the XPIDL source we
              // can go directly to the implementation rather than the pure
              // virtual decl that gets upgraded to a def.  (Unfortunately we
              // currently don't have a way to easily tell that that is what
              // happened to downgrade the def, although I guess we could
              // hardcode an assumption...)
              overrideJumpifyHelper(slotJumpref);
            }
          }

          // If there were multiple language bindings that we think might exist,
          // then generate a single roll-up search.
          if (allSearchSyms.length > 1 && implKind !== "StaticPrefs") {
            // Eat the default search if this was IDL, as currently the "search"
            // endpoint search for the synthetic symbol will only do upsells
            // which is not what people are used to.
            if (implKind === "IDL") {
              searches.shift();
              // Do put the synthetic symbol at the start of the explicit symbol
              // list, however.
              allSearchSyms.unshift(sym);
            }
            searches.push([`${implKind} ${symInfo.meta.kind} ${symInfo.pretty}`, allSearchSyms.join(",")]);
          }
        }

        overrideJumpifyHelper(symInfo);

        for (const search of searches) {
          searchMenuItems.push({
            html: this.fmt("Search for <strong>_</strong>", search[0]),
            href: `/${tree}/search?q=symbol:${encodeURIComponent(
              search[1]
            )}&redirect=false`,
            icon: "search",
            section: "symbol-searches",
            confidence,
          });
        }

        if (Settings.semanticInfo.enabled) {
          for (const jumpref of diagrammableSyms) {
            if ((jumpref?.meta?.kind === "class" || jumpref?.meta?.kind === "struct") &&
                (jumpref?.meta?.fields?.length || jumpref?.meta?.supers?.length)) {
              let queryString = `field-layout:'${jumpref.pretty}'`;
              searchMenuItems.push({
                html: this.fmt("Class layout of <strong>_</strong>", jumpref.pretty),
                href: `/${tree}/query/default?q=${encodeURIComponent(queryString)}`,
                // TODO: pick out a custom icon for this; "tasks" was great but
                // is already used for sticky highlight and so we now expect it
                // to have muscle memory implications so we can't repurpose it.
                icon: "docs",
                section: "layout",
                confidence,
              });
            }
          }
        }

        if (Settings.diagramming.enabled) {
          for (const jumpref of diagrammableSyms) {
            // Always offer to diagram uses of things
            let queryString = `calls-to:'${jumpref.pretty}' depth:4`;
            // TODO: Try dog-fooding with using the symbol-specific variant of this
            // whose query syntax is below.  The rationale for using pretty
            // identifiers is that they are more stable and more readable than
            // symbols.  It might be most practical to allow specializing a link
            // to just a single symbol from the page itself or in a sidebar
            // affordance, especially since it's hard to concisely express the
            // differences in signatures for overloads (although we have some
            // tentative plans to).
            //queryString = `calls-to-sym:'${jumpref.sym}' depth:4`;
            extraMenuItems.push({
              html: this.fmt("Uses diagram of <strong>_</strong>", jumpref.pretty),
              href: `/${tree}/query/default?q=${encodeURIComponent(queryString)}`,
              icon: "brush",
              section: "diagrams",
              confidence,
            });

            // Always offer to diagram uses of things
            queryString = `calls-from:'${jumpref.pretty}' depth:4`;
            extraMenuItems.push({
              html: this.fmt("Calls diagram of <strong>_</strong>", jumpref.pretty),
              href: `/${tree}/query/default?q=${encodeURIComponent(queryString)}`,
              icon: "brush",
              section: "diagrams",
              confidence,
            });

            if ((jumpref?.meta?.kind === "class" || jumpref?.meta?.kind === "struct") &&
                jumpref?.meta?.fields?.length) {
              // Offer class diagrams for classes/structs that have fields.
              queryString = `class-diagram:'${jumpref.pretty}' depth:4`;
              extraMenuItems.push({
                html: this.fmt("Class diagram of <strong>_</strong>", jumpref.pretty),
                href: `/${tree}/query/default?q=${encodeURIComponent(queryString)}`,
                icon: "brush",
                section: "diagrams",
                confidence,
              });
            }

            let showInheritance = false;
            if (jumpref?.meta?.kind === "method" &&
                (jumpref?.meta?.overrides?.length || jumpref?.meta?.overriddenBy?.length)) {
              showInheritance = true;
            } else if (jumpref?.meta?.kind === "class" &&
                       (jumpref?.meta?.supers?.length || jumpref?.meta?.subclasses?.length)) {
              showInheritance = true;
            }

            // Offer inheritance diagrams for methods that are involved in an
            // override hierarchy.  This does not currently work for classes
            // despite the name demanding it.  (cmd_traverse would like a minor
            // cleanup.)
            if (showInheritance) {
              queryString = `inheritance-diagram:'${jumpref.pretty}' depth:4`;
              extraMenuItems.push({
                html: this.fmt("Inheritance diagram of <strong>_</strong>", jumpref.pretty),
                href: `/${tree}/query/default?q=${encodeURIComponent(queryString)}`,
                icon: "brush",
                section: "diagrams",
                confidence,
              });
            }
          }
        }
      }

      const tokenText = symbolToken.textContent;
      stickyItems.push({
        html: "Sticky highlight",
        action: () => { Hover.stickyHighlight(symbols, tokenText); },
        icon: "tasks",
        section: "highlights",
      });
    }

    let remainingExpansionMenuItems = []
    for (const key in expansions) {
      for (const platform in expansions[key]) {
        const expansion = expansions[key][platform]
        let html;
        if (onlyOneExpansion) {
          html = `Expansion: <code>${expansion}</code>`;
        } else {
          html = `Expansion on ${platform}: <code>${expansion}</code>`;
        }
        remainingExpansionMenuItems.push({
          html: html,
          classNames: ["contextmenu-expansion-preview"],
          action: () => {
            this.hide();
            BlamePopup.expansionIndex = [key, platform];
            BlamePopup.blameElement = expansionToken;
            BlameStripHoverHandler.keepVisible = true;
          },
        });
      }
    }

    let menuItems = [];
    menuItems.push(...remainingExpansionMenuItems)
    menuItems.push(...jumpMenuItems)
    menuItems.push(...searchMenuItems);

    let word = getTargetWord();
    if (word) {
      // A word was clicked on.
      menuItems.push({
        html: this.fmt("Search for the substring <strong>_</strong>", word),
        href: `/${tree}/search?q=${encodeURIComponent(word)}&redirect=false`,
        icon: "font",
        section: "text-searches",
      });
    }

    menuItems.push(...stickyItems);

    menuItems.push(...extraMenuItems);

    if (!menuItems.length) {
      return;
    }

    this.populateMenu(this.menu, menuItems);

    let x = event.clientX + window.scrollX;
    let y = event.clientY + window.scrollY;

    let viewportHeight = window.innerHeight;
    let spaceTowardsBottom = viewportHeight - event.clientY;
    let spaceTowardsTop = viewportHeight - spaceTowardsBottom;

    // Position the menu towards the bottom, and if that overflows and there's
    // more space to the top, flip it.
    this.menu.classList.remove("bottom");
    this.menu.style.bottom = "";
    this.menu.style.top = y + "px";
    this.menu.style.left = x + "px";
    this.menu.style.maxHeight = "none";

    this.menu.style.display = "";
    this.menu.style.opacity = "0";

    let rect = this.menu.getBoundingClientRect();
    // If it overflows, either flip it or constrain its height.
    if (rect.height > spaceTowardsBottom) {
      if (spaceTowardsTop > spaceTowardsBottom) {
        // Position it towards the top.
        this.menu.classList.add("bottom");
        this.menu.style.bottom = viewportHeight - y + "px";
        this.menu.style.top = "";
        if (rect.height > spaceTowardsTop) {
          this.menu.style.maxHeight = spaceTowardsTop + "px";
        }
      } else {
        // Constrain its height.
        this.menu.style.maxHeight = spaceTowardsBottom + "px";
      }
    }

    // Now the menu is correctly positioned, show it.
    this.menu.style.opacity = "";
    this.menu.focus();

    // Context menu doesn't focus on the item by default.
    this.menu.addEventListener("keydown", event => {
      if (event.target != this.menu) {
        return;
      }

      switch (event.key) {
        case "ArrowUp":
        case "Up": {
          const column = this.columns[0];
          this.focusItem(column[column.length - 1].link);
          break;
        }
        case "ArrowDown":
        case "Down":
          const column = this.columns[0];
          this.focusItem(column[0].link);
          break;
      }
    });
  }

  get active() {
    return this.menu.style.display != "none";
  }
})();

var Hover = new (class Hover {
  constructor() {
    this.items = [];
    this.graphItems = [];
    this.hoveredElem = null;
    this.sticky = false;
    window.addEventListener("mousedown", (evt) => {
      // Constrain de-highlighting to the primary mouse button; in particular,
      // scrolling via the middle mouse button should not disable the sticky
      // state.  Unfortunately I think scrolling with the primary mouse button
      // on the modern normally-hidden scrollbars, but I'm being conservative
      // with this change.
      if (this.sticky && evt.button === 0) {
        this.deactivateDiagram();
        this.deactivate();
      }
    });

    window.addEventListener("mousemove", event => this._handleMouseMove(event));
  }

  _handleMouseMove(event) {
    if (ContextMenu.active || this.sticky) {
      return;
    }

    let elem = event.target?.closest("[data-symbols]");
    // Don't recompute things if we're still hovering over the same element.
    if (elem === this.hoveredElem) {
      return;
    }
    if (!elem) {
      this.deactivateDiagram();
      return this.deactivate();
    }

    let symbolNames = this.symbolsFromString(elem.getAttribute("data-symbols"));
    // We're hovering over a graph so we also want to hover related graph nodes.
    // We will still also potentially want to highlight any document spans as
    // well.
    if (elem.tagName === "g") {
      this.activateDiagram(elem);
    }

    this.activate(symbolNames, elem.textContent);
    this.hoveredElem = elem;
  }

  symbolsFromString(symbolStr) {
    if (!symbolStr || symbolStr == "?") {
      // XXX why the `?` special-case?
      return [];
    }
    return symbolStr.split(",");
  }

  deactivate() {
    for (let item of this.items) {
      item.classList.remove("hovered");
    }
    this.hoveredElem = null;
    this.items = [];
    this.sticky = false;
  }

  activate(symbols, visibleToken) {
    this.deactivate();
    this.items = this.findReferences(symbols, visibleToken);
    for (let item of this.items) {
      item.classList.add("hovered");
    }
  }

  findReferences(symbolNames, visibleToken) {
    if (!symbolNames.length) {
      return [];
    }

    let symbols = new Set(symbolNames);

    return [...document.querySelectorAll("span[data-symbols]")].filter(span => {
      // XXX The attribute check is cheaper, probably should be before.
      return (
        span.textContent == visibleToken &&
        this.symbolsFromString(span.getAttribute("data-symbols")).some(symbol =>
          symbols.has(symbol)
        )
      );
    });
  }

  #edgeReverseMap
  // Derive a map from edges to the source and target nodes by processing the
  // GRAPH_EXTRA node data on first use.  This could be generated on the server
  // but since the data is easily derived and we expect our graphs to be
  // O(1000), we don't expect this computation to be too bad.
  #ensureEdgeReverseMap() {
    if (this.#edgeReverseMap) {
      return;
    }

    this.#edgeReverseMap = new Map();
    if (!GRAPH_EXTRA?.[0]) {
      return;
    }

    for (const [node, nodeInfo] of Object.entries(GRAPH_EXTRA[0].nodes)) {
      for (const inEdge of nodeInfo.in_edges) {
        let edgeInfo = this.#edgeReverseMap.get(inEdge);
        if (!edgeInfo) {
          this.#edgeReverseMap.set(inEdge, [undefined, node]);
        } else {
          edgeInfo[1] = node;
        }
      }
      for (const outEdge of nodeInfo.out_edges) {
        let edgeInfo = this.#edgeReverseMap.get(outEdge);
        if (!edgeInfo) {
          this.#edgeReverseMap.set(outEdge, [node, undefined]);
        } else {
          edgeInfo[0] = node;
        }
      }
    }
  }

  activateDiagram(elem) {
    this.deactivateDiagram();

    let id;
    if (elem.id) {
      id = elem.id;
    } else {
      id = elem.parentElement.id;
    }
    if (id.startsWith("a_")) {
      id = id.substring(2);
    }

    const applyStyling = (targetId, clazzes) => {
      let maybeTarget = document.getElementById(targetId);
      // For the table rows, the id ends up on a "g" container with an "a_"
      // prefix.  We want to locate the a_ prefix and then adjust to its sole
      // child for consistency.
      if (!maybeTarget) {
        maybeTarget = document.getElementById(`a_${targetId}`);
        if (!maybeTarget) {
          return;
        }
        maybeTarget = maybeTarget.children[0];
      }
      maybeTarget.classList.add(...clazzes);

      this.graphItems.push([maybeTarget, clazzes])
    };

    // ## Hovered Edge
    if (id.startsWith("Gide")) {
      const edgeExtra = GRAPH_EXTRA[0].edges[id];
      if (!edgeExtra) {
        return;
      }

      this.#ensureEdgeReverseMap();

      const curEdgeHover = ["hovered-cur-edge"];
      elem.classList.add(...curEdgeHover);
      this.graphItems.push([elem, curEdgeHover]);

      let [srcNode, targNode] = this.#edgeReverseMap.get(id);

      const defaultInNodeHover = ["hovered-in-node"];
      applyStyling(srcNode, defaultInNodeHover);

      const defaultOutNodeHover = ["hovered-out-node"];
      applyStyling(targNode, defaultOutNodeHover);

      return;
    }

    let nodeExtra = GRAPH_EXTRA[0].nodes[id];
    if (!nodeExtra) {
      return;
    }

    // ## Hovered Node
    const curNodeHover = ["hovered-cur-node"];
    elem.classList.add(...curNodeHover);
    this.graphItems.push([elem, curNodeHover]);

    const defaultInNodeHover = ["hovered-in-node"];
    for (const [nid, clazzes] of nodeExtra.in_nodes) {
      applyStyling(nid, clazzes.length ? clazzes : defaultInNodeHover);
    }
    const defaultOutNodeHover = ["hovered-out-node"];
    for (const [nid, clazzes] of nodeExtra.out_nodes) {
      applyStyling(nid, clazzes.length ? clazzes : defaultOutNodeHover);
    }

    const inEdgeHover = ["hovered-in-edge"];
    for (const eid of nodeExtra.in_edges) {
      applyStyling(eid, inEdgeHover);
    }

    const outEdgeHover = ["hovered-out-edge"];
    for (const eid of nodeExtra.out_edges) {
      applyStyling(eid, outEdgeHover);
    }
  }

  deactivateDiagram() {
    for (const [item, clazzes] of this.graphItems) {
      item.classList.remove(...clazzes);
    }
    this.graphItems = [];
  }

  stickyHighlight(symbols, visibleToken) {
    ContextMenu.hide();
    this.activate(symbols, visibleToken);
    this.sticky = true;
  }
})();

function getTargetWord() {
  let selection = window.getSelection();
  if (!selection.isCollapsed) {
    return null;
  }

  let offset = selection.focusOffset;
  let node = selection.anchorNode;
  let string = node?.nodeValue;

  if (!string?.length) {
    return null;
  }

  function isWordChar(character) {
    // TODO: this could be more non-ascii friendly.
    //
    // Notable Changes:
    // - We have added "#" to deal with JS private symbols.  This will widen
    //   C preprocessor directives to include the leading #, which makes sense.
    //   This will also impact use of the "stringizing operator" for macros,
    //   where it won't be what we want.
    return /[#A-Z0-9_]/i.test(character);
  }

  if (offset < string.length && !isWordChar(string[offset])) {
    // Not really in a word.
    return null;
  }

  let start = offset;
  let end = offset;

  while (start > 0 && isWordChar(string[start - 1])) {
    --start;
  }
  while (end < string.length && isWordChar(string[end])) {
    ++end;
  }

  if (end <= start) {
    return null;
  }

  return string.substring(start, end);
}

var TreeSwitcherMenu = new (class TreeSwitcherMenu extends ContextMenuBase {
  constructor() {
    super();
    this.button = document.getElementById("tree-switcher");
    this.menu = document.getElementById("tree-switcher-menu");

    if (!this.button || !this.menu) {
      return;
    }

    this.button.addEventListener("click", () => {
      if (this.isShown()) {
        this.hide();
      } else {
        this.setupMenu();
        this.show();
        this.focusCurrentTree();
      }
    });
  }

  show() {
    this.menu.style.display = "flex";
    this.button.setAttribute("aria-expanded", "true");
  }

  isShown() {
    return this.menu.style.display == "flex";
  }

  hide() {
    super.hide();
    this.button.setAttribute("aria-expanded", "false");
  }

  hideOnMouseDown(event) {
    if (event.target.closest("#tree-switcher-menu")) {
      return;
    }
    if (event.target.closest("#tree-switcher")) {
      return;
    }

    this.hide();
  }

  setupMenu() {
    const columns = [];
    const columnBoxes = [];

    for (const groups of TREE_LIST) {
      const columnBox = document.createElement("div");
      const column = [];
      for (const group of groups) {
        const menuItems = [];

        const groupIdPart = group.name.toLowerCase().replace(/[^a-z0-9]/g, "-");
        const groupId = "tree-switcher-group-" + groupIdPart;
        const groupListId = "tree-switcher-group-list-" + groupIdPart;

        const label = document.createElement("label");
        label.id = groupId;
        label.setAttribute("for", groupListId);
        label.classList.add("context-menu-group-label");
        label.textContent = group.name;
        column.push({
          label,
        });
        columnBox.append(label);

        const list = document.createElement("ul");
        list.id = groupListId;
        list.setAttribute("role", "group");
        for (const item of group.items) {
          const label = item.label ? item.label : item.value;
          const tree = item.value;

          const li = this.createMenuItem({
            html: label,
            classNames: ["indent"],
            href: document.location.pathname.replace(/^\/[^\/]+\//, `/${tree}/`)
              + document.location.search
              + document.location.hash,
            attrs: {
              "data-tree": tree,
            },
          });

          li.setAttribute("aria-labelledby", groupId);

          list.append(li);
          column.push({
            link: li.firstChild,
          });
        }
        columnBox.append(list);
      }
      columns.push(column);
      columnBoxes.push(columnBox);
    }

    this.columns = columns;

    this.menu.replaceChildren(...columnBoxes);
  }

  getCurrentTree() {
    const m = document.location.pathname.match(/^\/([^\/]+)\//);
    if (m) {
      return m[1];
    }

    // Fallback
    return "mozilla-central";
  }

  focusCurrentTree() {
    const tree = this.getCurrentTree();
    const item = this.menu.querySelector(`a[data-tree="${tree}"]`);
    if (!item) {
      this.menu.focus();
    }

    this.focusItem(item);
  }
});

```

## static/css/font-icons.css
```
@font-face {
  font-family: 'icons';
  src: url('../font/icons.eot?28110584');
  src: url('../font/icons.eot?28110584#iefix') format('embedded-opentype'),
       url('../font/icons.woff2?28110584') format('woff2'),
       url('../font/icons.woff?28110584') format('woff'),
       url('../font/icons.ttf?28110584') format('truetype'),
       url('../font/icons.svg?28110584#icons') format('svg');
  font-weight: normal;
  font-style: normal;
}
/* Chrome hack: SVG is rendered more smooth in Windozze. 100% magic, uncomment if you need it. */
/* Note, that will break hinting! In other OS-es font will be not as sharp as it could be */
/*
@media screen and (-webkit-min-device-pixel-ratio:0) {
  @font-face {
    font-family: 'icons';
    src: url('../font/icons.svg?28110584#icons') format('svg');
  }
}
*/
[class^="icon-"]:before, [class*=" icon-"]:before {
  font-family: "icons";
  font-style: normal;
  font-weight: normal;
  speak: never;

  display: inline-block;
  text-decoration: inherit;
  width: 1em;
  margin-right: .2em;
  text-align: center;
  /* opacity: .8; */

  /* For safety - reset parent styles, that can break glyph codes*/
  font-variant: normal;
  text-transform: none;

  /* fix buttons height, for twitter bootstrap */
  line-height: 1em;

  /* Animation center compensation - margins should be symmetric */
  /* remove if not needed */
  margin-left: .2em;

  /* you can be more comfortable with increased icons size */
  /* font-size: 120%; */

  /* Font smoothing. That was taken from TWBS */
  -webkit-font-smoothing: antialiased;
  -moz-osx-font-smoothing: grayscale;

  /* Uncomment for 3D effect */
  /* text-shadow: 1px 1px 1px rgba(127, 127, 127, 0.3); */
}

.icon-cog:before { content: '\e800'; } /* 'î €' */
.icon-down-dir:before { content: '\e801'; } /* 'î ' */
.icon-right-dir:before { content: '\e802'; } /* 'î ‚' */
.icon-left-dir:before { content: '\e803'; } /* 'î ƒ' */
.icon-ok:before { content: '\e804'; } /* 'î „' */
.icon-search:before { content: '\e805'; } /* 'î …' */
.icon-export:before { content: '\e806'; } /* 'î †' */
.icon-font:before { content: '\e807'; } /* 'î ‡' */
.icon-brush:before { content: '\e808'; } /* 'î ˆ' */
.icon-tasks:before { content: '\f0ae'; } /* 'ï‚®' */
.icon-docs:before { content: '\f0c5'; } /* 'ïƒ…' */
.icon-export-alt:before { content: '\f14d'; } /* 'ï…' */

```

## static/css/icons.css
```
/* icons */
.deprecated-icon.folder, .deprecated-icon-folder {
  background: transparent url(../icons/mimetypes/folder.png) 0.5em 7px
    no-repeat;
  padding: 0.5em 0.5em 0.5em 30px; /* 30px = 16px icon + 7px padding above + a little more whitespace */
}

.folder-content {
  clear: both;
}

.mimetype-fixed-container {
  background-color: transparent;
  background-position: 0.5em center;
  background-repeat: no-repeat;
  background-size: 16px 16px;
  padding: 0.5em 0.5em 0.5em 30px;
  /* default to the unknown icon if there is no specific filetype selector;
     this has the same precedence as the mimtype-icon-* values so this must
     be defined before the per-file icon selectors. */
     background-image: url(../icons/mimetypes/unknown.png);
}

.mimetype-floating-container {
  background-color: transparent;
  background-repeat: no-repeat;
  float: right;
  padding: 8px; /* Makes the container 16x16 in order to show full icon. */
  /* same rationale as for .mimetype-fixed-container */
  background-image: url(../icons/mimetypes/unknown.png);
}
.mimetype-bullet {
  background-color: transparent;
  background-repeat: no-repeat;
  display: inline-block;
  padding: 8px; /* Makes the container 16x16 in order to show full icon. */
  vertical-align: text-bottom;
  /* same rationale as for .mimetype-fixed-container */
  background-image: url(../icons/mimetypes/unknown.png);
}

.mimetype-no-icon {
  background-image: none;
}

/* Previously the panel button rows were getting their padding from ".icon"
   which is now mimetype-fixed-container, but that was semantically sketchy and
   we needed the class name to be clearly about font icons with containers
   identified separately. */
.panel .item {
  padding: 0.5em 0.5em 0.5em 30px;
  background-color: transparent;
}

.panel label.panel-accel {
  display: block;
  width: 240px;
  padding: 0 0.5rem;
}

.panel .accel,
.panel .copy {
  font-size: 10px;
  border-radius: 2px;
  padding: 1px 3px;
  margin: 0 10px;
  border: 1px solid var(--page-border-color);
  background-color: var(--button-background);
  color: var(--button-color);
}

.panel .accel {
  /* simulate a keyboard key effect */
  font-family: monospace;
  box-shadow: 0 1px rgba(0, 0, 0, .8);
}

.panel .copy {
  height: 18px;
  width: 18px;
  padding: 1px;
  margin: 0 3px;
  cursor: pointer;
  vertical-align: middle;
}

.panel .copy.indicator {
  display: inline-block;
  cursor: unset;
}

.panel .copy:hover {
  background-color: var(--button-hover-background);
}

.panel .copy:active {
  background-color: var(--button-active-background);
}

.panel .copy svg {
  fill: currentColor;
}

.panel .copy:not(.copied) .tick-icon,
.panel .copy.copied .copy-icon {
  display: none;
}

/* MimeType Icons */
.mimetype-icon-build {
  background-image: url(../icons/mimetypes/build.png);
}
.mimetype-icon-c {
  background-image: url(../icons/mimetypes/c.png);
}
.mimetype-icon-conf {
  background-image: url(../icons/mimetypes/conf.png);
}
.mimetype-icon-cpp {
  background-image: url(../icons/mimetypes/cpp.png);
}
.mimetype-icon-cs {
  background-image: url(../icons/mimetypes/cs.png);
}
.mimetype-icon-css {
  background-image: url(../icons/mimetypes/css.png);
}
.mimetype-icon-diff {
  background-image: url(../icons/mimetypes/diff.png);
}
.mimetype-icon-folder {
  background: transparent url(../icons/mimetypes/folder.png) 0.5em 7px no-repeat;
  padding: 0.5em 0.5em 0.5em 30px; /* 30px = 16px icon + 7px padding above + a little more whitespace */
}
.mimetype-icon-h {
  background-image: url(../icons/mimetypes/h.png);
}
.mimetype-icon-html {
  background-image: url(../icons/mimetypes/html.png);
}
.mimetype-icon-iso {
  background-image: url(../icons/mimetypes/iso.png);
}
.mimetype-icon-image {
  background-image: url(../icons/mimetypes/image.png);
}
.mimetype-icon-java {
  background-image: url(../icons/mimetypes/java.png);
}
.mimetype-icon-js, .mimetype-icon-jsm, .mimetype-icon-mjs {
  background-image: url(../icons/mimetypes/js.png);
}
.mimetype-icon-mm {
  background-image: url(../icons/mimetypes/mm.png);
}
.mimetype-icon-php {
  background-image: url(../icons/mimetypes/php.png);
}
.mimetype-icon-py {
  background-image: url(../icons/mimetypes/py.png);
}
.mimetype-icon-rb {
  background-image: url(../icons/mimetypes/rb.png);
}
.mimetype-icon-sh {
  background-image: url(../icons/mimetypes/sh.png);
}
.mimetype-icon-svg {
  background-image: url(../icons/mimetypes/svg.png);
}
.mimetype-icon-tex {
  background-image: url(../icons/mimetypes/tex.png);
}
.mimetype-icon-txt {
  background-image: url(../icons/mimetypes/txt.png);
}
.mimetype-icon-ui {
  background-image: url(../icons/mimetypes/ui.png);
}
.mimetype-icon-unknown {
  background-image: url(../icons/mimetypes/unknown.png);
}
.mimetype-icon-vs {
  background-image: url(../icons/mimetypes/vs.png);
}
.mimetype-icon-xml {
  background-image: url(../icons/mimetypes/xml.png);
}

```

## static/css/mozsearch.css
```
/**
 * normalize - Remove most spacing between table cells.
 */
table {
  border-collapse: collapse;
  border-spacing: 0;
}

/* apply a natural box layout model to all elements */
* {
  box-sizing: border-box;
}

:root {
  --page-background: #fff;
  --page-foreground: #333;
  --page-subtle-foreground: #666;
  --page-border-color: #333;

  --button-color: var(--page-color);
  --button-background: #f5f5f5;
  --button-hover-background: white;
  --button-active-background: #d5d5d5;

  --link-color: #0095dd;
  --link-hover-color: #00539f;

  --search-box-background-image: linear-gradient(to bottom, #f8f8f8, #eaeaea);
  --revision-text-color: #656565;

  --panel-header-background: #333;
  --panel-header-color: #fff;

  --list-hover-background: #e0e0e0;
  --list-subtle-background: var(--line-stuck-background);
  --list-hover-color: var(--page-foreground);

  --info-box-test-skip-info-background: #eee;
  --info-box-error-background: #ffeeee;
  --info-box-warning-background: #ffffee;
  --info-box-info-background: #eeffee;

  --table-header-color: #555;
  --table-header-background: #f5f5f5;
  --table-even-row-background: #f5f5f5;

  --blame-popup-background: #404040;
  --blame-popup-color: #fff;
  --blame-light-gray: lightgray;
  --blame-dark-gray: darkgray;

  --context-menu-new-section: #ccc;
  --context-menu-same-section: #f8f8f8;

  --line-highlight-background: rgb(255, 255, 204);
  --line-stuck-background: #ffe;

  --syntax-type-color: teal;
  --syntax-comment-color: darkred;
  --syntax-reserved-color: blue;
  --syntax-string-color: green;
  --syntax-regex-color: #6d7b8d;
  --syntax-symbol-highlight: yellow;

  /* Our red-blue color scheme is from
   * https://colorbrewer2.org/#type=diverging&scheme=RdBu&n=11 and was chosen
   * for being "color blind friendly" while also being reasonably intuitive thanks
   * to a correspondence to the stereotypical red/green color theme (that's not
   * color blind friendly!). */
  --cov-interpolated-background: #f7f7f7;
  --cov-miss-background: #f4a582;
  --cov-hit-background-1: #d1e5f0;
  --cov-hit-background-2: #92c5de;
  --cov-hit-background-3: #4393c3;
  --cov-hit-background-4: #2166ac;
  --cov-hit-background-5: #053061;

  --cov-hit-unhovered-background: var(--cov-hit-background-1);
  --cov-miss-unhovered-background: var(--cov-miss-background);

  --diff-minus-line-background: rgb(255, 204, 204);
  --diff-plus-line-background: rgb(153, 204, 255);

  --result-context-color: royalblue;

  --select-arrow-svg: url("../images/select-arrow-light.svg");

  --diagram-depth-hue: 270deg;

  --base-font-size: 12px;
  --base-line-height: 1.5;
  --source-line-height: 1.3;

  --search-box-padding: 0.8rem;
  --search-box-old-rev-padding-bottom: 0.4rem;

  --revision-padding-top: 0.2rem;

  --checkbox-margin: 3px;
  --checkbox-height: 14px;

  --search-box-fieldset-min-height: calc(
    2 * (
      var(--checkbox-margin)
      + var(--checkbox-height)
      + var(--checkbox-margin)
    )
  );
}

@supports (color-scheme: dark) {
@media (prefers-color-scheme: dark) {
  :root {
    --page-background: rgb(28, 27, 34);
    --page-foreground: rgb(251, 251, 254);
    --page-subtle-foreground: #ccc;
    --page-border-color: ThreeDLightShadow;

    --search-box-background-image: linear-gradient(to bottom, #444, #333);
    --revision-text-color: #ddd;

    --button-background: rgb(43, 42, 51);
    --button-hover-background: rgb(82, 82, 94);
    --button-active-background: rgb(91, 91, 102);

    --link-color: #1897db;
    --link-hover-color: #016cbf;

    --list-hover-background: var(--button-hover-background);

    --info-box-test-skip-info-background: #161b22;
    --info-box-error-background: #57161c;
    --info-box-warning-background: #664d03;
    --info-box-info-background: #0a3722;

    --table-header-color: var(--page-foreground);
    --table-header-background: rgb(43, 42, 51);
    --table-even-row-background: rgb(34, 37, 44);

    --blame-light-gray: #595959;
    --blame-dark-gray: #393939;

    --context-menu-new-section: #444;
    --context-menu-same-section: #181818;

    --line-highlight-background: #3c3111;
    --line-stuck-background: #221c09;

    --syntax-comment-color: GrayText;
    --syntax-symbol-highlight: #5d4d1d;
    /* Shamelessly taken from source.chromium.org's dark theme */
    --syntax-type-color: #79a9c4;
    --syntax-reserved-color: #d8884b;
    --syntax-string-color: #97c67b;
    --syntax-regex-color: #b98eff;

    /* Hit colors from the dark bits from https://colorbrewer2.org/#type=sequential&scheme=Blues&n=9
     * Miss background from the same link as the light colors (the darkest red in there). */
    --cov-interpolated-background: #595959;
    --cov-miss-background: #67001f;
    --cov-hit-background-1: #6baed6;
    --cov-hit-background-2: #4292c6;
    --cov-hit-background-3: #2171b5;
    --cov-hit-background-4: #08519c;
    --cov-hit-background-5: #08306b;

    --diff-minus-line-background: #42161c;
    --diff-plus-line-background: #172c47;

    --result-context-color: #7892df;

    --select-arrow-svg: url("../images/select-arrow-dark.svg");

    color-scheme: dark;
  }
}
}

@media (forced-colors) {
  :root {
    --blame-popup-background: Canvas;
    --blame-popup-color: CanvasText;
    --blame-light-gray: ButtonFace;
    --blame-dark-gray: ButtonText;
  }
}

:root {
  font: var(--base-font-size)/var(--base-line-height) Arial, Helvetica, sans-serif;

  background-color: var(--page-background);
  color: var(--page-foreground);
}

body {
  margin: 0;
}

.context-menu .confidence-cppTemplateHeuristic {
  background-color: var(--list-subtle-background);
}

/* Link styling */
a {
  color: var(--link-color);
  text-decoration: none;
}

a:hover {
  color: var(--link-hover-color);
  text-decoration: underline;
}

/* Some links look like list items */
.panel section .item,
.folder-content a {
  text-decoration: none;
  color: unset;
}

/* .folder-content is handled with the whole row getting hover styles, so it doesn't need this. */
.panel section .item:is(:hover, :focus),
.context-menu a:is(:hover, :focus) {
  background-color: var(--list-hover-background);
  color: var(--list-hover-color);
  outline: none;
}

/* Others look more seamless (they only show underlines when hovered) */
.file a,
.results tr:where(:not(.result-head)) a {
  color: unset;
}

/* Some links always show underlines, but use the underlying text color */
#revision a,
.info-box a,
.blame-popup a {
  color: unset;
  text-decoration: underline;
}

#content details {
  margin-left: 2rem;
}

/* summary headings should not start a new block and should not have crazy big margins */
details > summary > h1,
details > summary > h2,
details > summary > h3,
details > summary > li > h3 {
  display: inline-block;
  margin: 4px 0px;
}
details > summary > li {
  display: inline-block;
  margin-left: 10px;
}

details > summary > h1 {
  font-size: 16px;
}
details > summary > h2 {
  font-size: 14px;
}
details > summary > h3,
details > summary > li > h3 {
  font-size: var(--base-font-size);
}


h4 {
  margin: 0 0 0.5rem 0;
}
caption {
  text-align: left;
}
table {
  margin: 0;
  padding: 0;
  width: 100%;
  overflow: auto;
}
table.folder-content {
  margin: 0.5rem 2rem;
  width: calc(100% - 4rem);
}
table#file {
  width: 100%;
}
table.folder-content thead tr:first-child {
  color: var(--table-header-color);
  background-color: var(--table-header-background);
  padding: 0.5em;
}
table th {
  font-weight: bold;
  padding: 0.5em;
  text-align: left;
}
.folder-content th:first-child {
  padding-left: 0.5em;
}
.folder-content th {
  padding-left: 0;
  padding-right: 0.5em;
  padding-top: 0.2em;
  padding-bottom: 0.2em;
}
table.folder-content tr:nth-child(even) {
  background-color: var(--table-even-row-background);
}
table.folder-content tbody tr:hover,
table tbody tr:hover {
  background-color: var(--list-hover-background);
  color: var(--list-hover-color);
}
table.folder-content td {
  padding: 0;
}
table.folder-content a {
  display: block; /* Make entire cells clickable. */
  padding-top: 0.5em;
  padding-right: 0.5em;
  padding-bottom: 0.5em;
}
table td {
  padding: 0.5em;
}
table.folder-content td.description {
  max-width: 60vw;
  padding-right: 2em;
}
table.folder-content td.description a {
  white-space: nowrap;
  overflow: hidden;
  text-overflow: ellipsis;
}
td.code {
  padding: 0 0 0 0.5em;
}
td#line-numbers {
  padding: 0;
}

.source-line-with-number {
  padding: 0rem 1rem;
  line-height: var(--source-line-height);
  display: flex;
}

.line-number::before {
  content: attr(data-line-number);
}

.line-number {
  display: inline-block;
  cursor: pointer;
  text-align: right;
  padding: 0 0.5rem;
  /* this was originally set on the ".file td:first-child" */
  color: #aaa;
  width: 8ex;
  flex-shrink: 0;
  /* This prevents shift-clicking in non-gecko browsers from selecting all
   * text in between the lines */
  -moz-user-select: none;
  -webkit-user-select: none;
  -ms-user-select: none;
  user-select: none;
}
.highlighted {
  background: none repeat scroll content-box 0 0 var(--line-highlight-background) !important;
}
.source-line {
  margin: 0;
  padding: 0;
  display: inline-block;
  white-space: pre;
  flex-grow: 1;
}

.source-line-with-number.stuck {
  background-color: var(--line-stuck-background);
}
.source-line-with-number.last-stuck {
  box-shadow: 0px 4px 8px rgba(0, 0, 0, .9);
}

.source-line a,
g[data-symbols] {
  cursor: pointer;
}
.code code {
  padding: 0 0.5rem;
  width: 100%;
}
.code code a {
  cursor: context-menu;
}
mark {
  position: relative;
  padding: 0.5rem;
}

/**
 * New fixed header mechanism!
 *
 * Previously, the search box stayed stuck at the top of the screen via
 * use of "position: fixed".  This was problematic with browser default
 * anchor positioning, but became untenable when we wanted to start using
 * "position: sticky" to show nesting scopes because we needed to specify "top"
 * positions for each nested sticky level, and it was hard to know for sure how
 * big the search box would actually be without involving JS.
 *
 * So our solution is to use flexbox to create a non-scrolling fixed-header,
 * allotting the remaining space to a scrolling region.
 */

html {
  height: 100%;
}
body {
  height: 100%;
  overflow: hidden;
  display: flex;
  flex-direction: column;
}

#fixed-header {
  flex-shrink: 0;
  display: inline-flex;
}

#scrolling {
  flex-grow: 1;
  overflow-y: auto;
}

/**
 * Nesting!
 */

.nesting-depth-0 {
  --nesting-level: 0;
}
.nesting-depth-1 {
  --nesting-level: 1;
}
.nesting-depth-2 {
  --nesting-level: 2;
}
.nesting-depth-3 {
  --nesting-level: 3;
}
.nesting-depth-4 {
  --nesting-level: 4;
}
.nesting-depth-5 {
  --nesting-level: 5;
}
.nesting-depth-6 {
  --nesting-level: 6;
}
.nesting-depth-7 {
  --nesting-level: 7;
}
.nesting-depth-8 {
  --nesting-level: 8;
}
.nesting-depth-9 {
  --nesting-level: 9;
}

.nesting-sticky-line {
  /* be sticky, contained within a .nesting-container */
  position: sticky;
  /* even with our "stuck" and "last-stuck" classes, it's important to have an
   opaque background color for these because those classes don't take effect
   immediately. */
  background-color: var(--page-background-color);
  top: calc(var(--nesting-level) * var(--base-font-size) * var(--source-line-height));
  z-index: calc(100 - var(--nesting-level));
}
/* Search results */

.search-result-header {
  padding: 0 2rem;
}

table.results {
  margin: 0.5rem 2rem;
  width: calc(100% - 4rem);
}

/* Make context lines more subtle. */
.results tr.before-context-line,
.results tr.after-context-line {
  opacity: 0.5;
}

/* Delineate the transition between consecutive groups. */
tr.after-context-line + tr.before-context-line {
  border-top: 1px solid gray;
}

.results tr:hover,
.file tr:hover {
  background: none;
}
.results tr,
.file tr {
  border: 0;
  line-height: var(--source-line-height);
}
.file tr .line-number,
.file tr .code pre code {
  /* don't let bolded code increase the line-height - see bug 1322740 comment 9 */
  max-height: calc(var(--base-font-size) * var(--source-line-height));
}
.left-column,
.file td:first-child {
  text-align: right;
  color: #aaa;
  width: 8ex;
}
.results td {
  padding: 0.1rem 0.5rem;
}
.results a {
  cursor: pointer;
  display: inline-block;
}
.file a:target {
  background-color: gray;
  color: white;
  padding: 0 0.5rem;
}
.results code {
  white-space: pre;
}
.result-head td {
  padding-top: 0.3rem; /* Additional padding to visually separate results.*/
  padding-bottom: 0;
}
.result-head td.left-column {
  padding-right: 0.2rem; /* Moves the icon closer to the path by reducing the padding.*/
}

.result-context {
  font-style: italic;
  margin-left: 10px;
  padding-left: 4px;
  padding-right: 4px;
  color: var(--result-context-color);
}

.result-upsearch {
  padding-left: 8px;
  color: var(--link-color);
}

.section {
  text-align: right;
  font-size: 150%;
}

.result-pathkind {
  font-size: 150%;
}

.result-kind {
  font-size: var(--base-font-size);
  font-weight: 800;
}

.expando {
  cursor: pointer;
  color: var(--page-foreground);
}
/* End search results */

.file td {
  padding: 0;
}
.file pre {
  margin: 0;
}

/* Context menu */
.context-menu {
  /* absolutely positioned within the containing block of the <body> relative
     to the click event */
  position: absolute;
  background-color: var(--page-background);
  color: var(--page-color);
  margin: 0;
  padding: 0;
  border: 1px solid var(--page-border-color);
  border-radius: 6px;
  list-style: none;
  z-index: 102;
  overflow: auto;
  transition: opacity .2s ease;
}
.context-menu:not(.bottom) {
  border-top-left-radius: 0;
}
.context-menu.bottom {
  border-bottom-left-radius: 0;
}
.context-menu:focus {
  outline: none;
}
.context-menu a {
  display: block;
}
.contextmenu-expansion-preview {
  max-width: 600px;
  overflow: hidden;
  text-overflow: ellipsis;
}
.context-menu code {
  white-space: nowrap;
}
.context-menu-group-label {
  display: block;
  padding: 0.5em;
  font-weight: bold;
}
.content {
  padding: 0.5rem 0rem;
}

/* Breadcrumbs */
.path-separator {
  margin: 0 0.2rem;
}
.breadcrumbs {
  display: inline-block;
  margin: 0;
  padding: 1rem;
  text-align: left;
}

#tree-switcher {
  pointer-events: auto;
  background-color: transparent;
  background-image: var(--select-arrow-svg);
  background-position: center center;
  background-repeat: no-repeat;
  appearance: none;
  border: none;
  vertical-align: middle;
  width: 24px;
  height: 24px;
  border-radius: 2px;
}
#tree-switcher:hover {
  background-color: var(--list-hover-background);
}
#tree-switcher:active {
  background-color: var(--button-active-background);
}

#tree-switcher-menu {
  display: flex;
  flex-direction: row;
}

#tree-switcher-menu ul {
  margin: 0;
  padding: 0;
  list-style: none;
}

#tree-switcher-menu ul + ul {
  border-inline-start: 1px solid var(--page-border-color);
}

#tree-switcher-menu .indent {
  padding-inline-start: 24px;
  padding-inline-end: 24px;
}

/* Footer */
.footer {
  color: #999;
  font-size: 1rem;
  margin: 1.2rem;
  justify-content: center;
}

/* Navigation panel */
.panel {
  /* (position:fixed is always relative to the viewport's initial containing
     block, so it doesn't matter where this node lives in the DOM, apart from
     accessibility tree purposes.) */
  position: fixed;
  /* We position the navigation panel just below the search box so that when
   * it is collapsed it exists in the margins of the page so that we can show
   * it on all pages.
   *
   * Previously it was styled to be floating on the page which was only
   * appropriate on source listing pages where it would usually be in the
   * right margin of the page. But now we want it visible on all pages.
   *
   * Note that on pages where we display the indexed revision as part of the
   * search box, this offset needs to be adjusted, and we do that below using
   * the ".old-rev" selector. */
  top: calc(
      var(--search-box-padding)
      + var(--search-box-fieldset-min-height)
      + var(--search-box-padding)
  );
  right: 22px;
  background-color: var(--page-background);
  border: 1px solid var(--page-border-color);
  min-width: 12rem;
  overflow-y: auto;
  overflow-x: hidden;
  max-height: calc(100% - 150px);
  z-index: 102;
}
:root.old-rev .panel {
  /* Adjust the navigation panel location for the presence of the indexed revision.
   * See above for more. */
  top: calc(
      var(--search-box-padding)
      + var(--search-box-fieldset-min-height)
      + var(--revision-padding-top)
      + var(--base-font-size) * var(--base-line-height)
      + var(--search-box-old-rev-padding-bottom)
  );
}
#panel-toggle {
  display: inline-block;
  background-color: var(--panel-header-background);
  color: var(--panel-header-color);
  margin: 0;
  padding: 0.5rem 0.2rem 0.5rem 0.7rem;
  border: 0;
  width: 100%;
  text-align: left;
  cursor: pointer;
}
#show-settings {
  float: right;
  color: var(--panel-header-color);
}
.navpanel-icon {
  display: inline-block;
  margin-right: 0.5rem;
  transform: rotate(-90deg);
  transition: all 0.1s;
}
.navpanel-icon.expanded {
  transform: rotate(0deg);
  transition: all 0.1s;
}
.panel h4 {
  margin: 0;
  padding: 0.5rem;
}
.panel ul {
  margin: 0;
  padding: 0;
  list-style: none;
}
.panel a.class {
  background-position: 4px 8px;
}
.panel a.method {
  background-position: 4px 6px;
}
.panel a.field {
  background-position: 2px 10px;
}
.panel section .item {
  display: inline-block;
  width: 100%;
  cursor: pointer;
}
.panel section button.item {
  font: unset;
  appearance: unset;
  border: unset;
  text-align: unset;
}
.panel section .item:disabled,
.panel section .item.disabled {
  opacity: 0.5;
  cursor: not-allowed;
}

.panel .selected-symbol-section {
  width: 240px;
  display: grid;
  grid-template-columns: 200px 30px;
  gap: 10px;
}

.panel .selected-symbol-section .selected-symbol-box {
  position: relative;
}

.panel .selected-symbol-section .selected-symbol-box .selected-symbol-ns {
  position: absolute;
  top: -1em;
  padding-inline-start: 26px;
  height: 0.8em;
  font-size: 0.8em;
  white-space: pre;
}
.panel .selected-symbol-section .selected-symbol-box .selected-symbol-local {
  padding-inline-start: 30px;
  overflow: hidden;
  text-overflow: ellipsis;
  white-space: pre;
}

.panel .selected-symbol-section .copy-box.disabled {
  opacity: 0.5;
  cursor: not-allowed;
}

/* Info Boxes */
.info-box {
  border: 1px solid rgba(0, 0, 0, .5);
  border-radius: 8px;
  padding: 0.8rem;
  margin: 0.5rem;
  font-size: 110%;
}
.info-box-error {
  background-color: var(--info-box-error-background);
}
.info-box-warning {
  background-color: var(--info-box-warning-background);
}
.info-box-info {
  background-color: var(--info-box-info-background);
}
.info-box ul {
  padding-inline-start: 20px;
  margin: 0;
}
.info-box .test-skip-info {
  font-family: monospace;
  background-color: var(--info-box-test-skip-info-background);
  padding: 5px;
  margin: -5px;
}

.no-results {
  clear: both;
  font-style: italic;
}
#fetch-results {
  display: none;
}

/* Hovering over symbol names */
span[data-symbols]:hover {
  cursor: pointer;
}
span[data-symbols].hovered,
span[data-symbols].selected {
  background-color: var(--syntax-symbol-highlight);
}

/* Help screen */
.intro, .settings-page {
  font: 14px/var(--base-line-height) Arial, Helvetica, sans-serif;
  margin-left: 10%;
  margin-right: 10%;
  margin-bottom: 10%;
}

.intro td,
.intro th {
  text-align: center;
  vertical-align: middle;
}
.intro td[yes]::before {
  content: "\2713";
}

.contextmenu-link:before {
  padding-right: 0.4em;
}
.context-menu .contextmenu-link {
  text-decoration: none;
  color: var(--page-subtle-foreground);
  padding: 0.5em;
}
.contextmenu-link > strong {
  color: var(--page-foreground);
  font-weight: normal;
}

.contextmenu-row ~ .contextmenu-row.contextmenu-same-section  {
  border-top: 1px solid var(--context-menu-same-section);
}

.contextmenu-row ~ .contextmenu-row.contextmenu-new-section  {
  border-top: 1px solid var(--context-menu-new-section);
}

/* ## Line number goto ##

   `createSyntheticAnchor` in code-highlighter.js creates a synthetic anchor
   element for the current hash string (which may be more than just "#200" and
   instead could be "#200-220" or "#220,225,227" or something like that) and
   gives it this class.

   The element is made a child of the .line-number element.

   See the method's documentation for more context.
*/
.goto {
  /* This needs to be large enough so that the "position: sticky"
     stuck lines don't obscure the line we're trying to display.
     Also, we add an extra line of padding to compensate for the box-shadow.
     Note: The default must be -1 * the offset for the non-stuck case.
   */
  scroll-margin-top: calc((var(--nesting-level, -2) + 2) * var(--base-font-size) * var(--source-line-height));
}

.nesting-container > .goto {
  /* The anchor for the first-line of a sticky container doesn't need to
     account for the sticky line itself, but still needs a line of padding
     for the box-shadow.  Disclaimer: This is only necessary for nesting
     levels >= 1 and results in an extra line of padding for == 0, but the
     complexity to fix it doesn't seem worth it.
   */
  scroll-margin-top: calc((var(--nesting-level) + 1) * var(--base-font-size) * var(--source-line-height));
}

/* ## Blame ## */
.blame-strip {
  display: block;
  width: 20px;
  height: 100%;
  padding: 0;
  margin: 0;
  touch-action: none; /* fallback for FF pre-85 which didn't support touch-action:pinch-zoom */
  touch-action: pinch-zoom;
  @media (forced-colors) {
    border-inline: 1px solid ButtonText;
    &:hover {
      background-color: SelectedItem;
      border-color: SelectedItemText;
    }
    &:active {
      border-color: ButtonText;
    }
  }
}
/* blame zebra stripes: we alternate colors whenever the revision a line is from
   changes between lines. */
.c1 {
  background-color: var(--blame-light-gray);
}
.c2 {
  background-color: var(--blame-dark-gray);
}

.blame-popup {
  position: absolute;
  /* positioned by BlamePopup in blame.js */
  top: 0;
  left: 0;
  padding: 10px;
  background-color: var(--blame-popup-background);
  color: var(--blame-popup-color);
  box-shadow: 3px 3px 3px rgba(0, 0, 0, .5);
  border-radius: 3px;
  text-align: left;
  z-index: 101;
  cursor: auto;
  @media (prefers-contrast) {
    border: 1px solid CanvasText;
  }
}
.blame-popup .blame-entry {
  width: 600px;
}
.blame-popup code {
  background-color: var(--page-background);
  color: var(--page-foreground);
  display: block;
  padding: 0.5em;
  white-space: pre;
  max-height: 80vh;
  overflow-y: scroll;
}
.deemphasize {
  color: GrayText;
}
.minus-line {
  background-color: var(--diff-minus-line-background);
}
.plus-line {
  background-color: var(--diff-plus-line-background);
}

/* Search box */
input,
select {
  padding: 0.5em;
}
input[placeholder],
input::placeholder {
  text-overflow: ellipsis;
}
#search-box #query {
  width: 100%;
}
#search-box #path {
  width: 100%;
}
#query {
  /* Leave room for spinner. */
  padding-right: 40px;
  @media (forced-colors) {
    background-color: Field;
    border: 1px solid ButtonText;
    color: FieldText;
  }
}
.in-progress #spinner {
  background: transparent url("../images/spinner-large.gif") 0 0 / contain
    no-repeat;
}
#spinner {
  display: inline-block;
  position: absolute;
  top: 10px;
  right: 10px;
  padding-left: 2rem;
  width: auto;
  bottom: 10px;
}
.access-key {
  text-decoration: underline;
}
#search-box {
  background-image: var(--search-box-background-image);
  border-bottom: 1px solid var(--page-border-color);
  padding: var(--search-box-padding);
  width: 100%;
}
#search-box > fieldset {
  min-height: var(--search-box-fieldset-min-height);
}

#search-box > fieldset {
  position: relative;
  margin: 0;
  padding: 0;
  border: 0;

  display: flex;
  flex-direction: row;
  align-items: center;
}
:root.old-rev #search-box {
  padding-bottom: var(--search-box-old-rev-padding-bottom);
}
#revision {
  padding-top: 0;
}
:root.old-rev #revision {
  padding-top: var(--revision-padding-top);
  color: var(--revision-text-color);
}
.v-flex-container {
  display: flex;
  flex-direction: column;
}
#query-section {
  flex: 6;
  position: relative;
}
#option-section {
  flex-grow: 0;
  flex-shrink: 0;
  flex-basis: auto;

  justify-content: center;
  padding: 0 2rem 0 0;
  /*min-width: 9rem;*/
  /*align-self: center;*/
}
#option-section input[type="checkbox"] {
  margin: var(--checkbox-margin);
  height: var(--checkbox-height);
}
#path-section {
  flex: 3;
}
/*http://developer.yahoo.com/blogs/ydn/posts/2012/10/clip-your-hidden-content-for-better-accessibility/*/
.visually-hidden {
  position: absolute;
  clip: rect(1px 1px 1px 1px); /* IE6, IE7 */
  clip: rect(1px, 1px, 1px, 1px);
  padding: 0;
  border: 0;
  height: 1px;
  width: 1px;
  overflow: hidden;
}
/* Message bubble */
.bubble {
  border: 1px solid #ccc;
  border-radius: 9px;
  color: #6c6c6c;
  display: none;
  font-size: 85%;
  padding: 0.1rem 30px 0.1rem 34px;
  position: absolute;
  top: 0.25rem;
}
/* The big triangle */
.bubble:before {
  content: "";
  position: absolute;
  border-style: solid;
  border-color: #ccc transparent;
  /* reduce the damage in FF3.0 */
  display: block;
  width: 0;

  top: -9px; /* value = - border-top-width - border-bottom-width */
  bottom: auto;
  right: auto;
  left: 11px; /* controls horizontal position */
  border-width: 0 9px 9px;
}
/* The small triangle */
.bubble:after {
  content: "";
  position: absolute;
  border-style: solid;
  border-color: #fff transparent;
  /* reduce the damage in FF3.0 */
  display: block;
  width: 0;

  top: -8px; /* value = - border-top-width - border-bottom-width */
  bottom: auto;
  right: auto;
  left: 11px; /* value = (:before left) + (:before border-left) - (:after border-left) */
  border-width: 0 9px 9px;
}
.bubble.info {
  background: white url(../images/info.gif) 12px 0 / 3ex no-repeat;
}
.bubble.warning {
  background: white url("../images/warning.gif") 10px -2px / 3.5ex no-repeat;
}
.bubble.error {
  background: white url(../images/error.gif) 12px 0 / 3ex no-repeat;
}
.zero-size-container {
  position: relative;
  height: 0;
  width: 100%;
}
@media only screen and (min-width: 740px) {
  .bubble {
    margin-right: 16%; /* 100% minus the above */
  }
}

@media only screen and (min-device-width: 320px) {
  .bubble {
    margin-right: 38%; /* 100% minus the above */
  }
  input[type="submit"] {
    margin-right: 14%;
  }
}

.svg-preview {
  padding: 0 1rem;
}

.svg-preview > h4 {
  margin-bottom: 0;
}

.svg-preview > a {
  display: block;
}

.svg-preview > a > img {
  min-height: 50px;
  max-height: 300px;
  min-width: 50px;
  max-width: 300px;
  box-sizing: content-box;
  border: 1px dashed #555;
  margin-top: 1em;
  margin-bottom: 1em;
}

#svg-preview-background-checkerboard:checked ~ a > img {
  background-image: linear-gradient(
      45deg,
      #555 25%,
      transparent 25%,
      transparent 75%,
      #555 75%,
      #555 100%
    ),
    linear-gradient(
      45deg,
      #555 25%,
      transparent 25%,
      transparent 75%,
      #555 75%,
      #555 100%
    );
  background-size: 10px 10px;
  background-position: 0 0, 5px 5px;
}

#svg-preview-background-light:checked ~ a > img {
  background-color: #d8d8d8;
  color-scheme: light;
}

#svg-preview-background-dark:checked ~ a > img {
  background-color: #181818;
  color-scheme: dark;
}

.syn_def {
  font-weight: 600;
}

.syn_type {
  color: var(--syntax-type-color);
}

.syn_string {
  color: var(--syntax-string-color);
}

.syn_comment {
  color: var(--syntax-comment-color);
}

.syn_tag,
.syn_reserved {
  color: var(--syntax-reserved-color);
}

.syn_regex {
  color: var(--syntax-regex-color);
}

/* ## Code Coverage Styling ## */

/* Color themes are currently from the Color Brewer 2.0 Project which is for
 * cartography and other uses.  Themes below are using "diverging"
 * "colorblind safe" 11-count colors.  The (middle) 6th color is used as the
 * uncovered foreground color, the adjacent (lightest) 5th and 7th colors used
 * for background colors, and 2nd and 10th colors used for foreground colors
 * unless otherwise noted.
 *
 * Our visual presentation of coverage data has two modes:
 * 1. The user is not hovering over coverage cells.  In this case, we want
 *    as little visual noise as possible so we display interpolated coverage
 *    data with only hits and misses (and uncovered at the top and bottom where
 *    interpolation is not appropriate.)
 * 2. The user is hovering over a coverage cell.  In this case we aren't
 *    concerned about the coverage data being distracting and we want to show
 *    more detail.  We no longer show interpolated values and we also vary the
 *    color of hits so that lines with more coverage hits are shown with darker
 *    colors.
 *
 * To this end, we have two variables --cov-hit-unhovered-background and
 * --cov-miss-unhovered-background that have valid color values when we're not
 * hovering and are made to be empty when we're hovering, allowing us to fail
 * over to a second set of colors.
 */
:root.coverage-details-shown {
  --cov-miss-unhovered-background: unset;
  --cov-hit-unhovered-background: unset;
}

.cov-strip {
  display: block;
  width: 5px;
  height: 100%;
  padding: 0;
  margin: 0;
  touch-action: none; /* fallback for FF pre-85 which didn't support touch-action:pinch-zoom */
  touch-action: pinch-zoom;
}

.cov-interpolated.cov-miss {
  background-color: var(--cov-miss-unhovered-background, var(--cov-interpolated-background));
}
.cov-interpolated.cov-hit {
  background-color: var(--cov-hit-unhovered-background, var(--cov-interpolated-background));
}

.cov-known.cov-miss {
  background-color: var(--cov-miss-background);
}

.cov-log10-0,
.cov-log10-1 {
  background-color: var(--cov-hit-unhovered-background, var(--cov-hit-background-1));
}
.cov-log10-2,
.cov-log10-3 {
  background-color: var(--cov-hit-unhovered-background, var(--cov-hit-background-2));
}
.cov-log10-4,
.cov-log10-5 {
  background-color: var(--cov-hit-unhovered-background, var(--cov-hit-background-3));
}
.cov-log10-6,
.cov-log10-7 {
  background-color: var(--cov-hit-unhovered-background, var(--cov-hit-background-4));
}
.cov-log10-8,
.cov-log10-9 {
  background-color: var(--cov-hit-unhovered-background, var(--cov-hit-background-5));
}

/* ## Class Diagram Styling ## */
/* Everything about diagrams needs some kind of dark mode support.
 *
 * Our diagram pointer background colors come from the ColorBrewer2 diverging
 * PiYG scheme with 11 steps: https://colorbrewer2.org/#type=diverging&scheme=PiYG&n=11
 */
g.ptr-strong > polygon {
  fill: #7fbc41;
}
g.ptr-unique > polygon {
  fill: #b8e186;
}
g.ptr-weak > polygon {
  fill: #fde0ef;
}
g.ptr-raw > polygon {
  fill: #f1b6da;
}
g.ptr-ref > polygon, g.ptr-contains > polygon {
  fill: #e6f5d0;
}

g.hovered-cur-node > polygon {
  fill: #fbf;
}
g.hovered-out-node > polygon {
  fill: #bbf;
}
g.hovered-in-node > polygon {
  fill: #bfb;
}

g.hovered-cur-edge > path:not(.clicktarget),
g.hovered-cur-edge > polygon {
  stroke: #eae;
  stroke-opacity: 1;
  stroke-width: 2px;
}

g.edge > path.clicktarget {
  stroke:rgba(0,0,0,0);
  stroke-width: 10px;
}

g.edge.hovered-out-edge > path,
g.edge.hovered-out-edge > polygon {
  stroke: blue;
  stroke-width: 2px;
  stroke-opacity: 1;
}
g.edge.hovered-in-edge > path,
g.edge.hovered-in-edge > polygon {
  stroke: green;
  stroke-width: 2px;
  stroke-opacity: 1;
}

g.edge > path {
  stroke-opacity: 0.2;
}

g.blurry g.edge polygon,
g.blurry g.edge path {
  stroke: none
}

g.blurry g.node text {
  fill: none;
}

g.blurry g.cluster polygon {
  stroke: none;
}
g.blurry g.cluster text {
  fill: none;
}

/* The depth mechanism was initially introduce for experimental metablob
 * blurring, but we can use the 0 depth to help emphasize the roots of the
 * graph without the blurring and in a way that doesn't conflict with uses of
 * color.
 */
.diagram-depth-0 {
  --diagram-depth: 0;
  stroke-width: 2;
}
.diagram-depth-1 {
  --diagram-depth: 1;
}
.diagram-depth-2 {
  --diagram-depth: 2;
}
.diagram-depth-3 {
  --diagram-depth: 3;
}
.diagram-depth-4 {
  --diagram-depth: 4;
}
.diagram-depth-5 {
  --diagram-depth: 5;
}
.diagram-depth-6 {
  --diagram-depth: 6;
}
.diagram-depth-7 {
  --diagram-depth: 7;
}
.diagram-depth-8 {
  --diagram-depth: 8;
}
.diagram-depth-9 {
  --diagram-depth: 9;
}
.diagram-depth-10 {
  --diagram-depth: 10;
}
.diagram-depth-11 {
  --diagram-depth: 10;
}
.diagram-depth-12 {
  --diagram-depth: 10;
}
.diagram-depth-13 {
  --diagram-depth: 10;
}

g.blurry.depth-mode g.node polygon {
  fill: hwb(var(--diagram-depth-hue) calc(40% + 60% * var(--diagram-depth) / 3) 0%);
  stroke: hwb(var(--diagram-depth-hue) calc(40% + 60% * var(--diagram-depth) / 3) 0%);
}

g.true-diag g.node polygon {
  fill: white;
}

/* Field Layout */

#symbol-tree-table-col-selector,
#symbol-tree-table-list {
  padding: 0 2rem;
}

#symbol-tree-table-list .type-cell {
  display: none;
}

#symbol-tree-table-list.hide-name .name-cell {
  display: none;
}

#symbol-tree-table-list.show-type .type-cell {
  display: table-cell;
}

#symbol-tree-table-list.hide-line .line-cell {
  display: none;
}

.symbol-tree-table {
  border-collapse: separate;
}

.symbol-tree-table .field-type {
  display: inline-block;
  max-width: 40vw;
}

.symbol-tree-table .class-size {
  font-weight: bold;
}

.symbol-tree-table .class-size,
.symbol-tree-table .field-offset,
.symbol-tree-table .field-size,
.symbol-tree-table .field-hole,
.symbol-tree-table .field-padding {
  white-space: pre;
}

.symbol-tree-table + .symbol-tree-table {
  margin-top: 2em;
}

.symbol-tree-table h3 {
  margin: 0;
}

.symbol-tree-table td, .symbol-tree-table th {
  border-color: var(--page-border-color);
  border-style: solid;
  border-width: 0px;
}

.symbol-tree-table tr:first-child th {
  border-block-start-width: 1px;
}

.symbol-tree-table th,
.symbol-tree-table td {
  border-block-end-width: 1px;
  border-inline-start-width: 1px;
}

.symbol-tree-table td.base-class-false {
  border-block-end-width: 0px;
}

.symbol-tree-table td.class-size-cell {
  border-inline-start-width: 0px;
}

.symbol-tree-table tr.class-alignment-and-size td {
  border-inline-start-width: 0px;
}

#symbol-tree-table-list:not(.hide-name) tr.class-alignment-and-size td.name-cell {
  border-inline-start-width: 1px;
}
#symbol-tree-table-list.hide-name.show-type tr.class-alignment-and-size td.type-cell {
  border-inline-start-width: 1px;
}
#symbol-tree-table-list.hide-name:not(.show-type):not(.hide-line) tr.class-alignment-and-size td.line-cell {
  border-inline-start-width: 1px;
}

.symbol-tree-table th:last-child,
.symbol-tree-table td:last-child {
  border-inline-end-width: 1px;
}

.symbol-tree-table tr:first-child th:first-child,
#symbol-tree-table-list.hide-name.show-type tr:first-child th.type-cell,
#symbol-tree-table-list.hide-name:not(.show-type) tr:first-child th.line-cell {
  border-start-start-radius: 4px;
}
.symbol-tree-table tr:first-child th:last-child {
  border-start-end-radius: 4px;
}
.symbol-tree-table tr:last-child td:first-child,
#symbol-tree-table-list.hide-name.show-type tr:last-child td.type-cell,
#symbol-tree-table-list.hide-name:not(.show-type) tr:last-child td.line-cell {
  border-end-start-radius: 4px;
}
.symbol-tree-table tr:last-child td:last-child {
  border-end-end-radius: 4px;
}

.symbol-tree-table th {
  color: var(--table-header-color);
  background-color: var(--table-header-background);
}

#symbol-tree-table-col-selector {
  text-align: end;
}

#symbol-tree-table-col-selector label {
  display: inline-block;
}

#symbol-tree-table-col-selector label + label {
  margin: 0 0 0 8px;
}

/* Query result debugging */

#query-debug-results-json[aria-hidden] {
  display: none;
}

```

## tools/src/symbol_graph_edge_kind.rs
```
#[derive(Clone)]
pub enum EdgeKind {
    Default, // solid line, closed arrow ("normal")
    // These value are meant to be UML-ish
    Inheritance,    // solid line, open arrow ("onormal")
    Implementation, // dashed line, open arrow ("onormal")
    Composition,    // solid line, closed diamond ("diamond")
    Aggregation,    // solid line, open diamond ("odiamond")
    // These are more specific searchfox concepts
    IPC,           // dotted line, weird vee arrow ("vee")
    CrossLanguage, // JNI-like; solid line, left-half-closed arrow ("lnormal")
}

```

## tools/src/output.rs
```
/**
 * Common rust HTML output logic.
 **/
use std::io::Write;
use std::path::Path;

extern crate chrono;
use crate::file_format::analysis_manglings::make_file_sym_from_path;
use crate::url_encode_path::url_encode_path;

use self::chrono::{DateTime, Local};

pub struct Options<'a> {
    pub title: &'a str,
    pub tree_name: &'a str,
    pub revision: Option<(&'a str, &'a str)>,
    pub include_date: bool,
    /// Extra classes to include on the content element.  This allows less padding to be used on
    /// source listings where we have particular styling needs for "position: sticky" but want
    /// every other display to have normal padding.
    pub extra_content_classes: &'a str,
}

pub fn choose_icon(path: &str) -> String {
    let ext: &str = match Path::new(path).extension() {
        Some(ext) => ext.to_str().unwrap(),
        None => "",
    };
    if ext == "jsm" {
        return "js".to_string();
    }
    if ["cpp", "h", "c", "js", "py"]
        .iter()
        .any(|x: &&str| *x == ext)
    {
        return ext.to_string();
    }
    "".to_string()
}

pub fn file_url(opt: &Options, path: &str) -> String {
    format!("/{}/source/{}", opt.tree_name, url_encode_path(path))
}

pub fn generate_breadcrumbs(
    opt: &Options,
    writer: &mut dyn Write,
    path: &str,
    generate_symbol: bool,
) -> Result<(), &'static str> {
    let mut breadcrumbs = format!("<a href=\"{}\">{}</a>", file_url(opt, ""), opt.tree_name);

    breadcrumbs.push_str(r#"<button id="tree-switcher" title="Open tree switcher menu" aria-expanded="false" aria-haspopup="true" aria-controls="tree-switcher-menu"></button>"#);
    breadcrumbs.push_str(r#"<div id="tree-switcher-menu" title="Tree switcher" role="menu" class="context-menu" style="display: none"></div>"#);

    let mut path_so_far = "".to_string();

    if !path.is_empty() {
        for name in path.split('/') {
            breadcrumbs.push_str("<span class=\"path-separator\">/</span>");
            path_so_far.push_str(name);
            breadcrumbs.push_str(&format!(
                "<a href=\"{}\">{}</a>",
                file_url(opt, &path_so_far),
                name
            ));
            path_so_far.push('/');
        }
    }

    if generate_symbol {
        breadcrumbs.push_str(&format!(
            "  <span data-symbols=\"{}\">(file symbol)</span>",
            make_file_sym_from_path(path)
        ));
    }

    writeln!(*writer, "<div class=\"breadcrumbs\">{}</div>", breadcrumbs)
        .map_err(|_| "Write err")?;

    Ok(())
}

/// `generate_formatted` input type that allows for hierarchical indentation and
/// not having to call to_string() on everything.
#[derive(Clone)]
pub enum F {
    /// Indents its children by one 2-spaced level.
    /// Use like `F::Indent(vec![...])`.
    Indent(Vec<F>),
    /// Doesn't indent its children.
    /// Use like `F::Seq(vec![...])`.
    Seq(Vec<F>),
    /// For when you don't have a 'static lifetime string literal that's part of
    /// the program source.  Frequently this is the result of a `format!` call.
    /// Use like `F::T(format!(r#"<h>{}</h>"#, some_var))`.
    T(String),
    /// For string literals in the program.  Avoid having to type `to_string()`!
    /// Use like `F::S("<div>")` or `F::S(r#"<a href="/look-quotes">foo</a>"#)`.
    S(&'static str),
}

pub fn generate_formatted(
    writer: &mut dyn Write,
    formatted: &F,
    indent: u32,
) -> Result<(), &'static str> {
    match *formatted {
        F::Indent(ref seq) => {
            for f in seq {
                generate_formatted(writer, f, indent + 1)?;
            }
            Ok(())
        }
        F::Seq(ref seq) => {
            for f in seq {
                generate_formatted(writer, f, indent)?;
            }
            Ok(())
        }
        F::T(ref text) => {
            for _ in 0..indent {
                write!(writer, "  ").map_err(|_| "Write err")?;
            }
            writeln!(writer, "{}", text).map_err(|_| "Write err")?;
            Ok(())
        }
        F::S(text) => {
            for _ in 0..indent {
                write!(writer, "  ").map_err(|_| "Write err")?;
            }
            writeln!(writer, "{}", text).map_err(|_| "Write err")?;
            Ok(())
        }
    }
}

pub fn generate_header(opt: &Options, writer: &mut dyn Write) -> Result<(), &'static str> {
    let css = ["mozsearch.css", "icons.css", "font-icons.css"];
    let css_tags = css.iter().map(|c| {
        F::T(format!(
            r#"<link href="/{}/static/css/{}" rel="stylesheet" media="screen"/>"#,
            opt.tree_name, c
        ))
    });

    let mut head_seq = vec![
        F::S(r#"<meta charset="utf-8" />"#),
        F::S(r#"<meta name="color-scheme" content="light dark">"#),
        F::T(format!(
            r#"<link href="/{}/static/icons/search.png" rel="shortcut icon">"#,
            opt.tree_name
        )),
        F::T(format!("<title>{}</title>", opt.title)),
    ];
    head_seq.extend(css_tags);

    let fieldset = vec![
        F::S(r#"<div id="query-section">"#),
        F::Indent(vec![
            F::S(r#"<label for="query" class="query_label visually-hidden">Find</label>"#),
            F::T(format!(
                r#"<input type="text" name="q" value="" maxlength="2048" id="query" accesskey="s" title="Search" placeholder="Search {}" autocomplete="off" />"#,
                opt.tree_name
            )),
            F::S(r#"<div class="zero-size-container">"#),
            F::Indent(vec![
                F::S(r#"<div class="bubble" id="query-bubble">"#),
                F::S("</div>"),
            ]),
            F::S("</div>"),
            F::S(r#"<section id="spinner"></section>"#),
        ]),
        F::S("</div>"),
        F::S(r#"<div id="option-section" class="v-flex-container">"#),
        F::Indent(vec![
            F::S(r#"<label for="case">"#),
            F::Indent(vec![F::S(
                r#"<input type="checkbox" name="case" id="case" class="option-checkbox" value="true" accesskey="c"/><span class="access-key">C</span>ase-sensitive"#,
            )]),
            F::S("</label>"),
            F::S(r#"<label for="regexp">"#),
            F::Indent(vec![F::S(
                r#"<input type="checkbox" name="regexp" id="regexp" class="option-checkbox" value="true" accesskey="r"/><span class="access-key">R</span>egexp search"#,
            )]),
            F::S("</label>"),
        ]),
        F::S("</div>"),
        F::S(r#"<div id="path-section">"#),
        F::Indent(vec![
            F::S(r#"<label for="path" class="query_label visually-hidden">Path</label>"#),
            F::S(
                r#"<input type="text" name="path" value="" maxlength="2048" id="path" accesskey="p" title="Path" placeholder="Path filter (supports globbing and ^, $)" autocomplete="off" />"#,
            ),
            F::S(r#"<div class="zero-size-container">"#),
            F::Indent(vec![
                F::S(r#"<div class="bubble" id="path-bubble">"#),
                F::S("</div>"),
            ]),
            F::S("</div>"),
        ]),
        F::S("</div>"),
    ];

    let revision = match opt.revision {
        Some((rev_id, rev_desc)) => vec![
            F::T(format!(
                r#"<span id="rev-id">Showing <a href="/{}/commit/{}">{}</a>:</span>"#,
                opt.tree_name,
                rev_id,
                &rev_id[..8]
            )),
            F::T(format!(r#"<span id="rev-desc">{}</span>"#, rev_desc)),
        ],
        None => vec![],
    };

    let form = vec![
        F::S("<fieldset>"),
        F::Indent(fieldset),
        F::S("</fieldset>"),
        F::S(
            "<!-- disabled to avoid enter-submits behavior that conflicts with JS search logic -->",
        ),
        F::S(r#"<input type="submit" value="Search" disabled class="visually-hidden" />"#),
        F::S(r#"<div id="revision">"#),
        F::Indent(revision),
        F::S("</div>"),
    ];

    let root_class = if opt.revision.is_some() {
        "old-rev"
    } else {
        ""
    };

    let f = F::Seq(vec![
        F::S("<!DOCTYPE html>"),
        F::T(format!(r#"<html lang="en-US" class="{}">"#, root_class)),
        F::S("<head>"),
        F::Indent(head_seq),
        F::S("</head>"),
        F::S(""),
        F::S("<body>"),
        F::Indent(vec![
            F::S(r#"<div id="fixed-header">"#),
            F::T(format!(
                r#"<form method="get" action="/{}/search" class="search-box" id="search-box">"#,
                opt.tree_name
            )),
            F::Indent(form),
            F::S("</form>"),
            F::S("</div>"),
            F::S(r#"<div id="scrolling">"#),
            F::T(format!(
                r#"<div id="content" class="content {}" data-no-results="No results for current query.">"#,
                opt.extra_content_classes
            )),
        ]),
    ]);

    generate_formatted(writer, &f, 0)?;

    Ok(())
}

pub fn generate_footer(
    opt: &Options,
    tree_name: &str,
    path: &str,
    writer: &mut dyn Write,
) -> Result<(), &'static str> {
    let mut date = F::Seq(vec![]);
    if opt.include_date {
        let local: DateTime<Local> = Local::now();
        let time_str = local.to_rfc2822();

        date = F::Seq(vec![
            F::S(r#"<div id="foot" class="footer">"#),
            F::Indent(vec![F::T(format!(
                r#"This page was generated by Searchfox <span class="pretty-date" data-datetime="{}"></span>."#,
                time_str
            ))]),
            F::S("</div>"),
        ]);
    }

    let scripts = [
        "settings.js",
        "search.js",
        "context-menu.js",
        "panel.js",
        "code-highlighter.js",
        "blame.js",
    ];
    let script_tags: Vec<_> = scripts
        .iter()
        .map(|s| {
            F::T(format!(
                r#"<script src="/{}/static/js/{}"></script>"#,
                opt.tree_name, s
            ))
        })
        .collect();

    let f = F::Seq(vec![
        F::Indent(vec![
            F::Indent(vec![F::S("</div>")]),
            date,
            F::T(format!(
                r#"<span id="data" data-root="/" data-search="/{}/search" data-tree="{}" data-path="{}"></span>"#,
                tree_name, tree_name, path
            )),
            F::S(r#"<script src="/tree-list.js"></script>"#),
            F::Seq(script_tags),
            F::S("</div>"), // close out #scrolling
            F::S("</body>"),
        ]),
        F::S("</html>"),
    ]);

    generate_formatted(writer, &f, 0)?;

    Ok(())
}

pub struct PanelItem {
    pub title: String,
    pub link: String,
    /// This is a pattern which will be appended to the URL, where `{}` is
    /// replaced by the line number.
    pub update_link_lineno: &'static str,
    pub accel_key: Option<char>,
    pub copyable: bool,
}

pub struct PanelSection {
    pub name: String,
    pub items: Vec<PanelItem>,
    pub raw_items: Vec<String>,
}

static COPY_ICONS: &str =
    r#"<span class="icon-docs copy-icon"></span><span class="icon-ok tick-icon"></span>"#;

/// Generate HTML for a panel containing the given sections and write it to the
/// provided writer.  This is expected to be called once per document.
pub fn generate_panel(
    opt: &Options,
    writer: &mut dyn Write,
    sections: &[PanelSection],
    collapsed: bool,
) -> Result<(), &'static str> {
    let sections = sections
        .iter()
        .map(|section| {
            let items = section
                .items
                .iter()
                .map(|item| {
                    let update_attr = if !item.update_link_lineno.is_empty() {
                        format!(
                            r#" data-update-link="{}" data-link="{}""#,
                            item.update_link_lineno, item.link
                        )
                    } else {
                        String::new()
                    };
                    let accel = if let Some(key) = item.accel_key {
                        format!(r#" <span class="accel">{}</span>"#, key)
                    } else {
                        String::new()
                    };
                    let data_accel = if let Some(key) = item.accel_key {
                        format!(r#" data-accel="{key}""#)
                    } else {
                        String::new()
                    };
                    let is_link = !item.link.is_empty();
                    let copy = if item.copyable {
                        if is_link {
                            format!(
                                r#"<button class="copy" title="Copy to clipboard">{}</button>"#,
                                COPY_ICONS
                            )
                        } else {
                            format!(r#"<span class="icon copy indicator">{}</span>"#, COPY_ICONS)
                        }
                    } else {
                        String::new()
                    };
                    let tag = if is_link { "a" } else { "button" };
                    let href = if is_link {
                        format!(r#" href="{}""#, item.link)
                    } else {
                        String::new()
                    };
                    F::Seq(vec![
                        F::S("<li>"),
                        F::T(format!(
                            r#"<{}{}{} title="{}" class="icon item"{}>{}{}{}</{}>"#,
                            tag,
                            href,
                            data_accel,
                            item.title,
                            update_attr,
                            item.title,
                            accel,
                            copy,
                            tag
                        )),
                        F::S("</li>"),
                    ])
                })
                .collect::<Vec<_>>();

            let raw_items = section
                .raw_items
                .iter()
                .map(|raw_item| F::T(raw_item.to_string()))
                .collect::<Vec<_>>();

            F::Seq(vec![
                F::T(format!("<h4>{}</h4>", section.name)),
                F::S("<ul>"),
                F::Seq(items),
                F::Seq(raw_items),
                F::S("</ul>"),
            ])
        })
        .collect::<Vec<_>>();

    let f = F::Seq(vec![
        F::S(r#"<div class="panel" id="panel">"#),
        F::Indent(vec![
            F::S(r#"<button id="panel-toggle">"#),
            F::Indent(vec![
                F::T(format!(
                    r#"<span class="navpanel-icon icon-down-dir{}" aria-hidden="false"></span>"#,
                    if collapsed { "" } else { " expanded" }
                )),
                F::S("Navigation"),
                F::T(format!(
                    r#"<a id="show-settings" title="Go to settings page" href="/{}/pages/settings.html"><span class="navpanel-icon icon-cog expanded" aria-hidden="false"></span></a>"#,
                    opt.tree_name
                )),
            ]),
            F::S("</button>"),
            F::T(format!(
                r#"<section id="panel-content" aria-expanded="{}" aria-hidden="{}"{}>"#,
                if collapsed { "false" } else { "true" },
                if collapsed { "true" } else { "false" },
                if collapsed {
                    r#" style="display: none""#
                } else {
                    ""
                }
            )),
            F::S(
                r#"<label class="panel-accel"><input type="checkbox" id="panel-accel-enable" checked="checked">Enable keyboard shortcuts</label>"#,
            ),
            F::Seq(sections),
            F::S("</section>"),
        ]),
        F::S("</div>"),
    ]);

    generate_formatted(writer, &f, 0)?;

    Ok(())
}

pub fn generate_svg_preview(writer: &mut dyn Write, url: &str) -> Result<(), &'static str> {
    let f = F::Seq(vec![
        F::S(r#"<div class="svg-preview">"#),
        F::Indent(vec![
            F::S("<h4>SVG Preview (Scaled)</h4>"),
            F::S(r#"<input id="svg-preview-background-default" type="radio" name="svg-preview-background" value="default" checked>"#),
            F::S(r#"<label for="svg-preview-background-default">Default</label>"#),
            F::S(r#"<input id="svg-preview-background-checkerboard" type="radio" name="svg-preview-background" value="checkerboard">"#),
            F::S(r#"<label for="svg-preview-background-checkerboard">Checkerboard</label>"#),
            F::S(r#"<input id="svg-preview-background-light" type="radio" name="svg-preview-background" value="light">"#),
            F::S(r#"<label for="svg-preview-background-light">Light</label>"#),
            F::S(r#"<input id="svg-preview-background-dark" type="radio" name="svg-preview-background" value="dark">"#),
            F::S(r#"<label for="svg-preview-background-dark">Dark</label>"#),
            F::T(format!(r#"<a href="{}">"#, url)),
            F::Indent(vec![F::T(format!(
                r#"<img src="{0}" alt="Preview of {0}"/>"#,
                url
            ))]),
            F::S("</a>"),
        ]),
        F::S("</div>"),
    ]);

    generate_formatted(writer, &f, 0)?;
    Ok(())
}

```

## tools/src/logging.rs
```
use std::{collections::HashMap, sync::Mutex};

use serde_json::{json, Map, Value};
use tokio::{
    sync::oneshot::{self, Receiver, Sender},
    task::JoinHandle,
};
use tracing::{info, info_span, span::Span};
use tracing_forest::{processor::from_fn, traits::*, tree::Tree, worker_task};
use tracing_subscriber::{fmt::format::FmtSpan, EnvFilter, Layer, Registry};
use uuid::Uuid;

#[allow(dead_code)]
struct LogGlobal {
    handle: JoinHandle<()>,
}

lazy_static! {
    static ref SPAN_MAP: Mutex<HashMap<uuid::Uuid, Sender<Tree>>> = Mutex::new(HashMap::new());
    static ref LOG_GLOBAL: Mutex<Option<LogGlobal>> = Mutex::new(None);
}

/// Mechanism for creating a logging span that, using tracing-forest, will
/// aggregate a hierarchy of all of everything that was nested under the span
/// as long as all `tokio::spawn`ed tasks have had `tracing::Instrument` used to
/// call `.instrument(some_span_to_wrap_the_task)` or `.in_current_span()`.
///
/// It is necessary to have called `init_logging()` below to have set up the
/// necessary machinery which will install tracing-forest as the subscriber.
pub struct LoggedSpan {
    pub span: Span,
    rx: Receiver<Tree>,
}

pub fn render_forest_to_value(tree: &Tree) -> Value {
    match tree {
        Tree::Span(span) => {
            json!({
                "name": span.name(),
                "nodes": span.nodes().iter().map(render_forest_to_value).collect::<Vec<Value>>(),
            })
        }
        Tree::Event(event) => {
            let mut obj = Map::new();
            if let Some(msg) = event.message() {
                obj.insert("message".to_string(), json!(msg));
            }
            for field in event.fields() {
                obj.insert(field.key().to_string(), json!(field.value()));
            }
            json!(obj)
        }
    }
}

impl LoggedSpan {
    pub fn new_logged_span(name: &str) -> LoggedSpan {
        let id = Uuid::new_v4();

        //let id_str = id.as_simple().to_string();
        let span = info_span!(parent: None, "logged_span", name, uuid = %id);
        info!("logged_span_start");
        let (tx, rx) = oneshot::channel();

        {
            let mut span_map = SPAN_MAP.lock().unwrap();
            span_map.insert(id, tx);
        }

        LoggedSpan { span, rx }
    }

    pub async fn retrieve(self) -> Tree {
        info!("logged_span_end");
        drop(self.span);

        self.rx.await.unwrap()
    }

    pub async fn retrieve_serde_json(self) -> Value {
        let tree = self.retrieve().await;
        render_forest_to_value(&tree)
    }
}

/// Initialize logging; for now we currently always use a hard-coded value of
/// tools=trace for the `LoggedSpan` mechanism because that's all we care about,
/// but if you set the environment variable `RUST_LOG` to a non-empty value, we
/// will enable pretty/verbose logging (although we can change that if desired).
//#[allow(unused_must_use)]
pub fn init_logging() {
    {
        let global_opt = LOG_GLOBAL.lock().unwrap();
        if global_opt.is_some() {
            return;
        }
    }

    let mut layers = Vec::new();
    // If RUST_LOG is present and *non-empty* then interpret it and use it.
    // Because of limitations in our shell-scripts for our tests, we will
    // frequently set RUST_LOG unconditionally but potentially with an empty
    // value, and we don't want that to be interpreted as a desire to enable
    // logging.
    if let Ok(rustlog) = std::env::var("RUST_LOG") {
        if !rustlog.is_empty() {
            if let Ok(env_filter) = EnvFilter::try_from_default_env() {
                let layer = tracing_subscriber::fmt::layer()
                    .with_span_events(FmtSpan::ENTER | FmtSpan::EXIT)
                    //.pretty()
                    .compact()
                    // We primarily expect this to go in our log which can be
                    // excerpted for email purposes, and so ANSI isn't helpful
                    // for this.
                    .with_ansi(false)
                    // In general we don't care about the wall time that much,
                    // and it takes up a lot of columns, especially in tracing
                    // which includes sub-second granularities.
                    //
                    // Also, if we leave time enabled, we have to fix
                    // send-warning-email.py to deal with the sub-seconds.
                    .without_time()
                    // I had enabled the thread ids for diagnosing complicated
                    // async issues, but ideally we won't see this much, so this
                    // will just be noise most of the time.
                    //.with_thread_ids(true)
                    .with_filter(env_filter)
                    .boxed();
                layers.push(layer);
            }
        }
    }

    let handle = tokio::spawn(
        worker_task()
            .set_global(true)
            .map_receiver(|_| {
                // Return our new receiver to replace the initial receiver.
                from_fn(|tree| {
                    // For every tree we receive, see if it has a UUID that we're
                    // looking for, and if so, put it in the map so that it can be
                    // extracted.
                    if let Tree::Span(span) = &tree {
                        let mut span_map = SPAN_MAP.lock().unwrap();

                        let id = span.uuid();
                        if let Some(tx) = span_map.remove(&id) {
                            // Sending can fail if the receiver has been dropped.
                            // This can happen if the LoggedSpan is dropped without retrieving its
                            // results.  We expect this to happen in cases where `?` is used in
                            // the same scope as the LoggedSpan is held.
                            let _ = tx.send(tree);
                        }
                    }
                    Ok(())
                })
            })
            .build_with(|layer| {
                layers.push(layer.boxed());
                Registry::default()
                    .with(layers)
                    // This needs to be static, so I'm hackily currently just adding binaries
                    // that call this method to the list, but it could make sense to do
                    // something more clever with a macro, etc.
                    .with(EnvFilter::new("crossref=trace,tools=trace"))
            })
            // set this up to run forever?
            .on(async {
                tokio::signal::ctrl_c().await.expect("Ctrl-C sad!");
            }),
    );

    {
        let mut global_opt = LOG_GLOBAL.lock().unwrap();
        *global_opt = Some(LogGlobal { handle });
    }
}

```

## tools/src/tree_sitter_support/cst_tokenizer.rs
```
use std::borrow;
use std::path::Path;

use include_dir::{include_dir, Dir};

use crate::file_format::history::syntax_files_struct::FileStructureRow;

static QUERIES_DIR: Dir = include_dir!("$CARGO_MANIFEST_DIR/languages/tokenizer_queries");

fn load_language_queries(
    ts_lang: &tree_sitter::Language,
    lang_str: &str,
) -> Result<tree_sitter::Query, String> {
    match QUERIES_DIR.get_file(format!("{}.scm", lang_str)) {
        Some(file) => {
            let maybe_contents = file.contents_utf8().map(borrow::Cow::from);
            match maybe_contents {
                Some(contents) => {
                    tree_sitter::Query::new(ts_lang, &contents).map_err(|ts_err| ts_err.message)
                }
                _ => Err(format!("No queries for lang: {}", lang_str)),
            }
        }
        _ => Err(format!("No queries for lang: {}", lang_str)),
    }
}

pub struct HyperTokenized {
    pub lang: String,
    pub tokenized: Vec<String>,
    pub structure: Vec<FileStructureRow>,
}

/// Process a source file with tree-sitter to derive the structurally-bound
/// syntax tokens and an outline of the structure of the file.
pub fn hypertokenize_source_file(
    filename: &str,
    source_contents: &str,
) -> Result<HyperTokenized, String> {
    let ext = match Path::new(filename).extension() {
        Some(ext) => ext.to_str().unwrap(),
        None => "",
    };

    let mut tokenized = Vec::new();
    let mut structure = Vec::new();

    let mut parser = tree_sitter::Parser::new();
    // ### atom_nodes ###
    //
    // We borrow difftastic's terminology to deal with awkward tree-sitter nodes
    // like tree-sitter-cpp's `string_literal` where we want to use the contents
    // of the node and ignore the fact that it has children because there are
    // only children for the opening and closing `"` characters but no node for
    // the actual contents of the string.
    //
    // See https://github.com/tree-sitter/tree-sitter/issues/1156 for more
    // information on the underlying tree-sitter issue.
    //
    // Specific example details:
    // - `#include "big_header.h"` has 3 children:
    //   - `#include"`: 0 children
    //   - `"big_header.h"`: 2 children, both of which are the quotes?!  This
    //     differs from `<stdlib.h>` which is just a single monolithic string
    //     with no children.
    //   - `\n`: 0 children
    //
    // ### ignore_nodes
    //
    // As noted in https://github.com/tree-sitter/tree-sitter-c/issues/97 the
    // C preprocessor nodes currently are weird and include the trailing
    // newline.  For our purposes, we never actually want to emit a newline
    // token, so it's easy enough for us to just forbid that node.
    let (lang, ts_lang, ts_query_filename, atom_nodes, ignore_nodes) = match ext {
        "c" | "cc" | "cpp" | "cxx" | "h" | "hh" | "hxx" | "hpp" => {
            let ts_lang: tree_sitter::Language = tree_sitter_cpp::LANGUAGE.into();
            let string_literal = ts_lang.id_for_node_kind("string_literal", true);
            let char_literal = ts_lang.id_for_node_kind("char_literal", true);
            let newline = ts_lang.id_for_node_kind("\n", false);
            (
                "cpp",
                ts_lang,
                "cpp",
                vec![string_literal, char_literal],
                vec![newline],
            )
        }
        "js" | "jsm" | "json" | "mjs" | "sjs" | "ts" => (
            "js",
            tree_sitter_typescript::LANGUAGE_TYPESCRIPT.into(),
            "typescript",
            vec![],
            vec![],
        ),
        "jsx" | "tsx" => (
            "js",
            tree_sitter_typescript::LANGUAGE_TSX.into(),
            "typescript",
            vec![],
            vec![],
        ),
        "py" | "build" | "configure" => (
            "py",
            tree_sitter_python::LANGUAGE.into(),
            "python",
            vec![],
            vec![],
        ),
        "rs" => (
            "rust",
            tree_sitter_rust::LANGUAGE.into(),
            "rust",
            vec![],
            vec![],
        ),
        // Explicitly skip things we know are binary; this list copied from "langauages.rs"
        "ogg" | "ttf" | "xpi" | "png" | "bcmap" | "gif" | "ogv" | "jpg" | "jpeg" | "bmp"
        | "icns" | "ico" | "mp4" | "sqlite" | "jar" | "webm" | "webp" | "woff" | "class"
        | "m4s" | "mgif" | "wav" | "opus" | "mp3" | "otf" => {
            return Err("Binary files can't be tokenized".to_string());
        }
        _ => {
            return Ok(HyperTokenized {
                lang: "none".to_string(),
                tokenized: source_contents
                    .split_whitespace()
                    .map(|s| format!("% {}", s))
                    .collect(),
                structure: vec![],
            });
        }
    };
    parser
        .set_language(&ts_lang)
        .expect("Error loading grammar");
    let container_query = load_language_queries(&ts_lang, ts_query_filename)?;

    let name_capture_ix = container_query.capture_index_for_name("name").unwrap();
    let container_capture_ix = container_query.capture_index_for_name("container").unwrap();

    let parse_tree = match parser.parse(source_contents.as_bytes(), None) {
        Some(t) => t,
        _ => {
            return Err("Parse failed!".to_string());
        }
    };

    // The cursor traversal logic here is derived from the tree-sitter-cli
    // parse_file_at_path logic: https://github.com/tree-sitter/tree-sitter/blob/master/cli/src/parse.rs
    //
    // A good resource if you are interested in what this class is doing is to instead look at
    // https://github.com/Wilfred/difftastic/blob/master/src/parse/tree_sitter_parser.rs which
    // I discovered after running into problems with the node modeling of tree-sitter-cpp's
    // `string_literal` node and found https://github.com/tree-sitter/tree-sitter/issues/1156
    // and related issues and discussion.  Note that it is explicitly mapping tree-sitter's
    // pseudo-CST to its own tree rep, whereas we are just linearizing tokens here, but the
    // general desire to have all tokens remains.
    let mut cursor = parse_tree.walk();
    let mut _depth = 0;
    let mut visited_children = false;

    let mut query_cursor = tree_sitter::QueryCursor::new();
    let mut query_matches = query_cursor.matches(
        &container_query,
        parse_tree.root_node(),
        source_contents.as_bytes(),
    );

    let mut next_container_match = query_matches.next();
    let mut next_container_id = usize::MAX;
    if let Some(container_match) = &next_container_match {
        next_container_id = container_match
            .nodes_for_capture_index(container_capture_ix)
            .next()
            .unwrap()
            .id();
    }

    let mut context_stack: Vec<String> = vec![];
    let empty_context = "%".to_string();
    let mut context_pretty = empty_context.clone();
    let mut id_stack: Vec<usize> = vec![];

    loop {
        let node = cursor.node();
        if visited_children {
            if cursor.goto_next_sibling() {
                visited_children = false;
            } else if cursor.goto_parent() {
                visited_children = true;
                _depth -= 1;

                if let Some(container_id) = id_stack.last() {
                    if cursor.node().id() == *container_id {
                        context_stack.pop();
                        context_pretty = if context_stack.is_empty() {
                            empty_context.clone()
                        } else {
                            context_stack.join("::")
                        };
                        id_stack.pop();
                    }
                }
            } else {
                break;
            }
        } else {
            // We are considering this node for the first time and before any of
            // its children.

            // Handle if this is our next container.
            if node.id() == next_container_id {
                let pattern_index = next_container_match.as_ref().unwrap().pattern_index;
                let name_node = next_container_match
                    .as_ref()
                    .unwrap()
                    .nodes_for_capture_index(name_capture_ix)
                    .next()
                    .unwrap();
                let name = name_node.utf8_text(source_contents.as_bytes()).unwrap();
                context_stack.push(name.to_string());
                context_pretty = if context_stack.is_empty() {
                    empty_context.clone()
                } else {
                    context_stack.join("::")
                };
                // We're assuming there's only one `#set!` directive right now and that it's
                // "structure.kind" and that it exists.  We do require it to exist, but...
                // TODO: It likely makes sense to preprocess the query by iterating over
                // its patterns and explicitly mapping based on the key so that we can
                // have the kind already available as a string we can clone.
                let structure_kind = container_query
                    .property_settings(pattern_index)
                    .first()
                    .unwrap()
                    .value
                    .as_ref()
                    .unwrap()
                    .to_string();
                structure.push(FileStructureRow {
                    pretty: context_pretty.clone(),
                    // TODO: This should come from a `#set!` directive too but this nuance
                    // won't matter for a bit, so I'm punting because there's a potential
                    // the SCM queries would need to get a little more complex in order to
                    // differentiate between decl and def and when making the change it
                    // would probably be ideal to add more test coverage.
                    is_def: true,
                    kind: structure_kind.to_string(),
                });
                id_stack.push(next_container_id);

                next_container_match = query_matches.next();
                if let Some(container_match) = &next_container_match {
                    next_container_id = container_match
                        .nodes_for_capture_index(container_capture_ix)
                        .next()
                        .unwrap()
                        .id();
                } else {
                    next_container_id = usize::MAX;
                }
            }
            let node_kind_id = node.kind_id();
            if ignore_nodes.contains(&node_kind_id) {
                // ignore this node!
                visited_children = true;
            } else if !atom_nodes.contains(&node_kind_id) && cursor.goto_first_child() {
                visited_children = false;
                _depth += 1;
            } else {
                let token = node.utf8_text(source_contents.as_bytes()).unwrap().trim();
                // Comments don't get further tokenized and are marked as extra, so for now we
                // only perform additional whitespace tokenization for "extra" nodes.  This
                // may turn out to be wrong.
                if token.is_empty() {
                    // ignore empty tokens!
                } else if node.is_extra() {
                    // TODO: probably better to use the regex crate here to avoid a bunch of empty
                    // matches for consecutive whitespace.
                    for piece in token.split(char::is_whitespace) {
                        if piece.is_empty() {
                            continue;
                        }
                        tokenized.push(format!("{} {}", context_pretty, piece));
                    }
                } else {
                    tokenized.push(format!("{} {}", context_pretty, token));
                }
                visited_children = true;
            }
        }
    }

    Ok(HyperTokenized {
        lang: lang.to_string(),
        tokenized,
        structure,
    })
}

```

## tools/src/tree_sitter_support/mod.rs
```
pub mod cst_tokenizer;

```

## tools/src/glob_helper.rs
```
/*
This file combines the synchronous gold-standards of
https://crates.io/crates/globset and https://crates.io/crates/walkdir (both
from the ripgrep author) to perform filtered tree enumeration and wraps them
with a call to tokio::block_in_place to avoid gumming up the scheduling works.

The only current desired consumer for this file is
the `test_check_insta`mechanism, but it could also make sense to be used for
other purposes.  It's also possible the crates ecosystem will gain a popular
crate we can just use directly.

Note that although the web UI has a mechanism to search known files, that
currently is based on shelling out to grep to filter `repo-files` and
`objdir-files`, not actually perform a filesystem traversal and we want to keep
that as filtering pre-canned data, not as a filesystem traversal.
*/

use std::path::Path;

use globset::Glob;
use tokio::task;
use walkdir::WalkDir;

/// Given a path to a root dir to traverse and a glob pattern, block in place
/// and return a sorted list of all files matching the glob relative to the root
/// dir.  Return value looks like ("relative/path/", "file.ext").
///
/// Note: This currently requires that there be no top-level files that match
/// the glob because of laziness about construction of the relative path.  We
/// will panic when this laziness eventually becomes a problem.
///
/// Everything operates in terms of strings because callers currently like to
/// use `format!` to build paths and we don't have to deal with adversarial
/// paths here.
pub fn block_in_place_glob_tree(root: &str, glob: &str) -> Vec<(String, String)> {
    task::block_in_place(|| {
        let mut paths = vec![];

        let glob = Glob::new(glob).unwrap().compile_matcher();

        let root_path = Path::new(root);
        for entry in WalkDir::new(root_path) {
            let entry = entry.unwrap();
            if glob.is_match(entry.path()) && entry.file_type().is_file() {
                let rel_path = entry.path().strip_prefix(root_path).unwrap();
                paths.push((
                    // We want a trailing slash for simplified string formatting.
                    format!("{}/", rel_path.parent().unwrap().to_str().unwrap()),
                    rel_path.file_name().unwrap().to_str().unwrap().to_string(),
                ));
            }
        }

        paths.sort();

        paths
    })
}

```

## tools/src/bin/scip-indexer.rs
```
extern crate clap;
extern crate env_logger;
#[macro_use]
extern crate log;
extern crate protobuf;
extern crate scip;
extern crate tools;

use clap::Parser;
use lazy_static::lazy_static;
use regex::Regex;
use scip::types::descriptor::Suffix;
use serde_json::Map;
use std::collections::BTreeSet;
use std::fs::{self, File};
use std::io;
use std::io::BufReader;
use std::path::{Path, PathBuf};
use tools::file_format::analysis::{
    AnalysisKind, AnalysisSource, AnalysisStructured, AnalysisTarget, LineRange, Location,
    SourceRange, SourceTag, StructuredFieldInfo, StructuredMethodInfo, StructuredOverrideInfo,
    StructuredSuperInfo, StructuredTag, TargetTag, WithLocation,
};
use tools::file_format::config;
use ustr::{ustr, Ustr, UstrMap, UstrSet};

/// Normalize illegal symbol characters into underscores.
fn sanitize_symbol(sym: &str) -> String {
    // Downstream processing of the symbol doesn't deal well with
    // these characters, so replace them with underscores.
    fn is_special_char(c: char) -> bool {
        matches!(c, ',' | ' ' | '\n')
    }
    sym.replace(is_special_char, "_").trim_matches('_').into()
}

fn create_output_dir(output_file: &Path) -> io::Result<()> {
    let mut output_dir = output_file.to_owned();
    output_dir.pop();
    fs::create_dir_all(output_dir)
}

#[derive(Parser)]
struct ScipIndexerCli {
    /// Path to the variable-expanded config file
    #[clap(value_parser)]
    config_file: String,

    /// The tree in the config file we're cross-referencing
    #[clap(value_parser)]
    tree_name: String,

    #[arg(long, value_parser)]
    subtree_name: Option<String>,

    /// Relative path from the root of the searchfox source tree to the SCIP
    /// index's contents.  If the SCIP index is from the root then this should
    /// be ".", otherwise it should be the relative path like "js-subtree/".
    #[arg(long, value_parser)]
    subtree_root: String,

    /// Platform name if this is per-platform.
    #[arg(long, value_parser)]
    platform: Option<String>,

    /// rustc analysis directories or scip inputs
    #[arg(value_parser)]
    inputs: Vec<PathBuf>,
}

// https://docs.rs/scip/latest/scip/types/struct.Occurrence.html#structfield.range
fn scip_range_to_searchfox_location(range: &[i32]) -> Location {
    // Searchfox uses 1-indexed lines, 0-indexed columns.
    let line_start = range[0] as u32 + 1;
    let col_start = range[1] as u32;
    let line_end = if range.len() == 3 {
        line_start
    } else {
        range[2] as u32 + 1
    };
    let col_end = *range.last().unwrap() as u32;
    // Rust spans are multi-line... So we just use the start column as
    // the end column if it spans multiple rows, searchfox has fallback
    // code to handle this.
    let col_end = if line_start != line_end {
        col_start
    } else {
        col_end
    };
    Location {
        lineno: line_start,
        col_start,
        col_end,
    }
}

fn node_range_to_searchfox_location(range: tree_sitter::Range) -> Location {
    // Searchfox uses 1-indexed lines, 0-indexed columns while tree-sitter uses
    // 0-based for both.
    Location {
        lineno: range.start_point.row as u32 + 1,
        col_start: range.start_point.column as u32,
        col_end: range.end_point.column as u32,
    }
}
fn node_range_to_searchfox_range(range: tree_sitter::Range) -> SourceRange {
    // Searchfox uses 1-indexed lines, 0-indexed columns while tree-sitter uses
    // 0-based for both.
    SourceRange {
        start_lineno: range.start_point.row as u32 + 1,
        start_col: range.start_point.column as u32,
        end_lineno: range.end_point.row as u32 + 1,
        end_col: range.end_point.column as u32,
    }
}

fn write_line(mut file: &mut File, data: &impl serde::Serialize) {
    use std::io::Write;
    serde_json::to_writer(&mut file, data).unwrap();
    file.write_all(b"\n").unwrap();
}

fn scip_roles_to_searchfox_analysis_kind(roles: i32) -> AnalysisKind {
    macro_rules! map_to_searchfox {
        ($scip:ident, $sfox:ident) => {
            if roles & scip::types::SymbolRole::$scip as i32 != 0 {
                return AnalysisKind::$sfox;
            }
        };
    }

    map_to_searchfox!(Definition, Def);
    map_to_searchfox!(Import, Use);
    // Read/Write would be pretty neat to have, but neither rust-analyzer or
    // scip-typescript generates these values.
    map_to_searchfox!(WriteAccess, Use);
    map_to_searchfox!(ReadAccess, Use);
    map_to_searchfox!(Generated, Use);
    // This would be very interesting if it works for rust-analyzer, as our
    // current file-level granularity for determining the pathkind for grouping
    // obviously will be wrong for tests in the source file, which is definitely
    // a rust idiom.   (Also a python idiom too!)
    map_to_searchfox!(Test, Use);
    map_to_searchfox!(Import, Use);

    AnalysisKind::Use
}

/// Our specifically handled languages for conditional logic.  We currently
/// require tree-sitter support for all supported languages.
enum ScipLang {
    Python,
    Rust,
    Typescript,
    Jvm,
}

enum PrettyAction {
    /// Don't include this in the pretty, but keep using what we've built.
    Omit,
    /// Append this to the current list of pieces.
    Append,
    /// Reset the list and use this as the first entry of the new list.
    ResetAndUse,
    /// The symbol name is known to be useless and some other means of inferring
    /// a pretty name (Ex: from the documentation) must be used.  Reset the list
    /// and use the alternate name source.  Note that it's expected this will be
    /// the last directive expected; re-evaluate if you need to use this rule
    /// more than once.
    UseAlternateSource,
}

/// Helper structure that we use to populate our tree-sitter queries.  See the
/// block comment in `analyze_using_scip` for context.
///
/// This structure captures the relevant data for queries that help us know
/// which SCIP occurrence should be the definition that starts a nesting range
/// both for position:sticky purposes and for context/contextsym purposes.
/// In general this just means knowing the node name for a given construct plus
/// the field names for the name and body.  Frequently the field name is "name",
/// but sometimes it can be something like "type".  Usually the body is "body".
///
/// Note that the query syntax is quite powerful and the names of captures could
/// potentially be used to encode metadata, so if making any changes to the type
/// here or adding a whole bunch of additional instances below, it's probably
/// worth considering moving to using externally stored ".scm" files which
/// have semantic capture groups instead of this current approach.  In
/// particular, alternations could be invaluable.  Also, we should avoid doing
/// anything that resembles reinventing `tree-sitter-highlight`.
///
/// The current approach has been chosen for prototyping expediency and for
/// ease of adding sidecar data, but as noted above and after having done
/// additional research, especially around conventions (and limited rust binding
/// support) for `#`-prefixed predicates, it's clear this is not the path
/// forward without a very good reason.  (Readability could be a good reason;
/// the s-expr syntax is powerful but probably unfamiliar to most people.  That
/// said, its use around tree-sitter is so common that it seems like a
/// reasonable thing to understand if touching tree-sitter related code.)
///
/// A reasonable hybrid approach, depending on how easy it is to add custom
/// predicates to the rust bindings, is to embed full s-exprs here but leave
/// our sidecar structure so that we can just use rust logic for what would
/// otherwise be potentially complex predicates.
struct SitterNesting {
    root_node_type: Vec<&'static str>,
    name_field: &'static str,
    body_field: &'static str,
    body_node: &'static str,
}

lazy_static! {
    // our list is manually derived from the tags.scm file:
    // https://github.com/tree-sitter/tree-sitter-python/blob/master/queries/tags.scm
    static ref PYTHON_NESTING: Vec<SitterNesting> = vec![
        SitterNesting {
            root_node_type: vec!["class_definition"],
            name_field: "name",
            body_field: "body",
            body_node: "",
        },
        SitterNesting {
            root_node_type: vec!["function_definition"],
            name_field: "name",
            body_field: "body",
            body_node: "",
        },
    ];
    // our list is manually derived from the tags.scm file:
    // https://github.com/tree-sitter/tree-sitter-rust/blob/master/queries/tags.scm
    static ref RUST_NESTING: Vec<SitterNesting> = vec![
        SitterNesting {
            root_node_type: vec!["struct_item"],
            name_field: "name",
            body_field: "body",
            body_node: "",
        },
        SitterNesting {
            root_node_type: vec!["enum_item"],
            name_field: "name",
            body_field: "body",
            body_node: "",
        },
        SitterNesting {
            root_node_type: vec!["union_item"],
            name_field: "name",
            body_field: "body",
            body_node: "",
        },
        SitterNesting {
            root_node_type: vec!["function_item"],
            name_field: "name",
            body_field: "body",
            body_node: "",
        },
        SitterNesting {
            root_node_type: vec!["trait_item"],
            name_field: "name",
            body_field: "body",
            body_node: "",
        },
        SitterNesting {
            root_node_type: vec!["mod_item"],
            name_field: "name",
            body_field: "body",
            body_node: "",
        },
        // TODO macro_definition lacks a body so the body needs to be the parent
        // node maybe?
        //
        // impl can also have a "trait" field, but symbol-wise I think the
        // important symbol for context is the struct type not the trait type.
        SitterNesting {
            root_node_type: vec!["impl_item"],
            name_field: "type",
            body_field: "body",
            body_node: "",
        },
    ];
    // tree-sitter support for typescript is a little weird because the
    // typescript languages (typescript and tsx) extend the javascript
    // language.
    //
    // for now we manually derive these from both base queries tags.scm files:
    // https://github.com/tree-sitter/tree-sitter-javascript/blob/master/queries/tags.scm
    // https://github.com/tree-sitter/tree-sitter-typescript/blob/master/queries/tags.scm
    static ref JS_NESTING: Vec<SitterNesting> = vec![
        // ### from the JS tags
        SitterNesting {
            root_node_type: vec!["method_definition"],
            name_field: "name",
            body_field: "body",
            body_node: "",
        },
        // There's an alt over class and class_declaration; class_declaration
        // becomes "class" if we add a "let blah = " ahead of it (and it stops
        // being a declaration).
        SitterNesting {
            root_node_type: vec!["class", "class_declaration"],
            name_field: "name",
            body_field: "body",
            body_node: "",
        },
        // There's also an alt over function/generators
        SitterNesting {
            root_node_type: vec![
                "function",
                "function_declaration",
                "generator_function",
                "generator_function_declaration"
            ],
            name_field: "name",
            body_field: "body",
            body_node: "",
        },
        // TODO: tags.scm has logic for lexical binds on arrow functions and
        // this is worth considering, although arguably this might also resemble
        // the lambda case.  But this is also beyond our current approach with
        // generated syntax.  Also, conceptually, the arrow functions should
        // already exist within a nesting scope, which raises the question of
        // whether it's actually desirable to treat them like C++ lambdas which
        // we also currently fold in.
        // TODO: There's also property-defined arrow functions.
        //
        // ### from the TS tags
        // XXX skipping function_signature because it lacks a directly available
        // body.
        // XXX skipping (abstract_)method_signature because it lacks a directly
        // available body.
        SitterNesting {
            root_node_type: vec!["abstract_class_declaration"],
            name_field: "name",
            body_field: "body",
            body_node: "",
        },
        SitterNesting {
            root_node_type: vec!["module"],
            name_field: "name",
            body_field: "body",
            body_node: "",
        },
        SitterNesting {
            root_node_type: vec!["interface_declaration"],
            name_field: "name",
            body_field: "body",
            body_node: "",
        },
    ];
    // https://github.com/tree-sitter/tree-sitter-java/blob/master/queries/tags.scm
    static ref JAVA_NESTING: Vec<SitterNesting> = vec![
        SitterNesting {
            root_node_type: vec!["class_declaration"],
            name_field: "name",
            body_field: "body",
            body_node: "",
        },
        SitterNesting {
            root_node_type: vec!["method_declaration"],
            name_field: "name",
            body_field: "body",
            body_node: "",
        },
        SitterNesting {
            root_node_type: vec!["interface_declaration"],
            name_field: "name",
            body_field: "body",
            body_node: "",
        },
    ];
    // no tags.scm in tree-sitter-kotlin
    static ref KOTLIN_NESTING: Vec<SitterNesting> = vec![
        SitterNesting {
            root_node_type: vec!["class_declaration"],
            name_field: "name",
            body_field: "",
            body_node: "class_body",
        },
        SitterNesting {
            root_node_type: vec!["function_declaration"],
            name_field: "name",
            body_field: "",
            body_node: "function_body",
        },
    ];
}

struct NestedSymbol {
    /// The symbol covering this nested range that should be used for
    /// contextsym.
    sym: Ustr,
    /// The pretty identifier of that symbol.
    pretty: Ustr,
    /// The range it covers; we really only need the last line, but include this
    /// for debugging.
    nesting_range: SourceRange,
}

fn compile_nesting_queries(
    lang: &tree_sitter::Language,
    nesting: &[SitterNesting],
) -> tree_sitter::Query {
    let query_pats: Vec<String> = nesting
        .iter()
        .map(|ndef| {
            let mut parts = vec![];
            let mut indent = "";
            if ndef.root_node_type.len() > 1 {
                parts.push("[".to_string());
                indent = "  ";
            }
            for node_type in &ndef.root_node_type {
                parts.push(format!(
                    "{}({} {}: (_) @name {}({}) @body)",
                    indent,
                    node_type,
                    ndef.name_field,
                    if !ndef.body_field.is_empty() {
                        format!("{}: ", ndef.body_field)
                    } else {
                        "".to_string()
                    },
                    if ndef.body_node.is_empty() {
                        "_"
                    } else {
                        ndef.body_node
                    }
                ));
            }
            if ndef.root_node_type.len() > 1 {
                parts.push("]".to_string());
            }
            parts.join("\n")
        })
        .collect();
    tree_sitter::Query::new(lang, &query_pats.join("\n")).unwrap()
}

// ### Process SCIP Descriptors
//
// Map the descriptor individually to build up canonical
// mozsearch symbol name.  In general, we maintain the SCIP
// descriptor syntax because there's no reason not to.  The main
// exception is we don't re-wrap backtick-enclosed values.
// (Interestingly, the scip lib's format_symbol_with method also
// does not re-wrap descriptors, but this is probably an
// oversight.)
//
// Note that this implementation inherently looks a lot like
// part of the scip crate's format_symbol_with impl out of
// necessity (we're reversing an explicit spec).  We don't call
// that method with a format option to just request the
// descriptors because that method is destructive and we both
// want to ensure stability and control of this mapping, like
// not emitting backticks.  This is very much a one-way mapping
// with policy decisions made here.
struct SymbolAnalysis {
    kind: Option<&'static str>,
    pretty: Ustr,
    norm_sym: Ustr,
    parent_sym: Option<Ustr>,
    contributes_to_parent: bool,
}

fn symbol_name(lang_name: &str, subtree_name: Option<&str>, scip_symbol: &str) -> Ustr {
    if let Some(subtree_name) = subtree_name {
        ustr(&format!("S_{}_{}_{}", lang_name, subtree_name, scip_symbol))
    } else {
        ustr(&format!("S_{}_{}", lang_name, scip_symbol))
    }
}

fn analyse_symbol(
    symbol: &scip::types::Symbol,
    lang: &ScipLang,
    lang_name: &str,
    subtree_name: Option<&str>,
    relative_path: &str,
    doc_name: Option<&str>,
    doc_namespace: Option<&str>,
) -> SymbolAnalysis {
    let mut pretty_pieces = vec![];
    let mut sym_pieces = vec![];
    let mut last_kind = None;
    let mut last_contributes_to_parent = false;
    let mut prev_kind = None;

    for descriptor in &symbol.descriptors {
        // Ignore descriptor enums from the future, skipping them.
        let suffix = match descriptor.suffix.enum_value() {
            // UnspecifiedSuffix is weird because it's an in-domain
            // value (it's explicitly part of the protobuf schema
            // for suffix), but since currently the suffix is only
            // built by parsing a string encoding of the enum, it
            // logically is similar in nature to the enum_value
            // being an Err.  Regardless, we skip it.
            Ok(Suffix::UnspecifiedSuffix) => {
                warn!("Experienced unspecified suffix on {}", symbol);
                continue;
            }
            Ok(v) => v,
            Err(_) => {
                warn!("Experienced weird suffix error on {}", symbol);
                continue;
            }
        };
        let escaped = sanitize_symbol(&descriptor.name);

        let (sym_piece, pretty_action, maybe_kind, contributes_to_parent) = match suffix {
            // Confusingly, package is deprecated in favor of
            // namespace, but right now the SCIP crate parses '/'
            // as Package, not Namespace.
            Suffix::Package | Suffix::Namespace => {
                // Pretty: For JS/TS the namespace includes the file path which
                // ends up way too verbose and now how humans would describe
                // things.
                //
                // TODO: Handle scip-typescript emitting symbols for file names
                // since our explicit heuristic above ends up leaving them
                // entirely with an empty pretty, and in that case we do want
                // to emit the path as a pretty, but we also want to emit a
                // "FILE_" symbol instead of a scip "S_" symbol.
                //
                // TODO: it would also be good to emit a "namespace" kind and/or
                // symbol for rust modules as we go? (For C++ we don't
                // emit a structured record but do emit an "NS_"-prefixed sym.)
                (
                    format!("{}/", escaped),
                    match lang {
                        ScipLang::Typescript => PrettyAction::Omit,
                        _ => PrettyAction::Append,
                    },
                    None,
                    false,
                )
            }
            Suffix::Type => (
                format!("{}#", escaped),
                PrettyAction::Append,
                Some("class"),
                false,
            ),
            Suffix::Term => (
                format!("{}.", escaped),
                PrettyAction::Append,
                Some("field"),
                true,
            ),
            Suffix::Method => (
                format!(
                    "{}({}).",
                    escaped,
                    sanitize_symbol(&descriptor.disambiguator)
                ),
                PrettyAction::Append,
                Some("method"),
                true,
            ),
            Suffix::TypeParameter => {
                // Not sure what cases this is used in...
                (
                    format!("[{}]", escaped),
                    PrettyAction::ResetAndUse,
                    None,
                    false,
                )
            }
            Suffix::Parameter => {
                // For now, at least, arguments don't get tracked by the parent.
                (
                    format!("({})", escaped),
                    PrettyAction::ResetAndUse,
                    Some("arg"),
                    false,
                )
            }
            Suffix::Macro => (
                format!("{}!", escaped),
                PrettyAction::Append,
                Some("macro"),
                false,
            ),
            Suffix::Meta => {
                // We see this used for fields in JS, at least when
                // preceded by a `Type#`.
                (
                    format!("{}:", escaped),
                    PrettyAction::Append,
                    Some("field"),
                    true,
                )
            }
            // Local is special because the symbol's "scheme" is
            // "local", so the suffix is interesting as a marker,
            // but doesn't actually exist on the descriptor as
            // an actual string suffix.
            Suffix::Local => {
                // We prefix the local with the relative path of the
                // doc, which should be the same as scip-typescript.
                // Conceptually, this is similar to what we do with
                // C++ where we hash over the filename/line and the
                // variable name.
                //
                // We also put a "#" on there to try and do a little
                // extra name-spacing.
                (
                    format!("{}/#{}", sanitize_symbol(relative_path), escaped),
                    PrettyAction::UseAlternateSource,
                    None,
                    false,
                )
            }
            // Suffix::UnspecifiedSuffix is not possible because we
            // excluded it above, but rust doesn't know that.
            Suffix::UnspecifiedSuffix => ("".to_owned(), PrettyAction::Omit, None, false),
        };
        prev_kind = last_kind;
        last_kind = maybe_kind;
        last_contributes_to_parent = contributes_to_parent;

        sym_pieces.push(sym_piece);

        match pretty_action {
            PrettyAction::Omit => {}
            PrettyAction::Append => {
                pretty_pieces.push(descriptor.name.clone());
            }
            PrettyAction::ResetAndUse => {
                pretty_pieces.clear();
                pretty_pieces.push(descriptor.name.clone());
            }
            PrettyAction::UseAlternateSource => {
                pretty_pieces.clear();
                if let Some(name) = doc_name {
                    pretty_pieces.push(name.to_string());
                } else {
                    pretty_pieces.push("unknown".to_string());
                }
            }
        }
    }

    // If we have an explicit doc namespace that provides context
    // the descriptors do not provide, then use that for all
    // pieces except the last piece we get from the descriptor.
    if let Some(namespace) = doc_namespace {
        if let Some(last_piece) = pretty_pieces.pop() {
            pretty_pieces = namespace.split("::").map(|s| s.to_string()).collect();
            pretty_pieces.push(last_piece);
        }
    }

    // We've standardized on "::" as the delimiter here even though
    // one might argue on a convention of using ".".  But crossref
    // currently requires "::" and this seems like a reasonable
    // convention.  Especially as the introduction of private JS
    // symbols prefixed with "#" has mooted the hacky syntax
    // previously used by mozsearch and used as a convention on MDN
    // URLs.
    let pretty = ustr(&pretty_pieces.join("::"));
    let norm_sym = symbol_name(lang_name, subtree_name, &sym_pieces.join(""));

    // Infer a parent sym if it seems to be a slice
    let parent_sym = if prev_kind == Some("class") && sym_pieces.len() >= 2 {
        Some(symbol_name(
            lang_name,
            subtree_name,
            &sym_pieces[..sym_pieces.len() - 1].join(""),
        ))
    } else {
        None
    };

    SymbolAnalysis {
        kind: last_kind,
        pretty,
        norm_sym,
        parent_sym,
        contributes_to_parent: last_contributes_to_parent,
    }
}

fn analyze_using_scip(
    tree_config: &config::TreeConfig,
    subtree_name: Option<&str>,
    subtree_root: &str,
    platform: &Option<String>,
    scip_file: PathBuf,
) {
    use protobuf::Message;
    use scip::types::*;

    let file = File::open(&scip_file).expect("Can't open scip file");
    let byte_count = file.metadata().expect("Failed to get file metadata").len();
    let mut file = BufReader::new(file);
    let mut file = protobuf::CodedInputStream::from_buf_read(&mut file);
    let index = Index::parse_from(&mut file).expect("Failed to read scip index");

    let mut scip_symbol_to_structured: UstrMap<AnalysisStructured> = UstrMap::default();
    let mut our_symbol_to_scip_sym: UstrMap<Ustr> = UstrMap::default();
    // When walking the relationship edges for doc.symbols we may learn about
    // superclasses or overridden methods that we never receive more information
    // for.  (For example. JDK and AndroidX classes.)  We keep track of these so
    // we can generate best-effort structured representations for these cases.
    let mut possible_unknown_scip_symbols: UstrMap<SymbolAnalysis> = UstrMap::default();

    let (lang_name, lang) = match index.metadata.tool_info.name.as_str() {
        "rust-analyzer" => ("rs", ScipLang::Rust),
        "scip-python" => ("py", ScipLang::Python),
        "scip-typescript" => ("js", ScipLang::Typescript),
        "scip-java" => ("jvm", ScipLang::Jvm),
        _ => {
            warn!("Unsupported language; we need tree-sitter support.");
            return;
        }
    };

    // ## First Pass: Process Symbol Definitions
    //
    // It's necessary to process all of the symbol definitions first because
    // occurrences can reference symbols defined in other documents.  (The
    // exception is locals inherently are file-local.)
    //
    // This also provides us a good opportunity to normalize the symbol names we
    // see and perform structural inference from the descriptors.  Structural
    // inference inherently involves mutating symbols as we go, so we do not
    // write anything out during this pass.  This is just as well as we can make
    // sure to only emit structured information at the point of definition
    // (which we do not know until we look at the occurrences).
    for doc in &index.documents {
        info!(
            "Processing symbols/definitions for '{}'",
            &doc.relative_path,
        );

        for scip_sym_info in &doc.symbols {
            // Process each symbol to:
            // - Derive a canonical mozsearch symbol name and map it.
            // - Derive structured analysis information that we will emit in the
            //   next pass if/when we see the definition.
            if let Ok(scip_sym) = scip::symbol::parse_symbol(&scip_sym_info.symbol) {
                // ### Extract Metadata from Documentation Markdown
                //
                // The documentation is what VS Code shows in tooltips for
                // symbols, which means there's dense information in there that
                // can be quite useful, but that it's intended for human
                // consumption, which means we need to regex it out.

                // XXX these should be made Option<u32>, but StructuredFieldInfo
                // needs to have its signature updated.  Right now this is hacky.
                let size_bytes = 0;
                //let mut align_bytes = 0;
                let offset_bytes = 0;

                // Until https://github.com/rust-lang/rust-analyzer/pull/16559
                // landed in rust-analyzer, we tried to use the doc string
                // containing the tunneled hover information to additionally
                // namespace the pretty identifier in an attempt to match our
                // original rust-analysis behavior.  This ended up only working
                // for fields (where we also would populate size_bytes and
                // offset_bytes above).  Bug 1881645 provides some more context
                // but the general situation is that this specific logic can
                // likely be removed as part of a nice clean-up, but it's also
                // worth revisiting the symbol and pretty mappings with more
                // intent.
                //
                // That said, there may be other SCIP languages where this could
                // still be a useful hack.
                let doc_namespace: Option<String> = None;

                // High confidence identifier name of what's being defined for
                // fallback use in the case of locals as extracted from the doc
                // strings.
                //
                // Because the doc strings are intended to be human-readable
                // rather than machine-readable, this may not always be
                // something we can reliably parse.  In particular, rust-analyzer
                // likes to excerpt the declaration, and our whole point in
                // using SCIP is to not be writing our own rust parser, although
                // we can probably evolve good-enough regexps, etc.
                //
                // TODO: Consider allowing for a fix-up pass when processing the
                // occurrences when we can potentially have the underlying token
                // available and/or a full tree-sitter parse.
                let mut fallback_kind = None;
                let mut doc_name = None;
                let mut type_pretty = None;

                lazy_static! {
                    // used for fields, methods, arguments/parameters
                    static ref RE_TS_TYPED: Regex =
                        Regex::new(r"^```ts\n([^ ]+) (.+): ([^\n]+)\n```$").unwrap();
                    // used for modules, classes
                    static ref RE_TS_UNTYPED: Regex =
                        Regex::new(r"^```ts\n([^ ]+) (.+)\n```$").unwrap();
                    // used for modules, classes
                    static ref RE_KT_FUNCTION: Regex =
                        Regex::new(r"^```kt\n([^ ]+) ([^ ]+) fun ([^ ]+)\(.*\)(?:: (.*))?\n```$").unwrap();
                }

                // TODO: Consider trying to do something where the documentation
                // is actually a real docstring.
                for (i, doc) in scip_sym_info.documentation.iter().enumerate() {
                    if i == 0 {
                        match &lang {
                            ScipLang::Python => {
                                // TODO: try and extract some info from here;
                                // It looks like this could be very descriptor-dependent.
                            }
                            ScipLang::Rust => {
                                // We no longer do anything for rust.  See the
                                // doc_namespace comments above.
                            }
                            ScipLang::Typescript => {
                                if let Some(caps) = RE_TS_TYPED.captures(doc) {
                                    if let Some(s) = caps.get(1) {
                                        fallback_kind =
                                            Some(s.as_str().trim_matches(|c| c == '(' || c == ')'));
                                    }
                                    if let Some(s) = caps.get(2) {
                                        doc_name = Some(s.as_str().to_string());
                                    }
                                    if let Some(s) = caps.get(3) {
                                        type_pretty = Some(ustr(s.as_str()));
                                    }
                                } else if let Some(caps) = RE_TS_UNTYPED.captures(doc) {
                                    if let Some(s) = caps.get(2) {
                                        doc_name = Some(s.as_str().to_string());
                                    }
                                }
                            }
                            ScipLang::Jvm => {
                                if let Some(caps) = RE_KT_FUNCTION.captures(doc) {
                                    fallback_kind = Some("method");
                                    if let Some(s) = caps.get(3) {
                                        doc_name = Some(s.as_str().to_string());
                                    }
                                }
                            }
                        }
                    }
                    // Otherwise this is an extracted docstring, which we can't
                    //use yet.
                }

                // Signature documentation is new and a more reliable source of
                // type information.
                if let Some(doc) = scip_sym_info.signature_documentation.as_ref() {
                    if !doc.text.is_empty() {
                        type_pretty = Some(ustr(&doc.text));
                    }
                }

                let mut symbol_info = analyse_symbol(
                    &scip_sym,
                    &lang,
                    lang_name,
                    subtree_name,
                    &doc.relative_path,
                    doc_name.as_deref(),
                    doc_namespace.as_deref(),
                );

                let mut supers = vec![];
                let mut overrides = vec![];

                // SCIP provides the full transitive closure of relationships,
                // but our current model favors only having the immediate links.
                // TODO: filter out indirect ancestors
                for rel in &scip_sym_info.relationships {
                    let Ok(rel_scip_sym) = scip::symbol::parse_symbol(&rel.symbol) else {
                        info!("bad relationship symbol: {}", rel.symbol);
                        continue;
                    };
                    let parent_symbol_info = analyse_symbol(
                        &rel_scip_sym,
                        &lang,
                        lang_name,
                        subtree_name,
                        &doc.relative_path,
                        None,
                        None,
                    );

                    // If our symbol is a local (or if we failed to unwrap its kind for any reason),
                    // fallback to the kind of the symbol it is related to
                    if symbol_info.kind.is_none() {
                        symbol_info.kind = parent_symbol_info.kind;
                    }
                    match symbol_info.kind {
                        Some("class") => {
                            supers.push(StructuredSuperInfo {
                                sym: ustr(&parent_symbol_info.norm_sym),
                                offset_bytes: 0,
                                props: vec![],
                            });
                        }
                        Some("method") => {
                            overrides.push(StructuredOverrideInfo {
                                sym: ustr(&parent_symbol_info.norm_sym),
                            });
                        }
                        _ => {}
                    }
                    // Add this symbol to consideration for needing a fake structured rep generated
                    // if we don't already know about it.  (We will also remove things from the map
                    // as we add entries to scip_symbol_to_structured.)
                    let urel_sym = ustr(&rel.symbol);
                    if !scip_symbol_to_structured.contains_key(&urel_sym) {
                        possible_unknown_scip_symbols.insert(urel_sym, parent_symbol_info);
                    }
                }

                // Ensure that supers and overrides are sorted to avoid flaky tests
                supers.sort_unstable_by_key(|r| r.sym);
                overrides.sort_unstable_by_key(|r| r.sym);

                let structured = AnalysisStructured {
                    structured: StructuredTag::Structured,
                    pretty: symbol_info.pretty,
                    sym: symbol_info.norm_sym,
                    type_pretty,
                    kind: ustr(symbol_info.kind.or(fallback_kind).unwrap_or("")),
                    subsystem: None,
                    parent_sym: symbol_info.parent_sym,
                    slot_owner: None,
                    impl_kind: ustr("impl"),
                    size_bytes: if size_bytes > 0 {
                        Some(size_bytes)
                    } else {
                        None
                    },
                    alignment_bytes: None,
                    own_vf_ptr_bytes: None,
                    binding_slots: vec![],
                    ontology_slots: vec![],
                    supers,
                    methods: vec![],
                    fields: vec![],
                    overrides,
                    props: vec![],
                    labels: BTreeSet::default(),

                    idl_sym: None,
                    subclass_syms: vec![],
                    overridden_by_syms: vec![],
                    variants: vec![],
                    extra: Map::default(),
                };
                // for local symbols we use our own symbol because the SCIP symbol is not
                // actually unique; we also need to update our helper mapping.
                if scip_sym_info.symbol.starts_with("local ") {
                    scip_symbol_to_structured.insert(symbol_info.norm_sym, structured);
                    // I don't think there should potentially be such a key, but there's no harm.
                    possible_unknown_scip_symbols.remove(&symbol_info.norm_sym);
                    our_symbol_to_scip_sym.insert(symbol_info.norm_sym, symbol_info.norm_sym);
                } else {
                    let usym = ustr(&scip_sym_info.symbol);
                    scip_symbol_to_structured.insert(usym, structured);
                    possible_unknown_scip_symbols.remove(&usym);
                    our_symbol_to_scip_sym.insert(symbol_info.norm_sym, usym);
                }

                if symbol_info.contributes_to_parent {
                    if let Some(psym) = symbol_info.parent_sym {
                        if let Some(scip_psym) = our_symbol_to_scip_sym.get(&psym) {
                            if let Some(pstruct) = scip_symbol_to_structured.get_mut(scip_psym) {
                                match &symbol_info.kind {
                                    Some("method") => {
                                        pstruct.methods.push(StructuredMethodInfo {
                                            pretty: symbol_info.pretty,
                                            sym: symbol_info.norm_sym,
                                            props: vec![],
                                            labels: BTreeSet::default(),
                                            // TODO: see about trying to extract args from markdown?
                                            args: vec![],
                                        });
                                    }
                                    Some("field") => {
                                        pstruct.fields.push(StructuredFieldInfo {
                                            line_range: ustr(""),
                                            pretty: symbol_info.pretty,
                                            sym: symbol_info.norm_sym,
                                            type_pretty: type_pretty.unwrap_or_else(|| ustr("")),
                                            type_sym: ustr(""),
                                            offset_bytes,
                                            bit_positions: None,
                                            size_bytes: if size_bytes > 0 {
                                                Some(size_bytes)
                                            } else {
                                                None
                                            },
                                            labels: BTreeSet::default(),
                                            pointer_info: vec![],
                                        });
                                    }
                                    _ => {}
                                }
                            }
                        }
                    }
                }
            } else {
                warn!(
                    "Unable to parse SCIP symbol: {}\n:{:?}",
                    scip_sym_info.symbol,
                    scip::symbol::parse_symbol(&scip_sym_info.symbol)
                );
            }
        }
    }

    let analysis_root = Path::new(&tree_config.paths.index_path).join(match platform {
        None => "analysis".to_string(),
        Some(platform) => format!("analysis-{}", platform),
    });

    // We keep track of what SCIP symbols we have emitted structured records
    // for so we can emit what's leftover at the end.
    let mut emitted_scip_structured: UstrSet = UstrSet::default();

    for doc in &index.documents {
        let searchfox_path = Path::new(&doc.relative_path).to_owned();
        let searchfox_path = Path::new(&subtree_root).to_owned().join(&searchfox_path);

        let output_file = analysis_root.join(&searchfox_path);
        if let Err(err) = create_output_dir(&output_file) {
            error!(
                "Couldn't create dir for: {}, {:?}",
                output_file.display(),
                err
            );
            continue;
        }
        let mut file = match File::create(&output_file) {
            Ok(f) => f,
            Err(err) => {
                error!(
                    "Couldn't open output file: {}, {:?}",
                    output_file.display(),
                    err
                );
                continue;
            }
        };

        let source_fname =
            tree_config.find_source_file(&format!("{}/{}", subtree_root, &doc.relative_path));
        let source_contents = match std::fs::read(source_fname.clone()) {
            Ok(f) => f,
            Err(_) => {
                error!("Unable to open source file {}", source_fname);
                continue;
            }
        };

        // XXX Because typescript has different languages for typescript and tsx,
        // we currently need to create the parser and the queries on a per-file
        // basis.  We should revisit this if profiling shows this is a big problem.
        // Same thing with Java and Kotlin
        let mut parser = tree_sitter::Parser::new();
        let (ts_lang, ts_nesting): (tree_sitter::Language, &Vec<SitterNesting>) = match &lang {
            ScipLang::Python => (tree_sitter_python::LANGUAGE.into(), &PYTHON_NESTING),
            ScipLang::Rust => (tree_sitter_rust::LANGUAGE.into(), &RUST_NESTING),
            ScipLang::Typescript => {
                if doc.relative_path.ends_with(".tsx") || doc.relative_path.ends_with(".jsx") {
                    (tree_sitter_typescript::LANGUAGE_TSX.into(), &JS_NESTING)
                } else {
                    (
                        tree_sitter_typescript::LANGUAGE_TYPESCRIPT.into(),
                        &JS_NESTING,
                    )
                }
            }
            ScipLang::Jvm => {
                if doc.relative_path.ends_with(".kt") {
                    (tree_sitter_kotlin_ng::LANGUAGE.into(), &KOTLIN_NESTING)
                } else {
                    (tree_sitter_java::LANGUAGE.into(), &JAVA_NESTING)
                }
            }
        };
        parser
            .set_language(&ts_lang)
            .expect("Error loading grammar");
        let ts_query = compile_nesting_queries(&ts_lang, ts_nesting);
        let name_capture_ix = ts_query.capture_index_for_name("name").unwrap();
        let body_capture_ix = ts_query.capture_index_for_name("body").unwrap();

        let parse_tree = match parser.parse(&source_contents[..], None) {
            Some(t) => t,
            _ => {
                warn!("tree-sitter parse failed, skipping file.");
                continue;
            }
        };

        let mut query_cursor = tree_sitter::QueryCursor::new();
        let mut query_matches =
            query_cursor.matches(&ts_query, parse_tree.root_node(), &source_contents[..]);
        let mut next_parse_match;
        let mut next_parse_loc = Location::default();
        let mut next_parse_nesting = SourceRange::default();

        let mut nesting_stack: Vec<NestedSymbol> = vec![];

        // Be chatty about the files we're outputting so that it's easier to follow
        // the path of rust analysis generation.
        info!(
            "Processing occurrences for '{}' to '{}'",
            searchfox_path.display(),
            output_file.display()
        );

        for occurrence in &doc.occurrences {
            // We need to normalize locals to include the file path, consistent
            // with how we handled them in the prior pass.  Note that this is
            // not actually an official SCIP symbol, but because the local symbols
            // are not namespaced, we just store it using our normalized version.
            let (is_local, norm_scip_sym) = if occurrence.symbol.starts_with("local ") {
                (
                    true,
                    symbol_name(
                        lang_name,
                        subtree_name,
                        &format!(
                            "{}/#{}",
                            sanitize_symbol(&doc.relative_path),
                            &occurrence.symbol[6..]
                        ),
                    ),
                )
            } else {
                (false, ustr(&occurrence.symbol))
            };

            let sinfo = match scip_symbol_to_structured.get(&norm_scip_sym) {
                Some(s) => s,
                None => {
                    // For occurences that don't match any symbol, we create a new structured fake,
                    // save it, and return it.

                    let symbol = match scip::symbol::parse_symbol(&occurrence.symbol) {
                        Ok(s) => s,
                        Err(e) => {
                            error!("{:?}", e);
                            continue;
                        }
                    };

                    let symbol_info = analyse_symbol(
                        &symbol,
                        &lang,
                        lang_name,
                        subtree_name,
                        &doc.relative_path,
                        None,
                        None,
                    );

                    let fake = AnalysisStructured {
                        structured: StructuredTag::Structured,
                        pretty: symbol_info.pretty,
                        sym: symbol_info.norm_sym,
                        type_pretty: None,
                        kind: ustr(symbol_info.kind.unwrap_or("")),
                        subsystem: None,
                        parent_sym: symbol_info.parent_sym,
                        slot_owner: None,
                        // Introducing the concept of "external" here to make it
                        // more obvious when we generate a fake structured
                        // record and that it is missing much of the expected
                        // metadata.
                        impl_kind: ustr("external"),
                        size_bytes: None,
                        alignment_bytes: None,
                        own_vf_ptr_bytes: None,
                        binding_slots: vec![],
                        ontology_slots: vec![],
                        supers: vec![],
                        methods: vec![],
                        fields: vec![],
                        overrides: vec![],
                        props: vec![],
                        labels: BTreeSet::default(),

                        idl_sym: None,
                        subclass_syms: vec![],
                        overridden_by_syms: vec![],
                        variants: vec![],
                        extra: Map::default(),
                    };
                    scip_symbol_to_structured.insert(norm_scip_sym, fake);
                    possible_unknown_scip_symbols.remove(&norm_scip_sym);
                    our_symbol_to_scip_sym.insert(symbol_info.norm_sym, norm_scip_sym);
                    scip_symbol_to_structured.get(&norm_scip_sym).unwrap()
                }
            };
            let loc = scip_range_to_searchfox_location(&occurrence.range);
            let kind = scip_roles_to_searchfox_analysis_kind(occurrence.symbol_roles);

            // ## Tree-Sitter Nesting Magic
            //
            // Goals:
            // 1. Provide `nesting_range` information for source records for
            //    relevant namespaces.
            // 2. Contribute context/contextsym information to target records.
            //    These should usually line up with the nesting information we
            //    want from the above.
            //
            // There are broadly 2 implementation approaches:
            // 1. Detailed AST traversal: We walk the AST ourselves and keep our
            //    cursor close to the occurrences.
            // 2. Use tree-sitter's query mechanism to have it give us a
            //    filtered set of the AST nodes that should correspond to the
            //    relevant nesting and namespace nesting levels.
            //
            // We're going with the 2nd approach because anything we do is going
            // to need to involve language-specific configuration data, and it
            // seems very silly to re-invent a worse version of the query
            // mechanism.
            //
            // A very nice thing about the query mechanism is it can tell you
            // which pattern index matched, which means we can define a little
            // data structure that we use to derive the query and also have
            // structured rust data that we can consult without any hacks.
            // (NB: As documented on the SitterNesting type, we probably should
            // be using a different approach based on `.scm` files.)

            // Pop off any nested symbols which don't include the current line.
            while let Some(nested) = nesting_stack.last() {
                if loc.lineno > nested.nesting_range.end_lineno {
                    nesting_stack.pop();
                } else {
                    break;
                }
            }

            // Check if this symbol starts a nesting range or if we need to skip.
            // We defer pushing any nesting we start to simplify the logic below
            // that's nesting aware; a symbol shouldn't be its own contextsym!

            // Skip any matched symbols that are earlier than our current symbol.
            while next_parse_loc < loc {
                next_parse_match = query_matches.next();
                (next_parse_loc, next_parse_nesting) = if let Some(pm) = &next_parse_match {
                    (
                        node_range_to_searchfox_location(
                            pm.nodes_for_capture_index(name_capture_ix)
                                .next()
                                .unwrap()
                                .range(),
                        ),
                        node_range_to_searchfox_range(
                            pm.nodes_for_capture_index(body_capture_ix)
                                .next()
                                .unwrap()
                                .range(),
                        ),
                    )
                } else {
                    (
                        Location {
                            lineno: u32::MAX,
                            col_start: 0,
                            col_end: 0,
                        },
                        SourceRange {
                            start_lineno: u32::MAX,
                            start_col: 0,
                            end_lineno: u32::MAX,
                            end_col: 0,
                        },
                    )
                }
            }

            // If we match the name, then push this.
            let starts_nest = if next_parse_loc == loc {
                // Note that it's possible this current approach could result
                // with us defining the start of 2+ nesting ranges on the same
                // line.  format.rs handles this and we similarly require any
                // other consumers to handle this.
                Some(NestedSymbol {
                    sym: sinfo.sym,
                    pretty: sinfo.pretty,
                    nesting_range: next_parse_nesting.clone(),
                })
            } else {
                None
            };

            let no_crossref = is_local && sinfo.supers.is_empty() && sinfo.overrides.is_empty();

            {
                let mut syntax = vec![kind.to_ustr()];
                if !sinfo.kind.is_empty() {
                    syntax.push(sinfo.kind);
                }
                let source_data = WithLocation {
                    data: AnalysisSource {
                        source: SourceTag::Source,
                        syntax,
                        pretty: ustr(&format!("{} {}", sinfo.kind, sinfo.pretty)),
                        sym: vec![sinfo.sym],
                        no_crossref,
                        nesting_range: if let Some(nest) = &starts_nest {
                            nest.nesting_range.clone()
                        } else {
                            SourceRange::default()
                        },
                        // TODO: Expose type information for fields/etc.
                        type_pretty: sinfo.type_pretty,
                        type_sym: None,
                        arg_ranges: vec![],
                        expansion_info: None,
                        confidence: None,
                    },
                    loc,
                };
                write_line(&mut file, &source_data);
            }

            // If this was the definition point, then write out the structured record.
            if kind == AnalysisKind::Def {
                emitted_scip_structured.insert(norm_scip_sym);
                write_line(&mut file, &WithLocation { data: sinfo, loc });
            }

            // TODO: Contextual info.

            if !no_crossref {
                let (contextsym, context) = if let Some(nested) = nesting_stack.last() {
                    (nested.sym, nested.pretty)
                } else {
                    (ustr(""), ustr(""))
                };

                let target_data = WithLocation {
                    data: AnalysisTarget {
                        target: TargetTag::Target,
                        kind,
                        pretty: sinfo.pretty,
                        sym: sinfo.sym,
                        context,
                        contextsym,
                        peek_range: LineRange {
                            start_lineno: 0,
                            end_lineno: 0,
                        },
                        arg_ranges: vec![],
                    },
                    loc,
                };
                write_line(&mut file, &target_data);
            }

            // Push the nesting now that we've finished processing the symbol
            // and this avoids any consultations of the stack above.
            if let Some(nested) = starts_nest {
                nesting_stack.push(nested);
            }
        }
    }

    // ## Create fake structured records for any remaining unknown scip symbols
    for symbol_info in possible_unknown_scip_symbols.into_values() {
        let fake = AnalysisStructured {
            structured: StructuredTag::Structured,
            pretty: symbol_info.pretty,
            sym: symbol_info.norm_sym,
            type_pretty: None,
            kind: ustr(symbol_info.kind.unwrap_or("")),
            subsystem: None,
            parent_sym: symbol_info.parent_sym,
            slot_owner: None,
            // (see previous use above for more context)
            impl_kind: ustr("external"),
            size_bytes: None,
            alignment_bytes: None,
            own_vf_ptr_bytes: None,
            binding_slots: vec![],
            ontology_slots: vec![],
            supers: vec![],
            methods: vec![],
            fields: vec![],
            overrides: vec![],
            props: vec![],
            labels: BTreeSet::default(),

            idl_sym: None,
            subclass_syms: vec![],
            overridden_by_syms: vec![],
            variants: vec![],
            extra: Map::default(),
        };
        scip_symbol_to_structured.insert(symbol_info.norm_sym, fake);
    }

    // ## Emit any external structured records we didn't already emit
    {
        // Let's name the analysis file after the SCIP file.
        let output_file = analysis_root.join(scip_file.file_name().unwrap());
        if let Err(err) = create_output_dir(&output_file) {
            error!(
                "Couldn't create dir for: {}, {:?}",
                output_file.display(),
                err
            );
            return;
        }
        let mut file = match File::create(&output_file) {
            Ok(f) => f,
            Err(err) => {
                error!(
                    "Couldn't open output file: {}, {:?}",
                    output_file.display(),
                    err
                );
                return;
            }
        };

        for (scip_sym, sinfo) in scip_symbol_to_structured {
            if !emitted_scip_structured.contains(&scip_sym) {
                write_line(
                    &mut file,
                    &WithLocation {
                        data: sinfo,
                        loc: Location {
                            lineno: 0,
                            col_start: 0,
                            col_end: 0,
                        },
                    },
                );
            }
        }
    }

    assert_eq!(file.pos(), byte_count, "Should've processed the whole file");
}

fn main() {
    env_logger::init();

    let cli = ScipIndexerCli::parse();

    let tree_name = &cli.tree_name;
    let cfg = config::load(&cli.config_file, false, Some(tree_name), None, None);
    let tree_config = cfg.trees.get(tree_name).unwrap();

    for file in cli.inputs {
        analyze_using_scip(
            tree_config,
            cli.subtree_name.as_deref(),
            &cli.subtree_root,
            &cli.platform,
            file,
        );
    }
}

```

## tools/src/bin/css-analyze.rs
```
use std::env;
use std::fs;
use tools::css_analyzer;

fn main() {
    let args: Vec<String> = env::args().collect();
    let base_path = args[1].clone();
    let path = args[2].clone();
    let full_path = format!("{base_path}/{path}");
    let text = match fs::read_to_string(full_path.clone()) {
        Ok(text) => text,
        _ => return,
    };
    let mut callback = |s| {
        println!("{}", s);
    };
    css_analyzer::analyze_css(path, 1, text, &mut callback);
}

```

## tools/src/bin/output-file.rs
```
use std::collections::HashMap;
use std::env;
use std::ffi::OsStr;
use std::fs;
use std::fs::File;
use std::io;
use std::io::BufRead;
use std::io::BufReader;
use std::io::BufWriter;
use std::io::Read;
use std::io::Seek;
use std::io::Write;
use std::path::Path;
use std::time::Instant;

use lazy_static::lazy_static;
use regex::Regex;
use tools::file_format::config;
use tools::file_format::per_file_info::read_detailed_file_info;
use tools::file_format::per_file_info::FileLookupMap;
use tools::templating::builder::build_and_parse;
use ustr::ustr;

extern crate env_logger;
extern crate log;
extern crate tools;
use crate::languages::FormatAs;
use tools::doc_trees_handler::find_doc_url;
use tools::file_format::analysis::{read_analysis, read_source};
use tools::file_format::crossref_lookup::CrossrefLookupMap;
use tools::format::{create_markdown_panel_section, format_file_data};
use tools::languages;
use tools::url_encode_path::url_encode_path;

use tools::output::{PanelItem, PanelSection};

extern crate flate2;
use flate2::write::GzEncoder;
use flate2::Compression;

fn main() {
    env_logger::init();

    let args: Vec<_> = env::args().collect();
    // TODO: refactor _fname_args out of existence; we now require paths to come via stdin.
    let (base_args, _fname_args) = args.split_at(5);

    let mut stdout = io::stdout().lock();

    let pre_config = Instant::now();
    let tree_name = &base_args[2];
    let cfg = config::load(
        &base_args[1],
        true,
        Some(tree_name),
        Some(base_args[3].to_string()),
        Some(base_args[4].to_string()),
    );
    writeln!(
        stdout,
        "Config file read, duration: {}us",
        pre_config.elapsed().as_micros() as u64
    )
    .unwrap();

    let tree_config = cfg.trees.get(tree_name).unwrap();

    let pre_templates = Instant::now();
    let source_file_info_boxes_liquid_str = cfg
        .read_tree_config_file_with_default("source_file_info_boxes.liquid")
        .unwrap();
    let source_file_info_boxes_template = build_and_parse(&source_file_info_boxes_liquid_str);
    let source_file_other_tools_panel_liquid_str = cfg
        .read_tree_config_file_with_default("source_file_other_tools_panels.liquid")
        .unwrap();
    let source_file_other_tools_panel_template =
        build_and_parse(&source_file_other_tools_panel_liquid_str);
    writeln!(
        stdout,
        "Tree templates read, duration: {}us",
        pre_templates.elapsed().as_micros() as u64
    )
    .unwrap();

    let jumpref_path = format!("{}/jumpref", tree_config.paths.index_path);
    let jumpref_extra_path = format!("{}/jumpref-extra", tree_config.paths.index_path);

    let pre_jumpref = Instant::now();
    let jumpref_lookup_map = CrossrefLookupMap::new(&jumpref_path, &jumpref_extra_path);

    writeln!(
        stdout,
        "Jumpref opened, duration: {}us",
        pre_jumpref.elapsed().as_micros() as u64
    )
    .unwrap();

    let pre_lookup_map = Instant::now();
    let file_lookup_path = format!(
        "{}/concise-per-file-info.json",
        tree_config.paths.index_path
    );
    let file_lookup_map = FileLookupMap::new(&file_lookup_path);
    writeln!(
        stdout,
        "FileLookupMap loadded, duration: {}us",
        pre_lookup_map.elapsed().as_micros() as u64
    )
    .unwrap();

    let pre_blame_prep = Instant::now();
    let (blame_commit, head_oid) = match tree_config.git {
        Some(ref git) => {
            let head_oid = git.repo.refname_to_id("HEAD").unwrap();
            let blame_commit = if let Some(ref blame_repo) = git.blame_repo {
                let blame_oid = blame_repo.refname_to_id("HEAD").unwrap();
                Some(blame_repo.find_commit(blame_oid).unwrap())
            } else {
                None
            };
            (blame_commit, Some(head_oid))
        }
        None => (None, None),
    };

    let head_commit =
        head_oid.and_then(|oid| tree_config.git.as_ref().unwrap().repo.find_commit(oid).ok());

    writeln!(
        stdout,
        "Blame prep done, duration: {}us",
        pre_blame_prep.elapsed().as_micros() as u64
    )
    .unwrap();

    let mut extension_mapping = HashMap::new();
    extension_mapping.insert("cpp", ("header", vec!["h", "hh", "hpp", "hxx"]));
    extension_mapping.insert("cc", ("header", vec!["h", "hh", "hpp", "hxx"]));
    extension_mapping.insert("cxx", ("header", vec!["h", "hh", "hpp", "hxx"]));
    extension_mapping.insert("h", ("source", vec!["cpp", "cc", "cxx"]));
    extension_mapping.insert("hh", ("source", vec!["cpp", "cc", "cxx"]));
    extension_mapping.insert("hpp", ("source", vec!["cpp", "cc", "cxx"]));
    extension_mapping.insert("hxx", ("source", vec!["cpp", "cc", "cxx"]));

    let mut stdin = io::stdin().lock();

    let mut path_buf = String::new();
    #[allow(clippy::unit_cmp)]
    while () == path_buf.clear() && stdin.read_line(&mut path_buf).unwrap() > 0 {
        let path = ustr(path_buf.trim_end());
        let file_start = Instant::now();
        writeln!(stdout, "File '{}'", path).unwrap();

        let output_fname = format!("{}/file/{}", tree_config.paths.index_path, path);
        let gzip_output_fname = format!("{}.gz", output_fname);
        let source_fname = tree_config.find_source_file(&path);

        let format = languages::select_formatting(&path);

        // Create a zero length output file with the normal name for nginx
        // try_files reasons... UNLESS the file we're dealing with already has
        // a ".gz" suffix.  (try_files isn't aware of the gzip_static magic and
        // so looks for a file with the exact non-.gz suffix, which means it has
        // to exist and so we normally need to create one.)
        //
        // The general problem scenario are tests where there's a file "FOO" and
        // its gzipped variant "FOO.gz" in the tree.  In that case there's an
        // overlap for "FOO.gz".  When processing "FOO.gz" we will write to
        // "FOO.gz.gz" but also want the file "FOO.gz" to exist.  And for "FOO"
        // we will write to "FOO.gz" and want "FOO" to exist.  If our heuristic
        // is to not create the zero-length file for "FOO.gz", we win and don't
        // have to worry about pathological races as long as the source file
        // "FOO" exists.  But if it doesn't our try_files logic will never allow
        // the user to view the (gibberish for humans) "FOO.gz" source file.
        //
        // So we use that heuristic.  Because I'm a human.  A lazy, lazy human.
        // Robots or non-lazy humans are welcome to contribute better fixes for
        // this and will be showered with praise.
        if !output_fname.ends_with(".gz") {
            File::create(output_fname).unwrap();
        }
        let output_file = File::create(gzip_output_fname).unwrap();
        let raw_writer = BufWriter::new(output_file);
        let mut writer = GzEncoder::new(raw_writer, Compression::default());

        let source_file = match File::open(source_fname.clone()) {
            Ok(f) => f,
            Err(_) => {
                writeln!(stdout, "Unable to open source file '{}'", source_fname).unwrap();
                continue;
            }
        };

        let path_wrapper = Path::new(&source_fname);
        let metadata = fs::symlink_metadata(path_wrapper).unwrap();
        if metadata.file_type().is_symlink() {
            let dest = fs::read_link(path_wrapper).unwrap();
            write!(writer, "Symlink to '{}'", dest.to_str().unwrap()).unwrap();
            continue;
        }

        let mut reader = BufReader::new(&source_file);

        if let FormatAs::Binary = format {
            let _ = io::copy(&mut reader, &mut writer);
            continue;
        };

        let pre_analysis_load = Instant::now();
        let analysis_fname = format!("{}/analysis/{}", tree_config.paths.index_path, path);
        let analysis = read_analysis(&analysis_fname, &mut read_source);
        writeln!(
            stdout,
            "  Analysis load duration: {}us",
            pre_analysis_load.elapsed().as_micros() as u64
        )
        .unwrap();

        let pre_per_file_info = Instant::now();
        let concise_info = file_lookup_map.lookup_file_from_ustr(&path).unwrap();
        let detailed_info = read_detailed_file_info(&path, &tree_config.paths.index_path).unwrap();
        writeln!(
            stdout,
            "  Per-file info load duration: {}us",
            pre_per_file_info.elapsed().as_micros() as u64
        )
        .unwrap();

        let pre_file_read = Instant::now();
        let mut input = String::new();
        match reader.read_to_string(&mut input) {
            Ok(_) => {}
            Err(_) => {
                let mut bytes = Vec::new();
                reader.seek(std::io::SeekFrom::Start(0)).unwrap();
                match reader.read_to_end(&mut bytes) {
                    Ok(_) => {
                        input.push_str(&bytes.iter().map(|c| *c as char).collect::<String>());
                    }
                    Err(e) => {
                        writeln!(
                            stdout,
                            "Unable to read source file '{}': {:?}",
                            source_fname, e
                        )
                        .unwrap();
                        continue;
                    }
                }
            }
        }
        writeln!(
            stdout,
            "  File contents read duration: {}us",
            pre_file_read.elapsed().as_micros() as u64
        )
        .unwrap();

        let extension = path_wrapper
            .extension()
            .unwrap_or(OsStr::new(""))
            .to_str()
            .unwrap();
        let show_header = match extension_mapping.get(extension) {
            Some((description, try_extensions)) => {
                let mut result = None;
                for try_ext in try_extensions {
                    let try_buf = path_wrapper.with_extension(try_ext);
                    let try_path = try_buf.as_path();
                    if try_path.exists() {
                        let (path_base, _) = path.split_at(path.len() - extension.len() - 1);
                        result = Some((
                            description.to_owned(),
                            format!("/{}/source/{}.{}", tree_name, path_base, try_ext),
                        ));
                        break;
                    }
                }
                result
            }
            None => None,
        };

        let mut panel = vec![];

        let mut source_panel_items = vec![];
        if let Some((other_desc, other_path)) = show_header {
            source_panel_items.push(PanelItem {
                title: format!("Go to {} file", other_desc),
                link: url_encode_path(other_path.as_str()),
                update_link_lineno: "",
                accel_key: None,
                copyable: false,
            });
        };

        if !path.contains("__GENERATED__") {
            if let Some((product, component)) = concise_info.bugzilla_component {
                source_panel_items.push(PanelItem {
                    title: format!("File a bug in {} :: {}", product, component),
                    link: format!(
                        "https://bugzilla.mozilla.org/enter_bug.cgi?product={}&component={}",
                        product.replace("&", "%26"),
                        component.replace("&", "%26")
                    ),
                    update_link_lineno: "",
                    accel_key: None,
                    copyable: true,
                });
            }
        }

        if !source_panel_items.is_empty() {
            panel.push(PanelSection {
                name: "Source code".to_owned(),
                items: source_panel_items,
                raw_items: vec![],
            });
        };

        let encoded_path = url_encode_path(path.as_str());

        if let Some(oid) = head_oid {
            if !path.contains("__GENERATED__") {
                let mut vcs_panel_items = vec![];
                vcs_panel_items.push(PanelItem {
                    title: "Permalink".to_owned(),
                    link: format!("/{}/rev/{}/{}", tree_name, oid, encoded_path),
                    update_link_lineno: "#{}",
                    accel_key: Some('Y'),
                    copyable: true,
                });
                vcs_panel_items.push(PanelItem {
                    title: "Remove the Permalink".to_owned(),
                    link: format!("/{}/source/{}", tree_name, encoded_path),
                    update_link_lineno: "#{}",
                    accel_key: None,
                    copyable: false,
                });

                let gh_log_link = tree_config
                    .paths
                    .github_repo
                    .as_ref()
                    .map(|gh_root| format!("{}/commits/HEAD/{}", gh_root, encoded_path));
                let hg_log_link = tree_config
                    .paths
                    .hg_root
                    .as_ref()
                    .map(|hg_root| format!("{}/log/tip/{}", hg_root, encoded_path));
                if let Some(link) = gh_log_link {
                    vcs_panel_items.push(PanelItem {
                        title: "Git log".to_owned(),
                        link,
                        update_link_lineno: "",
                        accel_key: hg_log_link.is_none().then_some('L'),
                        copyable: true,
                    });
                }
                if let Some(link) = hg_log_link {
                    vcs_panel_items.push(PanelItem {
                        title: "Mercurial log".to_owned(),
                        link,
                        update_link_lineno: "",
                        accel_key: Some('L'),
                        copyable: true,
                    });
                }

                let gh_raw_link = tree_config
                    .paths
                    .github_repo
                    .as_ref()
                    .map(|gh_root| format!("{}/raw/HEAD/{}", gh_root, encoded_path));
                let hg_raw_link = tree_config
                    .paths
                    .hg_root
                    .as_ref()
                    .map(|hg_root| format!("{}/raw-file/tip/{}", hg_root, encoded_path));
                if let Some(link) = gh_raw_link.or(hg_raw_link) {
                    vcs_panel_items.push(PanelItem {
                        title: "Raw".to_owned(),
                        link,
                        update_link_lineno: "",
                        accel_key: Some('R'),
                        copyable: true,
                    });
                }

                if tree_config.paths.git_blame_path.is_some() {
                    vcs_panel_items.push(PanelItem {
                        title: "Blame".to_owned(),
                        link: "javascript:alert('Hover over the gray bar on the left to see blame information.')".to_owned(),
                        update_link_lineno: "",
                        accel_key: None,
                        copyable: false,
                    });
                }
                panel.push(PanelSection {
                    name: "Revision control".to_owned(),
                    items: vcs_panel_items,
                    raw_items: vec![],
                });
            }
        }

        panel.push(create_markdown_panel_section(true));

        let mut tools_items = vec![];
        if let Some(ref hg_root) = tree_config.paths.hg_root {
            tools_items.push(PanelItem {
                title: "HG Web".to_owned(),
                link: format!("{}/file/tip/{}", hg_root, encoded_path),
                update_link_lineno: "#l{}",
                accel_key: None,
                copyable: false,
            });
        }
        if let Some(ref ccov_root) = tree_config.paths.ccov_root {
            tools_items.push(PanelItem {
                title: "Code Coverage".to_owned(),
                link: format!(
                    "{}#revision=latest&path={}&view=file",
                    ccov_root, encoded_path
                ),
                update_link_lineno: "&line={}",
                accel_key: None,
                copyable: false,
            });
        }

        match Path::new(path.as_str()).extension().and_then(OsStr::to_str) {
            Some("md") | Some("rst") => {
                match find_doc_url(&cfg, path.as_str()) {
                    Some(url) => {
                        tools_items.push(PanelItem {
                            title: "Source Docs".to_owned(),
                            link: url,
                            update_link_lineno: "",
                            accel_key: None,
                            copyable: false,
                        });
                    }
                    None => {}
                }

                if let Some(ref github) = tree_config.paths.github_repo {
                    tools_items.push(PanelItem {
                        title: "GitHub Rendered view".to_owned(),
                        link: format!(
                            "{}/blob/{}/{}",
                            github,
                            head_oid.map_or("master".to_string(), |x| x.to_string()),
                            encoded_path
                        ),
                        update_link_lineno: "",
                        accel_key: None,
                        copyable: false,
                    });
                }
            }
            _ => {}
        }

        let liquid_globals = liquid::object!({
            "tree": tree_name,
            "path": &path,
            // Propagate config settings that aren't absolute paths.  We do some
            // renaming here compared to `TreeConfigPaths` for clarity.
            "config": {
                "coverage_url": &tree_config.paths.ccov_root.as_deref().unwrap_or(""),
                "github_repo_url": &tree_config.paths.github_repo.as_deref().unwrap_or(""),
                "hg_repo_url": &tree_config.paths.hg_root.as_deref().unwrap_or(""),
                "wpt_root": &tree_config.paths.wpt_root.as_deref().unwrap_or(""),
            },
            "concise": &concise_info,
            "detailed": &detailed_info,
        });

        lazy_static! {
            static ref RE_WHITESPACE_CLEANUP: Regex = Regex::new(r#"(\n *)+\n"#).unwrap();
        }

        let mut source_file_info_boxes = source_file_info_boxes_template
            .render(&liquid_globals)
            .unwrap();
        // It's really difficult to get whitespace right in the templates right now.
        // While it probably makes sense to just pass what we get from this through
        // a formatter in general, for now let's at least just use this exciting
        // regex to clean things up.
        source_file_info_boxes = RE_WHITESPACE_CLEANUP
            .replace_all(&source_file_info_boxes, "\n")
            .to_string();
        let source_file_other_tools_panels = source_file_other_tools_panel_template
            .render(&liquid_globals)
            .unwrap()
            .trim()
            .to_string();

        if !tools_items.is_empty() || !source_file_other_tools_panels.is_empty() {
            panel.push(PanelSection {
                name: "Other Tools".to_owned(),
                items: tools_items,
                raw_items: vec![source_file_other_tools_panels],
            });
        }

        match format_file_data(
            &cfg,
            tree_name,
            &panel,
            source_file_info_boxes,
            &head_commit,
            &blame_commit,
            &path,
            input,
            &jumpref_lookup_map,
            &analysis,
            &detailed_info.coverage_lines,
            &mut writer,
        ) {
            Ok(perf_info) => {
                writeln!(
                    stdout,
                    "  Format code duration: {}us",
                    perf_info.format_code_duration_us
                )
                .unwrap();
                writeln!(
                    stdout,
                    "  Blame lines duration: {}us",
                    perf_info.blame_lines_duration_us
                )
                .unwrap();
                writeln!(
                    stdout,
                    "  Commit info duration: {}us",
                    perf_info.commit_info_duration_us
                )
                .unwrap();
                writeln!(
                    stdout,
                    "  Format mixing duration: {}us",
                    perf_info.format_mixing_duration_us
                )
                .unwrap();
            }
            Err(err) => {
                // Make sure our output log file indicates what happened.
                writeln!(stdout, "  warning: format_file_data failed: {}", err).unwrap();
                // Also embed the error into the output file
                writeln!(writer, "<h3>format_file_data failed: {}</h3>", err).unwrap();
            }
        }

        writer.finish().unwrap();
        writeln!(
            stdout,
            "  Total writing duration: {}us",
            file_start.elapsed().as_micros() as u64
        )
        .unwrap();
    }
    writeln!(stdout, "Done writing files.").unwrap();
}

```

## tools/src/bin/rust-indexer.rs
```
extern crate clap;
extern crate env_logger;
#[macro_use]
extern crate log;
extern crate protobuf;
extern crate rls_analysis;
extern crate rls_data as data;
extern crate scip;
extern crate tools;

use crate::data::GlobalCrateId;
use crate::data::{DefKind, ImplKind};
use clap::Parser;
use rls_analysis::{AnalysisHost, AnalysisLoader, SearchDirectory};
use serde_json::to_string;
use std::borrow::Cow;
use std::collections::{BTreeSet, HashMap};
use std::fs::{self, File};
use std::hash::Hash;
use std::io;
use std::io::{BufRead, BufReader, Read, Seek};
use std::path::{Path, PathBuf};
use tools::file_format::analysis::{
    AnalysisKind, AnalysisSource, AnalysisTarget, LineRange, Location, SourceRange, SourceTag,
    TargetTag, WithLocation,
};
use ustr::ustr;

// Note: This file has been forked into scip-indexer.rs for generic SCIP
// ingestion purposes.  (And this comment is being added so git's copy
// detection has an easier time of realizing that!)

/// A global definition id in a crate.
///
/// FIXME(emilio): This key is kind of slow, because GlobalCrateId contains a
/// String. There's a "disambiguator" field which may be more than enough for
/// our purposes.
#[derive(Clone, Hash, Debug, Eq, PartialEq)]
pub struct DefId(GlobalCrateId, u32);

/// A map from global definition ids to the actual definition.
pub struct Defs {
    map: HashMap<DefId, data::Def>,
}

/// Local filesystem path mappings and metadata which exist for the following
/// purposes:
/// 1. Know where to output the analysis files.
///   - There is only ever one analysis output directory.
/// 2. Know how to locate rust source files in order to hackily extract strings
///    that should have been in the save-analysis files.
///    - After config scripts run and normalize things there are 2 source
///      directories: revision controlled source (cross-platform) and the
///      (per-platform) generated files directory.
#[derive(Debug)]
struct TreeInfo<'a> {
    /// Local filesystem path root for the analysis dir where rust-indexer.rs
    /// should write its output.
    out_analysis_dir: &'a Path,
    /// Local filesystem path root for the source tree.  In the searchfox path
    /// space presented to users, this means all paths not prefixed with
    /// `__GENERATED__`.
    srcdir: &'a Path,
    /// Local filesystem path root for the per-platform generated source tree.
    /// In the searchfox path space presented to users, this means paths
    /// prefixed with `__GENERATED__`.
    generated: &'a Path,
    /// The searchfox path space prefix for generated.
    generated_friendly: &'a Path,
}

fn construct_qualname(scope: &str, name: &str) -> String {
    // Some of the names don't start with ::, for example:
    //   __self_0_0$282
    //   <Loader>::new
    // Since we're gluing it to the "scope" (which might be a crate name)
    // we'll insert the :: to make it more readable
    let glue = if name.starts_with("::") { "" } else { "::" };
    format!("{}{}{}", scope, glue, name)
}

fn sanitize_symbol(sym: &str) -> String {
    // Downstream processing of the symbol doesn't deal well with
    // these characters, so replace them with underscores.
    fn is_special_char(c: char) -> bool {
        matches!(c, ',' | '.' | '(' | ')' | '-')
    }
    fn is_separator(c: char) -> bool {
        c.is_ascii_whitespace() || matches!(c, '#' | '/')
    }
    sym.replace(is_special_char, "_")
        .trim_matches('_')
        .replace(is_separator, "::")
        .trim_matches(':')
        .into()
}

fn pretty_symbol(sym: &str) -> Cow<str> {
    use scip::symbol::SymbolFormatOptions;
    if let Ok(sym) = scip::symbol::parse_symbol(sym) {
        return Cow::Owned(scip::symbol::format_symbol_with(
            sym,
            SymbolFormatOptions {
                include_scheme: false,
                include_package_manager: false,
                include_package_name: true,
                include_package_version: false,
                include_descriptor: true,
            },
        ));
    }
    Cow::Borrowed(sym)
}

// Given a definition, and the global crate id where that definition is found,
// return a qualified name that identifies the definition unambiguously.
fn crate_independent_qualname(def: &data::Def, crate_id: &data::GlobalCrateId) -> String {
    // For stuff with "no_mangle" functions or statics, or extern declarations,
    // we just use the name.
    //
    // TODO(emilio): Maybe there's a way to get the #[link_name] attribute from
    // here and make C++ agree with that? Though we don't use it so it may not
    // be worth the churn.
    fn use_unmangled_name(def: &data::Def) -> bool {
        match def.kind {
            DefKind::ForeignStatic | DefKind::ForeignFunction => true,
            DefKind::Static | DefKind::Function => {
                def.attributes.iter().any(|attr| attr.value == "no_mangle")
            }
            _ => false,
        }
    }

    if use_unmangled_name(def) {
        return def.name.clone();
    }

    construct_qualname(&crate_id.name, &def.qualname)
}

impl Defs {
    fn new() -> Self {
        Self {
            map: HashMap::new(),
        }
    }

    fn insert(&mut self, analysis: &data::Analysis, def: &data::Def) {
        let crate_id = analysis.prelude.as_ref().unwrap().crate_id.clone();
        let mut definition = def.clone();
        definition.qualname = crate_independent_qualname(def, &crate_id);

        let index = definition.id.index;
        let defid = DefId(crate_id, index);
        debug!("Indexing def: {:?} -> {:?}", defid, definition);
        let previous = self.map.insert(defid, definition);
        if let Some(previous) = previous {
            // This shouldn't happen, but as of right now it can happen with
            // some builtin definitions when highly generic types are involved.
            // This is probably a rust bug, just ignore it for now.
            debug!(
                "Found a definition with the same ID twice? {:?}, {:?}",
                previous, def,
            );
        }
    }

    /// Getter for a given local id, which takes care of converting to a global
    /// ID and returning the definition if present.
    fn get(&self, analysis: &data::Analysis, id: data::Id) -> Option<data::Def> {
        let prelude = analysis.prelude.as_ref().unwrap();
        let krate_id = if id.krate == 0 {
            prelude.crate_id.clone()
        } else {
            // TODO(emilio): This escales with the number of crates in this
            // particular crate, but it's probably not too bad, since it should
            // be a pretty fast linear search.
            let krate = prelude
                .external_crates
                .iter()
                .find(|krate| krate.num == id.krate);

            let krate = match krate {
                Some(k) => k,
                None => {
                    debug!("Crate not found: {:?}", id);
                    return None;
                }
            };

            krate.id.clone()
        };

        let id = DefId(krate_id, id.index);
        let result = self.map.get(&id).cloned();
        if result.is_none() {
            debug!("Def not found: {:?}", id);
        }
        result
    }
}

#[derive(Clone)]
pub struct Loader {
    deps_dirs: Vec<PathBuf>,
}

impl Loader {
    pub fn new(deps_dirs: Vec<PathBuf>) -> Self {
        Self { deps_dirs }
    }
}

impl AnalysisLoader for Loader {
    fn needs_hard_reload(&self, _: &Path) -> bool {
        true
    }

    fn fresh_host(&self) -> AnalysisHost<Self> {
        AnalysisHost::new_with_loader(self.clone())
    }

    fn set_path_prefix(&mut self, _: &Path) {}

    fn abs_path_prefix(&self) -> Option<PathBuf> {
        None
    }
    fn search_directories(&self) -> Vec<SearchDirectory> {
        self.deps_dirs
            .iter()
            .map(|pb| SearchDirectory {
                path: pb.clone(),
                prefix_rewrite: None,
            })
            .collect()
    }
}

fn def_kind_to_human(kind: DefKind) -> &'static str {
    match kind {
        DefKind::Enum => "enum",
        DefKind::Local => "local",
        DefKind::ExternType => "extern type",
        DefKind::Const => "constant",
        DefKind::Field => "field",
        DefKind::Function | DefKind::ForeignFunction => "function",
        DefKind::Macro => "macro",
        DefKind::Method => "method",
        DefKind::Mod => "module",
        DefKind::Static | DefKind::ForeignStatic => "static",
        DefKind::Struct => "struct",
        DefKind::Tuple => "tuple",
        DefKind::TupleVariant => "tuple variant",
        DefKind::Union => "union",
        DefKind::Type => "type",
        DefKind::Trait => "trait",
        DefKind::StructVariant => "struct variant",
    }
}

/// Potentially non-helpful mapping of impl kind.
fn impl_kind_to_human(kind: &ImplKind) -> &'static str {
    match kind {
        ImplKind::Inherent => "impl",
        ImplKind::Direct => "impl for",
        ImplKind::Indirect => "impl for ref",
        ImplKind::Blanket => "impl for where",
        _ => "impl for where deref",
    }
}

/// Given two spans, create a new super-span that encloses them both if the files match.  If the
/// files don't match, just return the first span as-is.
fn union_spans(a: &data::SpanData, b: &data::SpanData) -> data::SpanData {
    if a.file_name != b.file_name {
        return a.clone();
    }

    let (byte_start, line_start, column_start) = if a.byte_start < b.byte_start {
        (a.byte_start, a.line_start, a.column_start)
    } else {
        (b.byte_start, b.line_start, b.column_start)
    };

    let (byte_end, line_end, column_end) = if a.byte_end > b.byte_end {
        (a.byte_end, a.line_end, a.column_end)
    } else {
        (b.byte_end, b.line_end, b.column_end)
    };

    data::SpanData {
        file_name: a.file_name.clone(),
        byte_start,
        byte_end,
        line_start,
        line_end,
        column_start,
        column_end,
    }
}

/// For the purposes of trying to figure out the actual effective nesting range of some type of
/// definition, union its span (which just really covers the symbol name) plus the spans of all of
/// its descendants.  This should end up with a sufficiently reasonable line value.  This is a hack.
fn recursive_union_spans_of_def(
    def: &data::Def,
    file_analysis: &data::Analysis,
    defs: &Defs,
) -> data::SpanData {
    let mut span = def.span.clone();
    for id in &def.children {
        // It should already be the case that the children are in the same krate, but better safe
        // than sorry.
        if id.krate != def.id.krate {
            continue;
        }
        let kid = defs.get(file_analysis, *id);

        if let Some(ref kid) = kid {
            let rec_span = recursive_union_spans_of_def(kid, file_analysis, defs);
            span = union_spans(&span, &rec_span);
        }
    }

    span
}

/// Given a list of ids of defs, run recursive_union_spans_of_def on all of them and union up the
/// result.  Necessary for when dealing with impls.
fn union_spans_of_defs(
    initial_span: &data::SpanData,
    ids: &[data::Id],
    file_analysis: &data::Analysis,
    defs: &Defs,
) -> data::SpanData {
    let mut span = initial_span.clone();
    for id in ids {
        let kid = defs.get(file_analysis, *id);

        if let Some(ref kid) = kid {
            let rec_span = recursive_union_spans_of_def(kid, file_analysis, defs);
            span = union_spans(&span, &rec_span);
        }
    }

    span
}

/// If we unioned together a span that only covers 1 or 2 lines, normalize it to None because
/// nothing interesting will happen from a presentation perspective.  (If we had proper AST info
/// about the span, it would be appropriate to keep it and expose it, but this is all derived from
/// shoddy inference.)
fn ignore_boring_spans(span: &data::SpanData) -> Option<&data::SpanData> {
    match span {
        span if span.line_end.0 > span.line_start.0 + 1 => Some(span),
        _ => None,
    }
}

fn pretty_for_impl(imp: &data::Impl, qualname: &str) -> String {
    let mut pretty = impl_kind_to_human(&imp.kind).to_owned();
    pretty.push(' ');
    pretty.push_str(qualname);

    pretty
}

fn pretty_for_def(def: &data::Def, qualname: &str) -> String {
    let mut pretty = def_kind_to_human(def.kind).to_owned();
    pretty.push(' ');
    // We use the unsanitized qualname here because it's more human-readable
    // and the source-analysis pretty name is allowed to have commas and such
    pretty.push_str(qualname);

    pretty
}

fn visit_def(
    out_data: &mut BTreeSet<String>,
    kind: AnalysisKind,
    location: &data::SpanData,
    qualname: &str,
    def: &data::Def,
    context: Option<&str>,
    nesting: Option<&data::SpanData>,
) {
    let pretty = pretty_for_def(def, qualname);
    visit_common(
        out_data, kind, location, qualname, &pretty, context, nesting,
    );
}

fn visit_common(
    out_data: &mut BTreeSet<String>,
    kind: AnalysisKind,
    location: &data::SpanData,
    qualname: &str,
    pretty: &str,
    context: Option<&str>,
    nesting: Option<&data::SpanData>,
) {
    // Searchfox uses 1-indexed lines, 0-indexed columns.
    let col_end = if location.line_start != location.line_end {
        // Rust spans are multi-line... So we just use the start column as
        // the end column if it spans multiple rows, searchfox has fallback
        // code to handle this.
        location.column_start.zero_indexed().0
    } else {
        location.column_end.zero_indexed().0
    };
    let loc = Location {
        lineno: location.line_start.0,
        col_start: location.column_start.zero_indexed().0,
        col_end,
    };

    let sanitized = sanitize_symbol(qualname);
    let target_data = WithLocation {
        data: AnalysisTarget {
            target: TargetTag::Target,
            kind,
            pretty: ustr(&sanitized),
            sym: ustr(&sanitized),
            context: ustr(context.unwrap_or("")),
            contextsym: ustr(context.unwrap_or("")),
            peek_range: LineRange {
                start_lineno: 0,
                end_lineno: 0,
            },
            arg_ranges: vec![],
        },
        loc,
    };
    out_data.insert(to_string(&target_data).unwrap());

    let nesting_range = match nesting {
        Some(span) => SourceRange {
            // Hack note: These positions would ideally be those of braces.  But they're not, so
            // while the position:sticky UI stuff should work-ish, other things will not.
            start_lineno: span.line_start.0,
            start_col: span.column_start.zero_indexed().0,
            end_lineno: span.line_end.0,
            end_col: span.column_end.zero_indexed().0,
        },
        None => SourceRange {
            start_lineno: 0,
            start_col: 0,
            end_lineno: 0,
            end_col: 0,
        },
    };

    let source_data = WithLocation {
        data: AnalysisSource {
            source: SourceTag::Source,
            syntax: vec![],
            pretty: ustr(pretty),
            sym: vec![ustr(&sanitized)],
            no_crossref: false,
            nesting_range,
            // TODO: Expose type information for fields/etc.
            type_pretty: None,
            type_sym: None,
            arg_ranges: vec![],
            expansion_info: None,
            confidence: None,
        },
        loc,
    };
    out_data.insert(to_string(&source_data).unwrap());
}

/// Normalizes a searchfox user-visible relative file path to be an absolute
/// local filesystem path.  No attempt is made to validate the existence of the
/// path.  That's up to the caller.
fn searchfox_path_to_local_path(searchfox_path: &Path, tree_info: &TreeInfo) -> PathBuf {
    if let Ok(objdir_path) = searchfox_path.strip_prefix(tree_info.generated_friendly) {
        return tree_info.generated.join(objdir_path);
    }
    tree_info.srcdir.join(searchfox_path)
}

fn read_existing_contents(map: &mut BTreeSet<String>, file: &Path) {
    if let Ok(f) = File::open(file) {
        let reader = BufReader::new(f);
        for line in reader.lines() {
            map.insert(line.unwrap());
        }
    }
}

fn extract_span_from_source_as_buffer(
    reader: &mut File,
    span: &data::SpanData,
) -> io::Result<Box<[u8]>> {
    reader.seek(std::io::SeekFrom::Start(span.byte_start.into()))?;
    let len = (span.byte_end - span.byte_start) as usize;
    let mut buffer: Box<[u8]> = vec![0; len].into_boxed_slice();
    reader.read_exact(&mut buffer)?;
    Ok(buffer)
}

/// Given a reader and a span from that file, extract the text contained by the span.  If the span
/// covers multiple lines, then whatever newline delimiters the file has will be included.
///
/// In the event of a file read error or the contents not being valid UTF-8, None is returned.
/// We will log to log::Error in the event of a file read problem because this can be indicative
/// of lower level problems (ex: in vagrant), but not for utf-8 errors which are more expected
/// from sketchy source-files.
fn extract_span_from_source_as_string(reader: &mut File, span: &data::SpanData) -> Option<String> {
    match extract_span_from_source_as_buffer(reader, span) {
        Ok(buffer) => match String::from_utf8(buffer.into_vec()) {
            Ok(s) => Some(s),
            Err(_) => None,
        },
        // This used to error! but the error payload was always just
        // `Unable to read file: Custom { kind: UnexpectedEof, error: "failed to fill whole buffer" }`
        // which was not useful or informative and may be due to invalid spans
        // being told to us by save-analysis.
        Err(_) => None,
    }
}

fn create_output_dir(output_file: &Path) -> io::Result<()> {
    let mut output_dir = output_file.to_owned();
    output_dir.pop();
    fs::create_dir_all(output_dir)
}

fn analyze_file(
    searchfox_path: &PathBuf,
    defs: &Defs,
    file_analysis: &data::Analysis,
    tree_info: &TreeInfo,
) {
    use std::io::Write;

    debug!("Running analyze_file for {}", searchfox_path.display());

    let local_source_path = searchfox_path_to_local_path(searchfox_path, tree_info);

    if !local_source_path.exists() {
        info!(
            "Skipping nonexistent source file with searchfox path '{}' which mapped to local path '{}'",
            searchfox_path.display(),
            local_source_path.display()
        );
        return;
    };

    // Attempt to open the source file to extract information not currently available from the
    // analysis data.  Some analysis information may not be emitted if we are unable to access the
    // file.
    let maybe_source_file = match File::open(&local_source_path) {
        Ok(f) => Some(f),
        Err(_) => None,
    };

    let output_file = tree_info.out_analysis_dir.join(searchfox_path);
    let mut dataset = BTreeSet::new();
    read_existing_contents(&mut dataset, &output_file);
    if let Err(err) = create_output_dir(&output_file) {
        error!(
            "Couldn't create dir for: {}, {:?}",
            output_file.display(),
            err
        );
        return;
    }
    let mut file = match File::create(&output_file) {
        Ok(f) => f,
        Err(err) => {
            error!(
                "Couldn't open output file: {}, {:?}",
                output_file.display(),
                err
            );
            return;
        }
    };

    // Be chatty about the files we're outputting so that it's easier to follow
    // the path of rust analysis generation.
    info!(
        "Writing analysis for '{}' to '{}'",
        searchfox_path.display(),
        output_file.display()
    );

    for import in &file_analysis.imports {
        let id = match import.ref_id {
            Some(id) => id,
            None => {
                debug!(
                    "Dropping import {} ({:?}): {}, no ref",
                    import.name, import.kind, import.value
                );
                continue;
            }
        };

        let def = match defs.get(file_analysis, id) {
            Some(def) => def,
            None => {
                debug!(
                    "Dropping import {} ({:?}): {}, no def for ref {:?}",
                    import.name, import.kind, import.value, id
                );
                continue;
            }
        };

        visit_def(
            &mut dataset,
            AnalysisKind::Use,
            &import.span,
            &def.qualname,
            &def,
            None,
            None,
        )
    }

    for def in &file_analysis.defs {
        let parent = def
            .parent
            .and_then(|parent_id| defs.get(file_analysis, parent_id));

        if let Some(ref parent) = parent {
            if parent.kind == DefKind::Trait {
                let trait_dependent_name = construct_qualname(&parent.qualname, &def.name);
                visit_def(
                    &mut dataset,
                    AnalysisKind::Def,
                    &def.span,
                    &trait_dependent_name,
                    def,
                    Some(&parent.qualname),
                    None,
                )
            }
        }

        let crate_id = &file_analysis.prelude.as_ref().unwrap().crate_id;
        let qualname = crate_independent_qualname(def, crate_id);
        let nested_span = recursive_union_spans_of_def(def, file_analysis, defs);
        let maybe_nested = ignore_boring_spans(&nested_span);
        visit_def(
            &mut dataset,
            AnalysisKind::Def,
            &def.span,
            &qualname,
            def,
            parent.as_ref().map(|p| &*p.qualname),
            maybe_nested,
        )
    }

    // We want to expose impls as "def,namespace" with an inferred nesting_range for their
    // contents.  I don't know if it's a bug or just a dubious design decision, but the impls all
    // have empty values and no names, so to get a useful string out of them, we need to extract
    // the contents of their span directly.
    //
    // Because the name needs to be extracted from the source file, we omit this step if we were
    // unable to open the file.
    if let Some(mut source_file) = maybe_source_file {
        for imp in &file_analysis.impls {
            // (for simple.rs at least, there is never a parent)

            let name = match extract_span_from_source_as_string(&mut source_file, &imp.span) {
                Some(s) => s,
                None => continue,
            };

            let crate_id = &file_analysis.prelude.as_ref().unwrap().crate_id;
            let qualname = construct_qualname(&crate_id.name, &name);
            let pretty = pretty_for_impl(imp, &qualname);
            let nested_span = union_spans_of_defs(&imp.span, &imp.children, file_analysis, defs);
            let maybe_nested = ignore_boring_spans(&nested_span);
            // XXX visit_common currently never emits any syntax types; we want to pretend this is
            // a namespace once it does.
            visit_common(
                &mut dataset,
                AnalysisKind::Def,
                &imp.span,
                &qualname,
                &pretty,
                None,
                maybe_nested,
            )
        }
    }

    for ref_ in &file_analysis.refs {
        let def = match defs.get(file_analysis, ref_.ref_id) {
            Some(d) => d,
            None => {
                debug!(
                    "Dropping ref {:?}, kind {:?}, no def",
                    ref_.ref_id, ref_.kind
                );
                continue;
            }
        };
        visit_def(
            &mut dataset,
            AnalysisKind::Use,
            &ref_.span,
            &def.qualname,
            &def,
            /* context = */ None, // TODO
            /* nesting = */ None,
        )
    }

    for obj in &dataset {
        file.write_all(obj.as_bytes()).unwrap();
        writeln!(file).unwrap();
    }
}

// Replace any backslashes in the path with forward slashes.  Paths can be a
// combination of backslashes and forward slashes for windows platform builds
// because the paths are normalized by a sed script that will match backslashes
// and output front-slashes.  The sed script could be made smarter.
fn linuxized_path(path: &Path) -> PathBuf {
    if let Some(pathstr) = path.to_str() {
        if pathstr.find('\\').is_some() {
            // Pesky backslashes, get rid of them!
            let converted = pathstr.replace('\\', "/");
            // If we're seeing this, it means the paths weren't normalized and
            // now it's a question of minimizing fallout.
            if converted.find(":/") == Some(1) {
                // Starts with a drive letter, so let's turn this into
                // an absolute path
                let abs = "/".to_string() + converted.as_str();
                return PathBuf::from(abs);
            }
            // Turn it into a relative path
            return PathBuf::from(converted);
        }
    }
    // Already a valid path!
    path.to_path_buf()
}

fn analyze_crate(analysis: &data::Analysis, defs: &Defs, tree_info: &TreeInfo) {
    // Create and populate per-file Analysis instances from the provided per-crate Analysis file.
    let mut per_file = HashMap::new();

    let crate_name = &*analysis.prelude.as_ref().unwrap().crate_id.name;
    info!("Analyzing crate: '{}'", crate_name);
    debug!("Crate prelude: {:?}", analysis.prelude);

    macro_rules! flat_map_per_file {
        ($field:ident) => {
            for item in &analysis.$field {
                let file_analysis = per_file
                    .entry(linuxized_path(&item.span.file_name))
                    .or_insert_with(|| {
                        let prelude = analysis.prelude.clone();
                        let mut analysis = data::Analysis::new(analysis.config.clone());
                        analysis.prelude = prelude;
                        analysis
                    });
                file_analysis.$field.push(item.clone());
            }
        };
    }

    flat_map_per_file!(imports);
    flat_map_per_file!(defs);
    flat_map_per_file!(impls);
    flat_map_per_file!(refs);
    flat_map_per_file!(macro_refs);
    flat_map_per_file!(relations);

    for (searchfox_path, analysis) in per_file.drain() {
        // Absolute paths mean that the save-analysis data wasn't normalized
        // into the searchfox path convention, which means we can't generate
        // analysis data, so just skip.
        //
        // This will be the case for libraries built with cargo that have paths
        // that have prefixes that look like "/cargo/registry/src/github.com-".
        if searchfox_path.is_absolute() {
            info!(
                "Skipping absolute analysis path {}",
                searchfox_path.display()
            );
            continue;
        }
        analyze_file(&searchfox_path, defs, &analysis, tree_info);
    }
}

#[derive(Parser)]
struct RustIndexerCli {
    /// Points to the source root (FILES_ROOT)
    #[clap(value_parser)]
    src: PathBuf,

    /// Points to the directory where searchfox metadata should go (ANALYSIS_ROOT)
    #[clap(value_parser)]
    output: PathBuf,

    /// Points to the generated source files root (GENERATED)
    #[clap(value_parser)]
    generated: PathBuf,

    /// Whether the inputs are scip files or analysis directories.
    #[clap(short, long, value_parser)]
    scip: bool,

    /// Common prefix to the scip files. If given e.g., the objdir, we can infer
    /// that a given scip file in objdir/tools/rust.scip refers to tools/ rather
    /// than top-level srcdir locations.
    #[clap(long, value_parser)]
    scip_prefix: Option<PathBuf>,

    /// rustc analysis directories or scip inputs
    #[clap(value_parser)]
    inputs: Vec<PathBuf>,
}

fn analyze_using_rls(tree_info: &TreeInfo, inputs: Vec<PathBuf>) {
    let loader = Loader::new(inputs);

    let crates =
        rls_analysis::read_analysis_from_files(&loader, Default::default(), &[] as &[&str]);

    info!(
        "Crates: {:?}",
        crates.iter().map(|k| &k.id.name).collect::<Vec<_>>()
    );

    // Create and populate Defs, a map from Id to Def, across all crates before beginning analysis.
    // This is necessary because Def and Ref instances name Defs via Id.
    let mut defs = Defs::new();
    for krate in &crates {
        for def in &krate.analysis.defs {
            defs.insert(&krate.analysis, def);
        }
    }

    for krate in crates {
        analyze_crate(&krate.analysis, &defs, tree_info);
    }
}

// https://docs.rs/scip/latest/scip/types/struct.Occurrence.html#structfield.range
fn scip_range_to_searchfox_location(range: &[i32]) -> Location {
    // Searchfox uses 1-indexed lines, 0-indexed columns.
    let line_start = range[0] as u32 + 1;
    let col_start = range[1] as u32;
    let line_end = if range.len() == 3 {
        line_start
    } else {
        range[2] as u32 + 1
    };
    let col_end = *range.last().unwrap() as u32;
    // Rust spans are multi-line... So we just use the start column as
    // the end column if it spans multiple rows, searchfox has fallback
    // code to handle this.
    let col_end = if line_start != line_end {
        col_start
    } else {
        col_end
    };
    Location {
        lineno: line_start,
        col_start,
        col_end,
    }
}

fn write_line(mut file: &mut File, data: &impl serde::Serialize) {
    use std::io::Write;
    serde_json::to_writer(&mut file, data).unwrap();
    file.write_all(b"\n").unwrap();
}

fn scip_roles_to_searchfox_tags(roles: i32) -> Vec<AnalysisKind> {
    let mut values = vec![];

    macro_rules! map_to_searchfox {
        ($scip:ident, $sfox:ident) => {
            if roles & scip::types::SymbolRole::$scip as i32 != 0 {
                if values.last() != Some(&AnalysisKind::$sfox) {
                    values.push(AnalysisKind::$sfox);
                }
            }
        };
    }

    map_to_searchfox!(Definition, Def);
    map_to_searchfox!(Import, Use);
    map_to_searchfox!(WriteAccess, Use);
    map_to_searchfox!(ReadAccess, Use);
    map_to_searchfox!(Generated, Use);
    map_to_searchfox!(Test, Use);
    map_to_searchfox!(Import, Use);

    values
}

fn analyze_using_scip(tree_info: &TreeInfo, scip_prefix: Option<&PathBuf>, scip_file: PathBuf) {
    use protobuf::Message;
    use scip::types::*;

    let file = File::open(&scip_file).expect("Can't open scip file");
    let byte_count = file.metadata().expect("Failed to get file metadata").len();
    let mut file = BufReader::new(file);
    let mut file = protobuf::CodedInputStream::from_buf_read(&mut file);
    let index = Index::parse_from(&mut file).expect("Failed to read scip index");

    for doc in &index.documents {
        let searchfox_path = Path::new(&doc.relative_path).to_owned();
        let searchfox_path =
            match scip_prefix.and_then(|prefix| scip_file.strip_prefix(prefix).ok()) {
                Some(p) => {
                    let mut p = p.to_owned();
                    p.pop();
                    p.join(&searchfox_path)
                }
                None => searchfox_path,
            };

        let output_file = tree_info.out_analysis_dir.join(&searchfox_path);
        if let Err(err) = create_output_dir(&output_file) {
            error!(
                "Couldn't create dir for: {}, {:?}",
                output_file.display(),
                err
            );
            continue;
        }
        let mut file = match File::create(&output_file) {
            Ok(f) => f,
            Err(err) => {
                error!(
                    "Couldn't open output file: {}, {:?}",
                    output_file.display(),
                    err
                );
                continue;
            }
        };
        // Be chatty about the files we're outputting so that it's easier to follow
        // the path of rust analysis generation.
        info!(
            "Writing analysis for '{}' to '{}'",
            searchfox_path.display(),
            output_file.display()
        );

        // A map from local symbol to the index in doc.symbols for this document.
        let mut doc_symbols_to_index = HashMap::new();
        for (index, symbol) in doc.symbols.iter().enumerate() {
            doc_symbols_to_index.insert(symbol.symbol.clone(), index);
        }

        let lookup_symbol = |s: &str| -> Cow<SymbolInformation> {
            match doc_symbols_to_index.get(s) {
                Some(i) => Cow::Borrowed(&doc.symbols[*i]),
                None => {
                    debug!("Didn't find symbol {:?} in local symbol table", s);
                    // Fake it till you make it? We have no info for this
                    // symbol, so...
                    Cow::Owned(SymbolInformation {
                        symbol: s.to_owned(),
                        ..Default::default()
                    })
                }
            }
        };

        for occurrence in &doc.occurrences {
            let loc = scip_range_to_searchfox_location(&occurrence.range);
            let symbol = lookup_symbol(&occurrence.symbol);
            if scip::symbol::is_local_symbol(&symbol.symbol) {
                continue;
            }
            {
                let global = sanitize_symbol(&symbol.symbol);
                let pretty = pretty_symbol(&symbol.symbol);
                let source_data = WithLocation {
                    data: AnalysisSource {
                        source: SourceTag::Source,
                        // TODO: Fill syntax.
                        syntax: vec![],
                        pretty: ustr(&pretty),
                        sym: vec![ustr(&global)],
                        no_crossref: false,
                        // TODO(bug 1796870): Nesting.
                        nesting_range: SourceRange::default(),
                        // TODO: Expose type information for fields/etc.
                        type_pretty: None,
                        type_sym: None,
                        arg_ranges: vec![],
                        expansion_info: None,
                        confidence: None,
                    },
                    loc,
                };
                write_line(&mut file, &source_data);
            }

            let get_target_data = |sym: &str, kind: AnalysisKind| -> WithLocation<AnalysisTarget> {
                let global = sanitize_symbol(sym);
                let pretty = pretty_symbol(sym);
                WithLocation {
                    data: AnalysisTarget {
                        target: TargetTag::Target,
                        kind,
                        pretty: ustr(&pretty),
                        sym: ustr(&global),
                        // TODO: Contextual info.
                        context: ustr(""),
                        contextsym: ustr(""),
                        peek_range: LineRange {
                            start_lineno: 0,
                            end_lineno: 0,
                        },
                        arg_ranges: vec![],
                    },
                    loc,
                }
            };

            write_line(
                &mut file,
                &get_target_data(&symbol.symbol, AnalysisKind::Use),
            );

            for kind in scip_roles_to_searchfox_tags(occurrence.symbol_roles) {
                write_line(&mut file, &get_target_data(&symbol.symbol, kind));
            }

            for relationship in &symbol.relationships {
                let kind = if relationship.is_type_definition || relationship.is_implementation {
                    AnalysisKind::Def
                } else {
                    AnalysisKind::Use
                };
                write_line(&mut file, &get_target_data(&relationship.symbol, kind));
            }
        }

        println!("{}", doc.relative_path);
    }

    assert_eq!(file.pos(), byte_count, "Should've processed the whole file");
}

fn main() {
    env_logger::init();

    let cli = RustIndexerCli::parse();

    let tree_info = TreeInfo {
        srcdir: &cli.src,
        out_analysis_dir: &cli.output,
        generated: &cli.generated,
        generated_friendly: &PathBuf::from("__GENERATED__"),
    };

    info!("Tree info: {:?}", tree_info);

    if cli.scip {
        for file in cli.inputs {
            analyze_using_scip(&tree_info, cli.scip_prefix.as_ref(), file);
        }
    } else {
        analyze_using_rls(&tree_info, cli.inputs)
    }
}

```

## tools/src/bin/ipdl-analyze.rs
```
use std::collections::BTreeMap;
use std::env;
use std::fs::File;
use std::io::BufRead;
use std::io::BufReader;
use std::io::Write;
use std::path::Path;
use std::path::PathBuf;

extern crate env_logger;
extern crate getopts;
extern crate ipdl_parser;
extern crate tools;

use getopts::Options;
use serde_json::json;

use ipdl_parser::ast::ProtocolSide;
use tools::file_format::analysis::{
    read_analyses, read_target, AnalysisKind, AnalysisTarget, WithLocation,
};

use ipdl_parser::ast;
use ipdl_parser::parser;

type TargetAnalysis = Vec<WithLocation<Vec<AnalysisTarget>>>;

fn get_options_parser() -> Options {
    let mut opts = Options::new();
    opts.optmulti(
        "I",
        "include",
        "Additional directory to search for included protocol specifications",
        "DIR",
    );
    opts.reqopt(
        "b",
        "base-input-prefix",
        "Base directory where IPDL input files are found.",
        "BASE_DIR",
    );
    opts.reqopt(
        "a",
        "analysis-prefix",
        "Base directory where analysis output files are found.",
        "ANALYSIS_DIR",
    );
    opts.reqopt(
        "f",
        "files-list",
        "List of source files, probably `repo-files`.",
        "FILES_LIST",
    );
    opts.reqopt(
        "o",
        "objdir-list",
        "List of generated/objdir files, probably `objdir-files`.",
        "FILES_LIST",
    );
    opts
}

fn load_file_list(filenames_file: &str) -> BTreeMap<String, String> {
    BufReader::new(File::open(filenames_file).unwrap())
        .lines()
        // In theory we could use Path for this but I am too sleepy to deal with the resulting type
        // nightmare of file_name()'s return value. Also, I lost my rust book so I don't know rust.
        .map(|maybe_name| {
            let name = maybe_name.unwrap();
            (name.rsplit("/").next().unwrap().into(), name)
        })
        .collect()
}

fn mangle_simple(s: &str) -> String {
    format!("{}{}", s.len(), s)
}

fn mangle_nested_name(ns: &[String], protocol: &str, name: &str) -> String {
    format!(
        "_ZN{}{}{}E",
        ns.iter()
            .map(|id| mangle_simple(id))
            .collect::<Vec<_>>()
            .join(""),
        mangle_simple(protocol),
        mangle_simple(name)
    )
}

fn find_analysis<'a>(analysis: &'a TargetAnalysis, mangled: &str) -> Option<&'a AnalysisTarget> {
    // As a hack to deal with SendPFooConstructor having a single-arg variant
    // that takes an args struct which just news the actor and then calls the
    // 2-arg variant, passing the actor as the first variant, and because the
    // 1-arg variant comes before the 2-arg variant, we return the last variant
    // we see.  Will this cause even more problems?  Maybe!
    let mut best_piece = None;
    for datum in analysis {
        for piece in &datum.data {
            // Inline method definitions and pure virtual method declarations
            // will both be reported as definitions by the C++ indexer without a
            // declaration, so we need to accept both decls and defs.
            if (piece.kind == AnalysisKind::Decl || piece.kind == AnalysisKind::Def)
                && piece.sym.contains(mangled)
            {
                best_piece = Some(piece);
            }
        }
    }

    best_piece
}

fn output_ipc_data(
    outputf: &mut File,
    locstr: &str,
    ipc_pretty: &str,
    ipc_sym: &str,
    send_datum: &AnalysisTarget,
    recv_datum: &AnalysisTarget,
) {
    write!(
        outputf,
        "{}",
        json!({
            "loc": locstr,
            "target": 1,
            "kind": "idl",
            "pretty": ipc_pretty,
            "sym": ipc_sym,
        })
    )
    .unwrap();
    writeln!(outputf).unwrap();
    write!(
        outputf,
        "{}",
        json!({
            "loc": locstr,
            "source": 1,
            "syntax": "idl,ipc,def",
            "pretty": format!("ipc {}", ipc_pretty),
            "sym": ipc_sym,
        })
    )
    .unwrap();
    writeln!(outputf).unwrap();
    write!(
        outputf,
        "{}",
        json!({
            "loc": locstr,
            "structured": 1,
            "pretty": ipc_pretty,
            "sym": ipc_sym,
            // Note that this is different than the target record kind.
            "kind": "ipc",
            "implKind": "idl",
            "bindingSlots": [
                {
                    "slotKind": "send",
                    "slotLang": "cpp",
                    "ownerLang": "idl",
                    "sym": send_datum.sym,
                },
                {
                    "slotKind": "recv",
                    "slotLang": "cpp",
                    "ownerLang": "idl",
                    "sym": recv_datum.sym,
                },
            ]
        })
    )
    .unwrap();
    writeln!(outputf).unwrap();
}

#[allow(clippy::too_many_arguments)]
fn output_send_recv(
    outputf: &mut File,
    locstr: &str,
    protocol: &ast::Namespace,
    message: &ast::MessageDecl,
    is_ctor: bool,
    send_side: &str,
    send_analysis: &TargetAnalysis,
    recv_side: &str,
    recv_analysis: &TargetAnalysis,
) {
    let send_prefix = if message.send_semantics == ast::SendSemantics::Intr {
        "Call"
    } else {
        "Send"
    };
    let recv_prefix = if message.send_semantics == ast::SendSemantics::Intr {
        "Answer"
    } else {
        "Recv"
    };

    let ctor_suffix = if is_ctor { "Constructor" } else { "" };

    let mangled = mangle_nested_name(
        &protocol.namespaces,
        &format!("{}{}", protocol.name.id, send_side),
        &format!("{}{}{}", send_prefix, message.name.id, ctor_suffix),
    );
    let maybe_send_datum = find_analysis(send_analysis, &mangled);
    if maybe_send_datum.is_none() {
        println!("No analysis target found for send: {}", mangled);
    }

    // Depending on whether the protocol is a legacy virtual implementation or direct-call, the
    // "P" prefix of the protocol may need to be sliced off to find the symbol.  See the block
    // comment in `main()` for more info.
    //
    // Our hacky heuristic here is we try without the P and failover to trying with the P.
    let mangled_no_p = mangle_nested_name(
        &protocol.namespaces,
        &format!("{}{}", /* sliced */ &protocol.name.id[1..], recv_side),
        &format!("{}{}{}", recv_prefix, message.name.id, ctor_suffix),
    );
    let mangled_yes_p = mangle_nested_name(
        &protocol.namespaces,
        &format!("{}{}", /* not sliced */ protocol.name.id, recv_side),
        &format!("{}{}{}", recv_prefix, message.name.id, ctor_suffix),
    );
    let maybe_recv_datum = find_analysis(recv_analysis, &mangled_no_p)
        .or_else(|| find_analysis(recv_analysis, &mangled_yes_p));
    if maybe_recv_datum.is_none() {
        println!(
            "No analysis target found for recv: {} or {}",
            mangled_no_p, mangled_yes_p
        );
    }

    if let (Some(send_datum), Some(recv_datum)) = (maybe_send_datum, maybe_recv_datum) {
        let ipc_pretty = format!(
            "{}::{}::{}",
            protocol.namespaces.join("_"),
            protocol.name.id,
            message.name.id
        );
        let ipc_sym = format!(
            "IPC_{}_{}_{}",
            protocol.namespaces.join("_"),
            protocol.name.id,
            message.name.id
        );
        output_ipc_data(
            outputf,
            locstr,
            &ipc_pretty,
            &ipc_sym,
            send_datum,
            recv_datum,
        );
    }
}

fn main() {
    env_logger::init();

    let args: Vec<String> = env::args().collect();

    let opts = get_options_parser();

    let matches = match opts.parse(&args[1..]) {
        Ok(m) => m,
        Err(f) => panic!("{}", f.to_string()),
    };

    let mut include_dirs = Vec::new();
    for i in matches.opt_strs("I") {
        include_dirs.push(PathBuf::from(i))
    }

    let base_dir = matches.opt_str("b").unwrap();
    let analysis_dir = matches.opt_str("a").unwrap();
    let repo_file_list_fname = matches.opt_str("f").unwrap();
    let objdir_file_list_fname = matches.opt_str("o").unwrap();
    let base_path = Path::new(&base_dir);
    let analysis_path = Path::new(&analysis_dir);

    let mut file_names = Vec::new();
    for f in matches.free {
        file_names.push(PathBuf::from(f));
    }

    let repo_files_map = load_file_list(&repo_file_list_fname);
    let objdir_files_map = load_file_list(&objdir_file_list_fname);

    let maybe_tus = parser::parse(&include_dirs, file_names);

    if maybe_tus.is_none() {
        println!("Specification could not be parsed.");
        return;
    }

    let tus = maybe_tus.unwrap();

    for (_, tu) in tus {
        println!("Analyzing {:?}", tu.file_name);

        let path = tu.file_name.as_path();
        let relative = path.strip_prefix(base_path).unwrap();
        let absolute = analysis_path.join(relative);
        let mut outputf = File::create(absolute).unwrap();

        if let Some((ns, protocol)) = tu.protocol {
            // ## Analyses Linkage
            //
            // Originally, IPDL analysis worked by being able to know exactly where the protocol
            // autogenerated headers would end up.  These would contain (virtual) declarations for
            // all the Send and Recv methods.  Because of how searchfox indexes overridden methods,
            // this allowed locating the concrete recv methods.
            //
            // Bug 1512990 (ipc-devirt) changed this so that the recv methods would only exist on
            // the concrete implementation class and `thisCall` in `lower.py` would know to
            // static_cast to that class.  This broke finding the "recv" methods for any
            // implementations not grandfathered into the legacy virtual call mechanism.  The
            // legacy methods can be found in
            // https://searchfox.org/mozilla-central/source/ipc/ipdl/ipdl/direct_call.py in the
            // `VIRTUAL_CALL_CLASSES` set.  There's also a `DIRECT_CALL_OVERRIDES` section that
            // explains to the binding generator how to find the subclass's header file for
            // inclusion for build purposes.  Those entries only exist when the header file isn't
            // predictably publicly exposed via `EXPORTS.mozilla.dom`/similar in `moz.build` files
            // based on the IPDL namespace and protocol name.
            //
            // This IPDL analysis logic doesn't currently understand `direct_call.py`, but that
            // also doesn't quite matter because our analysis operates in terms of the source
            // locations of files, not where they get installed to.  Admittedly here, I (asuth),
            // am somewhat confused as to whether we have a reverse mapping somewhere or things
            // just work out.  In our objdirs, the install process generates symlinks back to the
            // original source location which makes the indexer's life easy.  And the symlinks
            // at least disappear by the time the webserver gets the merged objdir.  It's only
            // truly generated files that stick around (and get labeled as generated).
            //
            // In any event, the approach I'm taking here is to load up the contents of the
            // `repo-files` list and create a map from the filename to the (source) path.  If
            // we find such a mapping, we add it to the list of analysis files to read.
            //
            // The right solution is likely what's been proposed for WebIDL in bug 1416899 wherein
            // the IPDL compiler should just be creating JSON build artifacts for searchfox when
            // requested and it can perform symlink resolution as part of its process so we always
            // have source paths when possible.
            //
            // Another option for symbol resolution just to depend on the `crossref` process.
            // It knows about all symbols, so the main issue is that it also really wants to know
            // the byproducts of this analysis.  One possibility is to support a type of record
            // that is linked/fixed-up by the crossref process.
            //
            // ### Platform variations resulting from preprocessed IPDL files
            //
            // Some IPDL files like PContent.ipdl are preprocessed which both means the IPDL parser
            // has to deal with the existence of directives, but also that we need to deal with the
            // merge logic being unable to unify the files.  Previously we use predicted path
            // locations to find `PFoo{Parent,Child}.h`, but now we perform a lookup from the
            // `objdir-files` list identically to how we use `repo-files` list for
            // "Foo{Parent,Child}.h".

            // ### Parent Analyses
            let mut parent_ana_files = vec![];
            if let Some(parent_fname) = objdir_files_map.get(&format!("{}Parent.h", &ns.name.id)) {
                let parent_path = analysis_path
                    .join(parent_fname)
                    .to_string_lossy()
                    .into_owned();
                println!("  Reading Parent header {:?}", &parent_path);
                parent_ana_files.push(parent_path);
            } else {
                println!(
                    "  Unable to find Parent header for protocol: {}",
                    &ns.name.id
                );
            }
            if let Some(parent_impl_fname) =
                repo_files_map.get(&format!("{}Parent.h", &ns.name.id[1..]))
            {
                let parent_impl_path = analysis_path
                    .join(parent_impl_fname)
                    .to_string_lossy()
                    .into_owned();
                println!("  Reading Parent impl header {:?}", &parent_impl_path);
                parent_ana_files.push(parent_impl_path);
            } else {
                println!(
                    "  Unable to find Parent impl header for protocol: {}",
                    &ns.name.id
                );
            }

            let parent_analysis = read_analyses(parent_ana_files.as_slice(), &mut read_target);

            // ### Child Analyses
            let mut child_ana_files = vec![];
            if let Some(child_fname) = objdir_files_map.get(&format!("{}Child.h", &ns.name.id)) {
                let child_path = analysis_path
                    .join(child_fname)
                    .to_string_lossy()
                    .into_owned();
                println!("  Reading Child header {:?}", &child_path);
                child_ana_files.push(child_path);
            } else {
                println!(
                    "  Unable to find Child header for protocol: {}",
                    &ns.name.id
                );
            }
            if let Some(child_impl_fname) =
                repo_files_map.get(&format!("{}Child.h", &ns.name.id[1..]))
            {
                let child_impl_path = analysis_path
                    .join(child_impl_fname)
                    .to_string_lossy()
                    .into_owned();
                println!("  Reading Child impl header {:?}", &child_impl_path);
                child_ana_files.push(child_impl_path);
            } else {
                println!(
                    "  Unable to find Child impl header for protocol: {}",
                    &ns.name.id
                );
            }
            let child_analysis = read_analyses(child_ana_files.as_slice(), &mut read_target);

            let is_toplevel = protocol.managers.is_empty();

            for message in protocol.messages {
                let loc = &message.name.loc;
                let locstr = format!(
                    "{}:{}-{}",
                    loc.lineno,
                    loc.colno,
                    loc.colno + message.name.id.len()
                );

                if is_toplevel && message.name.id == "__delete__" {
                    continue;
                }

                let is_ctor = protocol.manages.iter().any(|e| e.id == message.name.id);

                if message.direction == ast::Direction::To(ProtocolSide::Child)
                    || message.direction == ast::Direction::Both
                {
                    output_send_recv(
                        &mut outputf,
                        &locstr,
                        &ns,
                        &message,
                        is_ctor,
                        "Parent",
                        &parent_analysis,
                        "Child",
                        &child_analysis,
                    );
                }

                if message.direction == ast::Direction::To(ProtocolSide::Parent)
                    || message.direction == ast::Direction::Both
                {
                    output_send_recv(
                        &mut outputf,
                        &locstr,
                        &ns,
                        &message,
                        is_ctor,
                        "Child",
                        &child_analysis,
                        "Parent",
                        &parent_analysis,
                    );
                }
            }
        }
    }
}

```

## tools/src/bin/pipeline-server.rs
```
use std::{
    collections::{BTreeMap, HashMap},
    env,
    sync::Arc,
};

use axum::{
    extract::{Path, Query},
    http::{header, HeaderMap, StatusCode},
    response::{Html, IntoResponse, Response},
    routing::get,
    Extension, Json, Router,
};
use axum_macros::debug_handler;
use liquid::Template;
use serde_json::Value;
use tools::{
    abstract_server::{make_all_local_servers, AbstractServer, ServerError},
    cmd_pipeline::{builder::build_pipeline_graph, PipelineValues},
    logging::{init_logging, LoggedSpan},
    query::chew_query::chew_query,
    templating::builder::build_and_parse_query_results,
};
use tracing::Instrument;

#[debug_handler]
async fn handle_query(
    local_servers: Extension<Arc<BTreeMap<String, Box<dyn AbstractServer + Send + Sync>>>>,
    templates: Extension<Arc<SomeTemplates>>,
    headers: HeaderMap,
    Path((tree, preset)): Path<(String, String)>,
    Query(params): Query<HashMap<String, String>>,
) -> Result<Response, ServerError> {
    let server = match local_servers.get(&tree) {
        Some(s) => s,
        None => {
            return Ok((StatusCode::NOT_FOUND, format!("No such tree: {}", tree)).into_response());
        }
    };

    if preset.as_str() != "default" {
        return Ok((StatusCode::NOT_FOUND, format!("No such preset: {}", preset)).into_response());
    }

    let maybe_log = params.contains_key("debug");
    let logged_span: Option<LoggedSpan> = if maybe_log {
        Some(LoggedSpan::new_logged_span("query"))
    } else {
        None
    };

    let query = match params.get("q") {
        Some(q) => q,
        None => {
            return Ok((StatusCode::BAD_REQUEST, "No 'q' parameter, no results!").into_response());
        }
    };

    let graph = {
        let _log_entered = logged_span
            .as_ref()
            .map(|lspan| lspan.span.clone().entered());

        let pipeline_plan = chew_query(query)?;

        build_pipeline_graph(server.clonify(), pipeline_plan)?
    };

    let result = match &logged_span {
        Some(lspan) => graph.run(true).instrument(lspan.span.clone()).await?,
        _ => graph.run(true).await?,
    };

    let accept = headers
        .get("accept")
        .map(|x| x.to_str().unwrap_or("text/html"));
    let make_html = !matches!(accept, Some("application/json"));

    let logs = match logged_span {
        Some(lspan) => lspan.retrieve_serde_json().await,
        _ => Value::Null,
    };

    // There are a bunch of ways to return headers to axum; this is the most
    // legible I found.
    let mut header_map = HeaderMap::new();
    header_map.insert(header::VARY, "Accept".parse().unwrap());

    if make_html {
        let sym_info_str = match &result {
            PipelineValues::GraphResultsBundle(grb) => {
                serde_json::to_string(&grb.symbols).unwrap_or_else(|_| "{}".to_string())
            }
            PipelineValues::SymbolTreeTableList(sttl) => {
                serde_json::to_string(&sttl.unioned_node_sets_as_jumprefs())
                    .unwrap_or_else(|_| "{}".to_string())
            }
            _ => "{}".to_string(),
        };

        let globals = liquid::object!({
            "results": result,
            "query": query.clone(),
            "preset": preset.clone(),
            "tree": tree.clone(),
            "logs": logs,
            "SYM_INFO_STR": sym_info_str,
        });

        let output = templates.query_results.render(&globals)?;
        Ok((header_map, Html(output)).into_response())
    } else {
        Ok((header_map, Json(result)).into_response())
    }
}

struct SomeTemplates {
    query_results: Template,
}

#[tokio::main]
async fn main() {
    init_logging();

    let local_servers = Arc::new(make_all_local_servers(&env::args().nth(1).unwrap()).unwrap());
    let templates = Arc::new(SomeTemplates {
        query_results: build_and_parse_query_results(),
    });

    // build our application with a single route
    let app = Router::new()
        .route("/:tree/query/:preset", get(handle_query))
        .layer(Extension(local_servers))
        .layer(Extension(templates));

    axum::Server::bind(&"0.0.0.0:8002".parse().unwrap())
        .serve(app.into_make_service())
        .await
        .unwrap();
}

```

## tools/src/bin/crossref.rs
```
use std::collections::BTreeMap;
use std::collections::BTreeSet;
use std::collections::HashMap;
use std::fs;
use std::fs::create_dir_all;
use std::fs::File;
use std::io::BufRead;
use std::io::BufReader;
use std::io::Write;

#[macro_use]
extern crate tracing;

extern crate clap;
use clap::Parser;
use itertools::Itertools;
use serde_json::{json, Map};
extern crate tools;
use tools::file_format::analysis::AnalysisStructured;
use tools::file_format::analysis::OntologySlotInfo;
use tools::file_format::analysis::OntologySlotKind;
use tools::file_format::analysis::StructuredPointerInfo;
use tools::file_format::analysis::StructuredTag;
use tools::file_format::analysis::{
    read_analysis, read_structured, read_target, AnalysisKind, AnalysisTarget, BindingSlotProps,
    LineRange, Location, SearchResult, StructuredBindingSlotInfo, TargetTag,
};
use tools::file_format::analysis_manglings::make_file_sym_from_path;
use tools::file_format::analysis_manglings::split_pretty;
use tools::file_format::config;
use tools::file_format::crossref_converter::convert_crossref_value_to_sym_info_rep;
use tools::file_format::ontology_mapping::OntologyRunnableMode;
use tools::file_format::ontology_mapping::{
    OntologyLabelOwningClass, OntologyMappingIngestion, OntologyPointerKind,
};
use tools::file_format::repo_data_ingestion::RepoIngestion;
use tools::logging::init_logging;
use tools::logging::LoggedSpan;
use tools::templating::builder::build_and_parse_ontology_ingestion_explainer;
use tools::templating::builder::build_and_parse_repo_ingestion_explainer;
use ustr::ustr;
use ustr::Ustr;
use ustr::UstrMap;
use ustr::UstrSet;

/// The size for a payload line (inclusive of leading indicating character and
/// newline) at which we store it externally in `crossref-extra` instead of
/// inline in the `crossref` file itself.
const EXTERNAL_STORAGE_THRESHOLD: usize = 1024 * 3;

#[derive(Parser)]
struct CrossrefCli {
    /// Path to the variable-expanded config file
    #[clap(value_parser)]
    config_file: String,

    /// The tree in the config file we're cross-referencing
    #[clap(value_parser)]
    tree_name: String,

    /// Path to the file containing a list of all of the known analysis files to
    /// ingest.  This is expected to be a subset of the contents of
    /// INDEX_ROOT/all-files which will be located using the config_file and
    /// tree_name.
    #[clap(value_parser)]
    analysis_files_list_path: String,

    /// Path to the file containing a list of all the files which doesn't have
    /// analysis but still needs FILE_* target.
    #[clap(value_parser)]
    other_resources_list_path: String,
}

type SearchResultTable = BTreeMap<Ustr, BTreeMap<AnalysisKind, BTreeMap<Ustr, Vec<SearchResult>>>>;
type PrettyTable = HashMap<Ustr, Ustr>;
type IdTable = UstrMap<UstrSet>;
type MetaTable = BTreeMap<Ustr, AnalysisStructured>;
type CalleesTable = BTreeMap<Ustr, BTreeMap<Ustr, (Ustr, BTreeSet<u32>)>>;
type FieldMemberUseTable = BTreeMap<Ustr, BTreeMap<Ustr, Vec<(Ustr, OntologyPointerKind)>>>;
type XrefLinkSubclass = Vec<(Ustr, Ustr)>;
type XrefLinkOverride = Vec<(Ustr, Ustr)>;
type XrefLinkSlots = BTreeMap<(Ustr, Ustr), (BindingSlotProps, Option<Ustr>)>;

#[allow(clippy::too_many_arguments)]
fn process_analysis_target(
    mut piece: AnalysisTarget,
    path: &Ustr,
    file_sym: &Ustr,
    lineno: usize,
    loc: &Location,
    table: &mut SearchResultTable,
    pretty_table: &mut PrettyTable,
    id_table: &mut IdTable,
    callees_table: &mut CalleesTable,
    lines: &[(String, u32)],
) {
    if piece.pretty.is_empty() {
        info!("Skipping empty pretty for symbol {}", piece.sym);
        return;
    }

    // XXX temporary include hack; we should fix this in the C++ indexer, but I want to
    // see how it works out.
    if piece.sym.starts_with("FILE_") && piece.contextsym.is_empty() {
        piece.context = *path;
        piece.contextsym = *file_sym;
    }

    let t1 = table.entry(piece.sym).or_default();
    let t2 = t1.entry(piece.kind).or_default();
    let t3 = t2.entry(*path).or_default();

    let (line, offset) = lines[lineno].clone();

    // Idempotently insert the symbol -> pretty symbol mapping into `pretty_table`.
    pretty_table.insert(piece.sym, piece.pretty);

    // If this is a use and there's a contextsym, we want to create a "callees"
    // entry under the contextsym.  We also want to invert the use of "context"
    // to be the symbol in question; it's not useful to name the context symbol
    // redundantly when it's the symbol we're attaching data to.
    if piece.kind == AnalysisKind::Use && !piece.contextsym.is_empty() {
        let callee_syms = callees_table.entry(piece.contextsym).or_default();
        let (from_path, callee_jump_lines) = callee_syms
            .entry(piece.sym)
            .or_insert_with(|| (*path, BTreeSet::new()));
        if from_path == path {
            callee_jump_lines.insert(loc.lineno);
        }
        // XXX otherwise weird things are happening, but I'm not
        // sure we need to warn on this.
    }

    t3.push(SearchResult {
        lineno: loc.lineno,
        bounds: (loc.col_start - offset, loc.col_end - offset),
        line,
        context: piece.context,
        contextsym: piece.contextsym,
        peek_range: piece.peek_range,
    });

    // Idempotently insert the pretty identifier -> symbol mapping as long as the pretty
    // symbol looks sane.  (Whitespace breaks the `identifiers` file's text format, so
    // we can't include them.)
    let ch = piece.sym.chars().next().unwrap();
    if !ch.is_ascii_digit() && !piece.sym.contains(' ') {
        // Split the pretty identifier into parts so for "foo::bar::Baz"
        // we can emit ["foo::bar::Baz", "bar::Baz", "Baz"] into our
        // identifiers table so people don't have to always type out
        // the full identifier.
        //
        // NOTE: We are passing "" as the symbol here in order to
        // avoid splitting paths (which detects a "FILE_" prefix),
        // but we may want to support multiple pretty delimiters
        // beyond "::" here in the future.  (Although there's
        // something to be said for normalizing on use of "::" for
        // everything but paths, and we sorta do this for scip-indexer
        // already.)
        let (components, delim) = split_pretty(piece.pretty.as_str(), "");
        for i in 0..components.len() {
            let sub = &components[i..components.len()];
            let sub = sub.join(delim);

            if !sub.is_empty() {
                let t1 = id_table.entry(ustr(&sub)).or_default();
                t1.insert(piece.sym);
            }
        }
    }
}

fn process_analysis_structured(
    mut piece: AnalysisStructured,
    subsystem: Option<Ustr>,
    meta_table: &mut MetaTable,
    xref_link_subclass: &mut XrefLinkSubclass,
    xref_link_override: &mut XrefLinkOverride,
    xref_link_slots: &mut XrefLinkSlots,
) {
    meta_table.entry(piece.sym).or_insert_with(|| {
        for super_info in &piece.supers {
            xref_link_subclass.push((super_info.sym, piece.sym));
        }

        for override_info in &piece.overrides {
            xref_link_override.push((override_info.sym, piece.sym));
        }

        // We remove all bindings infos from AnalysisStructured instances here
        // but add them back both ways when we iterate over xref_link_slots.
        for slot_info in piece.binding_slots.drain(..) {
            xref_link_slots.insert((piece.sym, slot_info.sym), (slot_info.props, subsystem));
        }
        if let Some(slot_info) = piece.slot_owner.take() {
            xref_link_slots.insert((slot_info.sym, piece.sym), (slot_info.props, subsystem));
        }

        piece.subsystem = subsystem;

        piece
    });
}

fn make_subsystem(
    path: &Ustr,
    file_sym: &Ustr,
    ingestion: &mut RepoIngestion,
    meta_table: &mut MetaTable,
    pretty_table: &mut PrettyTable,
    id_table: &mut IdTable,
) -> Option<Ustr> {
    let concise_info = ingestion.state.concise_per_file.get(path);

    if let Some(concise) = concise_info {
        let file_structured = AnalysisStructured {
            structured: StructuredTag::Structured,
            pretty: *path,
            sym: *file_sym,
            type_pretty: None,
            kind: ustr("file"),
            subsystem: concise.subsystem,
            // For most analytical purposes, we want to think of files as atomic,
            // so I don't think there is any upside to modeling the containing
            // directory as a parent.  Especially since we don't yet have a
            // `DIR_blah` symbol type yet or a clear reason to want one.
            parent_sym: None,
            slot_owner: None,
            impl_kind: ustr("impl"),
            size_bytes: None,
            alignment_bytes: None,
            own_vf_ptr_bytes: None,
            binding_slots: vec![],
            ontology_slots: vec![],
            supers: vec![],
            methods: vec![],
            fields: vec![],
            overrides: vec![],
            props: vec![],
            labels: BTreeSet::default(),

            idl_sym: None,
            subclass_syms: vec![],
            overridden_by_syms: vec![],
            variants: vec![],
            extra: Map::default(),
        };
        meta_table.insert(file_structured.sym, file_structured);
        pretty_table.insert(*file_sym, *path);
        let t1 = id_table.entry(*path).or_default();
        t1.insert(*file_sym);
        concise.subsystem
    } else {
        None
    }
}

fn line_to_buf_and_offset(line: String) -> (String, u32) {
    let line_cut = line.trim_end();
    let len = line_cut.len();
    let line_cut = line_cut.trim_start();
    let offset = (len - line_cut.len()) as u32;
    let buf: String = line_cut.chars().take(100).collect();
    (buf, offset)
}

/// Process all analysis files, deriving the `crossref`, `jumpref`, and `identifiers` output files.
/// See https://github.com/mozsearch/mozsearch/blob/master/docs/crossref.md for high-level
/// documentation on how this works (locally, `docs/crossref.md`).
///
/// ## Implementation
/// There are 3 phases of processing:
/// 1. Repo data ingestion aggregates any per-file information (bugzilla component
///    mappings, test information) and performs file-level classifications like
///    pre-computing a path_kind for every file.
/// 2. The analysis files are read, populating `table`, `pretty_table`, `id_table`, and
///    `meta_table` incrementally.  Primary cross-reference information comes from target records,
///    but the file is also processed for structured records in order to populate `meta_table` with
///    meta-information about the symbol.
/// 2. The table is consumed, generating both crossref and jumpref information.
///
/// ### Memory Management
/// Memory usage grows continually throughout phase 1.  Because we load many identical strings,
/// we use string interning so that all long-lived strings are reference-counted interned strings.

#[tokio::main]
async fn main() {
    // This will honor RUST_LOG, but more importantly enables our LoggedSpan
    // mechanism.
    //
    // Note that this marks us transitioning to an async multi-threaded runtime
    // for crossref, but as of the time of writing this, the logging
    // infrastructure is the only async/multi-threaded thing going on, but this
    // will hopefully open the door to more.  (In particular, the semantic
    // linkage mechanism discussed in https://bugzilla.mozilla.org/show_bug.cgi?id=1727789
    // and adjacent bugs would potentially like to see us re-processing the
    // analysis files in parallel after the initial crossref-building phase.)
    init_logging();

    let cli = CrossrefCli::parse();

    let tree_name = &cli.tree_name;
    let cfg = config::load(&cli.config_file, false, Some(tree_name), None, None);

    let tree_config = cfg.trees.get(tree_name).unwrap();

    let analysis_filenames_file = &cli.analysis_files_list_path;

    // This is just the list of analysis files.
    let analysis_relative_paths: Vec<Ustr> =
        BufReader::new(File::open(analysis_filenames_file).unwrap())
            .lines()
            .map(|x| ustr(&x.unwrap()))
            .collect();

    let all_files_list_path = format!("{}/all-files", tree_config.paths.index_path);
    let all_files_paths: Vec<Ustr> = fs::read_to_string(all_files_list_path)
        .unwrap()
        .lines()
        .map(ustr)
        .collect();

    let all_dirs_list_path = format!("{}/all-dirs", tree_config.paths.index_path);
    let all_dirs_paths: Vec<Ustr> = fs::read_to_string(all_dirs_list_path)
        .unwrap()
        .lines()
        .map(ustr)
        .collect();

    // ## Ingest Repo-Wide Information

    // This will buffer ALL of the tracing logging in our crate between now
    // and when we retrieve it to emit diagnostics.  To this end, we want
    // verbose logging to be conditioned on our "probe" mechanism, which means
    // that we only enable logs for specific values that match our probe, which
    // is currently controlled by environment variables like `PROBE_PATH` (but
    // where we could imagine that our trees might always designate a default
    // probe so that we could have a few instructive data points for people to
    // learn from rather than an excessive wall of text with no curation).
    let logged_ingestion_span = LoggedSpan::new_logged_span("repo_ingestion");
    let ingestion_entered = logged_ingestion_span.span.clone().entered();

    let per_file_info_toml_str = cfg
        .read_tree_config_file_with_default("per-file-info.toml")
        .unwrap();
    let mut ingestion = RepoIngestion::new(&per_file_info_toml_str)
        .expect("Your per-file-info.toml file has issues");
    ingestion.ingest_file_list_and_apply_heuristics(&all_files_paths, tree_config);
    ingestion.ingest_dir_list(&all_dirs_paths);

    ingestion
        .ingest_files(|root: &str, file: &str| {
            cfg.maybe_read_file_from_given_root(&cli.tree_name, root, file)
        })
        .unwrap();

    // After this point we will only have the concise information populated.
    // We're doing this to minimize our peak memory usage here, but if we find
    // that we actually want to add more data to the per-file detailed
    // information, we should probably evaluate what's happening in practice.
    // While we can always load the detailed information back in as we iterate
    // through the analysis files we consume, for now we're only storing the
    // coverage info and it might be reasonable to not bother writing out the
    // detailed files until the end when we write out the concise file.
    ingestion
        .state
        .write_out_and_drop_detailed_file_info(&tree_config.paths.index_path);

    // Consume the ingestion logged span, pass it through our repo-ingestion
    // explainer template, and write it do sik.
    drop(ingestion_entered);
    {
        let ingestion_json = logged_ingestion_span.retrieve_serde_json().await;
        let crossref_diag_dir = format!("{}/diags/crossref", tree_config.paths.index_path);
        let ingestion_diag_path = format!("{}/repo_ingestion.md", crossref_diag_dir);
        create_dir_all(crossref_diag_dir).unwrap();

        let globals = liquid::object!({
            "logs": vec![ingestion_json],
        });
        let explain_template = build_and_parse_repo_ingestion_explainer();
        let output = explain_template.render(&globals).unwrap();
        std::fs::write(ingestion_diag_path, output).unwrap();
    }

    // ## Load Ontology Config
    //
    // I moved this before the analysis ingestion thinking we might process some
    // rules as we ingest data.  (Specifically for `label_owning_class`.)  But
    // now it seems like it's probably reasonable to process that at the normal
    // post-analysis-ingestion time to avoid limiting our options there.  But
    // I'm leaving this loading ahead of the analysis ingestion because it does
    // seem preferable that if we're going to throw a fatal error due to a
    // misconfiguration that it's much better for us to do it earlier.
    let logged_ontology_span = LoggedSpan::new_logged_span("ontology");
    let ontology_entered = logged_ontology_span.span.clone().entered();

    let ontology_toml_str = cfg
        .read_tree_config_file_with_default("ontology-mapping.toml")
        .unwrap();
    let ontology = OntologyMappingIngestion::new(&ontology_toml_str)
        .expect("ontology-mapping.toml has issues");
    drop(ontology_entered);

    // ## Process all the analysis files
    let xref_file = format!("{}/crossref", tree_config.paths.index_path);
    let xref_ext_file = format!("{}/crossref-extra", tree_config.paths.index_path);
    let jumpref_file = format!("{}/jumpref", tree_config.paths.index_path);
    let jumpref_ext_file = format!("{}/jumpref-extra", tree_config.paths.index_path);
    let id_file = format!("{}/identifiers", tree_config.paths.index_path);

    // Nested table hierarchy keyed by: [symbol, kind, path] with Vec<SearchResult> as the leaf
    // values.
    let mut table = SearchResultTable::new();
    // Maps (raw) symbol to interned-pretty symbol string.  Each raw symbol is unique, but there
    // may be many raw symbols that map to the same pretty symbol string.
    let mut pretty_table = PrettyTable::new();
    // Reverse of pretty_table.  The key is the pretty symbol, and the value is a UstrSet of all
    // of the raw symbols that map to the pretty symbol.  Pretty symbols that start with numbers or
    // include whitespace are considered illegal and not included in the map.
    //
    // This table has been modified so that it is populated with the suffix variations immediately.
    // So for the symbol "foo::bar::Baz" we will add entries for "Baz", "bar::Baz", and
    // "foo::bar::Baz".  Previously we would only add the full variation and compute the suffixes
    // when writing its contents out, but we now need/want this for processing field type strings
    // because we do not currently have the fully qualified symbols available.  In the future
    // we hopefully will have better type representations for fields.
    //
    // An alternate approach would be for us to write the identifier table out earlier and just
    // memory map that for subsequent processing.  Not doing that right now because the ustr rep
    // potentially could end up comparable in memory usage if the identifer file is fully paged
    // in, and for performance we would want it fully paged in, so might as well use the memory
    // so we fail faster if we don't have the memory available.
    let mut id_table = IdTable::default();
    // Maps (raw) symbol to `SymbolMeta` info for this symbol.  Currently, we
    // require that the language analyzer created a "structured" record and we
    // use that, but it could make sense for us to automatically generate a stub
    // meta for symbols for which we didn't find a structured record.  A minor
    // awkwardness here is that we would really want to use the "source" records
    // for this (as we did prior to the introduction of the structured record
    // type), but we currently don't retain those.  (But we do currently read
    // the file 2x; maybe it would be better to read it once and have the
    // records grouped by type so we can improve that).
    let mut meta_table = MetaTable::new();
    // Maps the (raw) symbol making the calls to a BTreeMap whose keys are the
    // symbols being called and whose values are a tuple of the path where the
    // calls are happening and a BTreeSet of the lines in the path where these
    // calls happen.  This is used so that on graphs we can have the edges have
    // a source link that highlights all of the lines where the calls are
    // happening.
    //
    // The term "callees" used here makes most sense when dealing with
    // functions/similar, but it's not just for those cases.  We also use it for
    // field accesses, etc.  This was formerly dubbed "consumes" in prototyping,
    // but that was even more confusing.  Another rename may be in order.
    let mut callees_table = CalleesTable::new();
    // Maps the (raw) symbol corresponding to a type to a BTreeMap whose key
    // is the class referencing the type and whose values are a vec of tuples of
    // the form (field pretty, pointer kind).
    let mut field_member_use_table = FieldMemberUseTable::new();

    // As we process the source entries and build the SourceMeta, we keep a running list of what
    // cross-SourceMeta links need to be established.  We then process this after all of the files
    // have been processed and we know all symbols are known.

    // Pairs of [parent class sym, subclass sym] to add subclass to parent.
    let mut xref_link_subclass = XrefLinkSubclass::new();
    // Pairs of [parent method sym, overridden by sym] to add the override to the parent.
    let mut xref_link_override = XrefLinkOverride::new();
    // (owner symbol, slotted symbol) -> slot props
    // This is a BTreeMap and not a HashMap to force a stable ordering and avoid flaky tests.
    let mut xref_link_slots = XrefLinkSlots::new();

    for path in &analysis_relative_paths {
        println!("File {}", path);

        let analysis_fname = format!("{}/analysis/{}", tree_config.paths.index_path, path);
        let file_sym: Ustr = ustr(&make_file_sym_from_path(path));

        let subsystem = make_subsystem(
            path,
            &file_sym,
            &mut ingestion,
            &mut meta_table,
            &mut pretty_table,
            &mut id_table,
        );

        // We process the structured records before checking for the source file
        // to allow us to ingest the structured records from SCIP indexing that
        // do not actually correspond to a source file.  This is the case for
        // Java imports from the JDK/Kotlin/Android runtimes.
        let structured_analysis = read_analysis(&analysis_fname, &mut read_structured);
        for datum in structured_analysis {
            for piece in datum.data {
                // If we don't have a location for the structured record then this
                // is the SCIP external structured record case mentioned above and
                // we need to insert the pretty and id_table mappings since there
                // won't be a target record for the definition.
                if datum.loc.lineno == 0 {
                    pretty_table.insert(piece.sym, piece.pretty);
                    // TODO: extract out the logic from process_analysis_target so
                    // we can generate all the suffix variations here.  But for our
                    // current needs, just the exact pretty identifier is sufficient.
                    // (The ontology rule is always on the fully qualified pretty.)
                    let id_syms = id_table.entry(piece.pretty).or_insert(UstrSet::default());
                    id_syms.insert(piece.sym);
                    // We also need to make sure there's a top-level entry in
                    // the table, even if it's empty, so that when we're
                    // building the crossref, the structured record gets emitted.
                    table.entry(piece.sym).or_default();
                }
                process_analysis_structured(
                    piece,
                    subsystem,
                    &mut meta_table,
                    &mut xref_link_subclass,
                    &mut xref_link_override,
                    &mut xref_link_slots,
                );
            }
        }

        // Load the source file and chop it up into `lines` so that we extract
        // the `line` for each result.  In the future this could move to
        // dynamic extraction that uses the `peek_range` if available and this
        // line if it's not.
        let source_fname = tree_config.find_source_file(path);
        let source_file = match File::open(source_fname.clone()) {
            Ok(f) => f,
            Err(_) => {
                println!("Unable to open source file {}", source_fname);
                continue;
            }
        };
        let reader = BufReader::new(&source_file);
        // We operate in String space here on a per-file basis, but these will be
        // flattened to ustrs when converted into a SearchResult.  The intent here
        // is that because Ustr instances permanently retain all provided strings
        // that we don't tell it about Strings until we're sure they'll be retained
        // be a SearchResult.
        let lines: Vec<_> = reader
            .lines()
            .map(|l| match l {
                Ok(line) => line_to_buf_and_offset(line),
                Err(_) => (String::from(""), 0),
            })
            .collect();

        let analysis = read_analysis(&analysis_fname, &mut read_target);

        for datum in analysis {
            // If we're going to experience a bad line, skip out before
            // creating any structure.
            let lineno = (datum.loc.lineno - 1) as usize;
            if lineno >= lines.len() {
                println!("Bad line number in file {} (line {})", path, lineno);
                continue;
            }

            for piece in datum.data {
                process_analysis_target(
                    piece,
                    path,
                    &file_sym,
                    lineno,
                    &datum.loc,
                    &mut table,
                    &mut pretty_table,
                    &mut id_table,
                    &mut callees_table,
                    &lines,
                );
            }
        }
    }

    let other_resources_file = &cli.other_resources_list_path;

    let other_resources_relative_paths: Vec<Ustr> =
        BufReader::new(File::open(other_resources_file).unwrap())
            .lines()
            .map(|x| ustr(&x.unwrap()))
            .collect();

    for path in &other_resources_relative_paths {
        println!("File {}", path);

        let pretty = ustr(format!("file {}", path).as_str());
        let file_sym = ustr(&make_file_sym_from_path(path));

        let line_and_offset = {
            let source_fname = tree_config.find_source_file(path);
            let source_file = match File::open(source_fname.clone()) {
                Ok(f) => f,
                Err(_) => {
                    println!("Unable to open source file {}", source_fname);
                    continue;
                }
            };

            let mut reader = BufReader::new(&source_file);
            let mut line: String = String::default();
            match reader.read_line(&mut line) {
                Ok(_) => line_to_buf_and_offset(line),
                Err(_) => ("(binary file)".to_string(), 0_u32),
            }
        };
        let lines = vec![line_and_offset];

        let loc = Location {
            lineno: 0,
            col_start: 0,
            col_end: 0,
        };
        let piece = AnalysisTarget {
            target: TargetTag::Target,
            kind: AnalysisKind::Def,
            pretty,
            sym: file_sym,
            context: ustr(""),
            contextsym: ustr(""),
            peek_range: LineRange {
                start_lineno: 0,
                end_lineno: 0,
            },
            arg_ranges: vec![],
        };

        process_analysis_target(
            piece,
            path,
            &file_sym,
            0,
            &loc,
            &mut table,
            &mut pretty_table,
            &mut id_table,
            &mut callees_table,
            &lines,
        );

        let _ = make_subsystem(
            path,
            &file_sym,
            &mut ingestion,
            &mut meta_table,
            &mut pretty_table,
            &mut id_table,
        );
    }

    // ## Process deferred meta cross-referencing
    for (super_sym, sub_sym) in xref_link_subclass {
        if let Some(super_meta) = meta_table.get_mut(&super_sym) {
            super_meta.subclass_syms.push(sub_sym);
        }
    }

    for (method_sym, override_sym) in xref_link_override {
        if let Some(method_meta) = meta_table.get_mut(&method_sym) {
            method_meta.overridden_by_syms.push(override_sym);
        }
    }

    for ((owner_sym, slotted_sym), (props, subsystem)) in xref_link_slots {
        if let Some(owner) = meta_table.get_mut(&owner_sym) {
            owner.binding_slots.push(StructuredBindingSlotInfo {
                sym: slotted_sym,
                props,
            });
            if owner.subsystem.is_none() {
                owner.subsystem = subsystem;
            }
        }
        if let Some(slotted) = meta_table.get_mut(&slotted_sym) {
            slotted.slot_owner = Some(StructuredBindingSlotInfo {
                sym: owner_sym,
                props,
            });
            slotted.subsystem = subsystem;
        }
    }

    // ## Run Ontology Processing
    let ontology_entered = logged_ontology_span.span.clone().entered();

    info!("Processing ontology now that all analysis files have been read in.");

    // ### Extract field-processing rules to run over every class.
    let mut field_owning_class_rules: UstrMap<OntologyLabelOwningClass> = UstrMap::default();

    for (pretty_id, rule) in ontology.config.pretty.iter() {
        if let Some(label_owning_class) = &rule.label_owning_class {
            // We lookup by the type_pretty which currently will have "class " or "struct ""
            // prefixes.  In the interest of not having to mangle every type field, create
            // "class "-prefixed variants.  I'm not creating "struct "-prefixed variants
            // right now because most things should be classes.
            let type_prettied = format!("class {}", pretty_id);
            field_owning_class_rules.insert(ustr(&type_prettied), label_owning_class.clone());
        }
    }

    // ### Process class/fields using ontology type information
    for meta in meta_table.values_mut() {
        if meta.kind.as_str() == "class" || meta.kind.as_str() == "struct" {
            for field in &mut meta.fields {
                // In order to avoid getting confused by native types, require that we have some
                // typesym.  We won't have a typesym for native types.
                if field.type_sym.is_empty() {
                    continue;
                }

                // Note that the type_pretty will have a "class " prefix which is why we already
                // pre-transformed our rules when populating the rule map.
                if let Some(rule) = field_owning_class_rules.get(&field.type_pretty) {
                    for label_rule in &rule.labels {
                        meta.labels.insert(label_rule.label);
                    }
                }

                let (ptr_infos, type_labels) = ontology
                    .config
                    .maybe_parse_type_as_pointer(&field.type_pretty);
                for label in type_labels {
                    meta.labels.insert(label);
                }
                for (ptr_kind, pointee_pretty) in ptr_infos {
                    if let Some(pointee_syms) = id_table.get(&pointee_pretty) {
                        // We need to find the first symbol that's referring to a type.
                        // Conveniently, for C++, these will always start with `T_`,
                        // which is nice because we can't do a lookup in meta right now.
                        // TODO: Generalize to better understand what's a type, especially
                        // in JS.  It might be easiest to sidestep this problem by having
                        // the analyzer be emitting structured information for the field
                        // so that we're just working in symbol space in the first place.
                        let best_sym = pointee_syms.iter().find(|s| s.starts_with("T_"));
                        if let Some(sym) = best_sym {
                            field.pointer_info.push(StructuredPointerInfo {
                                kind: ptr_kind.clone(),
                                sym: *sym,
                            });

                            let member_uses = field_member_use_table.entry(*sym).or_default();
                            let use_details = member_uses.entry(meta.sym).or_default();
                            use_details.push((field.pretty, ptr_kind));
                        }
                    } else {
                        info!(
                            pretty = pointee_pretty.as_str(),
                            "Unable to map pretty identifier to symbols."
                        );
                    }
                }
            }
        }
    }

    // ### Process Ontology Rules
    for (pretty_id, rule) in ontology.config.pretty.iter() {
        // #### Labels we just slap on
        if !rule.labels.is_empty() {
            if let Some(root_syms) = id_table.get(pretty_id) {
                for sym in root_syms {
                    if let Some(sym_meta) = meta_table.get_mut(sym) {
                        for label in &rule.labels {
                            sym_meta.labels.insert(*label);
                        }
                    }
                }
            }
        }

        // #### Runnables
        if let Some(runnable_mode) = &rule.runnable {
            info!(" Processing pretty runnable rule for: {}", pretty_id);
            if let Some(root_method_syms) = id_table.get(pretty_id) {
                // The list of symbols to process for the runnable relationship.
                // We process the root syms to find their descendants, but we
                // don't actually process the root symbols.  These pending syms
                // will both be directly processed and have their children
                // appended as well.
                let mut pending_method_syms = vec![];
                let mut is_jvm = false;
                for sym in root_method_syms {
                    // XXX We should really have an easy way to figure out the
                    // implementation language from the structured record.  Right
                    // now we only really have that for binding slots.
                    if sym.starts_with("S_jvm_") {
                        is_jvm = true;
                    }
                    if let Some(sym_meta) = meta_table.get(sym) {
                        for over in &sym_meta.overridden_by_syms {
                            pending_method_syms.push(*over);
                        }
                    }
                }

                info!("  found {} initial method syms", pending_method_syms.len());

                // (this is LIFO traversal, which is fine for us)
                while let Some(method_sym) = pending_method_syms.pop() {
                    info!("  processing method sym: {}", method_sym);

                    // use the method to find its owning class
                    let class_sym = if let Some(method_meta) = meta_table.get(&method_sym) {
                        for over in &method_meta.overridden_by_syms {
                            pending_method_syms.push(*over);
                        }

                        match method_meta.parent_sym {
                            Some(p) => p,
                            _ => continue,
                        }
                    } else {
                        continue;
                    };

                    info!("  found class sym: {}", class_sym);

                    // ### use the class to find its constructors
                    let linkage_syms = match runnable_mode {
                        OntologyRunnableMode::Constructor => {
                            if let Some(class_meta) = meta_table.get(&class_sym) {
                                let mut syms = vec![];
                                // For C++ we expect the constructors to have the same name as the class;
                                // currently for C++ we don't actually emit a special "props" "constructor"
                                // value.
                                //
                                // For the JVM we expect constructors to have a pretty name of "<init>".
                                let constructor_name: &str = if is_jvm {
                                    "<init>"
                                } else {
                                    class_meta.pretty.rsplit("::").next().unwrap()
                                };

                                let constructor_pretty =
                                    ustr(&format!("{}::{}", class_meta.pretty, constructor_name));
                                for method in &class_meta.methods {
                                    // Skip constructors that aren't known; this can happen for the copy
                                    // constructor/etc.
                                    if method.pretty == constructor_pretty
                                        && table.contains_key(&method.sym)
                                    {
                                        syms.push(method.sym);
                                    }
                                }
                                syms
                            } else {
                                continue;
                            }
                        }
                        OntologyRunnableMode::Class => {
                            vec![class_sym]
                        }
                    };

                    info!("  found linkage syms: {:?}", linkage_syms);

                    // ### mutate each of the constructors to have the ontology slot
                    for con_sym in &linkage_syms {
                        if let Some(con_meta) = meta_table.get_mut(con_sym) {
                            // XXX we could track precedence for runnable rules so that
                            // we could remove lower precedence relationships here.  This
                            // would be relevant for WorkerRunnable.

                            con_meta.ontology_slots.push(OntologySlotInfo {
                                slot_kind: OntologySlotKind::RunnableMethod,
                                syms: vec![method_sym],
                            });
                        }
                    }

                    // ### mutate our method_sym to have the ontology slot to the constructors
                    if let Some(method_meta) = meta_table.get_mut(&method_sym) {
                        method_meta.ontology_slots.push(OntologySlotInfo {
                            slot_kind: OntologySlotKind::RunnableConstructor,
                            syms: linkage_syms,
                        })
                    }
                }
            }
        }

        // #### Class Labeling (Some)
        //
        // Some rules are processed as we process structured fields above.

        if let Some(label_rule) = &rule.label_containing_class {
            info!(
                " Processing pretty label_containing_class for: {}",
                pretty_id
            );
            if let Some(root_class_syms) = id_table.get(pretty_id) {
                let mut investigate_class_syms = vec![];
                // We don't care about the root itself, just its subclasses.
                for sym in root_class_syms {
                    if let Some(sym_meta) = meta_table.get(sym) {
                        for sub in &sym_meta.subclass_syms {
                            investigate_class_syms.push(*sub);
                        }
                    }
                }

                while let Some(class_sym) = investigate_class_syms.pop() {
                    let sym_meta = match meta_table.get(&class_sym) {
                        Some(m) => m,
                        None => continue,
                    };

                    for sub in &sym_meta.subclass_syms {
                        investigate_class_syms.push(*sub);
                    }

                    // The structured record currently doesn't have a reference
                    // to its containing symbol; we need to pop the last pretty
                    // segment and perform a lookup.
                    let (pieces, delim) = split_pretty(&sym_meta.pretty, &sym_meta.sym);
                    let containing_pieces = match pieces.split_last() {
                        Some((_, rest)) => rest,
                        None => continue,
                    };
                    let containing_pretty = containing_pieces.join(delim);
                    let containing_pretty_ustr = ustr(&containing_pretty);
                    if let Some(containing_syms) = id_table.get(&containing_pretty_ustr) {
                        for sym in containing_syms {
                            if let Some(containing_meta) = meta_table.get_mut(sym) {
                                for rule in &label_rule.labels {
                                    containing_meta.labels.insert(rule.label);
                                }
                            }
                        }
                    }
                }
            }
        }

        // #### Field Labeling
        //
        // We start from an ancestral class and find all of its subclasses and all of their fields.
        // For each field, we check its uses and see if they match the rules.  If so, we will plan
        // to add a label to the field on its class.  (Currently we do not do anythign to the
        // structured info for field symbol itself.)
        if let Some(label_rule) = &rule.label_containing_class_field_uses {
            info!(
                " Processing pretty label_containing_class_field_uses rule for: {}",
                pretty_id
            );
            if let Some(root_class_syms) = id_table.get(pretty_id) {
                let mut investigate_class_syms = vec![];
                // We don't care about the root itself, just its subclasses.
                for sym in root_class_syms {
                    if let Some(sym_meta) = meta_table.get(sym) {
                        for sub in &sym_meta.subclass_syms {
                            investigate_class_syms.push(*sub);
                        }
                    }
                }

                while let Some(class_sym) = investigate_class_syms.pop() {
                    let sym_meta = match meta_table.get(&class_sym) {
                        Some(m) => m,
                        None => continue,
                    };

                    for sub in &sym_meta.subclass_syms {
                        investigate_class_syms.push(*sub);
                    }

                    // The structured record currently doesn't have a reference
                    // to its containing symbol; we need to pop the last pretty
                    // segment and perform a lookup.
                    let (pieces, delim) = split_pretty(&sym_meta.pretty, &sym_meta.sym);
                    let containing_pieces = match pieces.split_last() {
                        Some((_, rest)) => rest,
                        None => continue,
                    };
                    let containing_pretty = containing_pieces.join(delim);
                    let containing_pretty_ustr = ustr(&containing_pretty);
                    if let Some(containing_syms) = id_table.get(&containing_pretty_ustr) {
                        for sym in containing_syms {
                            if let Some(containing_meta) = meta_table.get_mut(sym) {
                                for field in &mut containing_meta.fields {
                                    if let Some(kind_map) = table.get(&field.sym) {
                                        if let Some(path_hits) = kind_map.get(&AnalysisKind::Use) {
                                            for hits in path_hits.values() {
                                                for hit in hits {
                                                    for rule in &label_rule.labels {
                                                        if hit.context.ends_with(
                                                            rule.context_sym_suffix.as_str(),
                                                        ) {
                                                            field.labels.insert(rule.label);
                                                        }
                                                    }
                                                }
                                            }
                                        }
                                    }
                                }
                            }
                        }
                    }
                }
            }
        }
    }

    // Consume the ontology logged span, pass it through our ontology-ingestion
    // explainer template, and write it to disk.
    drop(ontology_entered);
    {
        let ingestion_json = logged_ontology_span.retrieve_serde_json().await;
        let crossref_diag_dir = format!("{}/diags/crossref", tree_config.paths.index_path);
        let ingestion_diag_path = format!("{}/ontology_ingestion.md", crossref_diag_dir);
        create_dir_all(crossref_diag_dir).unwrap();

        let globals = liquid::object!({
            "logs": vec![ingestion_json],
        });
        let explain_template = build_and_parse_ontology_ingestion_explainer();
        let output = explain_template.render(&globals).unwrap();
        std::fs::write(ingestion_diag_path, output).unwrap();
    }

    // ## Write out the crossref and jumpref databases.
    let mut xref_out = File::create(xref_file).unwrap();
    let mut xref_ext_out = File::create(xref_ext_file).unwrap();

    let mut jumpref_out = File::create(jumpref_file).unwrap();
    let mut jumpref_ext_out = File::create(jumpref_ext_file).unwrap();

    // We need to know offset positions in the `-extra` file.  File::tell is a
    // nightly-only experimental API as documented at
    // https://github.com/rust-lang/rust/issues/71213 which makes it preferable
    // to avoid (although I think we may already be dependent on use of nightly
    // for save-analysis purposes?).  Seek::seek with a relative offset of 0
    // seems to be the standard fallback but there are suggestions that can
    // trigger flushes in buffered writers, etc.  So for now we're just keeping
    // track of offsets ourselves and relying on our tests to make sure we don't
    // mess up.
    let mut xref_ext_offset: usize = 0;
    let mut jumpref_ext_offset: usize = 0;

    // Let's only report missing concise info at most once, as for those cases
    // where we have them (ex: NSS), there's usually a lot of symbols in the
    // file and we'd end up reporting the missing info a lot.
    let mut reported_missing_concise = UstrSet::default();

    for (id, id_data) in table {
        let mut kindmap = Map::new();
        for (kind, kind_data) in &id_data {
            let mut result = Vec::new();
            for (path, results) in kind_data {
                if let Some(concise_info) = ingestion.state.concise_per_file.get(path) {
                    result.push(json!({
                        "path": path,
                        "path_kind": concise_info.path_kind,
                        "lines": results,
                    }));
                } else {
                    // NSS seems to have an issue with auto-generated files we
                    // don't know about, so this can't be a warning because it's
                    // too spammy.
                    if reported_missing_concise.insert(*path) {
                        info!("Missing concise info for path '{}'", path);
                    }
                }
            }
            let kindstr = match *kind {
                AnalysisKind::Use => "uses",
                AnalysisKind::Def => "defs",
                AnalysisKind::Assign => "assignments",
                AnalysisKind::Decl => "decls",
                AnalysisKind::Forward => "forwards",
                AnalysisKind::Idl => "idl",
                AnalysisKind::Alias => "aliases",
            };
            kindmap.insert(kindstr.to_string(), json!(result));
        }
        if let Some(callee_syms) = callees_table.get(&id) {
            let mut callees = Vec::new();
            for (callee_sym, (call_path, call_lines)) in callee_syms {
                if let Some(meta) = meta_table.get(callee_sym) {
                    let mut obj = BTreeMap::new();
                    obj.insert("sym".to_string(), callee_sym.to_string());
                    if let Some(pretty) = pretty_table.get(callee_sym) {
                        obj.insert("pretty".to_string(), pretty.to_string());
                    }
                    obj.insert("kind".to_string(), meta.kind.to_string());
                    obj.insert(
                        "jump".to_string(),
                        format!("{}#{}", call_path, call_lines.iter().join(",")),
                    );
                    callees.push(json!(obj));
                }
            }
            kindmap.insert("callees".to_string(), json!(callees));
        }
        if let Some(fmu_syms) = field_member_use_table.get(&id) {
            let mut fmus = Vec::new();
            for (fmu_sym, fmu_field_infos) in fmu_syms {
                if let Some(meta) = meta_table.get(fmu_sym) {
                    let mut fields = vec![];
                    for (field_pretty, ptr_kind) in fmu_field_infos {
                        fields.push(json!({
                            "pretty": field_pretty,
                            "ptr": ptr_kind,
                        }));
                    }
                    fmus.push(json!({
                        "sym": fmu_sym,
                        "pretty": meta.pretty,
                        "fields": fields,
                    }));
                }
            }
            kindmap.insert("field-member-uses".to_string(), json!(fmus));
        }
        // Put the metadata in there too.
        let mut fallback_pretty = None;
        if let Some(meta) = meta_table.get(&id) {
            kindmap.insert("meta".to_string(), json!(meta));
        } else {
            fallback_pretty = pretty_table.get(&id);
        }

        let kindmap = json!(kindmap);
        {
            let id_line = format!("!{}\n", id);
            let inline_line = format!(":{}\n", kindmap);
            if inline_line.len() >= EXTERNAL_STORAGE_THRESHOLD {
                // ### External storage.
                xref_out.write_all(id_line.as_bytes()).unwrap();
                // We write out the identifier in the extra file as well so that it
                // can be interpreted in the same fashion.
                xref_ext_out.write_all(id_line.as_bytes()).unwrap();
                xref_ext_offset += id_line.len();

                let ext_offset_line = format!(
                    "@{:x} {:x}\n",
                    // Skip the leading ":"
                    xref_ext_offset + 1,
                    // Subtract off the leading ":" but keep the newline.
                    inline_line.len() - 1
                );
                xref_out.write_all(ext_offset_line.as_bytes()).unwrap();

                xref_ext_out.write_all(inline_line.as_bytes()).unwrap();
                xref_ext_offset += inline_line.len();
            } else {
                // ### Inline storage.
                xref_out.write_all(id_line.as_bytes()).unwrap();
                xref_out.write_all(inline_line.as_bytes()).unwrap();
            }
        }

        // Also write out/update the jumpref.
        let jumpref_info = convert_crossref_value_to_sym_info_rep(kindmap, &id, fallback_pretty);
        {
            let id_line = format!("!{}\n", id);
            let inline_line = format!(":{}\n", jumpref_info);
            if inline_line.len() >= EXTERNAL_STORAGE_THRESHOLD {
                // ### External storage.
                jumpref_out.write_all(id_line.as_bytes()).unwrap();
                // We write out the identifier in the extra file as well so that it
                // can be interpreted in the same fashion.
                jumpref_ext_out.write_all(id_line.as_bytes()).unwrap();
                jumpref_ext_offset += id_line.len();

                let ext_offset_line = format!(
                    "@{:x} {:x}\n",
                    // Skip the leading ":"
                    jumpref_ext_offset + 1,
                    // Subtract off the leading ":" but keep the newline.
                    inline_line.len() - 1
                );
                jumpref_out.write_all(ext_offset_line.as_bytes()).unwrap();

                jumpref_ext_out.write_all(inline_line.as_bytes()).unwrap();
                jumpref_ext_offset += inline_line.len();
            } else {
                // ### Inline storage.
                jumpref_out.write_all(id_line.as_bytes()).unwrap();
                jumpref_out.write_all(inline_line.as_bytes()).unwrap();
            }
        }
    }

    let mut idf = File::create(id_file).unwrap();
    for (id, syms) in id_table {
        for sym in syms {
            let line = format!("{} {}\n", id, sym);
            let _ = idf.write_all(line.as_bytes());
        }
    }

    ingestion
        .state
        .write_out_concise_file_info(&tree_config.paths.index_path);
}

```

## tools/src/bin/searchfox-tool.rs
```
use std::env::args_os;

use serde_json::{to_string_pretty, to_value, Value};
use tools::{
    abstract_server::{ErrorDetails, ErrorLayer, ServerError},
    cmd_pipeline::{builder::build_pipeline, parser::OutputFormat, PipelineValues},
};
use tracing_subscriber::{fmt, prelude::*, EnvFilter};

#[tokio::main]
async fn main() {
    tracing_subscriber::registry()
        .with(fmt::layer())
        .with(EnvFilter::from_default_env())
        .init();

    let mut os_args: Vec<String> = args_os()
        .map(|os| os.into_string().unwrap_or("".to_string()))
        .collect();

    // We're expecting a single argument
    if os_args.len() == 1 {
        println!("!!! NOTE !!!");
        println!(
            "This command expects a single argument that it can parse up; quote in your shell."
        );
        println!("Example: `searchfox-tool 'cmd1 --arg | cmd2 --arg | cmd3'");
        println!();
        println!(
            "The built-in help will work, but the arg parser gets invoked once for each pipe."
        );
        println!("---");

        os_args.push("--help".to_string())
    } else if os_args.len() > 2 {
        println!("!!! TOO MANY ARGS !!!");
        println!(
            "This command expects a single argument that it can parse up; quote in your shell."
        );
        println!("Example: `searchfox-tool 'cmd1 --arg | cmd2 --arg | cmd3'");
        println!("^^^");
        std::process::exit(2);
    }

    let (pipeline, output_format) = match build_pipeline(&os_args[0], &os_args[1]) {
        Ok(pipeline) => pipeline,
        Err(ServerError::StickyProblem(ErrorDetails {
            layer: ErrorLayer::BadInput,
            message,
        })) => {
            println!("{}", message);
            std::process::exit(1);
        }
        Err(err) => {
            panic!("You did not specify a good pipeline!\n {:?}", err);
        }
    };

    let results = pipeline.run(false).await;

    let emit_json = |val: &Value| {
        if output_format == OutputFormat::Concise {
            println!("{}", val);
        } else if output_format == OutputFormat::Pretty {
            if let Ok(pretty) = to_string_pretty(val) {
                println!("{}", pretty);
            }
        }
    };

    std::process::exit(match results {
        Ok(PipelineValues::Void) => {
            println!("Void result.");
            0
        }
        Ok(PipelineValues::IdentifierList(il)) => {
            for identifier in il.identifiers {
                println!("{}", identifier);
            }
            0
        }
        Ok(PipelineValues::SymbolList(sl)) => {
            emit_json(&to_value(sl).unwrap());
            0
        }
        Ok(PipelineValues::SymbolCrossrefInfoList(sl)) => {
            for symbol_info in sl.symbol_crossref_infos {
                emit_json(&symbol_info.crossref_info);
            }
            0
        }
        Ok(PipelineValues::SymbolGraphCollection(sgc)) => {
            emit_json(&sgc.to_json());
            0
        }
        Ok(PipelineValues::FlattenedResultsBundle(frb)) => {
            emit_json(&to_value(frb).unwrap());
            0
        }
        Ok(PipelineValues::GraphResultsBundle(grb)) => {
            emit_json(&to_value(grb).unwrap());
            0
        }
        Ok(PipelineValues::HtmlExcerpts(he)) => {
            for file_excerpts in he.by_file {
                //println!("HTML excerpts from: {}", file_excerpts.file);
                for str in file_excerpts.excerpts {
                    println!("{}", str);
                }
            }
            0
        }
        Ok(PipelineValues::TextFile(fb)) => {
            println!("{}", fb.contents);
            0
        }
        Ok(PipelineValues::JsonRecords(jr)) => {
            for file_records in jr.by_file {
                for value in file_records.records {
                    emit_json(&value);
                }
            }
            0
        }
        Ok(PipelineValues::JsonValue(jv)) => {
            emit_json(&jv.value);
            0
        }
        Ok(PipelineValues::JsonValueList(jvl)) => {
            emit_json(&to_value(jvl).unwrap());
            0
        }
        Ok(PipelineValues::FileMatches(fm)) => {
            emit_json(&to_value(fm).unwrap());
            0
        }
        Ok(PipelineValues::TextMatches(tm)) => {
            emit_json(&to_value(tm).unwrap());
            0
        }
        Ok(PipelineValues::BatchGroups(bg)) => {
            emit_json(&to_value(bg).unwrap());
            0
        }
        Ok(PipelineValues::SymbolTreeTableList(sttl)) => {
            emit_json(&to_value(sttl).unwrap());
            0
        }
        Err(err) => {
            println!("Pipeline Error!");
            println!("{:?}", err);
            1
        }
    });
}

```

## tools/src/bin/merge-analyses.rs
```
//! This tool merges the analysis data in the files provided as arguments,
//! and prints the merged analysis data to stdout. The "target" data lines
//! from the input files are emitted to the output, but normalized and
//! deduplicated. The "source" data lines are merged such that the `syntax`
//! and `sym` properties are unioned across all input lines that have a
//! matching (loc, pretty) tuple.
//! This ensures that for a given identifier, only a single context menu
//! item will be displayed for a given "pretty" representation, and that
//! context menu will link to all the symbols from all the input files that
//! match that.
//!
//! Note that as this code uses the analysis.rs code for parsing and printing,
//! the emitted output should always be in a consistent/normalized format.

use std::env;
use std::io::stdout;

extern crate regex;
use regex::Regex;

extern crate env_logger;

extern crate tools;
use tools::file_format::merger::merge_files;

fn main() {
    env_logger::init();

    let args: Vec<_> = env::args().skip(1).collect();

    if args.is_empty() {
        eprintln!("Usage: merge-analyses <filename> [<filename> [...]]");
        eprintln!("  This tool will merge the analysis data from the given files");
        eprintln!("  and print it to stdout; each line will be in a normalized format.");
        std::process::exit(1);
    }

    // The paths are relative, so don't look for a leading slash, but instead anchor at the front.
    let re_platform = Regex::new(r"^analysis-([^/]+)/").unwrap();

    // Build a list of platforms that parallels the list of files in `args`.
    let platforms: Vec<String> = args
        .iter()
        .enumerate()
        .map(|(i, fname)| {
            re_platform
                .captures(fname)
                .and_then(|c| c.get(1))
                .map_or(format!("platform-{}", i), |m| m.as_str().to_string())
        })
        .collect();

    merge_files(&args, &platforms, &mut stdout());
}

```

## tools/src/bin/web-server.rs
```
extern crate env_logger;
extern crate hyper;
extern crate tools;

use std::collections::HashMap;
use std::env;
use std::fs::{File, OpenOptions};
use std::io::BufReader;
use std::io::{Read, Write};
use std::path::Path;
use std::process::Command;
use std::sync::Mutex;

use hyper::header::{ContentType, Location};
use hyper::method::Method;
use hyper::mime::Mime;
use hyper::server::{Request, Response};
use hyper::status::StatusCode;
use hyper::uri;

use tools::blame;
use tools::file_format::config;
use tools::file_format::identifiers::IdentMap;
use tools::format;
use tools::git_ops;

use tools::url_encode_path::url_decode_path;

struct WebRequest<'a> {
    path: &'a str,
}

struct WebResponse {
    status: StatusCode,
    content_type: String,
    redirect_location: Option<String>,
    output: String,
}

impl Default for WebResponse {
    fn default() -> WebResponse {
        WebResponse {
            status: StatusCode::Ok,
            content_type: "text/plain".to_owned(),
            redirect_location: None,
            output: String::new(),
        }
    }
}

impl WebResponse {
    fn html(body: String) -> WebResponse {
        WebResponse {
            content_type: "text/html".to_owned(),
            output: body,
            ..WebResponse::default()
        }
    }

    fn json(body: String) -> WebResponse {
        WebResponse {
            content_type: "application/json".to_owned(),
            output: body,
            ..WebResponse::default()
        }
    }

    fn internal_error(body: String) -> WebResponse {
        WebResponse {
            status: StatusCode::InternalServerError,
            output: body,
            ..WebResponse::default()
        }
    }

    fn not_found() -> WebResponse {
        WebResponse {
            status: StatusCode::NotFound,
            output: "Not found".to_owned(),
            ..WebResponse::default()
        }
    }

    fn redirect(url: String) -> WebResponse {
        WebResponse {
            status: StatusCode::MovedPermanently,
            redirect_location: Some(url),
            ..WebResponse::default()
        }
    }
}

fn handle_static(path: String, content_type: Option<&str>) -> WebResponse {
    let source_file = match File::open(&path) {
        Ok(f) => f,
        Err(_) => {
            return WebResponse::not_found();
        }
    };
    let mut reader = BufReader::new(&source_file);
    let mut input = String::new();
    match reader.read_to_string(&mut input) {
        Ok(_) => {}
        Err(_) => {
            return WebResponse::not_found();
        }
    }

    let inferred_content_type = match Path::new(&path).extension() {
        Some(ext) => match ext.to_str().unwrap() {
            "css" => "text/css",
            "js" => "text/javascript",
            _ => "text/html",
        },
        None => "text/html",
    };
    let content_type = match content_type {
        Some(ct) => ct,
        None => inferred_content_type,
    };

    WebResponse {
        content_type: content_type.to_owned(),
        output: input,
        ..WebResponse::default()
    }
}

fn handle(
    cfg: &config::Config,
    ident_map: &HashMap<String, IdentMap>,
    req: WebRequest,
) -> WebResponse {
    let path = url_decode_path(req.path);
    let path = path[1..].split('/').collect::<Vec<_>>();

    if !path.is_empty() && path[0] == "static" {
        let path = cfg.mozsearch_path.clone() + req.path;
        return handle_static(path, None);
    }

    if path.len() < 2 {
        return WebResponse::not_found();
    }

    let tree_name = &path[0];
    let kind = &path[1];

    println!("DBG {:?} {} {}", path, tree_name, kind);

    match &kind[..] {
        "rev" => {
            if path.len() < 3 {
                return WebResponse::not_found();
            }

            let rev = &path[2];
            let path = path.clone().split_off(3);
            let path = path.join("/");

            let mut writer = Vec::new();
            match format::format_path(cfg, tree_name, rev, &path, &mut writer) {
                Ok(()) => WebResponse::html(String::from_utf8(writer).unwrap()),
                Err(err) => WebResponse::internal_error(err.to_owned()),
            }
        }

        "hgrev" => {
            let tree_config = &cfg.trees[*tree_name];
            let git_path = match tree_config.get_git_path() {
                Ok(git_path) => git_path,
                Err(_) => return WebResponse::not_found(),
            };

            let hg_rev = path[2];
            let output_result = Command::new("git")
                .arg("cinnabar")
                .arg("hg2git")
                .arg(hg_rev)
                .current_dir(git_path)
                .output();
            match output_result {
                Ok(output) if output.status.success() => WebResponse::redirect(format!(
                    "/{}/rev/{}/{}",
                    tree_name,
                    git_ops::decode_bytes(output.stdout).trim(),
                    path[3..].join("/")
                )),
                Ok(_) => WebResponse::not_found(),
                Err(err) => WebResponse::internal_error(format!("{:?}", err)),
            }
        }

        "source" => {
            let path = path.clone().split_off(2);
            let path = path.join("/");

            let tree_config = &cfg.trees[*tree_name];

            let path = format!("{}/file/{}", tree_config.paths.index_path, path);
            handle_static(path, Some("text/html"))
        }

        "diff" => {
            if path.len() < 3 {
                return WebResponse::not_found();
            }

            let rev = &path[2];
            let path = path.clone().split_off(3);
            let path = path.join("/");

            let mut writer = Vec::new();
            match format::format_diff(cfg, tree_name, rev, &path, &mut writer) {
                Ok(()) => WebResponse::html(String::from_utf8(writer).unwrap()),
                Err(err) => WebResponse::internal_error(err.to_owned()),
            }
        }

        "commit" => {
            if path.len() < 3 {
                return WebResponse::not_found();
            }

            let rev = &path[2];

            let mut writer = Vec::new();
            match format::format_commit(cfg, tree_name, rev, &mut writer) {
                Ok(()) => WebResponse::html(String::from_utf8(writer).unwrap()),
                Err(err) => WebResponse::internal_error(err.to_owned()),
            }
        }

        "commit-info" => {
            if path.len() < 3 {
                return WebResponse::not_found();
            }

            let rev = &path[2];
            match blame::get_commit_info(cfg, tree_name, rev) {
                Ok(json) => WebResponse::json(json),
                Err(err) => WebResponse::internal_error(err.to_owned()),
            }
        }

        "complete" => {
            if let Some(ids) = ident_map.get(&tree_name.to_string()) {
                let json = ids.lookup_json(path[2], false, false, 6);
                WebResponse::json(json)
            } else {
                WebResponse::not_found()
            }
        }

        _ => WebResponse::not_found(),
    }
}

fn main() {
    env_logger::init();

    let cfg = config::load(&env::args().nth(1).unwrap(), true, None, None, None);

    let ident_map = IdentMap::load(&cfg);

    let internal_data = Mutex::new((cfg, ident_map));

    let handler = move |req: Request, mut res: Response| {
        if req.method != Method::Get {
            *res.status_mut() = StatusCode::MethodNotAllowed;
            let resp = "Invalid method".to_string().into_bytes();
            if let Err(e) = res.send(&resp) {
                eprintln!("Error when replying to {}: {:?}", req.uri, e);
            }
            return;
        }

        let path = match req.uri {
            uri::RequestUri::AbsolutePath(path) => path,
            uri::RequestUri::AbsoluteUri(url) => url.path().to_owned(),
            _ => panic!("Unexpected URI"),
        };

        let guard = match internal_data.lock() {
            Ok(guard) => guard,
            Err(poisoned) => poisoned.into_inner(),
        };
        let (ref cfg, ref ident_map) = *guard;

        let response = handle(cfg, ident_map, WebRequest { path: &path });

        *res.status_mut() = response.status;
        let output = response.output.into_bytes();
        let mime: Mime = response.content_type.parse().unwrap();
        res.headers_mut().set(ContentType(mime));
        if let Some(loc) = response.redirect_location {
            res.headers_mut().set(Location(loc));
        }
        if let Err(e) = res.send(&output) {
            eprintln!("Error when replying to {}: {:?}", path, e);
        }
    };

    {
        // We *append* to the status file because other server components
        // also write to this file when they are done starting up, and we
        // don't want to clobber those messages.
        let mut status_out = OpenOptions::new()
            .append(true)
            .create(true)
            .open(env::args().nth(2).unwrap())
            .unwrap();
        writeln!(status_out, "web-server.rs loaded").unwrap();
    }

    println!("On 8001");
    // Use 4 threads instead of the 2 that would be automatically chosen on our
    // AWS boxes.
    let _listening = hyper::Server::http("0.0.0.0:8001")
        .unwrap()
        .handle_threads(handler, 4);
}

```

## tools/src/git_ops.rs
```
use git2::{Commit, Repository, TreeEntry};
use std::path::Path;

use crate::file_format::config::GitData;

// Helpers to do things with git2

fn latin1_to_string(bytes: Vec<u8>) -> String {
    bytes.iter().map(|&c| c as char).collect()
}

pub fn decode_bytes(bytes: Vec<u8>) -> String {
    match String::from_utf8(bytes.clone()) {
        Ok(s) => s,
        Err(_) => latin1_to_string(bytes),
    }
}

pub fn read_blob_entry(repo: &Repository, entry: &TreeEntry) -> String {
    let blob_obj = entry.to_object(repo).unwrap();
    let blob = blob_obj.as_blob().unwrap();
    let mut content = Vec::new();
    content.extend(blob.content());
    decode_bytes(content)
}

pub fn get_blame_lines(
    git_data: Option<&GitData>,
    blame_commit: &Option<Commit>,
    path: &str,
) -> Option<Vec<String>> {
    match (git_data, blame_commit) {
        (
            Some(&GitData {
                blame_repo: Some(ref blame_repo),
                ..
            }),
            Some(blame_commit),
        ) => {
            let blame_tree = blame_commit.tree().ok()?;

            match blame_tree.get_path(Path::new(path)) {
                Ok(blame_entry) => {
                    let blame_data = read_blob_entry(blame_repo, &blame_entry);
                    Some(blame_data.lines().map(str::to_string).collect::<Vec<_>>())
                }
                Err(_) => None,
            }
        }
        _ => None,
    }
}

```

## tools/src/url_map_handler.rs
```
use crate::file_format::analysis_manglings::mangle_file;
use crate::file_format::config::Config;
use crate::file_format::url_map::{read_url_map, URLMap, URLMapItem};
use std::sync::OnceLock;

pub fn get_file_paths_for_url(cfg: Option<&Config>, url: &str) -> Option<Vec<URLMapItem>> {
    static URL_MAP: OnceLock<URLMap> = OnceLock::new();

    if URL_MAP.get().is_none() {
        URL_MAP
            .set(match cfg {
                Some(cfg) => match &cfg.url_map_path {
                    Some(url_map_path) => read_url_map(url_map_path),
                    None => URLMap::new_empty(),
                },
                None => URLMap::new_empty(),
            })
            .unwrap();
    }

    let url_map_key = format!("URL_{}", mangle_file(url));
    URL_MAP.get().unwrap().get(&url_map_key)
}

```

## tools/src/file_utils.rs
```
use crate::abstract_server::{ErrorDetails, ErrorLayer, Result, ServerError};

pub fn write_file_ensuring_parent_dir(file_path: &str, contents: &str) -> Result<()> {
    let as_path = std::path::Path::new(file_path);
    let parent_path = match as_path.parent() {
        Some(p) => p,
        None => {
            return Err(ServerError::StickyProblem(ErrorDetails {
                layer: ErrorLayer::DataLayer,
                message: format!("Problem getting parent of '{}'", file_path),
            }));
        }
    };
    if let Err(e) = std::fs::create_dir_all(parent_path) {
        return Err(ServerError::StickyProblem(ErrorDetails {
            layer: ErrorLayer::DataLayer,
            message: format!("Problem creating parent of '{}': {}", file_path, e),
        }));
    }
    std::fs::write(as_path, contents)?;
    Ok(())
}

```

## tools/src/languages.rs
```
use std::collections::HashMap;
use std::path::Path;

#[derive(Debug, Clone, Default)]
pub struct LanguageSpec {
    pub reserved_words: HashMap<String, String>,
    pub hash_comment: bool,
    // In JS, private symbols are now a thing.
    pub hash_identifier: bool,
    pub c_style_comments: bool,
    pub backtick_strings: bool,
    pub regexp_literals: bool,
    pub triple_quote_literals: bool,
    pub c_preprocessor: bool,
    // Rust is mostly C-like, with a couple of differences.
    pub rust_tweaks: bool,
    pub cxx14_digit_separators: bool,
    pub markdown_slug: &'static str,
}

pub const SYN_RESERVED_CLASS: &str = "class=\"syn_reserved\" ";

fn make_reserved(v: &[&str]) -> HashMap<String, String> {
    let mut reserved_words = HashMap::new();
    for word in v {
        reserved_words.insert(word.to_string(), SYN_RESERVED_CLASS.into());
    }
    reserved_words
}

static RESERVED_WORDS_JS: &[&str] = &[
    "abstract",
    "else",
    "instanceof",
    "super",
    "boolean",
    "enum",
    "int",
    "switch",
    "break",
    "export",
    "interface",
    "synchronized",
    "byte",
    "extends",
    "let",
    "this",
    "case",
    "false",
    "long",
    "throw",
    "catch",
    "final",
    "native",
    "throws",
    "char",
    "finally",
    "new",
    "transient",
    "class",
    "float",
    "null",
    "true",
    "const",
    "for",
    "package",
    "try",
    "continue",
    "function",
    "private",
    "typeof",
    "debugger",
    "goto",
    "protected",
    "var",
    "default",
    "if",
    "public",
    "void",
    "delete",
    "implements",
    "return",
    "volatile",
    "do",
    "import",
    "short",
    "while",
    "double",
    "in",
    "static",
    "with",
    "get",
    "set",
];

static RESERVED_WORDS_CPP: &[&str] = &[
    "alignas",
    "alignof",
    "and",
    "and_eq",
    "asm",
    "atomic_cancel",
    "atomic_commit",
    "atomic_noexcept",
    "auto",
    "bitand",
    "bitor",
    "bool",
    "break",
    "case",
    "catch",
    "char",
    "char16_t",
    "char32_t",
    "class",
    "compl",
    "concept",
    "const",
    "constexpr",
    "const_cast",
    "continue",
    "decltype",
    "default",
    "delete",
    "do",
    "double",
    "dynamic_cast",
    "else",
    "enum",
    "explicit",
    "export",
    "extern",
    "false",
    "float",
    "for",
    "friend",
    "goto",
    "if",
    "inline",
    "int",
    "import",
    "long",
    "module",
    "mutable",
    "namespace",
    "new",
    "noexcept",
    "not",
    "not_eq",
    "nullptr",
    // Don't mark "operator" as a keyword so that people can click
    // on it.
    //"operator",
    "or",
    "or_eq",
    "private",
    "protected",
    "public",
    "register",
    "reinterpret_cast",
    "requires",
    "return",
    "short",
    "signed",
    "sizeof",
    "static",
    "static_assert",
    "static_cast",
    "struct",
    "switch",
    "synchronized",
    "template",
    "this",
    "thread_local",
    "throw",
    "true",
    "try",
    "typedef",
    "typeid",
    "typename",
    "union",
    "unsigned",
    "using",
    "virtual",
    "void",
    "volatile",
    "wchar_t",
    "while",
    "xor",
    "xor_eq",
    "#if",
    "#ifdef",
    "#ifndef",
    "#else",
    "#elif",
    "#endif",
    "#define",
    "#undef",
    "#include",
    "#error",
    "defined",
];

static RESERVED_WORDS_AIDL: &[&str] = &[
    "parcelable",
    "import",
    "package",
    "in",
    "out",
    "inout",
    "cpp_header",
    "const",
    "true",
    "false",
    "interface",
    "oneway",
    "enum",
    "union",
];

// From 'reserved' in ipc/ipdl/ipdl/parser.py
static RESERVED_WORDS_IPDL: &[&str] = &[
    "async",
    "both",
    "child",
    "class",
    "compress",
    "compressall",
    "from",
    "include",
    "intr",
    "manager",
    "manages",
    "namespace",
    "nested",
    "nullable",
    "or",
    "parent",
    "prio",
    "protocol",
    "refcounted",
    "moveonly",
    "returns",
    "struct",
    "sync",
    "union",
    "UniquePtr",
    "upto",
    "using",
    "verify",
];

static RESERVED_WORDS_IDL: &[&str] = &[
    "cenum",
    "const",
    "interface",
    "in",
    "inout",
    "out",
    "attribute",
    "raises",
    "readonly",
    "native",
    "typedef",
    "webidl",
    "array",
    "shared",
    "iid_is",
    "size_is",
    "retval",
    "boolean",
    "void",
    "octet",
    "short",
    "long",
    "unsigned",
    "float",
    "double",
    "char",
    "string",
    "wchar",
    "wstring",
    "nsid",
    "AUTF8String",
    "ACString",
    "AString",
    "jsval",
    "Promise",
    "ptr",
    "ref",
    "uuid",
    "scriptable",
    "builtinclass",
    "function",
    "noscript",
    "deprecated",
    "object",
    "main_process_scriptable_only",
    "binaryname",
    "notxpcom",
    "symbol",
    "implicit_jscontext",
    "nostdcall",
    "must_use",
    "infallible",
    "can_run_script",
];

static RESERVED_WORDS_WEBIDL: &[&str] = &[
    "module",
    "interface",
    "partial",
    "dictionary",
    "exception",
    "enum",
    "callback",
    "typedef",
    "implements",
    "const",
    "null",
    "true",
    "false",
    "serializer",
    "stringifier",
    "jsonifier",
    "unrestricted",
    "attribute",
    "readonly",
    "inherit",
    "static",
    "getter",
    "setter",
    "creator",
    "deleter",
    "legacycaller",
    "optional",
    "Date",
    "DOMString",
    "ByteString",
    "USVString",
    "any",
    "boolean",
    "byte",
    "double",
    "float",
    "long",
    "object",
    "octet",
    "Promise",
    "required",
    "sequence",
    "MozMap",
    "short",
    "unsigned",
    "void",
    "ArrayBuffer",
    "SharedArrayBuffer",
    "or",
    // While maplike/setlike/iterable are reserved words, they're used as
    // symbols for corresponding C++ namespace.
    // "maplike",
    // "setlike",
    // "iterable",
    "Exposed",
    "ChromeOnly",
    "ChromeConstructor",
    "Pref",
    "Func",
    "AvailableIn",
    "CheckAnyPermissions",
    "CheckAllPermissions",
    "JSImplementation",
    "HeaderFile",
    "NavigatorProperty",
    "AvailableIn",
    "Func",
    "CheckAnyPermissions",
    "CheckAllPermissions",
    "Deprecated",
    "NeedResolve",
    "OverrideBuiltins",
    "ChromeOnly",
    "Unforgeable",
    "UnsafeInPrerendering",
    "LegacyEventInit",
    "ProbablyShortLivingObject",
    "ArrayClass",
    "Clamp",
    "Constructor",
    "EnforceRange",
    "ExceptionClass",
    "Exposed",
    "ImplicitThis",
    "Global",
    "PrimaryGlobal",
    "LegacyArrayClass",
    "LegacyUnenumerableNamedProperties",
    "LenientSetter",
    "LenientThis",
    "NamedConstructor",
    "NewObject",
    "NoInterfaceObject",
    "OverrideBuiltins",
    "PutForwards",
    "Replaceable",
    "SameObject",
    "SecureContext",
    "Throws",
    "TreatNonObjectAsNull",
    "TreatNullAs",
    "Unforgeable",
    "Unscopable",
];

static RESERVED_WORDS_PYTHON: &[&str] = &[
    "and", "del", "from", "not", "while", "as", "elif", "global", "or", "with", "assert", "else",
    "if", "pass", "yield", "break", "except", "import", "print", "class", "exec", "in", "raise",
    "continue", "finally", "is", "return", "def", "for", "lambda", "try",
];

// List of Rust reserved words pulled from
// https://github.com/rust-lang/rust/blob/master/src/libsyntax/symbol.rs
static RESERVED_WORDS_RUST: &[&str] = &[
    "as", "box", "break", "const", "continue", "crate", "else", "enum", "extern", "false", "fn",
    "for", "if", "impl", "in", "let", "loop", "match", "mod", "move", "mut", "pub", "ref",
    "return", "self", "Self", "static", "struct", "super", "trait", "true", "type", "unsafe",
    "use", "where", "while", "abstract", "alignof", "become", "do", "final", "macro", "offsetof",
    "override", "priv", "proc", "pure", "sizeof", "typeof", "unsized", "virtual", "yield",
    "default", "union",
];

static RESERVED_WORDS_JAVA: &[&str] = &[
    "abstract",
    "continue",
    "for",
    "new",
    "switch",
    "assert",
    "default",
    "goto",
    "package",
    "synchronized",
    "boolean",
    "do",
    "if",
    "private",
    "this",
    "break",
    "double",
    "implements",
    "protected",
    "throw",
    "byte",
    "else",
    "import",
    "public",
    "throws",
    "case",
    "enum",
    "instanceof",
    "return",
    "transient",
    "catch",
    "extends",
    "int",
    "short",
    "try",
    "char",
    "final",
    "interface",
    "static",
    "void",
    "class",
    "finally",
    "long",
    "strictfp",
    "volatile",
    "const",
    "float",
    "native",
    "super",
    "while",
    "null",
    "true",
    "false",
];

// http://kotlinlang.org/docs/reference/keyword-reference.html
static RESERVED_WORDS_KOTLIN: &[&str] = &[
    "as",
    "as?",
    "break",
    "class",
    "continue",
    "do",
    "else",
    "false",
    "for",
    "fun",
    "if",
    "in",
    "!in",
    "interface",
    "is",
    "!is",
    "null",
    "object",
    "package",
    "return",
    "super",
    "this",
    "throw",
    "true",
    "try",
    "typealias",
    "val",
    "var",
    "when",
    "while",
    "by",
    "catch",
    "constructor",
    "delegate",
    "dynamic",
    "field",
    "file",
    "finally",
    "get",
    "import",
    "init",
    "param",
    "property",
    "receiver",
    "set",
    "setparam",
    "where",
    "actual",
    "abstract",
    "annotation",
    "companion",
    "const",
    "crossinline",
    "data",
    "enum",
    "expect",
    "external",
    "final",
    "infix",
    "inline",
    "inner",
    "internal",
    "lateinit",
    "noinline",
    "open",
    "operator",
    "out",
    "override",
    "private",
    "protected",
    "public",
    "reified",
    "sealed",
    "suspend",
    "tailrec",
    "vararg",
    "field",
    "it",
];

lazy_static! {
    static ref JS_SPEC : LanguageSpec = LanguageSpec {
        reserved_words: make_reserved(RESERVED_WORDS_JS),
        hash_identifier: true,
        c_style_comments: true,
        backtick_strings: true,
        regexp_literals: true,
        markdown_slug: "js",
        .. LanguageSpec::default()
    };

    static ref HTML_SPEC : LanguageSpec = LanguageSpec {
        markdown_slug: "html",
        .. JS_SPEC.clone()
    };

    static ref CPP_SPEC : LanguageSpec = LanguageSpec {
        reserved_words: make_reserved(RESERVED_WORDS_CPP),
        c_style_comments: true,
        c_preprocessor: true,
        cxx14_digit_separators: true,
        markdown_slug: "cpp",
        .. LanguageSpec::default()
    };

    static ref AIDL_SPEC : LanguageSpec = LanguageSpec {
        reserved_words: make_reserved(RESERVED_WORDS_AIDL),
        c_style_comments: true,
        .. LanguageSpec::default()
    };

    static ref IPDL_SPEC : LanguageSpec = LanguageSpec {
        reserved_words: make_reserved(RESERVED_WORDS_IPDL),
        c_style_comments: true,
        .. LanguageSpec::default()
    };

    static ref IDL_SPEC : LanguageSpec = LanguageSpec {
        reserved_words: make_reserved(RESERVED_WORDS_IDL),
        c_style_comments: true,
        .. LanguageSpec::default()
    };

    static ref WEBIDL_SPEC : LanguageSpec = LanguageSpec {
        reserved_words: make_reserved(RESERVED_WORDS_WEBIDL),
        c_style_comments: true,
        .. LanguageSpec::default()
    };

    static ref PYTHON_SPEC : LanguageSpec = LanguageSpec {
        reserved_words: make_reserved(RESERVED_WORDS_PYTHON),
        hash_comment: true,
        triple_quote_literals: true,
        markdown_slug: "py",
        .. LanguageSpec::default()
    };

    static ref RUST_SPEC : LanguageSpec = LanguageSpec {
        reserved_words: make_reserved(RESERVED_WORDS_RUST),
        hash_comment: true, // for now, for attributes
        c_style_comments: true,
        rust_tweaks: true,
        markdown_slug: "rust",
        .. LanguageSpec::default()
    };

    static ref JAVA_SPEC : LanguageSpec = LanguageSpec {
        reserved_words: make_reserved(RESERVED_WORDS_JAVA),
        c_style_comments: true,
        markdown_slug: "java",
        .. LanguageSpec::default()
    };

    static ref KOTLIN_SPEC : LanguageSpec = LanguageSpec {
        reserved_words: make_reserved(RESERVED_WORDS_KOTLIN),
        c_style_comments: true,
        .. LanguageSpec::default()
    };
}

#[derive(Debug)]
pub enum FormatAs {
    FormatCLike(&'static LanguageSpec),
    FormatTagLike(&'static LanguageSpec),
    CSS,
    Plain,
    StaticPrefs,
    Binary,
}

pub fn select_formatting(filename: &str) -> FormatAs {
    let ext = match Path::new(filename).extension() {
        Some(ext) => ext.to_str().unwrap(),
        None => "",
    };
    match ext {
        "c" | "cc" | "cpp" | "cxx" | "h" | "hh" | "hxx" | "hpp" | "inc" | "mm" | "m" => {
            FormatAs::FormatCLike(&CPP_SPEC)
        }
        "aidl" => FormatAs::FormatCLike(&AIDL_SPEC),
        "ipdl" | "ipdlh" => FormatAs::FormatCLike(&IPDL_SPEC),
        "idl" => FormatAs::FormatCLike(&IDL_SPEC),
        "webidl" => FormatAs::FormatCLike(&WEBIDL_SPEC),
        "js" | "jsm" | "json" | "mjs" | "sjs" => FormatAs::FormatCLike(&JS_SPEC),
        "py" | "build" | "configure" => FormatAs::FormatCLike(&PYTHON_SPEC),
        "rs" => FormatAs::FormatCLike(&RUST_SPEC),
        "java" => FormatAs::FormatCLike(&JAVA_SPEC),
        "kt" => FormatAs::FormatCLike(&KOTLIN_SPEC),

        "html" | "htm" | "xhtml" | "xht" | "xml" | "xul" => FormatAs::FormatTagLike(&HTML_SPEC),

        "css" => FormatAs::CSS,

        // Keep this list in sync with the binary types list in nginx-setup.py
        "ogg" | "ttf" | "xpi" | "png" | "bcmap" | "gif" | "ogv" | "jpg" | "jpeg" | "bmp"
        | "icns" | "ico" | "mp4" | "sqlite" | "jar" | "webm" | "webp" | "woff" | "class"
        | "m4s" | "mgif" | "wav" | "opus" | "mp3" | "otf" => FormatAs::Binary,

        _ => {
            let name = match Path::new(filename).file_name() {
                Some(name) => name.to_str().unwrap(),
                None => "",
            };

            match name {
                "StaticPrefList.yaml" => FormatAs::StaticPrefs,
                _ => FormatAs::Plain,
            }
        }
    }
}

```

## tools/src/css_analyzer.rs
```
use cssparser;

use crate::file_format::analysis::{
    AnalysisKind, AnalysisSource, AnalysisTarget, LineRange, Location, SourceRange, SourceTag,
    TargetTag, WithLocation,
};

// NOTE: This does the same as analysis_manglings::mangle_file without regex
//       dependency.  regex increases the wasm file size by ~800kB.
fn mangle_name(name: &str) -> String {
    let mut s = String::new();

    for c in name.bytes() {
        match c {
            b'A'..=b'Z' | b'a'..=b'z' | b'0'..=b'9' | b'_' | b'/' => {
                s.push(c as char);
            }
            _ => {
                s.push_str(format!("@{:02X}", { c }).as_str());
            }
        }
    }

    s
}

fn to_loc(
    first_line: u32,
    start: &cssparser::SourceLocation,
    end: &cssparser::SourceLocation,
) -> Location {
    // cssparser::SourceLocation uses 0-origin line and 1-origin column.
    // analysis::Location uses 1-origin line and 0-origin column.
    // first_line is 1-origin.
    Location {
        lineno: first_line + start.line,
        col_start: start.column - 1,
        col_end: end.column - 1,
    }
}

fn to_source(
    loc: Location,
    syntax: Vec<String>,
    pretty: String,
    sym: String,
) -> WithLocation<AnalysisSource<String>> {
    WithLocation {
        data: AnalysisSource {
            source: SourceTag::Source,
            syntax,
            pretty,
            sym: vec![sym],
            no_crossref: false,
            nesting_range: SourceRange::default(),
            type_pretty: None,
            type_sym: None,
            arg_ranges: vec![],
            expansion_info: None,
            confidence: None,
        },
        loc,
    }
}

fn to_target(
    loc: Location,
    kind: AnalysisKind,
    pretty: String,
    sym: String,
) -> WithLocation<AnalysisTarget<String>> {
    WithLocation {
        data: AnalysisTarget {
            target: TargetTag::Target,
            kind,
            pretty,
            sym,
            context: "".to_string(),
            contextsym: "".to_string(),
            peek_range: LineRange {
                start_lineno: 0,
                end_lineno: 0,
            },
            arg_ranges: vec![],
        },
        loc,
    }
}

fn analyze_css_block<F>(
    input: &mut cssparser::Parser,
    first_line: u32,
    is_curly_children: bool,
    is_inside_lhs: bool,
    callback: &mut F,
) where
    F: FnMut(String),
{
    use cssparser::Token::*;
    let mut start = input.current_source_location();
    let mut is_lhs = is_inside_lhs;
    let mut after_at_property = false;
    while let Ok(token) = input.next_including_whitespace_and_comments().cloned() {
        let end = input.current_source_location();
        let mut has_block = false;
        let mut is_curly = false;
        match token {
            Ident(name) => {
                if name.starts_with("--") {
                    let loc = to_loc(first_line, &start, &end);
                    let source_pretty = format!("property {}", name.as_ref());
                    let target_pretty = name.to_string();
                    let sym = format!("CSSPROP_{}", mangle_name(name.as_ref()));

                    let (syntax, kind) = if after_at_property {
                        after_at_property = false;
                        (
                            vec!["decl".to_string(), "cssprop".to_string()],
                            AnalysisKind::Decl,
                        )
                    } else if is_lhs {
                        (
                            vec!["def".to_string(), "cssprop".to_string()],
                            AnalysisKind::Def,
                        )
                    } else {
                        (
                            vec!["use".to_string(), "cssprop".to_string()],
                            AnalysisKind::Use,
                        )
                    };

                    let source = to_source(loc, syntax, source_pretty, sym.clone());
                    callback(serde_json::to_string(&source).unwrap());
                    let target = to_target(loc, kind, target_pretty, sym);
                    callback(serde_json::to_string(&target).unwrap());
                }
            }
            Colon => {
                if is_curly_children {
                    is_lhs = false;
                }
            }
            AtKeyword(name) => {
                if name == "property" {
                    after_at_property = true;
                }
            }
            Semicolon => {
                if is_curly_children {
                    is_lhs = true;
                }
            }
            QuotedString(s) | UnquotedUrl(s) => {
                let is_moz_src = s.starts_with("moz-src:///");
                if s.starts_with("chrome://") || s.starts_with("resource://") || is_moz_src {
                    let loc = to_loc(first_line, &start, &end);
                    let source_pretty = format!("file {}", s.as_ref());
                    let target_pretty = s.to_string();
                    let sym = if is_moz_src {
                        format!("FILE_{}", mangle_name(s.as_ref().get(11..).unwrap()))
                    } else {
                        format!("URL_{}", mangle_name(s.as_ref()))
                    };
                    let syntax = vec!["use".to_string(), "file".to_string()];
                    let kind = AnalysisKind::Use;

                    let source = to_source(loc, syntax, source_pretty, sym.clone());
                    callback(serde_json::to_string(&source).unwrap());
                    let target = to_target(loc, kind, target_pretty, sym);
                    callback(serde_json::to_string(&target).unwrap());
                }
            }
            CurlyBracketBlock => {
                has_block = true;
                is_curly = true;
            }
            Function(_) | ParenthesisBlock | SquareBracketBlock => {
                has_block = true;
            }
            _ => {}
        };

        if has_block {
            let mut is_child_lhs = false;
            if is_curly {
                is_child_lhs = true;
            }
            let _: Result<(), cssparser::ParseError<()>> = input.parse_nested_block(|input| {
                analyze_css_block(input, first_line, is_curly, is_child_lhs, callback);
                Ok(())
            });
        }

        start = end;
    }
}

pub fn analyze_css<F>(path: String, first_line: u32, text: String, callback: &mut F)
where
    F: FnMut(String),
{
    let mut input = cssparser::ParserInput::new(text.as_str());
    let mut input = cssparser::Parser::new(&mut input);

    if !path.is_empty() {
        let loc = Location {
            lineno: 1,
            col_start: 0,
            col_end: 0,
        };
        let pretty = format!("file {}", path);
        let sym = format!("FILE_{}", mangle_name(path.as_str()));
        let kind = AnalysisKind::Def;
        let target = to_target(loc, kind, pretty, sym);

        callback(serde_json::to_string(&target).unwrap());
    }

    analyze_css_block(&mut input, first_line, false, false, callback);
}

```

## tools/src/lib.rs
```
extern crate serde;
extern crate serde_json;

#[cfg(not(target_arch = "wasm32"))]
extern crate chrono;
#[cfg(not(target_arch = "wasm32"))]
extern crate clap;
#[cfg(not(target_arch = "wasm32"))]
extern crate git2;
#[cfg(not(target_arch = "wasm32"))]
extern crate include_dir;
#[cfg(not(target_arch = "wasm32"))]
extern crate itertools;
#[cfg(not(target_arch = "wasm32"))]
extern crate log;
#[cfg(not(target_arch = "wasm32"))]
#[macro_use]
extern crate lazy_static;
#[cfg(not(target_arch = "wasm32"))]
extern crate lexical_sort;
#[cfg(not(target_arch = "wasm32"))]
extern crate linkify;
#[cfg(not(target_arch = "wasm32"))]
extern crate liquid;
#[cfg(not(target_arch = "wasm32"))]
extern crate query_parser;
#[cfg(not(target_arch = "wasm32"))]
extern crate regex;
#[cfg(not(target_arch = "wasm32"))]
#[macro_use]
extern crate tracing;
#[cfg(not(target_arch = "wasm32"))]
extern crate tracing_subscriber;
#[cfg(not(target_arch = "wasm32"))]
extern crate uuid;

pub mod css_analyzer;
pub mod file_format;

#[cfg(not(target_arch = "wasm32"))]
pub mod abstract_server;
#[cfg(not(target_arch = "wasm32"))]
pub mod cmd_pipeline;
#[cfg(not(target_arch = "wasm32"))]
pub mod query;
#[cfg(not(target_arch = "wasm32"))]
pub mod templating;
#[cfg(not(target_arch = "wasm32"))]
pub mod tree_sitter_support;

#[cfg(not(target_arch = "wasm32"))]
pub mod blame;
#[cfg(not(target_arch = "wasm32"))]
pub mod describe;
#[cfg(not(target_arch = "wasm32"))]
pub mod doc_trees_handler;
#[cfg(not(target_arch = "wasm32"))]
pub mod file_utils;
#[cfg(not(target_arch = "wasm32"))]
pub mod format;
#[cfg(not(target_arch = "wasm32"))]
pub mod git_ops;
#[cfg(not(target_arch = "wasm32"))]
pub mod glob_helper;
#[cfg(not(target_arch = "wasm32"))]
pub mod languages;
#[cfg(not(target_arch = "wasm32"))]
pub mod links;
#[cfg(not(target_arch = "wasm32"))]
pub mod logging;
#[cfg(not(target_arch = "wasm32"))]
pub mod output;
#[cfg(not(target_arch = "wasm32"))]
mod symbol_graph_edge_kind;
#[cfg(not(target_arch = "wasm32"))]
pub mod tokenize;
#[cfg(not(target_arch = "wasm32"))]
pub mod url_encode_path;
#[cfg(not(target_arch = "wasm32"))]
pub mod url_map_handler;

```

## tools/src/blame.rs
```
use crate::file_format::config::Config;
use crate::links;

use serde_json::{json, to_string, Map};
use std::borrow::Cow;

use chrono::datetime::DateTime;
use chrono::naive::datetime::NaiveDateTime;
use chrono::offset::fixed::FixedOffset;

pub fn commit_header(commit: &git2::Commit) -> Result<(String, String), &'static str> {
    fn entity_replace(s: &str) -> String {
        s.replace("&", "&amp;").replace("<", "&lt;")
    }

    let msg = commit.message().ok_or("Invalid message")?;
    let mut iter = msg.split('\n');
    let header = iter.next().unwrap();
    let remainder = iter.collect::<Vec<_>>().join("\n");
    let header = links::linkify_commit_header(&entity_replace(header));
    Ok((header, entity_replace(&remainder)))
}

pub fn get_commit_info(cfg: &Config, tree_name: &str, revs: &str) -> Result<String, &'static str> {
    let tree_config = cfg.trees.get(tree_name).ok_or("Invalid tree")?;
    let git = tree_config.get_git()?;
    let mut infos = vec![];
    for rev in revs.split(',') {
        let commit_obj = git.repo.revparse_single(rev).map_err(|_| "Bad revision")?;
        let commit = commit_obj.as_commit().ok_or("Bad revision")?;
        let (msg, _) = commit_header(commit)?;

        let naive_t = NaiveDateTime::from_timestamp(commit.time().seconds(), 0);
        let tz = FixedOffset::east(commit.time().offset_minutes() * 60);
        let t: DateTime<FixedOffset> = DateTime::from_utc(naive_t, tz);
        let t = t.to_rfc2822();

        let sig = commit.author();
        let (name, email) = git
            .mailmap
            .lookup(sig.name().unwrap(), sig.email().unwrap());

        let msg = format!("{}\n<br><i>{} &lt;{}>, {}</i>", msg, name, email, t);

        let mut obj = Map::new();

        obj.insert("header".to_owned(), json!(msg));

        let parents = commit.parent_ids().collect::<Vec<_>>();
        if parents.len() == 1 {
            obj.insert("parent".to_owned(), json!(parents[0].to_string()));
        }

        obj.insert("date".to_owned(), json!(t));

        if let (Some(hg_path), Some(hg_id)) =
            (&tree_config.paths.hg_root, git.hg_map.get(&commit_obj.id()))
        {
            obj.insert(
                "fulldiff".to_owned(),
                json!(format!("{}/rev/{}", hg_path, hg_id)),
            );
        };

        infos.push(json!(obj));
    }

    Ok(to_string(&json!(infos)).unwrap())
}

#[derive(Debug)]
pub struct LineData<'a> {
    pub rev: Cow<'a, str>,
    pub path: Cow<'a, str>,
    pub lineno: Cow<'a, str>,
}

impl<'a> LineData<'a> {
    pub fn deserialize(line: &'a str) -> Self {
        let mut pieces = line.splitn(4, ':');
        let rev = pieces.next().unwrap();
        let path = pieces.next().unwrap();
        let lineno = pieces.next().unwrap();
        LineData {
            rev: Cow::Borrowed(rev),
            path: Cow::Borrowed(path),
            lineno: Cow::Borrowed(lineno),
        }
    }

    pub fn path_unchanged() -> Cow<'a, str> {
        Cow::Owned(String::from("%"))
    }

    pub fn is_path_unchanged(&self) -> bool {
        self.path == "%"
    }

    pub fn serialize(&self) -> String {
        // The trailing colon delimits an empty "author" field
        // that was never used.
        format!("{}:{}:{}:", self.rev, self.path, self.lineno)
    }
}

```

## tools/src/abstract_server/server_interface.rs
```
use async_trait::async_trait;
use axum::http::StatusCode;
use axum::response::{IntoResponse, Response};
use futures_core::stream::BoxStream;
use serde::Serialize;
use serde_json::Value;
use ustr::{ustr, Ustr};

use crate::file_format::repo_data_ingestion::ConcisePerFileInfo;

pub type Result<T> = std::result::Result<T, ServerError>;

// JSON parse errors are sticky data problems.
impl From<serde_json::Error> for ServerError {
    fn from(err: serde_json::Error) -> ServerError {
        ServerError::StickyProblem(ErrorDetails {
            layer: ErrorLayer::DataLayer,
            message: err.to_string(),
        })
    }
}

// RegExps that are part of our code will be unwrap()ed inline to panic in
// tests, and config file regexps should have their errors handled inline,
// leaving us able to assume (and transform) any remaining regexp errors as
// relating to user input.
impl From<regex::Error> for ServerError {
    fn from(err: regex::Error) -> ServerError {
        ServerError::StickyProblem(ErrorDetails {
            layer: ErrorLayer::BadInput,
            message: format!("bad regexp: {}", err),
        })
    }
}

impl From<tokio::task::JoinError> for ServerError {
    fn from(err: tokio::task::JoinError) -> ServerError {
        // There was a debugging case where I needed to uncomment this, but at
        // least when using pipeline-server, we get a useful backtrace in the
        // "pipeline-server.err" file and so this should not be necessary.
        /*
        if let Ok(reason) = err.try_into_panic() {
            // Resume the panic on the main task
            std::panic::resume_unwind(reason);
        }
        */
        ServerError::StickyProblem(ErrorDetails {
            layer: ErrorLayer::RuntimeInvariantViolation,
            message: format!("task panicked?: {}", err),
        })
    }
}

impl From<liquid::Error> for ServerError {
    fn from(err: liquid::Error) -> ServerError {
        ServerError::StickyProblem(ErrorDetails {
            layer: ErrorLayer::ConfigLayer,
            message: format!("Liquid error: {}", err),
        })
    }
}

impl IntoResponse for ServerError {
    fn into_response(self) -> Response {
        let body = format!("Error: {:#?}", self);
        (StatusCode::INTERNAL_SERVER_ERROR, body).into_response()
    }
}

/// `fetch_html` needs to fetch from a number of parallel directories in the
/// index directory, and this enum maps to those directories.
#[derive(Debug, Eq, PartialEq)]

pub enum HtmlFileRoot {
    /// `INDEX_ROOT/file` root, unified remotely.
    FormattedFile,
    /// `INDEX_ROOT/dir` root for local, unified remotely.
    FormattedDir,
    /// `INDEX_ROOT/templates` root for local, not available remotely.
    FormattedTemplate,
}

/// Express whether the error seems to be happening in the server or the data.
#[derive(Debug)]
pub enum ErrorLayer {
    /// The request itself has structural issues like a malformed URL.  This
    /// should not be used for cases where the user input results in a search
    /// miss (which should instead be part of the result payload), but is
    /// appropriate for cases where the user input makes it impossible to return
    /// a hit or a miss, like an incorrectly constructed pipeline.
    ///
    /// For example, we would want to throw an error in an "insta" check if the
    /// pipeline is not a valid pipeline.  Or similarly, a shell script invoking
    /// searchfox-tool wants to experience an error if the command pipeline is
    /// incorrect.
    ///
    /// This does potentially end up ambiguous in the web UI case if the web UI
    /// allows the user to construct pipelines that aren't validated before
    /// being sent to the server.  In that case we would want to treat the error
    /// akin to a search miss and not generate errors that would trip alarms.
    /// (Our "insta" checks of course help avoid such problems becoming serious
    /// silent errors, as they would/should not be quieted.)
    BadInput,
    /// The problem seems to involve configuration data, for example in the
    /// query to pipeline mappings.
    ConfigLayer,
    /// The error seems to involve server logic, so it may or may not be an issue
    /// with the underlying data.
    ServerLayer,
    /// The error seems to be related to the indexed data in question rather
    /// than the server, like the data was not indexed.
    DataLayer,
    /// Our data structure doesn't work like it's supposed to and we don't want
    /// to panic, so we return this instead.
    RuntimeInvariantViolation,
    /// We're not sure if it was a server issue or a data issue.
    UnknownLayer,
}

/// ServerError payload to provide details about what went wrong for
/// investigation purposes.  In the future, this could wrap the
/// underlying errors we've seen.
#[derive(Debug)]
pub struct ErrorDetails {
    /// Attempt to distinguish failures due to server bugs from failures due to
    /// indexing bugs.  For example a 500 response from a server would be a
    /// `ServerLayer` problem, but if a 404 was instead returned, that would be
    /// a `DataLayer` problem.
    pub layer: ErrorLayer,
    /// Stringified version of the lower level error.
    pub message: String,
}

/// Does a retry makes sense or not?
///
/// Actually performing retries could of course happen either below this
/// abstraction layer or above it.  The argument for above is that the
/// `cmd_pipeline` could make more informed scheduling decisions with
/// appropriately long back-offs than this lower layer would be able to.  But
/// that's all speculative at this point and this type is really being
/// introduced because we need a unifying error type.
#[derive(Debug)]
pub enum ServerError {
    /// An error that will persist for at least this index.  For example a 404.
    StickyProblem(ErrorDetails),
    /// An error that might go away if retried later.  For example a 504 "Gateway
    /// timeout".
    TransientProblem(ErrorDetails),
    Unsupported,
}

/// Livegrep/codesearch bounds
#[derive(Serialize)]
pub struct TextBounds {
    pub start: i32,
    pub end_exclusive: i32,
}

/// Livegrep/codesearch line hit results
#[derive(Serialize)]
pub struct TextMatchInFile {
    pub line_num: u32,
    pub bounds: TextBounds,
    // This will vary a lot and so can never be a Ustr.
    pub line_str: String,
}

#[derive(Serialize)]
pub struct TextMatchesByFile {
    pub file: Ustr,
    pub path_kind: Ustr,
    pub matches: Vec<TextMatchInFile>,
}

/// Livegrep/codesearch text search results clustered by file.
#[derive(Serialize)]
pub struct TextMatches {
    pub by_file: Vec<TextMatchesByFile>,
}

#[derive(Serialize)]
pub struct FileMatch {
    pub path: Ustr,
    pub concise: ConcisePerFileInfo<Ustr>,
}

impl FileMatch {
    pub fn get_containing_dir(&self) -> Ustr {
        match self.path.rfind('/') {
            // We don't want to include the trailing "/"
            Some(offset) => ustr(&self.path[0..offset]),
            None => ustr(""),
        }
    }
}

#[derive(Serialize)]
pub struct FileMatches {
    pub file_matches: Vec<FileMatch>,
}

pub enum SearchfoxIndexRoot {
    /// Already gzipped analysis files.  Note that `fetch_raw_analysis` exists
    /// and should be used in preference to this for reading file contents.
    CompressedAnalysis,
    /// The root of the config repo.
    ConfigRepo,
    /// The templates dir under the index root, home of the search template and
    /// the rendered help.html.  This differs from IndexPages because the pages
    /// directory is intended to be exposed directly to the web, whereas if we
    /// expose the templates pages, we do so individually via symlinks.
    IndexTemplates,
    /// The "pages" dir under the index root.  This directory is intended to be
    /// exposed to the web in its entirety, which differs from IndexTemplates,
    /// which is not.  This is the home of the settings page.
    IndexPages,
    /// Directory listings.
    UncompressedDirectoryListing,
}

pub struct TreeInfo {
    pub name: String,
}

/// Unified exposure for interacting with a local Searchfox index on disk or
/// a remote searchfox server over HTTPS talking to the web-server.
///
/// The primary goal is for our tests to verify both our on-disk representations
/// and that these are exposed to searchfox users correctly.  It's also our hope
/// that this can be used by searchfox contributors to investigate problems and
/// how things currently work more efficiently and enjoyably than manually doing
/// so.
///
/// ## Runtime Assumptions
///
/// We assume that we are operating in a tokio multi-threaded runtime and that
/// all blocking operations for any implementations of these traits should
/// responsibly make use of
/// https://docs.rs/tokio/1.5.0/tokio/task/index.html#blocking-and-yielding so
/// that full parallelism can be maintained.  In particular, this means that
/// mmap-based lookups which can fault and block on IO should likely use
/// https://docs.rs/tokio/1.5.0/tokio/task/index.html#block_in_place.
///
/// ## Abstraction Level / Library Use
///
/// Currently existing analysis-file processing and other logic:
/// - Uses synchronous I/O
///
/// In the end, it likely would make sense for the analysis mechanism to:
/// - Support async I/O
/// - Use async streams via https://docs.rs/tokio-stream/0.1.5/tokio_stream/
///   on a per-record or per-line granularity, quite possibly using our analysis
///   types for analysis records instead of untyped JSON.
///
/// But I'm introducing this interface right now in an attempt to provide
/// increased test coverage before making more extensive refactorings.  So for
/// now, this interface will do the simplest thing possible.
///
#[async_trait]
pub trait AbstractServer {
    /// Clone this server so that the command can kick off some parallelism.
    /// Currently the idea is that the server implementations are fairly cheap
    /// objects around either something that either:
    /// - Can safely be used from multiple threads through use of an Arc wrapper
    ///   around the data in question so we don't need to duplicate it.
    /// - Is something like libgit2 where everything we do is actually mutation
    ///   and there's no sane way to share it across multiple threads and so
    ///   we either start from a fresh state for each new clone or we have it
    ///   operate through some kind of request pool or something.
    ///
    /// So far we've only had to deal with immutable data so it hasn't mattered.
    fn clonify(&self) -> Box<dyn AbstractServer + Send + Sync>;

    /// Return info about this tree, primarily for templating purposes.
    fn tree_info(&self) -> Result<TreeInfo>;

    /// Convert a searchfox tree-local path into an absolute path on disk using
    /// the requested root.  This fundamentally only works for local indices.
    /// Note that many paths also have uncompressed (pre compress-outputs.sh)
    /// and compressed variants (post compress-outputs.sh).  In general the
    /// uncompressed variants only make sense for parts of the indexing process
    /// using searchfox-tool.  All checks will happen after compression has
    /// happened.
    ///
    /// TODO: Consider changing uses of this to instead operate in terms of
    /// read or write requests.  Currently this is only used for single-threaded
    /// synchronous stuff, but it could make sense to overhaul this if:
    /// - We're trying to move to parallelism or where the written files could
    ///   result in read data dependencies where they'd want to know when the
    ///   write happens.
    /// - We think this could make some kinds of testing easier or faster.
    fn translate_path(&self, root: SearchfoxIndexRoot, sf_path: &str) -> Result<String>;

    /// Fetch the contents of the analysis file for the given searchfox
    /// tree-local path, decompressing if it's compressed.
    async fn fetch_raw_analysis<'a>(&self, sf_path: &str) -> Result<BoxStream<'a, Value>>;

    /// Fetch the contents of a raw (not HTML rendered) source file
    /// corresponding to the indexed revision like you would get out of revision
    /// control.
    ///
    /// TODO: In the future this should probably take a revision descriptor so
    /// we can actually check the source file out if needed.
    async fn fetch_raw_source(&self, sf_path: &str) -> Result<String>;

    /// Fetch the lines in the rendered HTML file.
    ///
    /// Returns a tuple of a list of lines, 0-th item for line 1,
    /// and a JSON string for the SYM_INFO object.
    async fn fetch_formatted_lines(&self, sf_path: &str) -> Result<(Vec<String>, String)>;

    /// Fetch the contents of a rendered HTML file, decompressing if it's
    /// compressed.  If `is_file` is true, this will be from the INDEX/file
    /// sub-tree.  If `is_file` is false, we're treating it as a directory
    /// request and we'll fetch the relevant `index.html.gz` file from the
    /// INDEX/dir sub-tree.
    async fn fetch_html(&self, root: HtmlFileRoot, sf_path: &str) -> Result<String>;

    /// Retrieve the JSON contents of the crossref database for the given
    /// symbol.  Optionally performs the extra processing provided by
    /// `lazy_crossref.rs` on local indices; this should only be passed for
    /// specific use-cases that know they need the new experimental data.
    async fn crossref_lookup(&self, symbol: &str, extra_processing: bool) -> Result<Value>;

    /// Retrieve the JSON contents of the jumpref database for the given
    /// symbol.
    async fn jumpref_lookup(&self, symbol: &str) -> Result<Value>;

    /// Search the list of all files using a (potentially empty) regexp string
    /// and optionally enforcing a limit.  The underlying list of files should
    /// be equivalent to the union of the `repo-files` and `objdir-files`
    /// listings.
    ///
    /// This call's structure was chosen for consistency with the other search
    /// calls but it potentially could be reasonable to instead just have a
    /// primitive that allows the caller to request the file listings from the
    /// index dir root.  Symmetry broke in favor of this seeming like a more
    /// useful API that is decoupled from file formats.  Also, because it
    /// probably makes sense that file metadata lookup might want to be random
    /// access like `crossref_lookup` or that we might even just use
    /// `crossref_lookup` with the file path mangled into a searchfox internal
    /// symbol (like we've already started to do for C++ includes).
    ///
    /// It could also make sense for this API to eventually be more powerful and
    /// to support non-path constraints like tests being enabled/disabled/etc.
    /// but that would benefit from performing an analysis of filters we could
    /// feasibly provide and that people agree would be useful.
    ///
    /// Note that this will initially be local-only and whether it makes sense
    /// as a remote API really hinges on a rationale for not just remoting
    /// the new "query" mechanism.
    async fn search_files(
        &self,
        pathre: &str,
        include_dirs: bool,
        limit: usize,
    ) -> Result<FileMatches>;

    /// Given an identifier (prefix), return pairs of matching identifiers and
    /// symbols that correspond to those identifiers.
    ///
    /// If `exact_match` is true, then this is just a (potentially case-insensitive)
    /// lookup.  If it's false, then this is a prefix search that skips anything
    /// that looks like hierarchy traversal.  That is, if we are searching for
    /// a needle of "Foo", this will match "Food" and "Fool" but not
    /// "Food::Pizza" or "Food.Pizza" because `:` and `.` are considered
    /// indications of hierarchy traversal.
    async fn search_identifiers(
        &self,
        needle: &str,
        exact_match: bool,
        ignore_case: bool,
        match_limit: usize,
    ) -> Result<Vec<(Ustr, Ustr)>>;

    /// Given an re2 search pattern and additional config info, run a
    /// livegrep codesearch against an already-running codesearch server.  In
    /// the future while our rust code may be responsible for starting the
    /// codesearch server and keeping it running, for now that responsibility
    /// continues to fall to the `router.py` webserver using `codesearch.py`.
    async fn search_text(
        &self,
        pattern: &str,
        fold_case: bool,
        path: &str,
        limit: usize,
    ) -> Result<TextMatches>;

    async fn perform_query(&self, q: &str) -> Result<Value>;
}

```

## tools/src/abstract_server/remote_server.rs
```
use async_trait::async_trait;
use futures_core::stream::BoxStream;
use serde_json::{from_str, Value};
use url::{ParseError, Url};
use ustr::Ustr;

use super::{
    server_interface::{
        AbstractServer, ErrorDetails, ErrorLayer, FileMatches, Result, SearchfoxIndexRoot,
        ServerError,
    },
    HtmlFileRoot, TextMatches, TreeInfo,
};

/// reqwest won't return an error for an unhappy status code itself; someone
/// would need to call `Response::error_from_status`, so for now we'll generally
/// assume everything is some kind of transient problem.
impl From<reqwest::Error> for ServerError {
    fn from(err: reqwest::Error) -> ServerError {
        ServerError::TransientProblem(ErrorDetails {
            layer: ErrorLayer::ServerLayer,
            message: err.to_string(),
        })
    }
}

impl From<ParseError> for ServerError {
    fn from(err: ParseError) -> ServerError {
        ServerError::StickyProblem(ErrorDetails {
            layer: ErrorLayer::BadInput,
            message: err.to_string(),
        })
    }
}

#[allow(dead_code)]
#[derive(Clone, Debug)]
struct RemoteServer {
    tree_name: String,
    server_base_url: Url,
    tree_base_url: Url,
    source_base_url: Url,
    raw_analysis_base_url: Url,
    search_url: Url,
}

async fn get(url: Url) -> Result<reqwest::Response> {
    //println!("Using URL {}", url);
    let res = reqwest::get(url).await?;

    if !res.status().is_success() {
        if res.status().is_server_error() {
            return Err(ServerError::TransientProblem(ErrorDetails {
                layer: ErrorLayer::ServerLayer,
                message: format!("Server status of {}", res.status()),
            }));
        } else {
            return Err(ServerError::StickyProblem(ErrorDetails {
                layer: ErrorLayer::DataLayer,
                message: format!("Server status of {}", res.status()),
            }));
        }
    }

    Ok(res)
}

async fn get_json(url: Url) -> Result<reqwest::Response> {
    let client = reqwest::Client::new();
    let res = client
        .get(url)
        .header("Accept", "application/json")
        .send()
        .await?;

    if !res.status().is_success() {
        if res.status().is_server_error() {
            return Err(ServerError::TransientProblem(ErrorDetails {
                layer: ErrorLayer::ServerLayer,
                message: format!("Server status of {}", res.status()),
            }));
        } else {
            return Err(ServerError::StickyProblem(ErrorDetails {
                layer: ErrorLayer::DataLayer,
                message: format!("Server status of {}", res.status()),
            }));
        }
    }

    Ok(res)
}

#[async_trait]
impl AbstractServer for RemoteServer {
    fn clonify(&self) -> Box<dyn AbstractServer + Send + Sync> {
        Box::new(self.clone())
    }

    fn tree_info(&self) -> Result<TreeInfo> {
        Ok(TreeInfo {
            name: self.tree_name.clone(),
        })
    }

    fn translate_path(&self, _root: SearchfoxIndexRoot, _sf_path: &str) -> Result<String> {
        // Remote servers don't have local filesystem paths.
        Err(ServerError::Unsupported)
    }

    async fn fetch_raw_analysis<'a>(&self, sf_path: &str) -> Result<BoxStream<'a, Value>> {
        let url = self.raw_analysis_base_url.join(sf_path)?;
        let raw_str = get(url).await?.text().await?;
        let values: Result<Vec<Value>> = raw_str
            .lines()
            .map(|s| from_str(s).map_err(ServerError::from))
            .collect();
        Ok(Box::pin(tokio_stream::iter(values?)))
    }

    async fn fetch_formatted_lines(&self, _sf_path: &str) -> Result<(Vec<String>, String)> {
        Err(ServerError::Unsupported)
    }

    async fn fetch_raw_source(&self, _sf_path: &str) -> Result<String> {
        // I'm not sure we actually expose the underlying raw file?
        Err(ServerError::Unsupported)
    }

    async fn fetch_html(&self, root: HtmlFileRoot, sf_path: &str) -> Result<String> {
        // We don't have access to raw templates, so just call that unsupported.
        // Note that we could special-case for "help.html" here since it does
        // get explicitly exposed as "index.html", but it's also fine to only
        // validate this for local files.
        if root == HtmlFileRoot::FormattedTemplate {
            return Err(ServerError::Unsupported);
        }
        // Our tree-relative paths should not start with a slash
        let norm_path = sf_path.strip_prefix('/').unwrap_or(sf_path);
        // We don't both caring about the presence of ".." here because we don't
        // have any security-ish things to worry about for a public web server.

        let url = self.source_base_url.join(norm_path)?;
        let html = get(url).await?.text().await?;
        Ok(html)
    }

    async fn crossref_lookup(&self, _symbol: &str, _extra_processing: bool) -> Result<Value> {
        // Let's require local index for now; we'll expose this once this
        // mechanism is exposed to the web so we can talk to the corresponding
        // local server over https.
        //
        // That is, we could build this on top of the existing router.py, but
        // the legacy rep is definitely not what we want and although the
        // "sorch" endpoint that's an artifact of the fancy-branch prototype
        // is closer, it's probably better if that doesn't get stabilized.
        Err(ServerError::Unsupported)
    }

    async fn jumpref_lookup(&self, _symbol: &str) -> Result<Value> {
        // Same rationale for `crossref_lookup` above.
        Err(ServerError::Unsupported)
    }

    async fn search_files(
        &self,
        _pathre: &str,
        _is_dir: bool,
        _limit: usize,
    ) -> Result<FileMatches> {
        // Not yet; see interface comment.
        Err(ServerError::Unsupported)
    }

    async fn search_identifiers(
        &self,
        _needle: &str,
        _exact_match: bool,
        _ignore_case: bool,
        _match_limit: usize,
    ) -> Result<Vec<(Ustr, Ustr)>> {
        // Same rationale as crossref_lookup.
        Err(ServerError::Unsupported)
    }

    async fn search_text(
        &self,
        _pattern: &str,
        _fold_case: bool,
        _path: &str,
        _limit: usize,
    ) -> Result<TextMatches> {
        // It's not clear we ever want to implement this.
        Err(ServerError::Unsupported)
    }

    async fn perform_query(&self, q: &str) -> Result<Value> {
        let mut url = self.search_url.clone();
        // If adding more parameters, considering using `query_pairs_mut()`.
        url.set_query(Some(&format!("q={}", q)));
        let raw_str = get_json(url).await?.text().await?;
        match from_str(&raw_str) {
            Ok(json) => Ok(json),
            Err(err) => Err(ServerError::StickyProblem(ErrorDetails {
                layer: ErrorLayer::ServerLayer,
                message: err.to_string(),
            })),
        }
    }
}

pub fn make_remote_server(
    server_base_url: Url,
    tree_name: &str,
) -> Result<Box<dyn AbstractServer + Send + Sync>> {
    let tree_base_url = server_base_url.join(&format!("{}/", tree_name))?;
    let source_base_url = tree_base_url.join("source/")?;
    let raw_analysis_base_url = tree_base_url.join("raw-analysis/")?;
    let search_url = tree_base_url.join("search")?;

    Ok(Box::new(RemoteServer {
        tree_name: tree_name.to_string(),
        server_base_url,
        tree_base_url,
        source_base_url,
        raw_analysis_base_url,
        search_url,
    }))
}

```

## tools/src/abstract_server/lazy_crossref.rs
```
//! Lazy computation that could have taken place in crossref.rs but did not.
//!
//! Logic goes in here that likely should run in crossref.rs once we're:
//! - Certain we want the functionality.
//! - Largely done iterating on the logic.  crossref for m-c takes 21 minutes so
//!   it is hard to have a tight experimentation loop.  It's also the case that
//!   `make build-test-repo` is much slower now since we added Java/Kotlin
//!   support.
//!
//! Functionality currently under development:
//! - Argument string population.  Currently for C++ we emit the argRanges on
//!   source and target records but not the string payloads.  (This was partly
//!   done for cost reasons, but also because we want to be able to use the
//!   ranges to know what semantic records they cover.)
//!
//! Some shorter term potential functionality:
//! - Inferred thread usage for methods that are looked up; classes would be
//!   useful too but is something where it either needs to happen in crossref
//!   proper or the lazy crossref mechanism here needs to become stateful and
//!   cache some things.  That's likely a dangerous path in terms of the
//!   potential for it to stick around and get increasingly messy.
//!   - Arguably any commands that want to know things for classes should
//!     probably support arbitrary predicates/checks which can only be
//!     determined by performing on-the-fly per-method checks, so this should
//!     also not be a limiting factor although it would probably be a huge
//!     performance win if it was reliably precomputed for those use-cases.
//!
//! Some longer term potential functionality:
//! - Improved argument processing to leverage the semantic tokens.
//! - Some level of dataflow analysis.  Note that this would require the C++
//!   indexer to emit additional information and/or using tree-sitter to help
//!   detect writes/assignment.

use super::{local_index::LocalIndex, server_interface::Result};

use serde_json::Value;

/// Perform the actual lazy cross-reference process.
///
/// Note that while we make an effort to be efficient within this method in
/// terms of loading the contents of source files at most once, this is
/// fundamentally not efficient when multiple calls are made to this method
/// where it's highly likely we could have reused the line-parsed files where it
/// is very likely for there to be file overlap.  (That said, we do expect this
/// to be fine in most cases and this is a trade-off we are intentionally
/// making.)
pub async fn perform_lazy_crossref(_server: &LocalIndex, val: Value) -> Result<Value> {
    // Consume the "uses" array if present so we can transform it.
    /*
    let use_path_containers: Vec<PathSearchResult> = match val.get_mut("uses") {
        Some(uses) => from_value(uses.take()).unwrap_or_default(),
        _ => vec![]
    };

    let mut source_file: HashMap<String, Vec<String>>

    for use_path_container in &use_path_containers {

    }
    */

    // ## Figure out what source files we need to load

    // ### Determine files to load for uses

    // ## Load the source files in

    // ## Process Source files

    // ### Walk the argument excerpts

    Ok(val)
}

```

## tools/src/abstract_server/local_index.rs
```
use async_trait::async_trait;
use flate2::read::GzDecoder;
use futures_core::stream::BoxStream;
use serde_json::{from_str, Value};
use std::collections::BTreeMap;
use std::io::Read;
use std::time::Instant;
use tokio::fs::File;
use tokio::io::AsyncReadExt;
use tracing::trace;
use ustr::{ustr, Ustr};

use super::server_interface::{
    AbstractServer, ErrorDetails, ErrorLayer, FileMatches, HtmlFileRoot, Result,
    SearchfoxIndexRoot, ServerError, TextBounds, TextMatchInFile,
};
use super::{TextMatches, TextMatchesByFile, TreeInfo};

use crate::abstract_server::lazy_crossref::perform_lazy_crossref;
use crate::file_format::analysis::{read_analyses, read_source};
use crate::file_format::config::{load, TreeConfig, TreeConfigPaths};
use crate::file_format::crossref_lookup::CrossrefLookupMap;
use crate::file_format::identifiers::IdentMap;
use crate::file_format::per_file_info::FileLookupMap;
use crate::format::format_code;
use crate::languages::select_formatting;

pub mod livegrep {
    tonic::include_proto!("_");
}

use livegrep::code_search_client::CodeSearchClient;
use livegrep::Query;

/// IO errors amount to a 404 for our purposes which means a sticky problem.
impl From<std::io::Error> for ServerError {
    fn from(err: std::io::Error) -> ServerError {
        ServerError::StickyProblem(ErrorDetails {
            layer: ErrorLayer::ServerLayer,
            message: err.to_string(),
        })
    }
}

impl From<tonic::Status> for ServerError {
    fn from(status: tonic::Status) -> ServerError {
        // There are gRPC codes accessible via code() but for now, especially
        // since we lack the ability to restart the server, it seems safe to
        // assume any problem will not magically fix itself.
        ServerError::StickyProblem(ErrorDetails {
            layer: ErrorLayer::ServerLayer,
            message: status.to_string(),
        })
    }
}

impl From<tonic::transport::Error> for ServerError {
    fn from(err: tonic::transport::Error) -> ServerError {
        // There are gRPC codes accessible via code() but for now, especially
        // since we lack the ability to restart the server, it seems safe to
        // assume any problem will not magically fix itself.
        ServerError::StickyProblem(ErrorDetails {
            layer: ErrorLayer::ServerLayer,
            message: err.to_string(),
        })
    }
}

/// Read newline-delimited JSON that's been gzip-compressed.
async fn read_gzipped_ndjson_from_file(path: &str) -> Result<Vec<Value>> {
    let mut f = File::open(path).await?;
    // We read the entirety to a buffer because
    // https://github.com/serde-rs/json/issues/160 suggests that the buffered
    // reader performance is likely to be much worse.
    //
    // When we want to go async here,
    // https://github.com/rust-lang/flate2-rs/pull/214 suggests that we want to
    // use the `async-compression` crate.
    let mut buffer = Vec::new();
    f.read_to_end(&mut buffer).await?;

    let mut gz = GzDecoder::new(&buffer[..]);

    let mut raw_str = String::new();
    gz.read_to_string(&mut raw_str)?;

    // let mut raw_str = String::new();
    // f.read_to_string(&mut raw_str).await?;

    raw_str
        .lines()
        .map(|s| from_str(s).map_err(ServerError::from))
        .collect()
}

/// Helper to ensure that our path-ish use of &str's does not ever try and do
/// something that can escape a hackily constructed path.  We probably should
/// move to using path types more directly.
fn validate_absoluteish_path(path: &str) -> Result<()> {
    if path.split("/").any(|x| x == "..") {
        Err(ServerError::StickyProblem(ErrorDetails {
            layer: ErrorLayer::BadInput,
            message: "All paths must be absolute-ish".to_string(),
        }))
    } else {
        Ok(())
    }
}

#[allow(dead_code)]
#[derive(Clone, Debug)]
pub struct LocalIndex {
    // We only hold onto the TreeConfigPaths portion of the config because the
    // git data is not `Sync`.  Specifically, the compiler says:
    //
    // "within `TreeConfig`, the trait `Sync` is not implemented for
    // `*mut libgit2_sys::git_repository`"
    //
    // When we need to do local git stuff, we will be able to accomplish this by
    // creating a new `git2::Repository` on demand from the git path.  This is
    // already done in `build-blame.rs` for its compute threads and that's
    // likely the model we should use.
    config_paths: TreeConfigPaths,
    config_repo_path: String,
    tree_name: String,
    // Note: IdentMap internally handles the identifiers db not existing
    ident_map: Option<IdentMap>,
    // But for crossref, it's on us.
    crossref_lookup_map: Option<CrossrefLookupMap>,
    jumpref_lookup_map: Option<CrossrefLookupMap>,
    file_lookup_map: FileLookupMap,
}

impl LocalIndex {
    fn normalize_and_validate_path<'a>(&self, sf_path: &'a str) -> Result<&'a str> {
        // We normalize off any leading "/" mainly to support our test cases
        // being able to use "/" to indicate they're interested in a root dir.
        let norm_path = sf_path.strip_prefix('/').unwrap_or(sf_path);
        // We don't want anyone trying to construct a path that escapes the
        // sub-tree.
        validate_absoluteish_path(norm_path)?;

        Ok(norm_path)
    }
}

#[async_trait]
impl AbstractServer for LocalIndex {
    fn clonify(&self) -> Box<dyn AbstractServer + Send + Sync> {
        Box::new(self.clone())
    }

    fn tree_info(&self) -> Result<TreeInfo> {
        Ok(TreeInfo {
            name: self.tree_name.clone(),
        })
    }

    fn translate_path(&self, root: SearchfoxIndexRoot, sf_path: &str) -> Result<String> {
        match root {
            SearchfoxIndexRoot::CompressedAnalysis => Ok(format!(
                "{}/analysis/{}.gz",
                self.config_paths.index_path, sf_path
            )),
            SearchfoxIndexRoot::ConfigRepo => Ok(format!("{}/{}", self.config_repo_path, sf_path)),
            SearchfoxIndexRoot::IndexTemplates => Ok(format!(
                "{}/templates/{}",
                self.config_paths.index_path, sf_path
            )),
            SearchfoxIndexRoot::IndexPages => Ok(format!(
                "{}/pages/{}",
                self.config_paths.index_path, sf_path
            )),
            SearchfoxIndexRoot::UncompressedDirectoryListing => Ok(format!(
                "{}/dir/{}/index.html",
                self.config_paths.index_path, sf_path
            )),
        }
    }

    async fn fetch_raw_analysis<'a>(&self, sf_path: &str) -> Result<BoxStream<'a, Value>> {
        let norm_path = self.normalize_and_validate_path(sf_path)?;
        let full_path = self.translate_path(SearchfoxIndexRoot::CompressedAnalysis, norm_path)?;
        let values = read_gzipped_ndjson_from_file(&full_path).await?;
        Ok(Box::pin(tokio_stream::iter(values)))
    }

    async fn fetch_raw_source(&self, sf_path: &str) -> Result<String> {
        let norm_path = self.normalize_and_validate_path(sf_path)?;
        let full_path = if norm_path.starts_with("__GENERATED__/") {
            format!(
                "{}/{}",
                self.config_paths.objdir_path,
                norm_path.strip_prefix("__GENERATED__/").unwrap()
            )
        } else {
            format!("{}/{}", self.config_paths.files_path, norm_path)
        };

        let mut f = File::open(full_path).await?;
        let mut raw_str = String::new();
        f.read_to_string(&mut raw_str).await?;
        Ok(raw_str)
    }

    async fn fetch_formatted_lines(&self, sf_path: &str) -> Result<(Vec<String>, String)> {
        let norm_path = self.normalize_and_validate_path(sf_path)?;
        let source = self.fetch_raw_source(sf_path).await?;
        let analysis_path =
            self.translate_path(SearchfoxIndexRoot::CompressedAnalysis, norm_path)?;
        let analysis = read_analyses(&[analysis_path], &mut read_source);

        let jumpref_path = format!("{}/jumpref", self.config_paths.index_path);
        let jumpref_extra_path = format!("{}/jumpref-extra", self.config_paths.index_path);

        let jumpref_lookup_map = CrossrefLookupMap::new(&jumpref_path, &jumpref_extra_path);

        let (raw_lines, sym_json) = format_code(
            None,
            &jumpref_lookup_map,
            select_formatting(sf_path),
            sf_path,
            source.as_str(),
            &analysis,
        );

        let lines = raw_lines.into_iter().map(|line| line.line).collect();

        Ok((lines, sym_json))
    }

    async fn fetch_html(&self, root: HtmlFileRoot, sf_path: &str) -> Result<String> {
        let norm_path = self.normalize_and_validate_path(sf_path)?;
        let (full_path, is_gzipped) = match root {
            HtmlFileRoot::FormattedFile => (
                format!("{}/file/{}.gz", self.config_paths.index_path, norm_path),
                true,
            ),
            HtmlFileRoot::FormattedDir => {
                // Our tree-relative paths should not start with a slash

                // We want a trailing slash for directories, and the input is allowed
                // to do either.  The exception is that for the root directory, ""
                // is the right choice because our "no leading /" rule trumps our
                // "yes trailing /" rule for path manipulation.
                let norm_path = if norm_path.is_empty() {
                    "".to_string()
                } else if norm_path.ends_with('/') {
                    norm_path.to_string()
                } else {
                    format!("{}/", norm_path)
                };
                (
                    format!(
                        "{}/dir/{}index.html.gz",
                        self.config_paths.index_path, norm_path
                    ),
                    true,
                )
            }
            HtmlFileRoot::FormattedTemplate => (
                format!("{}/templates/{}", self.config_paths.index_path, norm_path),
                false,
            ),
        };

        if !is_gzipped {
            let mut f = File::open(full_path).await?;
            let mut raw_str = String::new();
            f.read_to_string(&mut raw_str).await?;
            return Ok(raw_str);
        }

        let mut f = File::open(full_path).await?;
        let mut buffer = Vec::new();
        f.read_to_end(&mut buffer).await?;

        // When we want to go async here,
        // https://github.com/rust-lang/flate2-rs/pull/214 suggests that we want
        // to use the `async-compression` crate.
        let mut gz = GzDecoder::new(&buffer[..]);

        let mut raw_str = String::new();
        gz.read_to_string(&mut raw_str)?;

        Ok(raw_str)
    }

    async fn crossref_lookup(&self, symbol: &str, extra_processing: bool) -> Result<Value> {
        let now = Instant::now();
        let result = match &self.crossref_lookup_map {
            Some(crossref) => crossref.lookup(symbol),
            None => Ok(Value::Null),
        };
        trace!(
            duration_us = now.elapsed().as_micros() as u64,
            "crossref_lookup: {}",
            symbol
        );
        if result.is_ok() && extra_processing {
            perform_lazy_crossref(self, result.unwrap()).await
        } else {
            result
        }
    }

    async fn jumpref_lookup(&self, symbol: &str) -> Result<Value> {
        let now = Instant::now();
        let result = match &self.jumpref_lookup_map {
            Some(jumpref) => jumpref.lookup(symbol),
            None => Ok(Value::Null),
        };
        trace!(
            duration_us = now.elapsed().as_micros() as u64,
            "jumpref_lookup: {}",
            symbol
        );
        result
    }

    async fn search_files(
        &self,
        pathre: &str,
        include_dirs: bool,
        limit: usize,
    ) -> Result<FileMatches> {
        self.file_lookup_map
            .search_files(pathre, include_dirs, limit)
    }

    async fn search_identifiers(
        &self,
        needle: &str,
        exact_match: bool,
        ignore_case: bool,
        match_limit: usize,
    ) -> Result<Vec<(Ustr, Ustr)>> {
        if let Some(ident_map) = &self.ident_map {
            let now = Instant::now();
            let mut results = vec![];
            for ir in ident_map.lookup(needle, exact_match, ignore_case, match_limit) {
                results.push((ir.symbol, ir.id));
            }
            trace!(
                duration_us = now.elapsed().as_micros() as u64,
                result_count = results.len(),
                "search_identifiers: {}",
                needle
            );
            Ok(results)
        } else {
            Ok(vec![])
        }
    }

    async fn search_text(
        &self,
        pattern: &str,
        fold_case: bool,
        path: &str,
        limit: usize,
    ) -> Result<TextMatches> {
        let now = Instant::now();

        let endpoint = format!("http://localhost:{}", self.config_paths.codesearch_port);
        trace!("search_text: connecting to {}", endpoint);

        let mut client = CodeSearchClient::connect(endpoint).await?;

        // Before multiple paths were allowed, an empty path constraint allowed
        // us to skip the match; now if we pass an empty path in a vec, that
        // will fail to match, so we want to pass an empty vec.
        let use_path = if path.is_empty() {
            vec![]
        } else {
            vec![path.into()]
        };

        let query = tonic::Request::new(Query {
            line: pattern.into(),
            file: use_path,
            repo: "".into(),
            tags: "".into(),
            fold_case,
            not_file: vec![],
            not_repo: "".into(),
            not_tags: "".into(),
            // 0 falls back to the default, I believe.
            max_matches: limit as i32,
            filename_only: false,
            // 0 should pick the default of 0.
            context_lines: 0,
        });

        trace!("search_text: connected, issuing query: {}", pattern);
        let response = client.search(query).await?.into_inner();

        trace!(
            duration_us = now.elapsed().as_micros() as u64,
            result_count = response.results.len(),
            "search_text: query completed: {}",
            pattern
        );

        let mut by_file: BTreeMap<String, TextMatchesByFile> = BTreeMap::new();
        for result in response.results {
            let left = result.bounds.as_ref().map_or(0, |b| b.left);
            let right = result.bounds.as_ref().map_or(0, |b| b.right);
            by_file
                .entry(result.path.to_string())
                .or_insert_with(|| {
                    let path = ustr(&result.path);
                    let path_kind = self
                        .file_lookup_map
                        .lookup_file_from_ustr(&path)
                        .map_or_else(|| ustr(""), |fi| fi.path_kind);
                    TextMatchesByFile {
                        file: path,
                        path_kind,
                        matches: vec![],
                    }
                })
                .matches
                .push(TextMatchInFile {
                    line_num: result.line_number as u32,
                    bounds: TextBounds {
                        start: left,
                        end_exclusive: right,
                    },
                    line_str: result.line,
                });
        }

        Ok(TextMatches {
            by_file: by_file.into_values().collect(),
        })
    }

    async fn perform_query(&self, _q: &str) -> Result<Value> {
        // TODO: For this to work, we want to be able to directly invoke the
        // underpinnings of the web server, which entails porting router.py into
        // web-server.rs, an act which may involve building it on some of this
        // infrastructure...
        Err(ServerError::Unsupported)
    }
}

fn fab_server(
    tree_config: TreeConfig,
    tree_name: &str,
    config_repo_path: &str,
) -> Result<Box<dyn AbstractServer + Send + Sync>> {
    let ident_path = format!("{}/identifiers", tree_config.paths.index_path);
    let ident_map = IdentMap::new(&ident_path);

    let crossref_path = format!("{}/crossref", tree_config.paths.index_path);
    let crossref_extra_path = format!("{}/crossref-extra", tree_config.paths.index_path);

    let crossref_lookup_map = CrossrefLookupMap::new(&crossref_path, &crossref_extra_path);

    let jumpref_path = format!("{}/jumpref", tree_config.paths.index_path);
    let jumpref_extra_path = format!("{}/jumpref-extra", tree_config.paths.index_path);

    let jumpref_lookup_map = CrossrefLookupMap::new(&jumpref_path, &jumpref_extra_path);

    let file_lookup_path = format!(
        "{}/concise-per-file-info.json",
        tree_config.paths.index_path
    );

    let file_lookup_map = FileLookupMap::new(&file_lookup_path);

    Ok(Box::new(LocalIndex {
        // We don't need the blame_map and hg_map (yet)
        config_paths: tree_config.paths,
        config_repo_path: config_repo_path.to_string(),
        tree_name: tree_name.to_string(),
        ident_map,
        crossref_lookup_map,
        jumpref_lookup_map,
        file_lookup_map,
    }))
}

pub fn make_local_server(
    config_path: &str,
    tree_name: &str,
) -> Result<Box<dyn AbstractServer + Send + Sync>> {
    let mut config = load(config_path, false, Some(tree_name), None, None);
    let tree_config = match config.trees.remove(tree_name) {
        Some(t) => t,
        None => {
            return Err(ServerError::StickyProblem(ErrorDetails {
                layer: ErrorLayer::BadInput,
                message: format!("bad tree name: {}", &tree_name),
            }))
        }
    };

    fab_server(tree_config, tree_name, &config.config_repo_path)
}

pub fn make_all_local_servers(
    config_path: &str,
) -> Result<BTreeMap<String, Box<dyn AbstractServer + Send + Sync>>> {
    let config = load(config_path, false, None, None, None);
    let mut servers = BTreeMap::new();
    for (tree_name, tree_config) in config.trees {
        let server = fab_server(tree_config, &tree_name, &config.config_repo_path)?;
        servers.insert(tree_name, server);
    }
    Ok(servers)
}

```

## tools/src/abstract_server/mod.rs
```
mod lazy_crossref;
mod local_index;
mod remote_server;
mod server_interface;

pub use local_index::{make_all_local_servers, make_local_server};
pub use remote_server::make_remote_server;
pub use server_interface::{
    AbstractServer, ErrorDetails, ErrorLayer, FileMatch, FileMatches, HtmlFileRoot, Result,
    SearchfoxIndexRoot, ServerError, TextMatches, TextMatchesByFile, TreeInfo,
};

```

## tools/src/links.rs
```
use itertools::Itertools;
use linkify::{LinkFinder, LinkKind};
use regex::Regex;
use std::borrow::Cow;

use crate::file_format::config::Config;
use crate::url_map_handler::get_file_paths_for_url;

/// Turn anything that looks like a link into an actual `<a href>` link using
/// the `linkify` crate and in-between those links use `linkify_bug_number` to
/// convert
pub fn linkify_comment(cfg: Option<&Config>, s: String) -> String {
    let mut finder = LinkFinder::new();
    finder.kinds(&[LinkKind::Url]);

    let mut last = 0;
    let mut result = String::new();
    for link in finder.links(&s) {
        result.push_str(&linkify_bug_numbers(&s[last..link.start()]));
        last = link.end();

        if link.as_str().starts_with("chrome://") || link.as_str().starts_with("resource://") {
            if let Some(items) = get_file_paths_for_url(cfg, link.as_str()) {
                // The corresponding symbol data is not added to SYM_INFO.
                // The context menu is supposed to generate a pseudo data
                // for the file.
                //
                // If we want to make this a real reference, an extra linkify
                // step should be performed before crossref, with generating
                // extra analaysis records while not writing the resulting
                // HTML to a file.
                result.push_str(&format!(
                    "<span data-symbols=\"{}\">{}</span>",
                    items.iter().map(|item| &item.sym).join(","),
                    link.as_str()
                ));
                continue;
            }
        } else if link.as_str().starts_with("moz-src:///") {
            result.push_str(&format!(
                "<span data-symbols=\"FILE_{}\">{}</span>",
                link.as_str().get(11..).unwrap(),
                link.as_str()
            ));
            continue;
        }

        result.push_str(&format!(
            "<a href=\"{}\">{}</a>",
            link.as_str(),
            linkify_bug_numbers(link.as_str())
        ));
    }

    if last == 0 {
        return linkify_bug_numbers(&s).into_owned();
    }

    result.push_str(&linkify_bug_numbers(&s[last..]));
    result
}

fn linkify_bug_numbers(s: &str) -> Cow<str> {
    lazy_static! {
        static ref BUG_NUMBER_REGEX: Regex =
            Regex::new(r"\b(?i)bug\s*(?P<bugno>[1-9][0-9]{2,6})\b").unwrap();
    }
    BUG_NUMBER_REGEX.replace_all(
        s,
        "<a href=\"https://bugzilla.mozilla.org/show_bug.cgi?id=$bugno\">$0</a>",
    )
}

pub fn linkify_commit_header(s: &str) -> String {
    lazy_static! {
        static ref BUG_NUMBER_REGEX: Regex = Regex::new(r"\b(?P<bugno>[1-9][0-9]{4,9})\b").unwrap();
        static ref SERVO_PR_REGEX: Regex = Regex::new(r"#(?P<prno>[1-9][0-9]*)\b").unwrap();
        static ref WPT_SYNC_REGEX: Regex = Regex::new(r"\[wpt PR (?P<prno>[1-9][0-9]*)\]").unwrap();
    }
    if s.starts_with("servo: ") {
        SERVO_PR_REGEX
            .replace_all(
                s,
                "#<a href=\"https://github.com/servo/servo/pull/$prno\">$prno</a>",
            )
            .into_owned()
    } else if s.contains("[wpt PR") {
        // Linkify the "Bug XYZ" bit first. It's slightly unfortunate that this
        // would linkify the whole "Bug <number>" word, but for other commits
        // it'd only linkify the number. It doesn't seem like a big deal though.
        let s = linkify_bug_numbers(s);
        WPT_SYNC_REGEX
            .replace_all(
                &s,
                r#"[<a href="https://github.com/web-platform-tests/wpt/pull/$prno">wpt PR $prno</a>]"#,
            )
            .into_owned()
    } else {
        BUG_NUMBER_REGEX
            .replace_all(
                s,
                "<a href=\"https://bugzilla.mozilla.org/show_bug.cgi?id=$bugno\">$bugno</a>",
            )
            .into_owned()
    }
}

#[test]
fn test_linkify_wpt_sync() {
    let linkified =
        linkify_commit_header("Bug 1643934 [wpt PR 24024] - Align prefers-color-scheme:no-preference tests with spec., a=testonly");
    assert_eq!(
        linkified,
        r#"<a href="https://bugzilla.mozilla.org/show_bug.cgi?id=1643934">Bug 1643934</a> [<a href="https://github.com/web-platform-tests/wpt/pull/24024">wpt PR 24024</a>] - Align prefers-color-scheme:no-preference tests with spec., a=testonly"#,
    );
}

#[test]
fn test_linkify_servo_pr() {
    let linkified =
        linkify_commit_header("servo: Merge #1234 - stylo: Report a specific error for invalid CSS color values (from jdm:valueerr); r=heycam");
    assert!(linkified.contains("github.com"), "{:?}", linkified);
}

#[test]
fn test_bug_number() {
    let linkified = linkify_bug_numbers("this is a link to bUg 12345");
    assert!(
        linkified.contains("bugzilla.mozilla.org"),
        "{:?}",
        linkified
    );
    assert!(linkified.contains(">bUg 12345</a>"), "{:?}", linkified);
}

#[test]
fn test_bug_number_inside_link() {
    let link = "http://example.org/browser/editor/libeditor/tests/bug629172.html";
    let linkified = linkify_comment(None, link.into());
    assert_eq!(linkified, "<a href=\"http://example.org/browser/editor/libeditor/tests/bug629172.html\">http://example.org/browser/editor/libeditor/tests/<a href=\"https://bugzilla.mozilla.org/show_bug.cgi?id=629172\">bug629172</a>.html</a>");
}

```

## tools/src/describe.rs
```
use std::path::Path;

use crate::languages::FormatAs;

use regex::Regex;

/// Tries to find some text that describes the contents of the file.
/// Adapted from DXR code at https://github.com/mozilla/dxr/blob/master/dxr/plugins/descriptor/__init__.py
pub fn describe_file(contents: &str, path: &Path, format: &FormatAs) -> Option<String> {
    // Only look in the first 5k chars of the file, since running regex matches on giant files
    // can be expensive and most likely the thing we're looking for will be early in the file.
    let substr_end = contents
        .char_indices()
        .nth(5000)
        .map(|ix| ix.0)
        .unwrap_or(contents.len());
    let substr = &contents[0..substr_end];

    // DXR also does a search for "filename: <description>" which I've never seen in any file so
    // I'm omitting that here. We can add it if needed.
    match format {
        FormatAs::CSS => describe_from_c_comment(substr),
        FormatAs::FormatTagLike(_) => describe_html(substr),
        FormatAs::FormatCLike(spec) => {
            if spec.rust_tweaks {
                describe_from_rust_comment(substr).or_else(|| describe_from_c_comment(substr))
            } else if spec.c_style_comments {
                describe_from_c_comment(substr)
            } else if spec.triple_quote_literals {
                describe_py(substr)
            } else {
                None
            }
        }
        FormatAs::Binary => None,
        FormatAs::StaticPrefs => None,
        FormatAs::Plain => {
            let stem = path.file_stem()?.to_str()?;
            if stem.eq_ignore_ascii_case("README") {
                describe_readme(substr)
            } else {
                None
            }
        }
    }
}

/// Returns the content of the title tag
fn describe_html(contents: &str) -> Option<String> {
    lazy_static! {
        static ref TITLE_REGEX: Regex = Regex::new(r#"(?i)<title>((?s).*?)</title>"#).unwrap();
    }
    TITLE_REGEX
        .captures(contents)
        .and_then(|c| c.get(1))
        .map(|m| m.as_str().to_string())
}

/// Returns the first docstring (single-quoted preferred)
fn describe_py(contents: &str) -> Option<String> {
    lazy_static! {
        static ref DOCSTRING_SINGLE_REGEX: Regex = Regex::new(r#"'''\s*((?s).*?)'''"#).unwrap();
        static ref DOCSTRING_DOUBLE_REGEX: Regex = Regex::new(r#""""\s*((?s).*?)""""#).unwrap();
    }
    DOCSTRING_SINGLE_REGEX
        .captures(contents)
        .or_else(|| DOCSTRING_DOUBLE_REGEX.captures(contents))
        .and_then(|c| c.get(1))
        .map(|m| m.as_str().to_string())
}

/// Returns the first nonempty line
fn describe_readme(contents: &str) -> Option<String> {
    contents
        .lines()
        .find(|s| !s.trim().is_empty())
        .map(str::to_string)
}

/// Returns the first Rust-style module doc-comment (`//!`).
fn describe_from_rust_comment(contents: &str) -> Option<String> {
    let mut description = String::new();
    let mut in_comment = false;
    for line in contents.lines() {
        if !line.starts_with("//!") {
            if !in_comment {
                continue;
            }
            return Some(description);
        }
        in_comment = true;
        if !description.is_empty() {
            description.push('\n');
        }
        description.push_str(line.trim_start_matches("//!"));
    }
    None
}

/// Returns the first C-style comment that's not vim/modeline/license boilerplate
fn describe_from_c_comment(contents: &str) -> Option<String> {
    lazy_static! {
        // Matches C-style comments, including any leading '*' characters on
        // wrapped lines. The LEADING_STARS regex is used to remove those.
        static ref COMMENT_REGEX: Regex = {
            Regex::new(r#"(?:/\*[*\s]*)((?s).*?)\*/"#).unwrap()
        };
        static ref LEADING_STARS: Regex = {
            Regex::new(r#"(?m)^\s*\*"#).unwrap()
        };
    }
    for captures in COMMENT_REGEX.captures_iter(contents) {
        if let Some(comment_match) = captures.get(1) {
            let comment_text = comment_match.as_str();
            if comment_text.contains("tab-width") ||
                comment_text.contains("vim:") ||
                // Checking common case-variants is probably cheaper than lowercasing
                // comment_text (which allocates a new string) and doing a lowercase search
                comment_text.contains("license") ||
                comment_text.contains("LICENSE") ||
                comment_text.contains("License")
            {
                continue;
            }
            return Some(LEADING_STARS.replace_all(comment_text, "").into_owned());
        }
    }
    None
}

```

## tools/src/file_format/analysis.rs
```
use std::collections::BTreeMap;
use std::collections::BTreeSet;
use std::collections::HashMap;
use std::collections::HashSet;
use std::fmt::Debug;
#[cfg(not(target_arch = "wasm32"))]
use std::fs::File;
use std::hash::Hash;
#[cfg(not(target_arch = "wasm32"))]
use std::io::BufRead;
#[cfg(not(target_arch = "wasm32"))]
use std::io::BufReader;
#[cfg(not(target_arch = "wasm32"))]
use std::io::Read;
use std::marker::PhantomData;
use std::ops::Deref;

use itertools::Itertools;

#[cfg(not(target_arch = "wasm32"))]
use flate2::read::GzDecoder;
use serde::{Deserialize, Deserializer, Serialize, Serializer};
#[cfg(not(target_arch = "wasm32"))]
use serde_json::from_str;
use serde_json::{from_value, Map, Value};
use serde_repr::*;

#[cfg(not(target_arch = "wasm32"))]
use ustr::{ustr, Ustr, UstrMap};
#[cfg(target_arch = "wasm32")]
type Ustr = String;

use super::ontology_pointer_kind::OntologyPointerKind;

#[derive(Copy, Clone, Default, Eq, PartialEq, PartialOrd, Ord, Debug)]
pub struct Location {
    /// 1-base lined-number.
    pub lineno: u32,
    /// 0-based start column, inclusive.
    pub col_start: u32,
    /// 0-based end column, inclusive.
    pub col_end: u32,
}

impl Location {
    pub fn is_file_target(&self) -> bool {
        self.lineno == 1 && self.col_start == 0 && self.col_end == 0
    }
}

#[derive(Clone, Default, Eq, PartialEq, PartialOrd, Ord, Debug)]
pub struct LineRange {
    /// 1-based starting line-number
    pub start_lineno: u32,
    /// 1-based ending line number.
    pub end_lineno: u32,
}

impl LineRange {
    pub fn is_empty(&self) -> bool {
        (self.start_lineno == 0 && self.end_lineno == 0)
            || (self.start_lineno == u32::MAX && self.end_lineno == u32::MAX)
    }
}

#[derive(Clone, Default, Eq, PartialEq, PartialOrd, Ord, Debug)]
pub struct SourceRange {
    /// 1-based starting line number, inclusive.
    pub start_lineno: u32,
    /// 0-based starting column number, inclusive.
    pub start_col: u32,
    /// 1-based ending line number, inclusive.
    pub end_lineno: u32,
    /// 0-based ending column number, inclusive.
    pub end_col: u32,
}

impl SourceRange {
    pub fn is_empty(&self) -> bool {
        // we allow both 0 and u32::MAX as sentinel values.
        self.start_lineno == 0 || self.start_lineno == u32::MAX
    }
}

impl SourceRange {
    /// Union the other SourceRange into this SourceRange.
    pub fn union(&mut self, other: SourceRange) {
        // A start_lineno of 0 represents an empty/omitted range.  The range is best effort and
        // so one range might be empty and the other not.
        if other.start_lineno == 0 {
            // Nothing to do if the other range is empty.
            return;
        }
        if self.start_lineno == 0 {
            // Clobber this range with the other range if we were empty.
            self.start_lineno = other.start_lineno;
            self.start_col = other.start_col;
            self.end_lineno = other.end_lineno;
            self.end_col = other.end_col;
            return;
        }

        if other.start_lineno < self.start_lineno {
            self.start_lineno = other.start_lineno;
            self.start_col = other.start_col;
        } else if other.start_lineno == self.start_lineno && other.start_col < self.start_col {
            self.start_col = other.start_col;
        }

        if other.end_lineno > self.end_lineno {
            self.end_lineno = other.end_lineno;
            self.end_col = other.end_col;
        } else if other.end_lineno == self.end_lineno && other.end_col > self.end_col {
            self.end_col = other.end_col;
        }
    }
}

#[derive(Debug, Serialize, Deserialize)]
pub struct WithLocation<T> {
    pub loc: Location,
    #[serde(flatten)]
    pub data: T,
}

#[derive(Clone, Copy, Debug, Eq, PartialEq, Ord, PartialOrd, Serialize, Deserialize)]
#[serde(rename_all = "lowercase")]
pub enum AnalysisKind {
    Use,
    Def,
    Assign,
    Decl,
    Forward,
    Idl,
    Alias,
}

#[cfg(not(target_arch = "wasm32"))]
impl AnalysisKind {
    pub fn to_ustr(&self) -> Ustr {
        // We could obviously precompute/LAZY_STATIC these
        match self {
            AnalysisKind::Use => ustr("use"),
            AnalysisKind::Def => ustr("def"),
            AnalysisKind::Assign => ustr("assign"),
            AnalysisKind::Decl => ustr("decl"),
            AnalysisKind::Forward => ustr("forward"),
            AnalysisKind::Idl => ustr("idl"),
            AnalysisKind::Alias => ustr("alias"),
        }
    }
}

/// This is intended to help model the self-describing nature of analysis
/// records where we have `"target": 1` at the start of the field.  A normal
/// single-value enum should take up no space... hopefully that's the case for
/// this too despite the involvement of `serde_repr` to encode the value as an
/// int.
#[derive(Debug, PartialEq, Serialize_repr, Deserialize_repr)]
#[repr(u8)]
pub enum TargetTag {
    Target = 1,
}

/// We use this trait instead of From<&str> to avoid explicit lifetime.
pub trait FromStr {
    fn from(s: &str) -> Self;
}

#[cfg(not(target_arch = "wasm32"))]
impl FromStr for Ustr {
    fn from(s: &str) -> Self {
        ustr(s)
    }
}

impl FromStr for String {
    fn from(s: &str) -> Self {
        s.to_string()
    }
}

fn string_or_ustr_is_empty<StrT>(s: &StrT) -> bool
where
    StrT: Deref<Target = str>,
{
    s.is_empty()
}

#[derive(Debug, Serialize, Deserialize)]
pub struct AnalysisTarget<StrT = Ustr>
where
    StrT: Clone + Debug + Default + Deref<Target = str> + FromStr + Hash + Ord + PartialEq,
{
    pub target: TargetTag,
    pub kind: AnalysisKind,
    #[serde(default)]
    pub pretty: StrT,
    #[serde(default)]
    pub sym: StrT,
    #[serde(default, skip_serializing_if = "string_or_ustr_is_empty")]
    pub context: StrT,
    #[serde(default, skip_serializing_if = "string_or_ustr_is_empty")]
    pub contextsym: StrT,
    #[serde(
        rename = "peekRange",
        default,
        skip_serializing_if = "LineRange::is_empty"
    )]
    pub peek_range: LineRange,
    #[serde(rename = "argRanges", default, skip_serializing_if = "Vec::is_empty")]
    pub arg_ranges: Vec<SourceRange>,
}

/// See TargetTag for more info
#[derive(Debug, Eq, PartialEq, Serialize_repr, Deserialize_repr)]
#[repr(u8)]
pub enum StructuredTag {
    Structured = 1,
}

#[derive(Debug, Serialize, Deserialize)]
pub struct StructuredSuperInfo<StrT = Ustr>
where
    StrT: Clone + Debug + Default + Deref<Target = str> + FromStr + Hash + Ord + PartialEq,
{
    #[serde(default)]
    pub sym: StrT,
    #[serde(rename = "offsetBytes", default)]
    pub offset_bytes: u32,
    #[serde(default)]
    pub props: Vec<StrT>,
}

#[derive(Debug, Serialize, Deserialize)]
pub struct StructuredArgInfo<StrT = Ustr>
where
    StrT: Clone + Debug + Default + Deref<Target = str> + FromStr + Hash + Ord + PartialEq,
{
    pub name: StrT,
    #[serde(rename = "type", default)]
    pub type_pretty: StrT,
    #[serde(rename = "typesym", default)]
    pub type_sym: StrT,
}

#[derive(Debug, Serialize, Deserialize)]
pub struct StructuredMethodInfo<StrT = Ustr>
where
    StrT: Clone + Debug + Default + Deref<Target = str> + FromStr + Hash + Ord + PartialEq,
{
    #[serde(default)]
    pub pretty: StrT,
    #[serde(default)]
    pub sym: StrT,
    #[serde(default)]
    pub props: Vec<StrT>,
    #[serde(default, skip_serializing_if = "BTreeSet::is_empty")]
    pub labels: BTreeSet<StrT>,
    #[serde(default)]
    pub args: Vec<StructuredArgInfo<StrT>>,
}

#[derive(Clone, Debug, Eq, Hash, PartialEq, Serialize, Deserialize)]
pub struct StructuredBitPositionInfo {
    pub begin: u32,
    pub width: u32,
}

#[derive(Debug, Serialize, Deserialize)]
pub struct StructuredOverrideInfo<StrT = Ustr> {
    #[serde(default)]
    pub sym: StrT,
}

#[derive(Clone, Debug, Serialize, Deserialize)]
pub struct StructuredFieldInfo<StrT = Ustr>
where
    StrT: Clone + Debug + Default + Deref<Target = str> + FromStr + Hash + Ord + PartialEq,
{
    /// The field definition's location in "PATH#line-line" or "PATH#line" format.
    ///
    /// If this field is defined in single line, "PATH#line" format is used,
    /// otherwise "PATH#line-line" format is used with first line and last line.
    ///
    /// If this field is defined in the same file as struct itself,
    /// PATH part is omitted.
    /// Otherwise PATH is the full path inside the repository.
    ///
    /// TODO: Use relative path from struct's file to reduce the size.
    #[serde(rename = "lineRange", default)]
    pub line_range: StrT,
    #[serde(default)]
    pub pretty: StrT,
    #[serde(default)]
    pub sym: StrT,
    #[serde(rename = "type", default)]
    pub type_pretty: StrT,
    #[serde(rename = "typesym", default)]
    pub type_sym: StrT,
    // XXX this should be made Optional like size_bytes.
    #[serde(rename = "offsetBytes", default)]
    pub offset_bytes: u32,
    #[serde(rename = "bitPositions")]
    pub bit_positions: Option<StructuredBitPositionInfo>,
    #[serde(rename = "sizeBytes")]
    pub size_bytes: Option<u32>,
    #[serde(default, skip_serializing_if = "BTreeSet::is_empty")]
    pub labels: BTreeSet<StrT>,
    #[serde(default, rename = "pointerInfo", skip_serializing_if = "Vec::is_empty")]
    pub pointer_info: Vec<StructuredPointerInfo<StrT>>,
}

#[derive(Clone, Debug, Serialize, Deserialize)]
pub struct StructuredPointerInfo<StrT = Ustr>
where
    StrT: Clone + Debug + Default + Deref<Target = str> + FromStr + Hash + Ord + PartialEq,
{
    pub kind: OntologyPointerKind,
    pub sym: StrT,
}

#[derive(Clone, Copy, Debug, Eq, PartialEq, Ord, PartialOrd, Serialize, Deserialize)]
#[serde(rename_all = "snake_case")]
pub enum BindingSlotKind {
    /// A class that directly implements or will be subclassed.
    Class,
    /// For situations like XPConnect interfaces reflected into JS and
    /// WebIDL where we are describing the symbol that exposes the IDL
    /// interface into the language, but where that symbol is not directly part
    /// of a class hierarchy.  I'm really not sure about the WebIDL case here,
    /// and it probably will want to depend on how we end up implementing the
    /// rest of the UX around here.  For now we will treat this like `Class`
    /// above for most purposes, but this may enable semantic linking to try
    /// and do XPConnect magic.
    InterfaceName,
    /// Callable.
    Method,
    /// A field/attribute/property that has JS XPIDL or WebIDL semantics where we only
    /// have a single symbol name but it could correspond to a property or any
    /// combination of a getter/setter.
    Attribute,
    /// An enum/const which is expected to be a value somehow.
    Const,
    /// An attribute for which we have a distinct symbol for a getter.
    Getter,
    /// An attribute for which we have a distinct symbol for a setter.
    Setter,
    /// An RPC/IPC send method which will have a corresponding Recv counterpart.
    Send,
    /// An RPC/IPC receive method which will have a corresponding Send
    /// counterpart.
    Recv,
    /// Future: Pref symbol specified in a WebIDL `Pref="foo"` annotation.
    ///
    EnablingPref,
    /// Future: Symbol specified in a WebIDL `Func=Class::Method` annotation.
    EnablingFunc,
}

#[derive(Clone, Copy, Debug, Eq, PartialEq, Ord, PartialOrd, Serialize, Deserialize)]
#[serde(rename_all = "lowercase")]
pub enum BindingSlotLang {
    Cpp,
    JS,
    Rust,
    Jvm,
}

#[derive(Clone, Copy, Debug, Eq, PartialEq, Ord, PartialOrd, Serialize, Deserialize)]
#[serde(rename_all = "lowercase")]
pub enum BindingOwnerLang {
    Idl,
    Prefs,
    Cpp,
    JS,
    Rust,
    Jvm,
}

#[derive(Clone, Copy, Debug, Eq, PartialEq, Ord, PartialOrd, Serialize, Deserialize)]
#[serde(rename_all = "lowercase")]
pub enum BindingImplKind {
    // The auto-generated binding.
    Binding,
    // The actual implementation called by the binding.
    Impl,
}

/// The binding slot mechanism is used to describe the exclusive relationship
/// between IDL symbols and their bindings as well as the non-exclusive
/// support relationships like enabling functions.
///
/// This type is used in 2 directions:
/// 1. From the IDL symbols via the "structured" `binding_slots` field.  In this
///    case the origin symbol will have an `impl_kind` of "idl" and the binding
///    slot target symbols will have non-idl values.
/// 2. On a exclusive symbol referenced via `binding_slots`, this type is also
///    used for the optional back-edge to the defining idl symbol.  This will
///    not be used for support slots like enabling functions where the tentative
///    plan is just to let the IDL file emit "uses" of the enabling func for
///    cross-reference purposes.  In this case the structure is indicating the
///    values which describe the relationship from the IDL symbol to the current
///    symbol.
#[derive(Clone, Copy, Debug, Serialize, Deserialize)]
pub struct BindingSlotProps {
    #[serde(rename = "slotKind")]
    pub slot_kind: BindingSlotKind,
    #[serde(rename = "slotLang")]
    pub slot_lang: BindingSlotLang,
    #[serde(rename = "implKind", default)]
    pub impl_kind: Option<BindingImplKind>,
    #[serde(rename = "ownerLang")]
    pub owner_lang: BindingOwnerLang,
}
#[derive(Debug, Serialize, Deserialize)]
pub struct StructuredBindingSlotInfo<StrT = Ustr>
where
    StrT: Clone + Debug + Default + Deref<Target = str> + FromStr + Hash + Ord + PartialEq,
{
    #[serde(flatten)]
    pub props: BindingSlotProps,
    #[serde(default)]
    pub sym: StrT,
}

#[derive(Clone, Copy, Debug, Eq, PartialEq, Ord, PartialOrd, Serialize, Deserialize)]
#[serde(rename_all = "snake_case")]
pub enum OntologySlotKind {
    /// For methods like nsIRunnable::Run, any overrides will have this slot
    /// which points at the constructors.  In the future this might be replaced
    /// or accompanied by a `RunnableDispatch` kind.
    ///
    /// Constructors will have the reciprocal `RunnableMethod` slot.
    ///
    /// The `syms` payload will be the list of symbols for the constructors
    /// for the immediate class.  We intentionally do not look up the superclass
    /// chain here, but that would likely be a side effect if the Run method
    /// calls its superclass run method.
    RunnableConstructor,
    /// For constructors of nsIRunnable/similar subclasses, this slot points at
    /// the run methods which will reference this constructor and its siblings
    /// via `RunnableConstructor`.
    RunnableMethod,
}

/// Evolving mechanism that allows trees to define high-level semantics that
/// allow eliding low-level implementation details in favor of expressing the
/// emergent control or data flow as humans understand it.  In particular, we
/// want simple annotations to provide a more useful understanding of the code
/// that raw static analysis would not be able to infer, at least on the level
/// we can currently implement it.
///
/// For example, nsIRunnable is a case where we know that overrides of the Run
/// method result from the construction of a runnable followed by its dispatch.
/// For now, we will just treat the creation of the runnable class as an implied
/// call to its Run method, but in the future with some static analysis combined
/// with limited symbolic execution, we could also track the code that hands
/// the runnable off to a more generic dispatch system.  (In general a core goal
/// is not to get tripped up by infrastructure code that touches everything.)
///
/// ### Ongoing Design Discussion
///
/// #### Locations / Existings Target Records
///
/// An question is how this mechanism should relate to target records which have
/// location information.  Currently these slots don't have any location info,
/// but effectively serve to repurpose existing records' symbols and contextsym.
/// Arguably the edges we are introducing exclusively for graphing purposes
/// should impact hit results in "search" style results.  For our current
/// "runnable" use case, this is something we can reasonably map to how we
/// handle subclasses/superclasses/overrides since we can straightforwardly map
/// to the entire kind slot of the related symbols.
///
/// But for something like handling preferences or observer notifications where
/// we are partitioning uses based on an argument, this would not be sufficient.
/// We would need a way to filter those hits either through labeling we do ahead
/// of time or that we can recompute on the fly from the `OntologySlotInfo` if
/// we use this model.  An alternate approach for those cases would be to
/// introduce synthetic symbols, which had been the hand-waving tentative plan
/// but which did not address the logistical glue layer and the relationship
/// between the low-level symbols versus the high-level symbols and hit results.
///
/// There is a spectrum in this space in terms of what low level symbols can be
/// usefully faceted versus situations where the results would be so voluminous
/// that normal faceting would likely be overwhelmed and there is an argument
/// for a different UI paradigm and pre-computation.  For example, observer
/// notifications have a sufficiently limited domain that faceting is
/// appropriate, but for preferences the domain is so huge and the usage so
/// extensive that a normal faceting UI would be of dubious utility because the
/// user should probably just keep typing if they are interested in a specific
/// preference.
#[derive(Debug, Serialize, Deserialize)]
pub struct OntologySlotInfo<StrT = Ustr>
where
    StrT: Clone + Debug + Default + Deref<Target = str> + FromStr + Hash + Ord + PartialEq,
{
    #[serde(rename = "slotKind")]
    pub slot_kind: OntologySlotKind,
    /// The symbols
    pub syms: Vec<StrT>,
}

/// The structured record type extracts out the necessary information to uniquely identify the
/// symbol and what is required for cross-referencing's establishment of hierarchy/links.  The rest
/// of the data in the JSON payload of the record (minus these fields) is re-encoded as a
/// JSON-formatted string.  It's fine to promote things out of the payload into the struct as
/// needed.
///
/// Structured records are merged by choosing one platform rep to be the canonical variant and
/// embedding the other variants observed under a `variants` attribute.  See `analysis.md` and
/// `merge-analyses.rs` for more details.
#[derive(Debug, Serialize, Deserialize)]
pub struct AnalysisStructured<StrT = Ustr>
where
    StrT: Clone + Debug + Default + Deref<Target = str> + FromStr + Hash + Ord + PartialEq,
{
    pub structured: StructuredTag,
    #[serde(default)]
    pub pretty: StrT,
    #[serde(default)]
    pub sym: StrT,
    // XXX Adding this right now for scip-indexer because we're using the analysis
    // rep as the canonical info to provide to the source record, and right now this
    // only exists on source records and fields.
    // TODO: have crossref.rs promote info into this from the source record as
    // appropriate; especially because at least initially in C++ we'll only have
    // this data from the source record.
    // TODO: consider whether we should have type_sym here too.
    pub type_pretty: Option<StrT>,
    #[serde(default)]
    pub kind: StrT,
    // Comes from the ConcisePerFileInfo where the structured record was found.
    #[serde(default)]
    pub subsystem: Option<StrT>,

    #[serde(rename = "parentsym", skip_serializing_if = "Option::is_none")]
    pub parent_sym: Option<StrT>,
    #[serde(rename = "slotOwner", skip_serializing_if = "Option::is_none")]
    pub slot_owner: Option<StructuredBindingSlotInfo<StrT>>,

    #[serde(rename = "implKind", default)]
    pub impl_kind: StrT,

    #[serde(rename = "sizeBytes")]
    pub size_bytes: Option<u32>,
    #[serde(rename = "alignmentBytes")]
    pub alignment_bytes: Option<u32>,
    #[serde(rename = "ownVFPtrBytes")]
    pub own_vf_ptr_bytes: Option<u32>,

    #[serde(rename = "bindingSlots", default)]
    pub binding_slots: Vec<StructuredBindingSlotInfo<StrT>>,
    #[serde(rename = "ontologySlots", default)]
    pub ontology_slots: Vec<OntologySlotInfo<StrT>>,
    #[serde(default)]
    pub supers: Vec<StructuredSuperInfo<StrT>>,
    #[serde(default)]
    pub methods: Vec<StructuredMethodInfo<StrT>>,
    // TODO: This really needs to be the union of all fields across all variants
    // to begin with; right now for the layout table we do manual stuff, but
    // this really is not sufficient.
    #[serde(default)]
    pub fields: Vec<StructuredFieldInfo<StrT>>,
    #[serde(default)]
    pub overrides: Vec<StructuredOverrideInfo<StrT>>,
    #[serde(default)]
    pub props: Vec<StrT>,
    #[serde(default, skip_serializing_if = "BTreeSet::is_empty")]
    pub labels: BTreeSet<StrT>,

    // ### Derived by cross-referencing
    #[serde(rename = "idlsym", skip_serializing_if = "Option::is_none")]
    pub idl_sym: Option<StrT>,
    // Note: Originally these (subclasses, overriddenBy) were meant to hold
    // { pretty, sym } for symmetry, but now the code and docs do reflect these
    // as being symbol only.
    #[serde(rename = "subclasses", default, skip_serializing_if = "Vec::is_empty")]
    pub subclass_syms: Vec<StrT>,
    #[serde(
        rename = "overriddenBy",
        default,
        skip_serializing_if = "Vec::is_empty"
    )]
    pub overridden_by_syms: Vec<StrT>,

    #[serde(default)]
    pub variants: Vec<AnalysisStructured<StrT>>,

    #[serde(flatten)]
    pub extra: Map<String, Value>,
}

impl<StrT> AnalysisStructured<StrT>
where
    StrT: Clone
        + Debug
        + serde::de::DeserializeOwned
        + Default
        + Deref<Target = str>
        + FromStr
        + Hash
        + Ord
        + PartialEq,
{
    // Retrieve the platforms from "extra" if present; this could arguably just
    // be serialized in the first place.
    pub fn platforms(&self) -> Vec<String> {
        match self.extra.get("platforms") {
            Some(val) => from_value(val.clone()).unwrap_or_default(),
            _ => vec![],
        }
    }

    pub fn per_platform(&self) -> Vec<(Option<String>, &Self)> {
        // XXX at least for things that are subclassed it seems like we can end up with multiple
        // structured representations right now, so we need to keep track of platforms we've seen
        // so we can avoid adding them a subsequent time.
        let mut seen = HashSet::new();

        let mut results = vec![];

        let main_platforms = self.platforms();
        if main_platforms.is_empty() {
            results.push((None, self));
        } else {
            for p in main_platforms {
                seen.insert(p.to_owned());
                results.push((Some(p.clone()), self));
            }
        }
        for v in &self.variants {
            for p in &v.platforms() {
                // Try and insert the platforms into the seen set; insert returns true
                // if the element is newly inserted.
                if !seen.insert(p.to_owned()) {
                    continue;
                }

                results.push((Some(p.clone()), v));
            }
        }
        results
    }
}

mod bool_as_int {
    use serde::{Deserialize, Deserializer, Serializer};

    pub fn serialize<S>(b: &bool, serializer: S) -> Result<S::Ok, S::Error>
    where
        S: Serializer,
    {
        serializer.serialize_i8(if *b { 1 } else { 0 })
    }

    pub fn deserialize<'de, D>(deserializer: D) -> Result<bool, D::Error>
    where
        D: Deserializer<'de>,
    {
        let i = i8::deserialize(deserializer)?;
        Ok(i != 0)
    }
}

struct SerializeVecString<StrT>
where
    StrT: Clone + Debug + Default + Deref<Target = str> + FromStr + Hash + Ord + PartialEq,
{
    phantom: PhantomData<StrT>,
}

impl<StrT> SerializeVecString<StrT>
where
    StrT: Clone + Debug + Default + Deref<Target = str> + FromStr + Hash + Ord + PartialEq,
{
    pub fn serialize<S>(arr: &[StrT], serializer: S) -> Result<S::Ok, S::Error>
    where
        S: Serializer,
    {
        let s = SerializeVecString::<StrT>::join(arr);
        serializer.serialize_str(&s)
    }

    pub fn deserialize<'de, D>(deserializer: D) -> Result<Vec<StrT>, D::Error>
    where
        D: Deserializer<'de>,
    {
        let s = String::deserialize(deserializer)?;
        Ok(s.split(',').map(|s| FromStr::from(s)).collect())
    }

    pub fn join(arr: &[StrT]) -> String {
        arr.iter()
            .map(|x| x.as_ref())
            .collect::<Vec<&str>>()
            .join(",")
    }
}

/// See TargetTag for more info
#[derive(Serialize_repr, Deserialize_repr, PartialEq, Debug)]
#[repr(u8)]
pub enum SourceTag {
    Source = 1,
}

fn bool_is_false(b: &bool) -> bool {
    !b
}

/// Maps tracking expansion information.
/// Both maps are keyd by `{symbol}(,{dependencies})*` then by platform.
#[derive(Debug, Serialize, Deserialize)]
#[serde(rename_all = "camelCase")]
pub enum ExpansionInfo {
    ExpandsTo(BTreeMap<String, BTreeMap<String, String>>),
    InExpansionAt(BTreeMap<String, BTreeMap<String, Vec<usize>>>),
}

/// Confidence Level
#[derive(Debug, Clone, Copy, Serialize, Deserialize, PartialEq, Eq, PartialOrd, Ord)]
#[serde(rename_all = "camelCase")]
pub enum ConfidenceLevel {
    CppTemplateHeuristic,
    Concrete,
}

enum ConfidenceIterator<'a> {
    Set(std::iter::Copied<std::slice::Iter<'a, ConfidenceLevel>>),
    Default(std::iter::RepeatN<ConfidenceLevel>),
}

impl Iterator for ConfidenceIterator<'_> {
    type Item = ConfidenceLevel;
    fn next(&mut self) -> Option<Self::Item> {
        match self {
            Self::Set(it) => it.next(),
            Self::Default(it) => it.next(),
        }
    }
}

#[derive(Debug, Serialize, Deserialize)]
pub struct AnalysisSource<StrT = Ustr>
where
    StrT: Clone + Debug + Default + Deref<Target = str> + FromStr + Hash + Ord + PartialEq,
{
    pub source: SourceTag,
    #[serde(
        serialize_with = "SerializeVecString::<StrT>::serialize",
        deserialize_with = "SerializeVecString::<StrT>::deserialize"
    )]
    pub syntax: Vec<StrT>,
    pub pretty: StrT,
    #[serde(
        serialize_with = "SerializeVecString::<StrT>::serialize",
        deserialize_with = "SerializeVecString::<StrT>::deserialize"
    )]
    pub sym: Vec<StrT>,
    #[serde(default, with = "bool_as_int", skip_serializing_if = "bool_is_false")]
    pub no_crossref: bool,
    #[serde(
        rename = "nestingRange",
        default,
        skip_serializing_if = "SourceRange::is_empty"
    )]
    pub nesting_range: SourceRange,
    /// For records that have an associated type (and aren't a type), this is the human-readable
    /// representation of the type that may have all kinds of qualifiers that searchfox otherwise
    /// ignores.  Not all records will have this type.
    #[serde(rename = "type", skip_serializing_if = "Option::is_none")]
    pub type_pretty: Option<StrT>,
    /// For records that have an associated type, we may be able to map the type to a searchfox
    /// symbol, and if so, this is that.  Even if the record has a `type_pretty`, it may not have a
    /// type_sym.
    #[serde(rename = "typesym", skip_serializing_if = "Option::is_none")]
    pub type_sym: Option<StrT>,
    #[serde(rename = "argRanges", default, skip_serializing_if = "Vec::is_empty")]
    pub arg_ranges: Vec<SourceRange>,
    #[serde(flatten, skip_serializing_if = "Option::is_none")]
    pub expansion_info: Option<ExpansionInfo>,
    /// Confidence level for each symbol.
    /// When Some it should have the same length as sym and defines the confidence level for each symbol.
    /// When None all symbols are assumed to have the highest confidence level.
    #[serde(skip_serializing_if = "Option::is_none")]
    pub confidence: Option<Vec<ConfidenceLevel>>,
}

impl<StrT> AnalysisSource<StrT>
where
    StrT: Clone + Debug + Default + Deref<Target = str> + FromStr + Hash + Ord + PartialEq,
{
    /// Merges the `syntax`, `sym`, `no_crossref`, and `nesting_range` fields from `other`
    /// into `self`. The `no_crossref` field can be different sometimes
    /// with different versions of clang being used across different
    /// platforms; in this case we only set `no_crossref` if all the versions
    /// being merged have the `no_crossref` field set.  The `nesting_range` can
    /// vary due to use of the pre-processor, including an extreme case where the
    /// ranges are non-overlapping.  We choose to union these ranges because
    /// `merge-analyses.rs` only merges adjacent source entries so the space
    /// between the ranges should simply be preprocessor directives.
    ///
    /// Also asserts that the `pretty` field is the same because otherwise
    /// the merge doesn't really make sense.
    pub fn merge(&mut self, mut other: AnalysisSource<StrT>) {
        assert_eq!(self.pretty, other.pretty);
        self.no_crossref &= other.no_crossref;
        self.syntax.append(&mut other.syntax);
        self.syntax.sort();
        self.syntax.dedup();
        // de-duplicate symbols without sorting the symbol list so we can maintain the original
        // ordering which can allow the symbols to go from most-specific to least-specific.  In
        // the face of multiple platforms with completely platform-specific symbols and where each
        // platform has more than one symbol, this doesn't maintain a useful overall order, but the
        // first symbol can still remain useful.  (And given in-order processing of platforms, the
        // choice of first symbol remains stable as long as the indexer's symbol ordering remains
        // stable.)
        //
        // This currently will give precedence to the order in "other" rather than "self", but
        // it's still consistent.
        if self.confidence.is_none() && other.confidence.is_none() {
            other.sym.append(&mut self.sym);
            self.sym.extend(other.sym.drain(0..).unique());
            // self.confidence stays None, everything is assumed to be Concrete
        } else {
            let confidence: Vec<_> = other.confidences().chain(self.confidences()).collect();

            other.sym.append(&mut self.sym);

            let mut confidences = HashMap::<StrT, ConfidenceLevel>::new();
            for (sym, confidence) in other.sym.into_iter().zip(confidence.into_iter()) {
                let entry = confidences.entry(sym);
                entry
                    .and_modify(|existing_confidence| {
                        *existing_confidence = confidence.max(*existing_confidence)
                    })
                    .or_insert_with_key(|sym| {
                        self.sym.push(sym.clone());
                        confidence
                    });
            }
            self.confidence = Some(self.sym.iter().map(|sym| confidences[sym]).collect());
        }

        self.nesting_range.union(other.nesting_range);
        // We regrettably have no guarantee that the types are the same, so just pick a type when
        // we have it.
        // I tried to make this idiomatic using "or" to overwrite the type, but it got ugly.
        if let Some(type_pretty) = other.type_pretty {
            self.type_pretty.get_or_insert(type_pretty);
        }
        if let Some(type_sym) = other.type_sym {
            self.type_sym.get_or_insert(type_sym);
        }

        use ExpansionInfo::*;
        match (&mut self.expansion_info, &mut other.expansion_info) {
            (_, None) => {}
            (expansion_info @ &mut None, m) => *expansion_info = m.take(),
            (Some(ExpandsTo(_)), Some(InExpansionAt(_)))
            | (Some(InExpansionAt(_)), Some(ExpandsTo(_))) => {
                panic!("Trying to merge an expansion and an expanded symbol.")
            }
            (&mut Some(ExpandsTo(ref mut a)), &mut Some(ExpandsTo(ref mut b))) => {
                for (k, mut v) in core::mem::take(b) {
                    a.entry(k).and_modify(|a_v| a_v.append(&mut v)).or_insert(v);
                }
            }
            (&mut Some(InExpansionAt(ref mut a)), &mut Some(InExpansionAt(ref mut b))) => {
                for (k, mut v) in core::mem::take(b) {
                    a.entry(k)
                        .and_modify(|a_v| {
                            for (k0, mut v0) in core::mem::take(&mut v) {
                                a_v.entry(k0)
                                    .and_modify(|a_v0| a_v0.append(&mut v0))
                                    .or_insert(v0);
                            }
                        })
                        .or_insert(v);
                }
            }
        }
    }

    pub fn confidences(&self) -> impl Iterator<Item = ConfidenceLevel> + use<'_, StrT> {
        match &self.confidence {
            Some(confidence) => ConfidenceIterator::Set(confidence.iter().copied()),
            None => ConfidenceIterator::Default(std::iter::repeat_n(
                ConfidenceLevel::Concrete,
                self.sym.len(),
            )),
        }
    }

    /// Source records' "pretty" field is prefixed with their SyntaxKind.  It's also placed in the
    /// "syntax" sorted array, but that string/array ends up empty when no_crossref is set, so
    /// it's currently easiest to get it from here.
    ///
    /// XXX note that the clang indexer can generate "enum constant" syntax kinds that possess a
    /// space, but that just means we lose the "constant" bit, not that we get confused about the
    /// pretty name.
    pub fn get_syntax_kind(&self) -> Option<&str> {
        // It's a given that we're using a standard ASCII space character.
        self.pretty.split(' ').next()
    }

    /// Returns the `sym` array joined with ",".  This convenience method exists
    /// because join() doesn't currently work on Ustr.
    pub fn get_joined_syms(&self) -> String {
        SerializeVecString::<StrT>::join(&self.sym)
    }
}

#[derive(Serialize, Deserialize)]
#[serde(untagged)]
pub enum AnalysisUnion<StrT = Ustr>
where
    StrT: Clone + Debug + Default + Deref<Target = str> + FromStr + Hash + Ord + PartialEq,
{
    Target(AnalysisTarget<StrT>),
    Source(AnalysisSource<StrT>),
    Structured(AnalysisStructured<StrT>),
}

pub fn parse_location(loc: &str) -> Location {
    let v: Vec<&str> = loc.split(":").collect();
    let lineno = v[0].parse::<u32>().unwrap();
    let (col_start, col_end) = if v[1].contains("-") {
        let v: Vec<&str> = v[1].split("-").collect();
        (v[0], v[1])
    } else {
        (v[1], v[1])
    };
    let col_start = col_start.parse::<u32>().unwrap();
    let col_end = col_end.parse::<u32>().unwrap();
    Location {
        lineno,
        col_start,
        col_end,
    }
}

impl Serialize for Location {
    fn serialize<S>(&self, serializer: S) -> Result<S::Ok, S::Error>
    where
        S: Serializer,
    {
        let s = if self.col_start == self.col_end {
            format!("{:05}:{}", self.lineno, self.col_start)
        } else {
            format!("{:05}:{}-{}", self.lineno, self.col_start, self.col_end)
        };
        serializer.serialize_str(&s)
    }
}

impl<'de> Deserialize<'de> for Location {
    fn deserialize<D>(deserializer: D) -> Result<Location, D::Error>
    where
        D: Deserializer<'de>,
    {
        let s = String::deserialize(deserializer)?;
        Ok(parse_location(&s))
    }
}

fn parse_line_range(range: &str) -> LineRange {
    let v: Vec<&str> = range.split("-").collect();
    if v.len() != 2 {
        return LineRange::default();
    }
    let start_lineno = v[0].parse::<u32>().unwrap();
    let end_lineno = v[1].parse::<u32>().unwrap();
    LineRange {
        start_lineno,
        end_lineno,
    }
}

impl Serialize for LineRange {
    fn serialize<S>(&self, serializer: S) -> Result<S::Ok, S::Error>
    where
        S: Serializer,
    {
        serializer.serialize_str(&format!("{}-{}", self.start_lineno, self.end_lineno))
    }
}

impl<'de> Deserialize<'de> for LineRange {
    fn deserialize<D>(deserializer: D) -> Result<LineRange, D::Error>
    where
        D: Deserializer<'de>,
    {
        let s = String::deserialize(deserializer)?;
        Ok(parse_line_range(&s))
    }
}

fn parse_source_range(range: &str) -> SourceRange {
    let v: Vec<&str> = range.split(&['-', ':'][..]).collect();
    if v.len() != 4 {
        return SourceRange::default();
    }
    let start_lineno = v[0].parse::<u32>().unwrap();
    let start_col = v[1].parse::<u32>().unwrap();
    let end_lineno = v[2].parse::<u32>().unwrap();
    let end_col = v[3].parse::<u32>().unwrap();
    SourceRange {
        start_lineno,
        start_col,
        end_lineno,
        end_col,
    }
}

impl Serialize for SourceRange {
    fn serialize<S>(&self, serializer: S) -> Result<S::Ok, S::Error>
    where
        S: Serializer,
    {
        serializer.serialize_str(&format!(
            "{}:{}-{}:{}",
            self.start_lineno, self.start_col, self.end_lineno, self.end_col
        ))
    }
}

impl<'de> Deserialize<'de> for SourceRange {
    fn deserialize<D>(deserializer: D) -> Result<SourceRange, D::Error>
    where
        D: Deserializer<'de>,
    {
        let s = String::deserialize(deserializer)?;
        Ok(parse_source_range(&s))
    }
}

#[cfg(not(target_arch = "wasm32"))]
pub fn read_analysis<T>(
    filename: &str,
    filter: &mut dyn FnMut(Value, &Location, usize) -> Option<T>,
) -> Vec<WithLocation<Vec<T>>> {
    read_analyses(vec![filename.to_string()].as_slice(), filter)
}

/// Load analysis data for one or more files, sorting and grouping by location, with data payloads
/// transformed via the provided `filter`, resulting in either AnalysisSource records being
/// returned (if `read_source` is provided) or AnalysisTarget (if `read_target`) and other record
/// types being ignored.
///
/// Analysis files ending in .gz will be automatically decompressed as they are
/// read.
///
/// Note that the filter function is invoked as records are read in, which means
/// that the sort order seen by the filter function is the order the file
/// already had.  It's only the return value that's sorted and grouped.
#[cfg(not(target_arch = "wasm32"))]
pub fn read_analyses<T>(
    filenames: &[String],
    filter: &mut dyn FnMut(Value, &Location, usize) -> Option<T>,
) -> Vec<WithLocation<Vec<T>>> {
    let mut result = Vec::new();
    for (i_file, filename) in filenames.iter().enumerate() {
        let file = match File::open(filename) {
            Ok(f) => f,
            Err(_) => {
                // TODO: This should be a warning again or have more explicit
                // propagation of this case to callers.  This was reduced from
                // a warning because we have a bunch of cases from
                // mozsearch-mozilla/shared/collapse-generated-files.sh being
                // invoked on "analysis-*/etc" expansions that don't match and
                // so are passed through directly because we're not using
                // "shopt -s nullglob" there.  Also crossref seems to sometimes
                // end up trying to ingest files that aren't there?  Both of
                // these things should be addressed if we want to turn this back
                // into a warning.
                info!("Error trying to open analysis file [{}]", filename);
                continue;
            }
        };
        // An analysis file that ends in .gz is compressed and should be
        // dynamically decompressed.
        let reader: Box<dyn Read> = if filename.ends_with(".gz") {
            Box::new(GzDecoder::new(file))
        } else {
            Box::new(file)
        };
        let reader = BufReader::new(reader);
        let mut lineno = 0;
        for line in reader.lines() {
            let line = line.unwrap();
            lineno += 1;
            let data: serde_json::Result<Value> = from_str(&line);
            let mut data = match data {
                Ok(data) => data,
                Err(e) => {
                    warn!(
                        "Error [{}] trying to read analysis from file [{}] line [{}]: [{}]",
                        e, filename, lineno, &line
                    );
                    continue;
                }
            };
            let obj = data.as_object_mut().unwrap();
            // Destructively pull the "loc" out before passing it into the filter.  This is for
            // read_structured which stores everything it doesn't directly process in `payload`.
            let loc = parse_location(obj.remove("loc").unwrap().as_str().unwrap());
            if let Some(v) = filter(data, &loc, i_file) {
                result.push(WithLocation { data: v, loc })
            }
        }
    }

    result.sort_by(|x1, x2| x1.loc.cmp(&x2.loc));

    let mut result2 = Vec::new();
    let mut last_loc = None;
    let mut last_vec = Vec::new();
    for r in result {
        match last_loc {
            Some(ll) => {
                if ll == r.loc {
                    last_loc = Some(ll);
                } else {
                    result2.push(WithLocation {
                        loc: ll,
                        data: last_vec,
                    });
                    last_vec = Vec::new();
                    last_loc = Some(r.loc);
                }
            }
            None => {
                last_loc = Some(r.loc);
            }
        }
        last_vec.push(r.data);
    }

    if let Some(ll) = last_loc {
        result2.push(WithLocation {
            loc: ll,
            data: last_vec,
        })
    }

    result2
}

pub fn read_target(obj: Value, _loc: &Location, _i_size: usize) -> Option<AnalysisTarget<Ustr>> {
    // XXX this shouldn't be necessary thanks to our tag, so this should be removable
    obj.get("target")?;

    from_value(obj).ok()
}

pub fn read_structured(
    obj: Value,
    _loc: &Location,
    _i_size: usize,
) -> Option<AnalysisStructured<Ustr>> {
    // XXX this shouldn't be necessary thanks to our tag, so this should be removable
    obj.get("structured")?;

    from_value(obj).ok()
}

pub fn read_source(obj: Value, _loc: &Location, _i_size: usize) -> Option<AnalysisSource<Ustr>> {
    // XXX this shouldn't be necessary thanks to our tag, so this should be removable
    obj.get("source")?;

    from_value(obj).ok()
}

#[derive(Debug, Serialize, Deserialize)]
pub struct Jump {
    pub id: Ustr,
    pub path: String,
    pub lineno: u64,
    pub pretty: String,
}

#[cfg(not(target_arch = "wasm32"))]
pub fn read_jumps(filename: &str) -> UstrMap<Jump> {
    let file = File::open(filename).unwrap();
    let reader = BufReader::new(&file);
    let mut result = UstrMap::default();
    let mut lineno = 1;
    for line in reader.lines() {
        let line = line.unwrap();
        let data: serde_json::Result<Value> = from_str(&line);
        let data = match data {
            Ok(data) => data,
            Err(_) => panic!("error on line {}: {}", lineno, &line),
        };
        lineno += 1;

        let array = data.as_array().unwrap();
        let id = ustr(array[0].as_str().unwrap());
        let data = Jump {
            id,
            path: array[1].as_str().unwrap().to_string(),
            lineno: array[2].as_u64().unwrap(),
            pretty: array[3].as_str().unwrap().to_string(),
        };

        result.insert(id, data);
    }
    result
}

/// This is the representation format for the path-lines per-kind results we
/// emit into the crossref database.  It is generic over `T` so that we can use
/// T=`Ustr` for easy string-interning in crossref.rs but so that we can also
/// deserialize the results as T=`String` in `cmd_compile_results` where we
/// ingest this format and the manual parsing logic ends up very verbose.
#[derive(Clone, Debug, Deserialize, Serialize)]
pub struct SearchResult {
    #[serde(rename = "lno")]
    pub lineno: u32,
    pub bounds: (u32, u32),
    pub line: String,
    pub context: Ustr,
    pub contextsym: Ustr,
    // We used to build up "peekLines" which we excerpted from the file here, but
    // this was never surfaced to users.  The plan at the time had been to try
    // and store specific file offsets that could be directly mapped/seeked, but
    // between effective caching of dynamic search results and good experiences
    // with lol_html, it seems like we will soon be able to just excerpt the
    // statically produced HTML efficiently enough through dynamic HTML
    // filtering.
    #[serde(
        rename = "peekRange",
        default,
        skip_serializing_if = "LineRange::is_empty"
    )]
    pub peek_range: LineRange,
}

#[derive(Clone, Debug, Deserialize)]
pub struct PathSearchResult {
    pub path: Ustr,
    pub path_kind: Ustr,
    pub lines: Vec<SearchResult>,
}

```

## tools/src/file_format/analysis_manglings.rs
```
use regex::{Captures, Regex};

pub fn mangle_file(filename: &str) -> String {
    lazy_static! {
        // The column portion can potentially be singular I think so we just
        // treat the half as its own group.
        static ref RE: Regex = Regex::new(r"[^a-zA-Z0-9_/]").unwrap();
    }
    RE.replace_all(filename, |c: &Captures| {
        format!("@{:02X}", c[0].as_bytes()[0])
    })
    .to_string()
}

pub fn make_file_sym_from_path(path: &str) -> String {
    format!("FILE_{}", mangle_file(path))
}

#[test]
fn test_mangle_file() {
    assert_eq!(mangle_file("path/foo.h"), "path/foo@2Eh");
    assert_eq!(make_file_sym_from_path("path/foo.h"), "FILE_path/foo@2Eh");
    assert_eq!(
        make_file_sym_from_path("subdir/header@with,many^strange~chars.h"),
        "FILE_subdir/header@40with@2Cmany@5Estrange@7Echars@2Eh"
    );
}

#[derive(Eq, PartialEq, Clone, Copy, Debug)]
enum SplitState {
    // We're not in a template argument and the last character we processed is
    // not interesting.
    NotInArg,
    // We're not in a template argument and we've seen one colon and we expect
    // the next thing we see to be a colon completing a delimiter.
    NotInArgSawColon,
    // We're in a template argument, with the specific depth expressed by
    // `arg_depth`, and the last character we processed is not interesting.
    InArg,
    // We're in a template argument and the last character we saw was a "<".
    // If we see another "<" we know it's a left-shift because although there
    // are in-domain reasons for multiple consecutive ">" characters, there are
    // none for for "<" (other than left shift).
    InArgSawLT,
}

/// Pretty identifier segmentation that improves on naive splitting on "::",
/// and which relies on the provided symbol for some context for cases like
/// files.  It's not clear if this should also handle situations like Python
/// modules currently being "." delimited (although we then use "::" to combine
/// on top of that).
///
/// The primary motivation for this helper is to deal with cases like
/// `TemplatedClass<Foo::Bar>::Method` where the naive splitting will go wrong.
/// Additionally, we have to handle real world cases with bitshifts like
/// `Array<std::pair<uint8_t, uint8_t>, 1 << sizeof(AnonymousContentKey) * 8>`.
///
/// Note that although searchfox effectively understands JS-style "Foo.bar"
/// hierarchy, this is currently accomplished via `js-analyze.js` emitting 2
/// records: `{ pretty: "Foo", sym: "#Foo", ...}` and `{ pretty: "Foo.bar", sym:
/// "Foo#bar", ...}`.  This approach will likely be revisited when we move to
/// using LSIF/similar indexing, in which case this method will likely want to
/// become language aware and we would start only emitting a single record for
/// a single symbol.
///
/// ## Observed problems:
///
/// On LLVM:
/// - "In arg state with depth 1 when ran out of chars." seems to be happening
///   on "llvm::raw_ostream::operator<<".
pub fn split_pretty(pretty: &str, sym: &str) -> (Vec<String>, &'static str) {
    // Split files based on their path delimiter.  It would be too weird for us
    // to map them to using "::".  We're also now using split_inclusive so the
    // directories can have a distinct trailing slash to distinguish them from
    // actual pretty symbols.  (Not that we should have a heterogeneous diagram
    // that has files and symbols at the same time, but I think this will help
    // make the diagram more legible since it will be immediately clear what
    // we're dealing with.)
    if sym.starts_with("FILE_") {
        return (
            pretty.split_inclusive("/").map(|s| s.to_string()).collect(),
            "",
        );
    }

    let mut pieces = vec![];
    let mut state = SplitState::NotInArg;
    let mut arg_depth = 0;
    let mut pretty_chars = pretty.chars();
    let mut token = String::new();
    loop {
        let next_c = pretty_chars.next();

        match (state, next_c) {
            (_, None) => {
                if state != SplitState::NotInArg {
                    info!(
                        "In arg state with depth {} when ran out of chars on {}.",
                        arg_depth, pretty,
                    );
                }
                if !token.is_empty() {
                    pieces.push(token);
                }
                break;
            }
            (SplitState::NotInArg, Some(':')) => {
                state = SplitState::NotInArgSawColon;
                // we will either end up eating this ":" if it's the first of a
                // pair, or put it back in if we see a different char in the
                // next state.
            }
            (SplitState::NotInArg, Some('<')) => {
                state = SplitState::InArg;
                arg_depth = 1;
                token.push('<');
            }
            (SplitState::NotInArg, Some('>')) => {
                // Don't warn on operator->, operator>, operator>=, operator>>, etc.
                // Also, we expect to get called on a bunch of garbage, so no warnings.
                if !token.starts_with("operator") {
                    info!("Saw '>' when not in an argument while parsing {}.", pretty);
                }
                token.push('>');
            }
            (SplitState::NotInArg, Some(c)) => {
                token.push(c);
            }
            (SplitState::NotInArgSawColon, Some(':')) => {
                pieces.push(token);
                token = String::new();
                state = SplitState::NotInArg;
            }
            (SplitState::NotInArgSawColon, Some(c)) => {
                // We expect to be called on a bunch of garbage, so no warnings.
                // In particular, we will have things like:
                // NSPRLogModulesParser_RawArg_Test::TestBody::(anonymous)::(lambda at /builds/worker/checkouts/gecko/xpcom/tests/gtest/TestNSPRLogModulesParser.cpp:130:19)
                // NameTableKey::(anonymous)::(unnamed union at /builds/worker/checkouts/gecko/xpcom/ds/nsStaticNameTable.cpp:33:3)
                info!(
                    "Saw a single colon when not in arg while parsing {}",
                    pretty
                );
                token.push(')');
                token.push(c);
                state = SplitState::NotInArg;
            }
            (SplitState::InArg, Some('<')) => {
                token.push('<');
                state = SplitState::InArgSawLT;
            }
            (SplitState::InArg, Some('>')) => {
                token.push('>');
                arg_depth -= 1;
                if arg_depth == 0 {
                    state = SplitState::NotInArg;
                }
            }
            (SplitState::InArg, Some(c)) => {
                token.push(c);
            }
            (SplitState::InArgSawLT, Some('<')) => {
                // Okay, this was almost certainly a left-shift, so we don't
                // bump the depth.
                token.push('<');
                state = SplitState::InArg;
            }
            (SplitState::InArgSawLT, Some(c)) => {
                token.push(c);
                // Since we didn't see two in a row, then we probably increased
                // our depth.
                arg_depth += 1;
                state = SplitState::InArg;
            }
        }
    }

    (pieces, "::")
}

#[test]
fn test_split_pretty() {
    let ts = |vs: Vec<&str>| -> Vec<String> { vs.into_iter().map(|s| s.to_string()).collect() };

    assert_eq!(
        split_pretty("foo/bar/Baz.h", "FILE_foo_bar_Baz.h"),
        (ts(vec!["foo/", "bar/", "Baz.h"]), "")
    );

    assert_eq!(
        split_pretty("mozilla::dom::locks::LockRequest", "T_LockRequest"),
        (ts(vec!["mozilla", "dom", "locks", "LockRequest"]), "::")
    );

    assert_eq!(
        split_pretty("nsCOMPtr::operator->", "_ZNK8nsCOMPtrptEv&redirect=false"),
        (ts(vec!["nsCOMPtr", "operator->"]), "::")
    );

    assert_eq!(
        split_pretty(
            "Deserializer<mozilla::UniquePtr<char[], JS::FreePolicy>>::Read",
            "T_blah"
        ),
        (
            ts(vec![
                "Deserializer<mozilla::UniquePtr<char[], JS::FreePolicy>>",
                "Read"
            ]),
            "::"
        )
    );

    assert_eq!(
        split_pretty(
            "Array<std::pair<uint8_t, uint8_t>, 1 << sizeof(AnonymousContentKey) * 8>::DoStuff",
            "T_blah"
        ),
        (
            ts(vec![
                "Array<std::pair<uint8_t, uint8_t>, 1 << sizeof(AnonymousContentKey) * 8>",
                "DoStuff"
            ]),
            "::"
        )
    );
}

```

## tools/src/file_format/url_map.rs
```
use serde::Deserialize;
use serde_json::from_reader;
use std::collections::HashMap;
use std::fs::File;

#[derive(Clone, Deserialize, Debug)]
pub struct URLMapItem {
    pub pretty: String,
    pub sym: String,
}

#[derive(Debug)]
pub struct URLMap {
    data: HashMap<String, Vec<URLMapItem>>,
}

impl URLMap {
    fn new(data: HashMap<String, Vec<URLMapItem>>) -> Self {
        Self { data }
    }

    pub fn new_empty() -> Self {
        Self {
            data: HashMap::new(),
        }
    }

    pub fn get(&self, sym: &String) -> Option<Vec<URLMapItem>> {
        self.data.get(sym).cloned()
    }
}

pub fn read_url_map(filename: &String) -> URLMap {
    let file = match File::open(filename) {
        Ok(f) => f,
        Err(_) => {
            info!("Error trying to open URL map file [{}]", filename);
            return URLMap::new_empty();
        }
    };

    let data: HashMap<String, Vec<URLMapItem>> = match from_reader(file) {
        Ok(result) => result,
        Err(_) => {
            info!("Error trying to read URL map file [{}]", filename);
            return URLMap::new_empty();
        }
    };

    URLMap::new(data)
}

```

## tools/src/file_format/history/syntax_symdex.rs
```
//! This file defines the ND-JSON records we write into files under
//! `history/syntax/symdex`.
//!
//! Symdex files are organized on a per-language basis and serve as very simple
//! cross-references from a pretty symbol identifier to the source files that
//! contain the declarations and definitions for that symbol and, if the symbol
//! is class-like, its members, fields, and any nested classes.  We currently
//! aren't planning to have entries for namespaces, but that's a weakly held
//! decision.
//!
//! Because the files represent an aggregation of information derived from
//! potentially multiple source files and our processing model only re-processes
//! files that are changed, our general approach for updating these files is:
//! - For every updated `history/syntax/files-struct` file that is updated
//!   because of its source file, we create a `SymdexRecord` from each of its
//!   `FileStructureRow` records and bin them based on their "pretty" and their
//!   parent's "pretty" if appropriate for the parent type.
//! - Once we've processed all the files, we process the resulting map structure
//!   on a per-symdex-file basis based on the "pretty" values:
//!   - We load the existing symdex file.
//!   - We filter out all `SymdexRecord` records for any files we have new
//!     records for.  (This saves us from having to compute any deltas.)
//!   - We append all the new records.
//!   - We sort the symdex records by their "pretty".
//!   - We write out the updated symdex file (including the leading header).

use serde::{Deserialize, Serialize};

use super::syntax_files_struct::FileStructureRow;

// First record in a symdex file; it wants to be completely independent of the
// rest of the contents of the file and so there's not much to put in here other
// than if we eventually want to handle overload / "stop symbol" semantics for
// some reason.
#[derive(Debug, Serialize, Deserialize)]
pub struct SymdexHeader {}

/// The contents of a FileStructuredRow with a path added.
#[derive(Debug, Eq, PartialEq, Ord, PartialOrd, Serialize, Deserialize)]
pub struct SymdexRecord {
    #[serde(flatten)]
    pub file_row: FileStructureRow,

    pub path: String,
}

```

## tools/src/file_format/history/syntax_files_struct.rs
```
//! This file defines the ND-JSON records we write into files under
//! `history/syntax/files-struct`.  The contents of the files are intended to
//! always reflect the current state of the tree for the source revision they
//! are derived from.  Our goal is to be purely functional in doing this; our
//! output is expected to be purely a function of the contents of the files.
//!
//! Currently we're using `String` instead of `Ustr` because the code is
//! intended to be run in parallel so lock contention is not helpful and any
//! memory pressure concerns would not come from strings where Ustr is likely to
//! be a major help.  (However, ropes would be quite useful.)

use serde::{Deserialize, Serialize};

/// This record is the first JSON record in the file and provides file-level
/// information.  Currently we emit "check the plug" debugging information like
/// what tree-sitter parser was used to derive the contents, as well as
/// indicating specifically when we did not recognize the file as a supported
/// type and therefore had no parser.
#[derive(Debug, Serialize, Deserialize)]
pub struct FileStructureHeader {
    // The tree-sitter language we parsed this as, or None if we did not parse
    // this file using tree-sitter.  If the value is None, we expect there will
    // be no `FileStructureRow` records following the header.
    pub lang: Option<String>,
}

/// The remainder of the records in the file after the header.  These are
/// expected to be emitted in the order they are encountered in the source file.
/// Accordingly, we don't encode any position information since we would expect
/// these to be subject to churn which makes diffs derived from the changes in
/// this file harder to usefully derive information from.
#[derive(Clone, Debug, Eq, PartialEq, Ord, PartialOrd, Serialize, Deserialize)]
pub struct FileStructureRow {
    /// The pretty identifier for this symbol.  It's currently assumed that we
    /// can get the pretty identifier for the containing namespace/class by
    /// popping off the last segment of the pretty idenfitier using "::" as the
    /// delimiter.
    pub pretty: String,

    /// Is this a definition?  If it's not a definition, it's a declaration.
    #[serde(rename = "isDef")]
    pub is_def: bool,

    /// The "structured" kind, or at least a best-effort attempt to match it.  We
    /// primarily care about "class", "method", and "field" as those have clear
    /// benefit to listing in the symdex.
    pub kind: String,
}

```

## tools/src/file_format/history/timeline_files_delta.rs
```
//! This file defines the ND-JSON records we write into files under
//! `history/timeline/files-delta`.

use serde::{Deserialize, Serialize};

use super::timeline_common::{DetailRecordRef, SummaryRecordRef, SymbolSyntaxDeltaGroup};

#[derive(Debug, Serialize, Deserialize)]
pub struct FileDeltaHeader {}

/// Details changes from a specific revision for the source file containing this
/// record.  We do not currently
#[derive(Debug, Serialize, Deserialize)]
pub struct FileDeltaDetailRecord {
    #[serde(flatten)]
    pub desc: DetailRecordRef,

    #[serde(flatten)]
    pub symbol_group: SymbolSyntaxDeltaGroup,
}

#[derive(Debug, Serialize, Deserialize)]
pub struct FileDeltaSummaryRecord {
    #[serde(flatten)]
    pub desc: SummaryRecordRef,

    #[serde(flatten)]
    pub symbol_group: SymbolSyntaxDeltaGroup,
}

/// Internally tagged enum for our detail and summary types.  This ends up
/// serializing as `{"type": "Detail" , ...}` or `{"type": "Summary", ...}`.
#[derive(Debug, Serialize, Deserialize)]
#[serde(tag = "type")]
pub enum FileDeltaRecord {
    Detail(FileDeltaDetailRecord),
    Summary(FileDeltaSummaryRecord),
}

```

## tools/src/file_format/history/timeline_tokens.rs
```
//! This file defines the ND-JSON records we write into files under
//! `history/timeline/tokens/ab/cd/` where "ab" and "cd" are pairs of characters
//! from the (lowercased) prefix of the token to help keep the file-system, or
//! at least directory listings, sane.
//!
//! The files are intended to support UX functionality along the lines of:
//! - `git log -S` by helping make it clear when there are net changes in the
//!   presence of certain tokens which indicates that logic isn't just being
//!   reformatted or moved around.
//! - Letting the user know if what they searched for is no longer in the tree,
//!   but when it was last in the tree and potentially identifying the likely
//!   multiple patches involved in the term being removed.
//! - General interest graphs of net changes in use of the token over time,
//!   aggregated by week.


use serde::{Deserialize, Serialize};

#[derive(Debug, Serialize, Deserialize)]
pub struct TokenDetailRecord {

}

/// Aggregated statistics
#[derive(Debug, Serialize, Deserialize)]
pub struct TokenSummaryRecord {

}

```

## tools/src/file_format/history/rev_summaries.rs
```
//! This file defines the JSON records we write into the (non-git)
//! `history/rev-summaries/by-source-rev/aa/bb` path structure where AA and BB
//! are the first 2 pairs of the lowercased git source revision hash that we
//! are summarizing.
//!
//! ## Storage: Not Git!
//!
//! This data is intentionally not stored in git because this
//! makes it easier to go directly from a user-provided revision to all of the
//! metadata we have about the revision without having to have a large in-memory
//! map or add a git on-disk map like git-cinnabar does for hg2git.  This also
//! saves us from having to use git to get a checkout of the revision, etc.  We
//! can also easily compress the files, but git can handle that, it just isn't
//! useful if the files change.  Note that we do expect these files to be
//! immutable except potentially in the face of backouts when we would probably
//! update the files.
//!
//! ## File Contents and Relation to File Deltas
//!
//! The revision summary is primarily an aggregation of the individual file
//! deltas.  We only write out a single JSON blob so we only need a record and
//! there's no need for a header.

use std::collections::BTreeMap;

use serde::{Deserialize, Serialize};

use super::timeline_common::SymbolSyntaxDeltaGroup;

/// Intended to be analogous to the payload of the `FileDeltaDetailRecord` for
/// the given file.  If that changes to be more than just a `symbol_group` then
/// both structs likely just want to be including the same new intermediate
/// struct.
#[derive(Debug, Serialize, Deserialize)]
pub struct RevFileSummaryRecord {
    #[serde(flatten)]
    pub symbol_group: SymbolSyntaxDeltaGroup,
}

#[derive(Debug, Serialize, Deserialize)]
pub struct RevSummaryRecord {
    /// The source git repo revision we're describing; this should also be our
    /// filename.
    pub source_rev: String,

    /// The "syntax" history git repo revision corresponding to this revision.
    pub syntax_rev: String,

    // The "timeline" history git repo corresponding to this revision.  This
    // file is expected to be written immediately after committing the given
    // revision so we can have it available.
    pub timeline_rev: String,

    /// The commit message.
    pub message: String,

    /// The commit/push date (versus the potentially misleading authorship date,
    /// if we have that too).
    pub iso_date: String,

    /// The author of the commit, not yet mail-mapped; this must ALWAYS be
    /// passed through a mail-mapping process before being passed to a display
    /// layer in order to avoid dead-naming people.
    pub unmapped_author: String,

    /// Basically the contents of all the `FileDetlaDetailRecords` for all the
    /// files changed in this revision.
    pub file_deltas: BTreeMap<String, RevFileSummaryRecord>,
}

```

## tools/src/file_format/history/io_helpers.rs
```
use itertools::Itertools;
use serde::{Deserialize, Serialize};
use serde_json::to_string;

pub fn read_record_file_contents<
    Header: for<'a> Deserialize<'a>,
    Record: for<'a> Deserialize<'a>,
>(
    contents: &[u8],
) -> Option<(Header, Vec<Record>)> {
    let contents_as_str = std::str::from_utf8(contents).unwrap();
    let mut lines = contents_as_str.lines();
    let header: Header = match lines.next() {
        Some(line) => serde_json::from_str(line).unwrap(),
        None => return None,
    };
    let mut records: Vec<Record> = vec![];
    for line in lines {
        records.push(serde_json::from_str(line).unwrap());
    }
    Some((header, records))
}

pub fn record_file_contents_to_string<Header: Serialize, Record: Serialize>(
    header: &Header,
    records: &[Record],
) -> String {
    let header_iter = std::iter::once(to_string(&header).unwrap());
    let records_iter = records.iter().map(|rec| to_string(rec).unwrap());
    header_iter.chain(records_iter).join("\n")
}

```

## tools/src/file_format/history/mod.rs
```
pub mod io_helpers;
pub mod rev_summaries;
pub mod syntax_files_struct;
pub mod syntax_symdex;
pub mod timeline_common;
pub mod timeline_files_delta;

```

## tools/src/file_format/history/timeline_common.rs
```
use std::collections::BTreeMap;

use serde::{Deserialize, Serialize};

#[derive(Debug, Serialize, Deserialize)]
pub struct DetailRecordRef {
    /// Source revision this record contains details for.
    pub source_rev: String,
    /// The syntax revision that corresponds to that source revision.
    pub syntax_rev: String,
    /// ISO 8601 date of the commit as told to us by git; git cinnabar seems to
    /// give us the autoland date, which is nice.
    pub iso_date: String,
}

#[derive(Debug, Serialize, Deserialize)]
pub struct SummaryRecordRef {
    /// List of all of the source revisions whose data is aggregated into this
    /// summary record ordered from newest to oldest.  It's possible to have a
    /// length of 1 as our policy is to aggregate at a week-based granularity
    /// for now.
    pub source_revs: Vec<String>,

    /// The timeline revision that precedes the creation of the revision that
    /// holds this summary record.  So if you look at this revision, you will
    /// find all of the detail records that were an input to the creation of
    /// this summary record.
    pub pred_timeline_rev: String,

    /// The [year, newest iso week inclusive, oldest iso week inclusive] time
    /// range that this summary is intended to cover.  For now we expect that
    /// all summary records will cover a single week, so the 2nd and 3rd values
    /// will be the same.  In the future we might imagine quantizing to a month
    /// granularity as a second pass, but it's not clear the additional
    /// decimation would be useful.
    ///
    /// Summary records should never overlap, so sorting
    pub iso_week_range: (u16, u8, u8),
}

#[derive(Debug, Serialize, Deserialize)]
pub struct TokenDeltaDetails {
    /// Number of times this token was present in a "+" diff delta that could
    /// not be attributed to a matching syntactically bound "-" and thereby
    /// counted as "moved".  Unlike something like `git log -S` which looks at
    /// the net change in tokens, it's completely possible for this record to
    /// have both a >0 "added" and "removed".
    pub added: u32,
    /// Fuzzy heuristic concept where we have reason to believe that a pair of
    /// "+" and "-" diff deltas for a token correspond to moved or very lightly
    /// refactored code.  Initially this means that the tokens had the same
    /// structural syntax binding scope, but in the future we could also
    /// potentially allow for changes in binding scope due to inferred method
    /// renames explaining the scope change.  Also keep in mind that because we
    /// initially will be only looking at what the diff deltas are, we are
    /// looking at the diff algorithm's attempt to find a minimal delta, but
    /// semantically it might be that some other greater number of changes
    /// should instead be counted as moved.
    pub moved: u32,
    /// Counterpart to "added"; the number of times this token was present in a
    /// "-" diff delta that was not attributed to "moved".
    pub removed: u32,
}

/// Indicate whether a symbol was added/changed/removed.
/// TODO: Figure out how to express a symbol being renamed.  While we could add
/// a "Renamed" here, it might instead make sense to stick with "Changed" and
/// instead have a list of change details (beyond the token_changes) which could
/// express the rename.
#[derive(Clone, Copy, Debug, Eq, PartialEq, Ord, PartialOrd, Serialize, Deserialize)]
#[serde(rename_all = "lowercase")]
pub enum ChangeKind {
    /// Newly added symbol/whatever.
    Added,
    /// The symbol/whatever existed before this and it still exists; look for
    /// details elsewhere.  This mainly exists to distinguish from added and
    /// removed which are more immediately useful.
    Changed,
    /// The symbol/whatever was removed.
    Removed,
}

/// Summarized changes at symbol granularity, with the "pretty" being assumed to
/// be stored externally in a map key that owns this value or in a wrapper if a
/// map is not involved.
#[derive(Debug, Serialize, Deserialize)]
pub struct SymbolSyntaxDelta {
    pub change: ChangeKind,

    /// Changes to tokens within the owning scope corresponding to this pretty
    /// identifier.
    pub token_changes: BTreeMap<String, TokenDeltaDetails>,
}

/// Holds aggregated changes to symbols.
#[derive(Debug, Serialize, Deserialize)]
pub struct SymbolSyntaxDeltaGroup {
    /// Maps symbols to the deltas observed related to the symbol.  Note that
    /// "%" is a sentinel corresponding to there being no scope
    /// which is arbitrarily derived from prior blame processing logic.
    pub symbol_deltas: BTreeMap<String, SymbolSyntaxDelta>,
}

```

## tools/src/file_format/merger.rs
```
use std::collections::hash_map::DefaultHasher;
use std::collections::BTreeMap;
use std::collections::HashMap;
use std::collections::HashSet;
use std::hash::Hash;
use std::hash::Hasher;

extern crate regex;
use serde_json::to_value;
use serde_json::{from_value, json, to_string, Value};

use ustr::Ustr;

use super::analysis::AnalysisUnion;
use super::analysis::{
    read_analyses, AnalysisSource, AnalysisStructured, ExpansionInfo, Location, WithLocation,
};

#[derive(Debug)]
pub struct HashedStructured {
    pub platforms: Vec<usize>,
    pub loc: Location,
    pub data: AnalysisStructured,
}

/// Given a list of files and a matching parallel list of platform identifiers,
/// merge the records and write them to the provided writer.
///
/// This logic was extracted out from `merge-analyses.rs` for the purpose of
/// being able to test its logic through the introduction of `cmd_merge_analyses`.
///
/// The logic could almost certainly be further unified into the `cmd_pipeline`
/// data model, with callers potentially altered to use searchfox-tool and
/// eliminate the standalone merge-analyses.rs binary.  But there's no urgency.
pub fn merge_files<W: std::io::Write>(filenames: &[String], platforms: &[String], mut writer: W) {
    let mut unique_targets = HashSet::new();
    // Maps from symbol name to a HashMap<u64 hash, HashedStructured>
    let mut structured_syms = BTreeMap::new();

    let src_data = read_analyses(
        filenames,
        &mut |obj: Value, loc: &Location, i_file: usize| {
            if let Ok(unified) = from_value::<AnalysisUnion<Ustr>>(obj) {
                match unified {
                    AnalysisUnion::Source(mut src) => {
                        // return source objects so that they come out of `read_analyses` for
                        // additional processing below.

                        // populate the platform key of expansions and expanded symbols
                        use ExpansionInfo::*;
                        match src.expansion_info {
                            Some(ExpandsTo(ref mut expansions)) => {
                                for expansions in expansions.values_mut() {
                                    if let Some(unnamed_expansions) = expansions.remove("") {
                                        expansions
                                            .insert(platforms[i_file].clone(), unnamed_expansions);
                                    }
                                }
                            }
                            Some(InExpansionAt(ref mut offsets)) => {
                                for offsets in offsets.values_mut() {
                                    if let Some(unnamed_offsets) = offsets.remove("") {
                                        offsets.insert(platforms[i_file].clone(), unnamed_offsets);
                                    }
                                }
                            }
                            None => {}
                        }

                        return Some(src);
                    }
                    AnalysisUnion::Target(tgt) => {
                        // for target objects, just print them back out, but use the `unique_targets`
                        // hashset to deduplicate them.
                        let target_str = to_string(&WithLocation {
                            data: tgt,
                            loc: *loc,
                        })
                        .unwrap();
                        if !unique_targets.contains(&target_str) {
                            writeln!(writer, "{}", target_str).unwrap();
                            unique_targets.insert(target_str);
                        }
                    }
                    AnalysisUnion::Structured(structured) => {
                        // Structured objects may have different data for different platforms.  We
                        // detect this by building a map for each symbol from the hash of the string
                        // representation of their JSON encoding to the AnalysisStructured
                        // representation.  If, after processing the files we find there was a single
                        // hash, then we emit that record as we originally found it.  However, if there
                        // were multiple hashes, we pick the last.
                        //
                        // We used to have AnalysisStructured be hashable, but the `extra` Map was
                        // not currently hashable due to https://github.com/serde-rs/json/issues/747
                        // and in reality we just want to hash the JSON string, but it's already
                        // been parsed into a Value, which is why we're not using the string.
                        let variants = structured_syms
                            .entry(structured.sym)
                            .or_insert(HashMap::new());
                        let json_str = to_string(&structured).unwrap();
                        let mut hasher = DefaultHasher::new();
                        json_str.hash(&mut hasher);
                        let hash_key = hasher.finish();
                        let hs = variants.entry(hash_key).or_insert(HashedStructured {
                            platforms: vec![],
                            loc: *loc,
                            data: structured,
                        });
                        hs.platforms.push(i_file);
                    }
                }
            }
            None
        },
    );

    // For each bucket of source data at a given location, sort the source data by
    // the `pretty` field. This allows us to walk through the bucket and operate
    // with the assumption that entries with the same (location, pretty) tuple are
    // adjacent. If we do run into such entries we merge them to union the tokens
    // in the `syntax` and `sym` fields.
    for mut loc_data in src_data {
        loc_data.data.sort_by(|s1, s2| s1.pretty.cmp(&s2.pretty));
        let mut last_entry: Option<AnalysisSource> = None;
        for analysis_entry in std::mem::take(&mut loc_data.data) {
            match last_entry {
                Some(mut e) => {
                    if e.pretty == analysis_entry.pretty {
                        // the (loc, pretty) tuple on `analysis_entry` matches that
                        // on `last_entry` so we merge them
                        e.merge(analysis_entry);
                        last_entry = Some(e);
                    } else {
                        loc_data.data.push(e);
                        last_entry = Some(analysis_entry);
                    }
                }
                None => {
                    last_entry = Some(analysis_entry);
                }
            }
        }
        if let Some(e) = last_entry {
            loc_data.data.push(e);
        }
        // We can't convert WithLocation<Vec<T>> directly to JSON; we need to
        // spread the loc to each individual piece of data.
        let loc = loc_data.loc;
        for datum in loc_data.data {
            writeln!(
                writer,
                "{}",
                to_string(&WithLocation { loc, data: datum }).unwrap()
            )
            .unwrap();
        }
    }

    for (_id, mut hmap) in structured_syms {
        if hmap.len() == 1 {
            // There was only one variant of the structured info, so we can just use it as-is.
            let (_hash, hs) = hmap.drain().next().unwrap();
            writeln!(
                writer,
                "{}",
                to_string(&WithLocation {
                    loc: hs.loc,
                    data: hs.data
                })
                .unwrap()
            )
            .unwrap();
        } else {
            // There are multiple variants, so we want to:
            // 1. Pick one of the variants as the canonical variant.  For now our heuristic is to
            //    pick the highest platform index.  This is because the platform list is currently
            //    accomplished via wildcard that puts "android-armv7" first and that's a 32-bit
            //    platform, and we'd rather our defaults be 64-bit.
            // 2. Using the `extras` field, populate a `platforms` value in
            //    the canonical variant as well a `variants` field.  This should
            //    allow round-tripping while also avoiding us actually doing
            //    anything with this surplus-ish info which we expect to only be
            //    consumed by front-end JS UI at this time for the purposes of
            //    showing differing memory layouts across platforms.
            //
            // Prior to the conversion to serde_json, the `extras` field was a
            // JSON-string `payload` field and we just did a lot of sketchy
            // gluing together of raw JSON string fragments.

            // Do a pass to pick the best hash.
            let mut best_hash = 0;
            let mut best_plat = 0;
            for (hash, hs) in hmap.iter() {
                let local_plat = hs.platforms.iter().max().unwrap();
                if local_plat >= &best_plat {
                    best_plat = *local_plat;
                    best_hash = *hash;
                }
            }

            let mut hs = hmap.remove(&best_hash).unwrap();
            hs.data.extra.insert(
                "platforms".to_string(),
                json!(hs
                    .platforms
                    .iter()
                    .map(|x| platforms[*x].clone())
                    .collect::<Vec<String>>()),
            );
            hs.data.extra.insert(
                "variants".to_string(),
                hmap.into_values()
                    .map(|mut variant| {
                        variant.data.extra.insert(
                            "platforms".to_string(),
                            json!(variant
                                .platforms
                                .iter()
                                .map(|x| platforms[*x].clone())
                                .collect::<Vec<String>>()),
                        );
                        to_value(&variant.data).unwrap()
                    })
                    .collect(),
            );

            writeln!(
                writer,
                "{}",
                to_string(&WithLocation {
                    loc: hs.loc,
                    data: hs.data
                })
                .unwrap()
            )
            .unwrap();
        }
    }
}

```

## tools/src/file_format/config.rs
```
use std::collections::{BTreeMap, HashMap, HashSet};
use std::fs::{self, File};
use std::io::BufReader;
use std::io::Read;
use std::str;

use serde::{Deserialize, Serialize};

use git2::{Oid, Repository};

#[derive(Clone, Debug, Serialize, Deserialize)]
#[serde(rename_all = "lowercase")]
pub enum TreeCaching {
    Everything,
    Codesearch,
    Nothing,
}

#[derive(Clone, Debug, Serialize, Deserialize)]
#[serde(rename_all = "lowercase")]
pub enum TreeErrorHandling {
    /// Keep going, don't stop the indexing process.
    Continue,
    /// Generate an error and stop the indexing process.
    Halt,
}

/// Schema for the config.json files for loading; used to derive the actual
/// `Config` instance which also ends up including things like git info.
#[derive(Clone, Debug, Deserialize)]
pub struct ConfigJson {
    pub mozsearch_path: String,
    pub config_repo: String,
    /// What tree is the default for purposes of choosing which tree gets
    /// searched when viewing the root index page (which is derived from
    /// help.html).
    pub default_tree: Option<String>,
    /// What type of EC2 instance type to use for the web-server when it's spun
    /// up.
    pub instance_type: Option<String>,
    pub trees: BTreeMap<String, TreeConfigPaths>,

    #[serde(default)]
    pub allow_webtest: Option<bool>,
}

#[derive(Clone, Debug, Serialize, Deserialize)]
pub struct TreeConfigPaths {
    /// Tree priority; higher numbers mean more important.  Used to control the
    /// order in which we apply the `cache` directive.
    pub priority: u32,
    pub on_error: TreeErrorHandling,
    pub cache: TreeCaching,
    /// Absolute path to the root of the tree's index, INDEX_ROOT.
    pub index_path: String,
    /// Absolute path to the root of the checked out source tree which should be
    /// a sub-directory of the `index_path`.
    pub files_path: String,
    /// Absolute path to where the `.git` sub-directory can be located; this
    /// should certainly be the same as `files_path`, and this will be a thing
    /// even if the canonical revision control system is mercurial.
    pub git_path: Option<String>,
    /// Absolute path to where the blame repo is which should be a sub-directory
    /// of the `index_path`.
    pub git_blame_path: Option<String>,
    /// Absolute path to where the history sub-tree lives; this should be a
    /// sub-directory of the `index_path`.
    pub history_path: Option<String>,
    /// Absolute path to where generated files can be found, and which will then
    /// be mapped into `"__GENERATED__"`.  This will usually be a sub-directory
    /// of the `index_path` but exceptions could be possible.
    pub objdir_path: String,
    /// List of the path prefixes where files may be missing at the point of
    /// gathering metadata.
    #[serde(default)]
    pub ignore_missing_path_prefixes: Vec<String>,
    /// If this is actually a mercurial repo, the URL of the hg server, no
    /// trailing `/`.
    pub hg_root: Option<String>,
    /// Coverage server URL.
    pub ccov_root: Option<String>,
    /// Relative path within the source tree that's really a WPT root.
    pub wpt_root: Option<String>,
    /// If this is actually a git repo hosted on github, its URL.  If the repo
    /// isn't github, we'll need to learn other URL mapping support.
    pub github_repo: Option<String>,
    /// Absolute path to where we store the livegrep index.
    pub codesearch_path: String,
    /// Manually allocated port number to host the livegrep server on, starting
    /// from 8081 why not.
    pub codesearch_port: u32,
    /// Definitions of SCIP-based indexes to ingest.  Currently it's expected
    /// that the build script will handle downloading or generating the indexes.
    #[serde(default)]
    pub scip_subtrees: BTreeMap<String, ScipSubtreeConfig>,
}

#[derive(Clone, Debug, Serialize, Deserialize)]
pub struct ScipSubtreeConfig {
    /// The path the SCIP index can be found at.
    pub scip_index_path: String,
    /// The tree-relative path where the files referenced by the index can be
    /// found.  For example, if there's a JS subtree that lives at
    /// "components/foo" and that's the root where the build script runs
    /// scip-typescript, then that's the value to put here, because the index
    /// will have paths relative to that directory.  (And while the index will
    /// have a "ProjectRoot", we want to handle the case where the SCIP index
    /// was generated on another machine.)
    ///
    /// Leave this empty if the subtree is actually at the root of the tree.
    pub subtree_root: String,
}

pub struct GitData {
    pub repo: Repository,
    pub blame_repo: Option<Repository>,

    pub blame_map: HashMap<Oid, Oid>, // Maps repo OID to blame_repo OID.
    pub hg_map: HashMap<Oid, String>, // Maps repo OID to Hg rev.

    pub mailmap: Mailmap,
    /// Revs that we want to skip over during blame computation
    pub blame_ignore: BlameIgnoreList,
}

pub struct TreeConfig {
    pub paths: TreeConfigPaths,
    pub git: Option<GitData>,
}

impl TreeConfig {
    pub fn get_git(&self) -> Result<&GitData, &'static str> {
        match self.git {
            Some(ref git) => Ok(git),
            None => Err("History data unavailable"),
        }
    }

    pub fn get_git_path(&self) -> Result<&str, &'static str> {
        match self.paths.git_path {
            Some(ref git_path) => Ok(git_path),
            None => Err("History data unavailable"),
        }
    }

    pub fn find_source_file(&self, path: &str) -> String {
        if path.starts_with("__GENERATED__") {
            return path.replace("__GENERATED__", &self.paths.objdir_path);
        }
        format!("{}/{}", &self.paths.files_path, path)
    }

    pub fn should_ignore_missing_file(&self, path: &str) -> bool {
        for prefix in &self.paths.ignore_missing_path_prefixes {
            if path.starts_with(prefix) {
                return true;
            }
        }
        false
    }
}

pub struct Config {
    pub trees: BTreeMap<String, TreeConfig>,
    pub mozsearch_path: String,
    pub config_repo_path: String,
    // FIXME: Move these to TreeConfig.
    pub url_map_path: Option<String>,
    pub doc_trees_path: Option<String>,
}

impl Config {
    /// Synchronously read the contents of a file in the given tree's config
    /// directory, falling back to `MOZSEARCH/config_defaults/FILENAME` if
    /// available.
    pub fn read_tree_config_file_with_default(
        &self,
        filename: &str,
    ) -> Result<String, &'static str> {
        let repo_specific_path = format!("{}/{}", self.config_repo_path, filename);
        if let Ok(data_str) = std::fs::read_to_string(repo_specific_path) {
            return Ok(data_str);
        }
        let default_path = format!("{}/config_defaults/{}", self.mozsearch_path, filename);
        if let Ok(data_str) = std::fs::read_to_string(default_path) {
            return Ok(data_str);
        }
        Err("Unable to read the requested file")
    }

    /// Synchronously attempt to locate and read the contents of the given file
    /// at the given root using the given tree as context.  Documentation on the
    /// roots can be found on `SourceDescriptor`.
    pub fn maybe_read_file_from_given_root(
        &self,
        tree: &str,
        root: &str,
        file: &str,
    ) -> Result<Option<String>, &'static str> {
        let tree = self.trees.get(tree).unwrap();

        let path_root = match root {
            "config_repo" => &self.config_repo_path,
            "files" => &tree.paths.files_path,
            "index" => &tree.paths.index_path,
            "mozsearch" => &self.mozsearch_path,
            "objdir" => &tree.paths.objdir_path,
            _ => {
                return Err("invalid root specified");
            }
        };

        let full_path = format!("{}/{}", path_root, file);
        match fs::metadata(&full_path) {
            Ok(_) => match fs::read_to_string(full_path) {
                Ok(str) => Ok(Some(str)),
                // We should maybe convert to our server Result error or at least
                // dynamic strings, but for these static strings, let's have fun
                // with how useless this is!
                _ => Err("some kind of read error I guess"),
            },
            _ => Ok(None),
        }
    }
}

pub fn index_blame(
    blame_repo: &Repository,
    head_ref: Option<Oid>,
) -> (HashMap<Oid, Oid>, HashMap<Oid, String>) {
    let mut walk = blame_repo.revwalk().unwrap();
    if let Some(oid) = head_ref {
        walk.push(oid).unwrap();
    } else {
        walk.push_head().unwrap();
    }

    let mut blame_map = HashMap::new();
    let mut hg_map = HashMap::new();
    for r in walk {
        let oid = r.unwrap();
        let commit = blame_repo.find_commit(oid).unwrap();

        let msg = commit.message().unwrap();
        let pieces = msg.split_whitespace().collect::<Vec<_>>();

        let orig_oid = Oid::from_str(pieces[1]).unwrap();
        blame_map.insert(orig_oid, commit.id());

        if pieces.len() > 2 {
            let hg_id = pieces[3].to_owned();
            hg_map.insert(orig_oid, hg_id);
        }
    }

    (blame_map, hg_map)
}

pub fn load(
    config_path: &str,
    need_indexes: bool,
    only_tree: Option<&str>,
    url_map_path: Option<String>,
    doc_trees_path: Option<String>,
) -> Config {
    let config_file = File::open(config_path).unwrap();
    let mut reader = BufReader::new(&config_file);
    let mut input = String::new();
    reader.read_to_string(&mut input).unwrap();
    let config: ConfigJson = serde_json::from_str(&input).unwrap();

    let mut trees = BTreeMap::new();
    for (tree_name, paths) in config.trees {
        if let Some(only_tree_name) = only_tree {
            if tree_name != only_tree_name {
                continue;
            }
        }

        let git = match (&paths.git_path, &paths.git_blame_path) {
            (Some(git_path), Some(git_blame_path)) => {
                let repo = Repository::open(git_path).unwrap();
                let mailmap = Mailmap::load(&repo);
                let blame_ignore = BlameIgnoreList::load(&repo);

                let blame_repo = Repository::open(git_blame_path).unwrap();
                let (blame_map, hg_map) = if need_indexes {
                    index_blame(&blame_repo, None)
                } else {
                    (HashMap::new(), HashMap::new())
                };

                Some(GitData {
                    repo,
                    blame_repo: Some(blame_repo),
                    blame_map,
                    hg_map,
                    mailmap,
                    blame_ignore,
                })
            }
            (Some(git_path), &None) => {
                let repo = Repository::open(git_path).unwrap();
                let mailmap = Mailmap::load(&repo);
                let blame_ignore = BlameIgnoreList::load(&repo);

                Some(GitData {
                    repo,
                    blame_repo: None,
                    blame_map: HashMap::new(),
                    hg_map: HashMap::new(),
                    mailmap,
                    blame_ignore,
                })
            }
            _ => None,
        };

        trees.insert(tree_name, TreeConfig { paths, git });
    }

    Config {
        trees,
        mozsearch_path: config.mozsearch_path,
        config_repo_path: config.config_repo,
        url_map_path,
        doc_trees_path,
    }
}

#[derive(Hash, Eq, PartialEq, Debug)]
struct MailmapKey(Option<String>, Option<String>);

/// Mapping from names and emails to replace to the real names and emails for
/// these authors.
pub struct Mailmap {
    /// Map from old name and email to real name and email
    entries: HashMap<MailmapKey, MailmapKey>,
}

impl Mailmap {
    // Look up an entry in the mailmap, and return the real name and email.
    pub fn lookup<'a>(&'a self, name: &'a str, email: &'a str) -> (&'a str, &'a str) {
        // Unfortunately, we need to actually own our key strings due to type
        // matching & the keys being tuple-structs. I doubt this will have any
        // meaningful perf impact.

        // Try to look up with both name & email.
        let mut key = MailmapKey(Some(name.to_owned()), Some(email.to_owned()));
        if let Some(MailmapKey(new_name, new_email)) = self.entries.get(&key) {
            return (
                new_name.as_ref().map(String::as_str).unwrap_or(name),
                new_email.as_ref().map(String::as_str).unwrap_or(email),
            );
        }

        // Try looking up only by email.
        key.0 = None;
        if let Some(MailmapKey(new_name, new_email)) = self.entries.get(&key) {
            return (
                new_name.as_ref().map(String::as_str).unwrap_or(name),
                new_email.as_ref().map(String::as_str).unwrap_or(email),
            );
        }

        // Not in the mailmap, return it as-is.
        (name, email)
    }

    /// Load the Mailmap for the given repository.
    pub fn load(repo: &Repository) -> Self {
        // Repo may not have a mailmap file, in which case we can just generate
        // an empty one.
        Mailmap::try_load(repo).unwrap_or_else(|| Mailmap {
            entries: HashMap::new(),
        })
    }

    fn parse_line(mut line: &str) -> Option<(MailmapKey, MailmapKey)> {
        fn nonempty(s: &str) -> Option<String> {
            if s.is_empty() {
                None
            } else {
                Some(s.to_owned())
            }
        }

        // Remove text after a '#' comment from the line.
        line = line.split('#').next().unwrap();

        // name_a is the optional string before the first email.
        let idx = line.find('<')?;
        let name_a = nonempty(line[..idx].trim());
        line = &line[idx + 1..];

        // email_a is the required string until the end of the email block.
        let idx = line.find('>')?;
        let email_a = line[..idx].trim().to_owned();
        line = &line[idx + 1..];

        // name_b and email_b are optional. name_b requires email_b.
        let (name_b, email_b) = if let Some(idx) = line.find('<') {
            let name_b = nonempty(line[..idx].trim());
            line = &line[idx + 1..];

            let idx = line.find('>')?;
            let email_b = line[..idx].trim().to_owned();
            line = &line[idx + 1..];

            (name_b, Some(email_b))
        } else {
            (None, None)
        };

        // If we have junk at the end of the line - ignore it.
        if !line.trim().is_empty() {
            return None;
        }

        // Determine which format was being used, and build up our old and new
        // mailmap keys.
        let old;
        let new;
        if let Some(email_b) = email_b {
            new = MailmapKey(name_a, Some(email_a));
            old = MailmapKey(name_b, Some(email_b));
        } else {
            new = MailmapKey(name_a, None);
            old = MailmapKey(None, Some(email_a));
        }

        Some((old, new))
    }

    fn try_load(repo: &Repository) -> Option<Self> {
        // Get current mailmap from the repository.
        let obj = repo.revparse_single("HEAD:.mailmap").ok()?;
        let blob = obj.peel_to_blob().ok()?;
        let data = str::from_utf8(blob.content()).ok()?;

        // Parse each entry in turn
        let mut entries = HashMap::new();
        for line in data.lines() {
            if let Some((old, new)) = Mailmap::parse_line(line) {
                entries.insert(old, new);
            }
        }

        Some(Mailmap { entries })
    }
}

#[derive(Default)]
pub struct BlameIgnoreList {
    entries: HashSet<String>,
}

impl BlameIgnoreList {
    pub fn load(repo: &Repository) -> Self {
        // Produce an empty list if we fail to load anything
        BlameIgnoreList::try_load(repo).unwrap_or_default()
    }

    fn try_load(repo: &Repository) -> Option<Self> {
        let obj = repo.revparse_single("HEAD:.git-blame-ignore-revs").ok()?;
        let blob = obj.peel_to_blob().ok()?;
        let data = str::from_utf8(blob.content()).ok()?;

        let mut entries = HashSet::new();
        for line in data.lines() {
            let trimmed = line.split('#').next().unwrap().trim();
            // I guess we could also verify these are actually revisions but
            // that will eat CPU cycles for not much real benefit
            if !trimmed.is_empty() {
                entries.insert(trimmed.to_owned());
            }
        }

        Some(BlameIgnoreList { entries })
    }

    pub fn should_ignore(&self, rev: &str) -> bool {
        self.entries.contains(rev)
    }
}

impl GitData {
    pub fn should_ignore_for_blame(&self, rev: &str) -> bool {
        // TODO: we might want to pull the commit message and check for
        // special annotations like "#skip-blame" or backouts as well.
        // For now just check the list.
        self.blame_ignore.should_ignore(rev)
    }
}

```

## tools/src/file_format/globbing_file_list.rs
```
use globset::{GlobBuilder, GlobMatcher};

/// Parses `.gitignore` and `.eslintignore` style files so that a boolean test
/// can be run against a given file and see if it matches the file.  In most
/// cases it's probably more wise to instead use the output of whatever tool
/// would normally process this file rather than trying to replicate what it is
/// doing, but this is intended to help people get to a prototype or 80%
/// solution quickly.
///
/// We understand the file format to consist of:
/// - Comment lines starting with `#` which are ignored.
/// - Whitespace lines (after trimming/stripping) which are ignored.
/// - Negation lines which start with `!` and which are followed by a glob.
/// - Escaped lines start with a backslash to allow paths that start with the
///   above characters; the backslash will be removed.
/// - Everything else should be a glob.
///
/// In terms of glob semantics:
/// - We use the `globset` crate for this because we already use it.  It's
///   actually more capable than some of these formats require, but we don't
///   really care.
/// - We strip leading and trailing slashes on the glob patterns before handing
///   them to globset because mozsearch paths currently will not contain those
///   and we are not going to distinguish between files and directories at this
///   time in the way .gitignore will.
/// - We currently only evaluate on the full paths, effectively anchoring the
///   globs to the root, regardless of where the file came from.  This could be
///   enhanced for fidelity (and maybe a different mode introduced?) if someone
///   really cares.
///   - Additionally to this, we don't bother approximating the `.gitignore`
///     semantics where a glob applies at any level of the path unless there is
///     a leading `/` or a `/` in the middle of a path.  (A `/` at the end of
///     the glob is to distinguish directories from files.)   We could perhaps
///     approximate this by prepending a `**` if there's no leading/middle `/`
///     but as per the above, we don't need the fidelity right now, but aren't
///     opposed to adding it.  (Although I'm not sure we want to add a crate
///     dep for it.)
///
/// The general implementation for evaluating a filename:
/// - We maintain a list of globs, each of which has a negation state.  This is
///   populated when the struct is instantiated.
/// - We maintain a "matched" state which is initially false for each eval.
/// - If we match a negated glob, we clear the "matched" state.
/// - If we match a non-negated glob, we set the "matched" state.
/// - We return the "matched" state at the end.
pub struct GlobbingFileList {
    globs: Vec<(bool, GlobMatcher)>,
}

impl GlobbingFileList {
    pub fn new(file_contents: String) -> Self {
        let mut globs = vec![];

        for mut line in file_contents.lines() {
            line = line.trim();
            let mut negated = false;

            if line.is_empty() || line.starts_with("#") {
                continue;
            } else if line.starts_with("\\") {
                line = &line[1..];
            } else if line.starts_with("!") {
                negated = true;
                line = &line[1..];
            }

            // If the line ends with a "/" then add a ** glob on the end to make
            // it match all of its children too.
            let use_line = if line.ends_with("/") {
                format!("{}**", line)
            } else {
                line.to_string()
            };

            info!(
                "  glob: '{}' normalized to '{}' negated: {}",
                line, &use_line, negated
            );
            if let Ok(glob) = GlobBuilder::new(&use_line).literal_separator(true).build() {
                globs.push((negated, glob.compile_matcher()));
            }
        }

        GlobbingFileList { globs }
    }

    pub fn is_match(&self, path: &str) -> bool {
        let mut matches = false;

        for (negated, matcher) in &self.globs {
            if matcher.is_match(path) {
                // (I feel this reads better than assigning `!*negated`.)
                matches = !(*negated);
            }
        }

        matches
    }
}

```

## tools/src/file_format/ontology_pointer_kind.rs
```
use serde::{Deserialize, Serialize};

#[derive(Eq, PartialEq, Clone, Debug, Deserialize, Serialize)]
#[serde(rename_all = "lowercase")]
pub enum OntologyPointerKind {
    Strong,
    Unique,
    Weak,
    Raw,
    Ref,
    // Ex: JS::{Handle, Heap, MutableHandle, Rooted}.
    GCRef,
    Contains,
}

```

## tools/src/file_format/ontology_mapping.rs
```
use serde::Deserialize;
use ustr::{ustr, Ustr, UstrMap};

use crate::symbol_graph_edge_kind::EdgeKind;

pub use super::ontology_pointer_kind::OntologyPointerKind;

#[derive(Deserialize)]
pub struct OntologyMappingConfig {
    #[serde(default)]
    pub pretty: UstrMap<OntologyRule>,
    #[serde(default)]
    pub types: UstrMap<OntologyType>,
}

#[derive(Deserialize)]
pub struct OntologyRule {
    /// When specified, treats the given symbol/identifier as an nsIRunnable::Run
    /// style method where overrides should be treated as runnables and have
    /// ontology slots allocated to point to the concrete constructors (C++) or
    /// the class (Java/Kotlin reflection idiom)
    pub runnable: Option<OntologyRunnableMode>,
    /// Given a base class, find all of its subclasses which are expected to be
    /// inner classes and label the outer class that contains them.  This mainly
    /// exists for detecting cycle collection where we have an inner class that
    /// is glued to the containing class by macros.
    pub label_containing_class: Option<OntologyLabelContainingClass>,
    /// Given a base class, find all of its subclasses which are expected to be
    /// inner classes, walk out to the containing class, then process all of its
    /// fields' uses to see if any of them have a contextsym matching the given
    /// "context_sym_suffix" and apply the labels if so.
    ///
    /// This very much exists for labeling cycle collected fields where the
    /// traversal/unlink logic lives on an inner class that's glued to the
    /// outer class with macros.  This could potentially be less hacky in terms
    /// of the suffix mechanism, but there's not a clear upside at this point.
    pub label_containing_class_field_uses: Option<OntologyLabelContainingClassFieldUses>,
    /// Given a class that can be directly used as a field on objects, whenever
    /// we see a field with this type, label the owning class with the given
    /// labels.
    pub label_owning_class: Option<OntologyLabelOwningClass>,
    /// Labels that we always apply to the class.
    #[serde(default)]
    pub labels: Vec<Ustr>,
}

#[derive(Eq, PartialEq, Deserialize)]
#[serde(rename_all = "lowercase")]
pub enum OntologyRunnableMode {
    /// The original mode, appropriate for C++ and XPCOM, we assume a reference
    /// to the constructor will result in the runnable subequently running.
    Constructor,
    /// Introduce for the "androidx::work::Worker::doWork" Kotlin idiom where
    /// we only see references to the class in the analysis data, and not the
    /// constructor.
    Class,
}

#[derive(Deserialize)]
pub struct OntologyLabelContainingClassFieldUses {
    #[serde(default)]
    pub labels: Vec<OntologyContextSymLabelRule>,
}

#[derive(Deserialize)]
pub struct OntologyLabelContainingClass {
    #[serde(default)]
    pub labels: Vec<OntologyAlwaysLabelRule>,
}

#[derive(Clone, Deserialize)]
pub struct OntologyLabelOwningClass {
    #[serde(default)]
    pub labels: Vec<OntologyAlwaysLabelRule>,
}

#[derive(Clone, Deserialize)]
pub struct OntologyAlwaysLabelRule {
    pub label: Ustr,
}

#[derive(Deserialize)]
pub struct OntologyContextSymLabelRule {
    pub context_sym_suffix: Ustr,
    pub label: Ustr,
}

#[derive(Eq, PartialEq, Deserialize)]
#[serde(rename_all = "lowercase")]
pub enum OntologyType {
    /// A type like Atomic or IntializedOnce that provides notable semantics and
    /// so we should apply a label, but where the decorator type itself is not
    /// the underlying type of interest and we should continue processing its
    /// arguments like they existed without the decorator.
    Decorator(OntologyTypeDecorator),
    Pointer(OntologyTypePointer),
    /// Currently we assume a container has a >1 multiplicity.  We don't bother
    /// with pointer kind because we expect that to be a characteristic of the
    /// contained type.
    Container,
    Value,
    Variant,
    Nothing,
}

#[derive(Eq, PartialEq, Deserialize)]
pub struct OntologyTypePointer {
    pub kind: OntologyPointerKind,
    #[serde(default)]
    pub arg_index: u32,
}

#[derive(Eq, PartialEq, Deserialize)]
pub struct OntologyTypeDecorator {
    pub labels: Vec<Ustr>,
}

pub struct OntologyMappingIngestion {
    pub config: OntologyMappingConfig,
}

impl OntologyMappingIngestion {
    pub fn new(config_str: &str) -> Result<Self, String> {
        let config: OntologyMappingConfig =
            toml::from_str(config_str).map_err(|err| err.to_string())?;

        Ok(OntologyMappingIngestion { config })
    }
}

#[derive(Eq, PartialEq, Clone, Copy, Debug)]
enum TypeParseState {
    /// We're parsing a type.
    Typish,
    /// We've most recently seen a ">" and now don't care about whitespace and
    /// just expect to see either ">" or ","
    Closing,
}

#[derive(Default)]
struct ShoddyType {
    is_const: bool,
    is_pointer: bool,
    is_ref: bool,
    is_tag: bool,
    is_nothing: bool,
    identifier: String,
    args: Vec<ShoddyType>,
    // We set this to true if we've already put it in the results list,
    consumed: bool,
}

pub fn pointer_kind_to_badge_info(
    kind: &OntologyPointerKind,
) -> (i32, EdgeKind, &'static str, &'static str) {
    match kind {
        // the muscle arm thing
        OntologyPointerKind::Strong => (0, EdgeKind::Aggregation, "\u{1f4aa}", "ptr-strong"),
        // a snowflake, which is unique
        OntologyPointerKind::Unique => (0, EdgeKind::Aggregation, "\u{2744}\u{fe0f}", "ptr-unique"),
        // A calendar contains a week, right?  I'm sorry.  I have no idea
        // what to do here.
        OntologyPointerKind::Weak => (0, EdgeKind::Aggregation, "\u{1f4d3}\u{fe0f}", "ptr-weak"),
        // Eh, raw pointers are bad.  Face screaming in fear.
        OntologyPointerKind::Raw => (0, EdgeKind::Aggregation, "\u{1f631}", "ptr-raw"),
        // The "&" gets escaped so we if we use "&amp;" here we see "&amp;" in the UI.
        OntologyPointerKind::Ref => (0, EdgeKind::Aggregation, "&", "ptr-ref"),
        // "ginger" (it's a root!)
        OntologyPointerKind::GCRef => (0, EdgeKind::Aggregation, "\u{1fada}", "ptr-ref"),
        OntologyPointerKind::Contains => (0, EdgeKind::Composition, "\u{1f4e6}", "ptr-contains"),
    }
}

pub fn label_to_badge_info(label: &str) -> Option<(i32, &str)> {
    // Ignore all class-diagram directives, these are processed by cmd_traverse.
    if label.starts_with("calls-diagram:") {
        return None;
    }
    if label.starts_with("class-diagram:") {
        return None;
    }
    if label.starts_with("uses-diagram:") {
        return None;
    }

    match label {
        // "atom symbol" for atomic refcount.  We also will have an "rc"
        // label, so we don't bother including its label
        "arc" => Some((10, "\u{269b}\u{fe0f}")),
        // "atomic" symbol for fields where there was an Atomic<>, although
        // usually these will be value types and won't show up in diagrams.
        "atomic" => Some((10, "\u{269b}\u{fe0f}")),
        // "link symbol" for "cc"
        "cc" => Some((11, "\u{1f517}")),
        // chains for ccrc; I think maybe "cc" and "ccrc" may be redundant
        "ccrc" => Some((5, "\u{26d3}\u{fe0f}")),
        // Link followed by a pencil, for "tracing"
        "cc-trace" => Some((20, "\u{1f517}\u{270f}\u{fe0f}")),
        // Link followed by a left-pointing magnifying glass
        "cc-traverse" => Some((21, "\u{1f517}\u{1f50d}")),
        // "Broken Chain" is an emoji 15.1 ZWJ sequence of chains and collision
        "cc-unlink" => Some((22, "\u{26d3}\u{fe0f}\u{200d}\u{1f4a5}")),
        // "abacus" for reference counted.
        "rc" => Some((11, "\u{1f9ee}")),
        _ => Some((100, label)),
    }
}

impl OntologyMappingConfig {
    /// Shoddily parse the type, looking up the types we find, seeing if this
    /// type seems to represent a pointer type.  If we identify a pointer type,
    /// we return the pointer kind (strong, unique, weak, raw) and the pretty
    /// identifier for the type which we can probably look up.
    ///
    /// The motivating situation here is:
    /// - For structured C++ fields, the "typesym" we have is just something
    ///   like "T_RefPtr" or "T_InitializedOnce" (plus namespace), and lacks
    ///   the information we actually need, so we currently need to parse it
    ///   out of the "type".  In the future we can potentially enhance the C++
    ///   indexer, but that's a non-trivial amount of work and out-of-scope at
    ///   the current time.
    /// - We just want to know the class being pointed at and the kind of the
    ///   pointer; we don't really care or want any extra type-magic like
    ///   if "InitializedOnce" or "Maybe" is used at this time.
    /// - In the future we may want to understand extra type annotations that
    ///   indicate if something is nullable, when it's initialized, etc. so it's
    ///   nice to support that.
    /// - We want to be able to distinguish "SafeRefPtr" from "RefPtr" which a
    ///   regex based solution might get tripped up on.
    ///
    /// So this is:
    /// - Intended to be slightly better than a regexp for being able to apply
    ///   simple type rules based on what we see in the type signature.
    /// - Not intended to grow or become more sophisticated than being able to
    ///   build a simple tree with very simple rules.  We have access to clang
    ///   and all the info it has, and we should just use that as the next step.
    ///
    /// TODO: Distinguish a ref to a strong pointer from just a ref.
    /// - We should already be able to do this, but this is more of a question of
    ///   how/whether to reflect this in the diagram.  Also, it raises the issue
    ///   of whetehr we should be potentially propagating more of `ShoddyType`
    ///   directly.
    ///
    /// TODO: Move to returning an explicit new return type in the option, which
    /// may or may not make sense to be wrapped in a vec.  Visually:
    /// - For maps the key and value may way to be on separate rows to avoid
    ///   arrow crossings.  The type name should be implicit in the target,
    ///   though.
    /// - Potential badges:
    ///   - Atom for atomic: U+269B \u269B
    ///   -
    ///
    /// Definitely real issues from llvm:
    /// - `llvm::MachineInstrBundleIterator::operator->` we get confused:
    ///   "Saw '>' when not in an argument"
    ///
    /// XXX Previously Pending issues that maybe I fixed some:
    /// - closing state hates commas and then the new type:
    ///   `AutoTArray<RefPtr<nsFrameSelection>, 1>`
    /// - `NotNull<nsCOMPtr<mozIStorageConnection> >` getting id clobber
    ///   `prev_id="nsCOMPtr" new_id=""`
    /// - `nsTArray<Accessible *>` the space trips us up.
    /// - Also maybe:
    ///   `Got an identifier when already had an identifier! type_str="Maybe<BufferPointer<BaselineFrame> >" prev_id="BufferPointer" new_id=""`
    /// - Also for arrays, seems like we should propagate that.
    ///   - `Vector<UniquePtr<ProfiledThreadData> >` is an interesting case of that.
    ///   - `const std::vector<HashMgr *> &`
    ///   - `Vector<char *>`
    ///   - `AutoTArray` in addition to `nsTArray`, `mozilla::Array`, nsTObserverArray
    /// - Also sets like HashSet
    /// - Native arrays?
    ///   - `"UniquePtr<char[]>"`
    ///   - `UniquePtr<nscoord[]>`
    /// - maybe just bail on functions because of complex signatures, ex tame:
    ///   `std::queue<std::function<void (void)> >`
    /// - Maybe just bail on unions?  ex:
    ///   `union AllocInfo::(anonymous at /builds/worker/checkouts/gecko/memory/build/mozjemalloc.cpp:3508:3)`
    /// - Similar with enums:
    ///   `enum (unnamed enum at /builds/worker/checkouts/gecko/xpcom/tests/gtest/TestMultiplexInputStream.cpp:503:3)`
    /// - Maybe need to "Evaluate" only on first arg for cases like
    ///   `UniquePtr<Utf8Unit[], JS::FreePolicy>` where right now we only call on the FreePolicy.
    ///
    /// Other domain situations:
    /// - `Rooted<AbstractGeneratorObject *>`
    /// - `Atomic<_Bool>`
    /// - DataMutex, StaticDataMutex
    ///   - So for these I think it makes sense for this to be a bool that gets mapped to an atomic emoji.
    /// - Maps: `nsTHashtable<CategoryLeaf>`, std::map
    ///   - For std::map need to reference key and value types!
    ///
    /// Complex scenarios:
    /// - `HashSet<gc::Cell *, DefaultHasher<gc::Cell *>, SystemAllocPolicy>` hates the closing state
    pub fn maybe_parse_type_as_pointer(
        &self,
        type_str: &str,
    ) -> (Vec<(OntologyPointerKind, Ustr)>, Vec<Ustr>) {
        let mut c = type_str.chars();
        let mut state = TypeParseState::Typish;
        let mut type_stack: Vec<ShoddyType> = vec![];
        let mut cur_type = ShoddyType::default();
        let mut token = String::new();

        let mut results: Vec<(OntologyPointerKind, Ustr)> = vec![];

        let mut labels_to_apply = vec![];

        loop {
            let next_c = c.next();

            match (state, next_c) {
                (TypeParseState::Typish, None) => break,
                // Whitespace can happen in a few cases:
                // - After "const", so token.len() > 0
                // - After a legit token just before a "*".
                // - After a ",", so token.len() == 0
                // - After a ">", but we handle that via the `Closing` state.
                (TypeParseState::Typish, Some(' ')) => {
                    if !token.is_empty() {
                        if token.as_str() == "const" {
                            cur_type.is_const = true;
                        } else if token.as_str() == "union" {
                            // we can't do anything useful with unions.
                            return (results, labels_to_apply);
                        } else if token.as_str() == "enum" {
                            // we can't do anything useful with enums.
                            return (results, labels_to_apply);
                        } else if token.as_str() == "class" || token.as_str() == "struct" {
                            cur_type.is_tag = true;
                        } else if token.chars().all(char::is_numeric) {
                            // If our current token is just numeric then we're quite
                            // possibly looking at something like
                            // "1 << sizeof(AnonymousContentKey) * 8" as a template
                            // arg.  This is a complex case that I think really
                            // emphasizes we should just move to having clang give
                            // us a structured representation of the types and stop
                            // fooling around.  So we're just going to early return
                            // in this case rather than go down a shoddy parsing
                            // rabbit hole.
                            return (results, labels_to_apply);
                        } else {
                            if !cur_type.identifier.is_empty() {
                                info!(
                                    type_str,
                                    prev_id = cur_type.identifier,
                                    new_id = token,
                                    "Got an identifier when already had an identifier!"
                                );
                            }
                            cur_type.identifier = token;
                        }
                        token = String::new();
                    }
                    // otherwise this is probably after a comma.
                }
                (TypeParseState::Typish, Some('*')) => {
                    cur_type.is_pointer = true;
                    token = String::new();
                }
                (TypeParseState::Typish, Some('&')) => {
                    cur_type.is_ref = true;
                }
                (TypeParseState::Typish, Some('<')) => {
                    if !cur_type.identifier.is_empty() {
                        info!(
                            type_str,
                            prev_id = cur_type.identifier,
                            new_id = token,
                            "Got an identifier when already had an identifier!"
                        );
                    }
                    cur_type.identifier = token;
                    token = String::new();

                    type_stack.push(cur_type);
                    cur_type = ShoddyType::default();
                }
                (TypeParseState::Typish, Some(',')) => {
                    if !cur_type.identifier.is_empty() {
                        info!(
                            type_str,
                            prev_id = cur_type.identifier,
                            new_id = token,
                            "Got an identifier when already had an identifier!"
                        );
                    }
                    cur_type.identifier = token;
                    token = String::new();

                    // Evaluate the types now that cur_type is updated.
                    //
                    // TODO: Consider unifying with the `>` closing a bit more.
                    // Right now this is just to help mark the nothing type,
                    // and we don't really need to process pointers here because
                    // those will have a > and then be in closing and then see a
                    // ',', but we can do better or at least add more comments.
                    let parent_name = ustr(&cur_type.identifier);
                    if let Some(OntologyType::Nothing) = self.types.get(&parent_name) {
                        cur_type.is_nothing = true;
                    }

                    if let Some(container_type) = type_stack.last_mut() {
                        container_type.args.push(cur_type);
                        cur_type = ShoddyType::default();
                    } else {
                        info!(type_str, "Hit comma with no parent type!");
                        return (results, labels_to_apply);
                    }
                }
                (TypeParseState::Typish, Some('>')) | (TypeParseState::Closing, Some('>')) => {
                    // In the closing state we don't process the token.
                    if state == TypeParseState::Typish && !token.is_empty() {
                        if !cur_type.identifier.is_empty() {
                            info!(
                                type_str,
                                prev_id = cur_type.identifier,
                                new_id = token,
                                "Got an identifier when already had an identifier!"
                            );
                        }
                        cur_type.identifier = token;
                        token = String::new();
                    }

                    // A type is being closed out, the cur_type goes in the parent,
                    // and the parent becomes the new cur_type.
                    let done_type = cur_type;
                    cur_type = match type_stack.pop() {
                        Some(t) => t,
                        None => {
                            info!(type_str, "Unpaired '>' encountered!");
                            return (results, labels_to_apply);
                        }
                    };
                    cur_type.args.push(done_type);

                    // Evaluate the types now that cur_type is updated.
                    let parent_name = ustr(&cur_type.identifier);
                    let process_args = match self.types.get(&parent_name) {
                        Some(OntologyType::Decorator(dec)) => {
                            for label in &dec.labels {
                                labels_to_apply.push(*label);
                            }
                            // Process the arguments on their own still.
                            true
                        }
                        Some(OntologyType::Container) => {
                            // TODO: we should be setting a multiplicity flag that
                            // should be propagated to the pointer info.

                            // Process the arguments on their own still.
                            true
                        }
                        Some(OntologyType::Pointer(ptr)) => {
                            if let Some(arg_type) = cur_type.args.get(ptr.arg_index as usize) {
                                let pointee_name = ustr(&arg_type.identifier);
                                info!(
                                    type_str,
                                    parent_name = cur_type.identifier,
                                    pointee_name = pointee_name.as_str(),
                                    "evaluating"
                                );

                                if arg_type.is_tag
                                    && self.types.get(&pointee_name) != Some(&OntologyType::Value)
                                {
                                    results.push((ptr.kind.clone(), pointee_name));
                                }
                                cur_type.consumed = true;
                            }
                            // We've notionally consumed the argument(s) here.  If the pointer type
                            // itself was something like a refcounted data structure type that in
                            // turn can hold other types (and is defined as a pointer), then we
                            // would have already processed tha type at its ">".
                            false
                        }
                        Some(OntologyType::Variant) => true,
                        Some(OntologyType::Nothing) => {
                            cur_type.is_nothing = true;
                            false
                        }
                        Some(OntologyType::Value) => {
                            // With the introduction of "container" we now intentionally want to
                            // ignore the arguments, although we have not yet done anything to
                            // precldue nested arguments from being suppressed.  It's still
                            // currently the case that a value type could have a `RefPtr<Foo>` and
                            // we'd process that.  It probably makes sense to wait for an example
                            // where that happens and we definitely don't want to process that.
                            //
                            // Another possibility is to consider the types in this case but
                            // generate some kind of diagnostic marker that the type defies our
                            // expectations and should potentially be reconsidered.  If those
                            // cases where this happens should indeed suppress the nested type,
                            // we would add a field to value or an alternate form of value that
                            // explicitly is intentionally suppressing its contents.
                            false
                        }
                        None => {
                            // We have no information about the parent type, so we're not
                            // going to do anything about its argument(s).
                            false
                        }
                    };
                    if process_args {
                        // Push all the non-nothing types that weren't already pushed.
                        for arg_type in &cur_type.args {
                            if arg_type.consumed || arg_type.is_nothing {
                                continue;
                            }
                            if arg_type.is_pointer {
                                results
                                    .push((OntologyPointerKind::Raw, ustr(&arg_type.identifier)));
                            } else if arg_type.is_ref {
                                results
                                    .push((OntologyPointerKind::Ref, ustr(&arg_type.identifier)));
                            } else if arg_type.is_tag {
                                if let Some(OntologyType::Value) =
                                    self.types.get(&ustr(&arg_type.identifier))
                                {
                                    // We don't record value types.
                                } else {
                                    results.push((
                                        OntologyPointerKind::Contains,
                                        ustr(&arg_type.identifier),
                                    ));
                                }
                            }
                        }
                    }

                    state = TypeParseState::Closing;
                }
                (TypeParseState::Typish, Some(c)) => {
                    token.push(c);
                }

                (TypeParseState::Closing, None) => {
                    assert_eq!(type_stack.len(), 0, "Should have closed all types.");
                    break;
                }
                (TypeParseState::Closing, Some(' ')) => {
                    // Whitespace doesn't mattern when closing.
                }
                (TypeParseState::Closing, Some(',')) => {
                    if let Some(container_type) = type_stack.last_mut() {
                        container_type.args.push(cur_type);
                        cur_type = ShoddyType::default();
                    } else {
                        info!(type_str, "Hit comma with no parent type!");
                        return (results, labels_to_apply);
                    }
                    // We're no longer closing.
                    state = TypeParseState::Typish;
                }
                (TypeParseState::Closing, Some('*')) => {
                    cur_type.is_pointer = true;
                }
                (TypeParseState::Closing, Some('&')) => {
                    cur_type.is_ref = true;
                }
                (TypeParseState::Closing, Some(c)) => {
                    info!(type_str, "Unexpected character in closing state: '{}'", c);
                }
            }
        }

        if !token.is_empty() {
            cur_type.identifier = token;
        }

        if results.is_empty() && !cur_type.consumed {
            if cur_type.is_pointer {
                info!(
                    type_str,
                    type_name = cur_type.identifier,
                    "fallback to pointer on exit"
                );
                results.push((OntologyPointerKind::Raw, ustr(&cur_type.identifier)));
            } else if cur_type.is_ref {
                info!(
                    type_str,
                    type_name = cur_type.identifier,
                    "fallback to ref on exit"
                );
                results.push((OntologyPointerKind::Ref, ustr(&cur_type.identifier)));
            } else if cur_type.is_tag {
                if let Some(OntologyType::Value) = self.types.get(&ustr(&cur_type.identifier)) {
                    // If the type is a value type, like nsTString, fall through to None.
                } else {
                    results.push((OntologyPointerKind::Contains, ustr(&cur_type.identifier)));
                }
            }
        }

        (results, labels_to_apply)
    }
}

#[test]
fn test_type_parser() {
    let test_config = r#"
[types."nsCOMPtr".pointer]
kind = "strong"

# explicitly not in the mozilla namespace
[types."RefPtr".pointer]
kind = "strong"

[types."mozilla::UniquePtr".pointer]
kind = "unique"

[types."UniquePtr".pointer]
kind = "unique"

[types."mozilla::WeakPtr".pointer]
kind = "weak"

[types."WeakPtr".pointer]
kind = "weak"

[types."nsClassHashtable".pointer]
kind = "unique"
arg_index = 1

[types."nsTArray".container]
[types."nsTString".value]

[types."mozilla::Atomic".decorator]
labels = ["atomic"]

# ### Variant Types ###
[types."mozilla::Variant".variant]

# ### Sentinel Nothing Types ###
[types."mozilla::Nothing".nothing]

[types."mozilla::Maybe".pointer]
kind = "contains"
"#;
    let ingestion = OntologyMappingIngestion::new(test_config).unwrap();
    let c = &ingestion.config;

    assert_eq!(c.maybe_parse_type_as_pointer("_Bool"), (vec![], vec![]));

    // Note that some of these real-world examples pre-date our change to use the
    // canonical type which gets us fully qualified namespaces, so these won't
    // match reality.
    assert_eq!(
        c.maybe_parse_type_as_pointer("class RefPtr<class outer::inner::Actual>"),
        (
            vec![(OntologyPointerKind::Strong, ustr("outer::inner::Actual"))],
            vec![]
        )
    );

    assert_eq!(
        c.maybe_parse_type_as_pointer("UniquePtr<class Poodle, JS::FreePolicy>"),
        (vec![(OntologyPointerKind::Unique, ustr("Poodle"))], vec![])
    );

    assert_eq!(
        c.maybe_parse_type_as_pointer("AutoTArray<RefPtr<class nsFrameSelection>, 1>"),
        (
            vec![(OntologyPointerKind::Strong, ustr("nsFrameSelection"))],
            vec![]
        )
    );

    assert_eq!(
        c.maybe_parse_type_as_pointer("NotNull<nsCOMPtr<class mozIStorageConnection> >"),
        (
            vec![(OntologyPointerKind::Strong, ustr("mozIStorageConnection"))],
            vec![]
        )
    );

    assert_eq!(
        c.maybe_parse_type_as_pointer("NotNull<nsCOMPtr<class mozIStorageConnection> >"),
        (
            vec![(OntologyPointerKind::Strong, ustr("mozIStorageConnection"))],
            vec![]
        )
    );

    assert_eq!(
        c.maybe_parse_type_as_pointer("union AllocInfo::(anonymous at /builds/worker/checkouts/gecko/memory/build/mozjemalloc.cpp:3508:3)"),
        (vec![], vec![])
    );

    assert_eq!(
        c.maybe_parse_type_as_pointer(
            "class nsClassHashtable<class nsCStringHashKey, class RegistrationDataPerPrincipal>"
        ),
        (
            vec![(
                OntologyPointerKind::Unique,
                ustr("RegistrationDataPerPrincipal")
            )],
            vec![]
        )
    );

    assert_eq!(
        c.maybe_parse_type_as_pointer("nsTArray<RefPtr<class SyntheticExample> >"),
        (
            vec![(OntologyPointerKind::Strong, ustr("SyntheticExample"))],
            vec![]
        )
    );

    assert_eq!(
        c.maybe_parse_type_as_pointer("nsTArray<class SyntheticExample *>"),
        (
            vec![(OntologyPointerKind::Raw, ustr("SyntheticExample"))],
            vec![]
        )
    );

    assert_eq!(
        c.maybe_parse_type_as_pointer("HashSet<RefPtr<class ServiceWorkerRegistrationInfo>, class PointerHasher<ServiceWorkerRegistrationInfo*>>"),
        (vec![(OntologyPointerKind::Strong, ustr("ServiceWorkerRegistrationInfo"))], vec![])
    );

    // const struct mozilla::dom::locks::LockRequest
    assert_eq!(
        c.maybe_parse_type_as_pointer("const struct mozilla::dom::locks::LockRequest"),
        (
            vec![(
                OntologyPointerKind::Contains,
                ustr("mozilla::dom::locks::LockRequest")
            )],
            vec![]
        )
    );

    assert_eq!(
        c.maybe_parse_type_as_pointer("class nsTString<char16_t>"),
        (vec![], vec![])
    );

    assert_eq!(
        c.maybe_parse_type_as_pointer("class mozilla::Variant<struct mozilla::Nothing, class RefPtr<class nsPIDOMWindowInner>, class nsCOMPtr<class nsIDocShell>, class mozilla::dom::WorkerPrivate *>"),
        (vec![
            (OntologyPointerKind::Strong, ustr("nsPIDOMWindowInner")),
            (OntologyPointerKind::Strong, ustr("nsIDocShell")),
            (OntologyPointerKind::Raw, ustr("mozilla::dom::WorkerPrivate"))
        ], vec![])
    );

    assert_eq!(
        c.maybe_parse_type_as_pointer("class mozilla::Atomic<class mozilla::dom::WorkerPrivate *>"),
        (
            vec![(
                OntologyPointerKind::Raw,
                ustr("mozilla::dom::WorkerPrivate")
            )],
            vec![ustr("atomic")]
        )
    );

    assert_eq!(
        c.maybe_parse_type_as_pointer("class mozilla::Maybe<class nsTString<char16_t> >"),
        (vec![], vec![])
    );

    assert_eq!(
        c.maybe_parse_type_as_pointer(
            "Array<std::pair<uint8_t, uint8_t>, 1 << sizeof(AnonymousContentKey) * 8>"
        ),
        (vec![], vec![])
    );
}

```

## tools/src/file_format/crossref_converter.rs
```
use bitflags::bitflags;
use serde_json::{from_value, json, Map, Value};
use ustr::Ustr;

use super::analysis::{BindingSlotKind, BindingSlotLang, StructuredBindingSlotInfo};

/// Transform a crossref Value that will be written into crossref into the
/// digested representation we emit into the SYM_INFO structure for source
/// listings and diagrams.  This method also takes some fallback information for
/// population of the representation if the value is Null or lacks a "meta"
/// key.  (Currently "meta" key contents only come from structured analysis
/// records, but this may change in the future.)
///
/// Note: We actually have quite strong invariants about the data we're consuming
/// here but some of the JSON processing logic is written in a more defensive
/// idiom (if let) than it needs to be, especially since for a lot of values we
/// just unwrap them.
pub fn convert_crossref_value_to_sym_info_rep(
    cross_val: Value,
    sym: &Ustr,
    fallback_pretty: Option<&Ustr>,
) -> Value {
    // Process a path hit-list if there's a single hit inside it, writing an
    // entry with the given jump_kind.
    let jumpify = |path_hits: Option<Value>, jump_kind: &str, jump_map: &mut Map<String, Value>| {
        if let Some(Value::Array(mut paths)) = path_hits {
            if paths.len() != 1 {
                return;
            }
            if let Value::Object(mut path_hit) = paths.remove(0) {
                let path_val = path_hit.remove("path").unwrap();
                let path = path_val.as_str().unwrap();
                if let Some(Value::Array(mut lines)) = path_hit.remove("lines") {
                    if lines.len() != 1 {
                        return;
                    }
                    if let Value::Object(line_hit) = lines.remove(0) {
                        let lno = line_hit.get("lno").unwrap().as_u64().unwrap();
                        jump_map.insert(jump_kind.to_string(), json!(format!("{}#{}", path, lno)));
                    }
                }
            }
        }
    };

    match cross_val {
        Value::Object(mut xref) => {
            let mut rep = Map::new();
            rep.insert("sym".to_string(), json!(sym.to_string()));

            if let Some((key, meta)) = xref.remove_entry("meta") {
                // Favor the "pretty" value from the meta since it may eventually
                // start doing more clever things.
                if let Some(Value::String(pretty)) = meta.get("pretty") {
                    rep.insert("pretty".to_string(), json!(pretty.clone()));
                } else {
                    rep.insert(
                        "pretty".to_string(),
                        json!(fallback_pretty.unwrap_or(sym).to_string()),
                    );
                }
                rep.insert(key, meta);
            } else {
                rep.insert(
                    "pretty".to_string(),
                    json!(fallback_pretty.unwrap_or(sym).to_string()),
                );
            }

            let mut jumps = Map::new();
            jumpify(xref.remove("idl"), "idl", &mut jumps);
            jumpify(xref.remove("defs"), "def", &mut jumps);
            jumpify(xref.remove("decls"), "decl", &mut jumps);

            // TODO: Need to handle the IDL search permutations issue that currently allows
            // the language indexer to define multiple symbol groupings.

            if !jumps.is_empty() {
                rep.insert("jumps".to_string(), json!(jumps));
            }

            json!(rep)
        }
        _ => {
            json!({
                "sym": sym.to_string(),
                "pretty": fallback_pretty.unwrap_or(sym).to_string(),
            })
        }
    }
}

bitflags! {
    #[derive(Clone, Copy)]
    /// Bitflag that allows us to express what additional traversals we want of
    /// a jumpref symbol as returned by
    /// `determine_desired_extra_syms_from_jumpref`.  And when populating such
    /// a map, the bitflags let us track what traversals we have already
    /// performed for a symbol.
    pub struct JumprefTraversals: u32 {
        /// We called `determine_desired_extra_syms_from_jumpref` and processed
        /// the results.
        const NormalExtra  = 0b00000001;
        /// We want to traverse any "recv" binding slots.
        const Receive      = 0b00000010;
        /// We want to traverse any overriddenBy values.
        const OverriddenBy = 0b00000100;
    }
}

/// Helper that processes a jumpref-formatted Value in order to extract any
/// binding slot symbols that should also be looked up.
pub fn determine_desired_extra_syms_from_jumpref(
    jumpref: &Value,
) -> Vec<(String, JumprefTraversals)> {
    let mut extra_syms = vec![];
    if let Some(owner) = jumpref.pointer("/meta/slotOwner") {
        let next_step = match owner["slotKind"].as_str() {
            Some("send") => JumprefTraversals::Receive,
            _ => JumprefTraversals::empty(),
        };
        if let Some(Value::String(sym)) = owner.get("sym") {
            extra_syms.push((sym.clone(), next_step));
        }
    }
    if let Some(Value::Array(slots)) = jumpref.pointer("/meta/bindingSlots") {
        for slot in slots {
            if let Ok(slot_info) = from_value::<StructuredBindingSlotInfo>(slot.clone()) {
                let next_step = match (slot_info.props.slot_kind, slot_info.props.slot_lang) {
                    // For IDL symbols in an .idl file, this implies XPIDL (we could check more
                    // thoroughly) and the binding slot will reference the pure virtual decl that
                    // we upgrade to a def.  That's not useful, so we also want to traverse its
                    // overridenBy edges so we can provide go to the actual impl definition.
                    (BindingSlotKind::Method, BindingSlotLang::Cpp) => {
                        JumprefTraversals::OverriddenBy
                    }
                    _ => JumprefTraversals::empty(),
                };
                extra_syms.push((slot_info.sym.to_string(), next_step));
            }
        }
    }
    if let Some(Value::Array(overridden)) = jumpref.pointer("/meta/overriddenBy") {
        if overridden.len() <= 2 {
            for over_info in overridden {
                if let Value::String(over_sym) = over_info {
                    // The override is all we need.
                    extra_syms.push((over_sym.to_owned(), JumprefTraversals::empty()));
                }
            }
        }
    }
    extra_syms
}

/// Given a jumpref with a next step, return any additional symbols that should
/// be looked up.  Currently, we do not allow additional next steps to be
/// requested because normally we only need the level of indirection of traversing
/// to the binding slot owner and then via one of its binding slots, for example
/// to get from the IPC send method to the IPC receive method.
pub fn extra_syms_next_step_lookups(
    jumpref: &Value,
    next_step: JumprefTraversals,
) -> Vec<(String, JumprefTraversals)> {
    let mut extra_syms = vec![];
    if next_step.contains(JumprefTraversals::Receive) {
        if let Some(Value::Array(slots)) = jumpref.pointer("/meta/bindingSlots") {
            for slot in slots {
                if let Ok(slot_info) = from_value::<StructuredBindingSlotInfo>(slot.clone()) {
                    if slot_info.props.slot_kind == BindingSlotKind::Recv {
                        extra_syms.push((slot_info.sym.to_string(), JumprefTraversals::empty()));
                    }
                }
            }
        }
    }
    if next_step.contains(JumprefTraversals::OverriddenBy) {
        // this is copied from the same logic in determine_desired_extra_syms_from_jumpref.
        if let Some(Value::Array(overridden)) = jumpref.pointer("/meta/overriddenBy") {
            if overridden.len() <= 2 {
                for over_info in overridden {
                    if let Value::String(over_sym) = over_info {
                        // The override is all we need.
                        extra_syms.push((over_sym.to_owned(), JumprefTraversals::empty()));
                    }
                }
            }
        }
    }

    extra_syms
}

```

## tools/src/file_format/crossref_lookup.rs
```
extern crate memmap;

use self::memmap::Mmap;
use std::fs::File;
use std::str;
use std::sync::Arc;
use std::{cmp::Ordering, collections::HashMap};

use serde_json::{from_slice, Value};

use crate::{
    abstract_server::Result,
    abstract_server::{ErrorDetails, ErrorLayer, ServerError},
};

use super::config::Config;

#[derive(Clone, Debug)]
pub struct CrossrefLookupMap {
    inline_mm: Arc<Mmap>,
    extra_mm: Arc<Mmap>,
}

const SPACE: u8 = b' ';
const NEWLINE: u8 = b'\n';
const ID_START: u8 = b'!';
const INLINE_STORED: u8 = b':';
const EXTERNALLY_STORED: u8 = b'@';

fn make_crossref_data_error(sym: &str) -> ServerError {
    ServerError::StickyProblem(ErrorDetails {
        layer: ErrorLayer::DataLayer,
        message: format!("bad crossref data for symbol: {}", sym),
    })
}

// This implementation is a port of `crossrefs.py` (which was adapted from
// `identifiers.py`) and informed by `identifiers.rs` (which presumably was
// adapted from `identifiers.py` as well).
impl CrossrefLookupMap {
    pub fn new(inline_path: &str, extra_path: &str) -> Option<CrossrefLookupMap> {
        let inline_file = File::open(inline_path).unwrap();
        let inline_mm = unsafe {
            match Mmap::map(&inline_file) {
                Ok(mmap) => Arc::new(mmap),
                Err(_) => return None,
            }
        };
        let extra_file = File::open(extra_path).unwrap();
        let extra_mm = unsafe {
            match Mmap::map(&extra_file) {
                Ok(mmap) => Arc::new(mmap),
                Err(_) => return None,
            }
        };
        Some(CrossrefLookupMap {
            inline_mm,
            extra_mm,
        })
    }

    pub fn load(config: &Config) -> HashMap<String, Option<CrossrefLookupMap>> {
        let mut result = HashMap::new();
        for (tree_name, tree_config) in &config.trees {
            println!("Loading crossref {}", tree_name);
            let inline_path = format!("{}/crossref", tree_config.paths.index_path);
            let extra_path = format!("{}/crossref-extra", tree_config.paths.index_path);
            let map = CrossrefLookupMap::new(&inline_path, &extra_path);
            result.insert(tree_name.clone(), map);
        }
        result
    }

    // Given a memory map and a position, expand from `pos` to find the identifier
    // line (`!` prefixed) that covers the position.  Returns (the identifier,
    // the offset of the `!` from the start of the identifier line, the offset of
    // the newline ending the identifier line).
    //
    // `pos` is either inside an identifier line or a payload line that follows an
    // identifier line, so we always walk backwards until we find an identifier.
    // We should never need to walk forward (to find the start of the identifier
    // line) because the result of any comparison should always tell the bisection
    // to bisect in the positive direction (because the file is sorted), which
    // should then find the subsequent record (if that's the one we're looking
    // for, etc.).
    fn get_id_line(&self, pos: usize) -> (&[u8], usize, usize) {
        let mut pos = pos;
        let bytes: &[u8] = self.inline_mm.as_ref();
        if bytes[pos] == NEWLINE {
            pos -= 1;
        }

        let mut start = pos;
        let mut end = pos;

        while start > 0 {
            if bytes[start - 1] == NEWLINE {
                if bytes[start] == ID_START {
                    break;
                } else {
                    // We're hitting a ":" and we need to reset end to this newlin
                    end = start - 1
                    // and we want to keep going...
                }
            }
            start -= 1;
        }

        // Start should now be pointing at the `!` of the identifier line.

        let size = self.inline_mm.len();
        while end < size && bytes[end] != NEWLINE {
            end += 1;
        }

        // end should now be pointing at the trailing newline.

        // Skip the leading `!`
        (&bytes[start + 1..end], start, end)
    }

    // Bisect the mmap to look for an exact symbol match `sym`, and returning the
    // payload line which may be either inline JSON or external offsets to be
    // retrieved from another map.
    fn bisect_for_payload(&self, search_sym: &[u8]) -> &[u8] {
        // We are always looking at a byte-range window within the mmap that is
        // a slice with bounds [0, mmap_end).
        let mut first = 0;
        let mmap_end = self.inline_mm.len();
        let bytes: &[u8] = self.inline_mm.as_ref();
        let mut count = mmap_end;

        while count > 0 {
            let step = count / 2;
            let mut pos = first + step;
            // Our range is currently [first, first + count), and we have chosen
            // a sample point `pos` that's the midpoint of the range.
            //
            // Because we take care to always adjust the byte-range window we
            // are looking at to eliminate both the key and value lines, we can
            // be certain that `pos` will not somehow fall within a line of text
            // we have already looked at.

            let (line_sym, line_start, line_end) = self.get_id_line(pos);

            match line_sym.cmp(search_sym) {
                Ordering::Equal => {
                    // Exact Match!  Extract the payload line.
                    let payload_start = line_end + 1;
                    let mut payload_end = payload_start + 1;
                    while payload_end < mmap_end && bytes[payload_end] != NEWLINE {
                        payload_end += 1;
                    }
                    return &bytes[payload_start..payload_end];
                }
                Ordering::Less => {
                    // ## Bisect latter half
                    // The line we found was less than our needle, so the answer is
                    // in the second half of our current range of [first, first + count).
                    // The second half of the range is [pos, first + count), but
                    // this also includes at least some of our payload line, and we
                    // want to skip that.  (Actually need, by our rules.)
                    //
                    // If `pos` was in an id line, then `line_end` will be greater
                    // than `pos` and we should use `line_end + 1` because it's
                    // already pointing at a newline.  If `pos` is greater, then it
                    // must be in the value line.
                    if pos <= line_end {
                        pos = line_end + 1
                    }

                    // Now scan forward until we find the newline ending the value.
                    while pos < mmap_end && bytes[pos] != NEWLINE {
                        pos += 1;
                    }
                    // move past the newline
                    pos += 1;

                    // Our new range now wants to be [pos, first + count).
                    // We want to halve the count, but we also want to compensate
                    // for the extra data to skip.  `pos` is effectively
                    // `first + step + value_length` so subtracting off `first` from
                    // `pos` gets us the step plus the extra length.
                    count -= pos - first;
                    // And now we want to be starting from the `pos`.
                    first = pos;
                }
                Ordering::Greater => {
                    // ## Bisect first half
                    // Halve count and subtract off the part of the identifier line that
                    // we can eliminate from consideration.
                    count = step - (pos - line_start)
                }
            }
        }

        &[]
    }

    pub fn lookup(&self, sym: &str) -> Result<Value> {
        let payload = self.bisect_for_payload(sym.as_bytes());
        let payload_len = payload.len();
        // Finding nothing (a miss!) is not an error and so is an in-band null.
        if payload_len == 0 {
            return Ok(Value::Null);
        }
        // Let's also rule out results that are too short and therefore must be
        // an error.
        if payload_len < 3 {
            return Err(make_crossref_data_error(sym));
        }

        let marker_char = payload[0];

        if marker_char == INLINE_STORED {
            return from_slice(&payload[1..]).or(Ok(Value::Null));
        } else if marker_char != EXTERNALLY_STORED {
            // Fail if we're seeing something other than an external ref.
            return Err(make_crossref_data_error(sym));
        }

        let mut space_pos = 2;
        while space_pos < payload_len && payload[space_pos] != SPACE {
            space_pos += 1;
        }

        let brace_offset = unsafe {
            usize::from_str_radix(str::from_utf8_unchecked(&payload[1..space_pos]), 16)
                .map_err(|_| make_crossref_data_error(sym))?
        };
        let length_with_newline = unsafe {
            usize::from_str_radix(str::from_utf8_unchecked(&payload[space_pos + 1..]), 16)
                .map_err(|_| make_crossref_data_error(sym))?
        };

        let extra_bytes: &[u8] = self.extra_mm.as_ref();
        Ok(from_slice(
            &extra_bytes[brace_offset..brace_offset + length_with_newline - 1],
        )?)
    }
}

```

## tools/src/file_format/per_file_info.rs
```
use std::fs::File;
use std::io::BufReader;
use std::sync::Arc;

use lexical_sort::natural_lexical_cmp;
use regex::Regex;
use serde::{Deserialize, Serialize};
use serde_json::{from_reader, Map, Value};
use ustr::{existing_ustr, Ustr, UstrMap};

use crate::abstract_server::{FileMatch, FileMatches, Result};

use super::repo_data_ingestion::{ConcisePerFileInfo, DetailedPerFileInfo};

/// Provides access to (concise) per-file info via a pre-loaded copy of
/// `concise-per-file-info.json` and any derived indices.  This exact same
/// information is also available inside the crossref database as
/// `FILE_`-prefixed symbols.
///
/// The reasons to favor using this implementation (or growing this
/// implementation):
/// - Searching for a subset of files in the tree, including using additional
///   constraints that can be pre-computed.
///   - The crate https://github.com/lun3x/multi_index_map has tentatively
///     been identified as a way to aid in precomputation.
/// - Up-front file I/O and object allocation versus crossref-lookup which
///   loads/allocates JSON each time.  This data is able to be shared immutably.
#[derive(Clone, Debug)]
pub struct FileLookupMap {
    // We are able to safely use a UstrMap here because we ensure that in cases
    // where we're dealing with non-Ustr values that we do not create new Ustrs
    // for paths that do not exist through use of `existing_ustr`.
    concise_per_file: Arc<UstrMap<ConcisePerFileInfo<Ustr>>>,
}

impl FileLookupMap {
    pub fn new(concise_file_path: &str) -> Self {
        let components_file = File::open(concise_file_path).unwrap();
        let mut reader = BufReader::new(&components_file);
        let map: UstrMap<ConcisePerFileInfo<Ustr>> = from_reader(&mut reader).unwrap();
        FileLookupMap {
            concise_per_file: Arc::new(map),
        }
    }

    /// File lookup for when you have an existing Ustr; under no circumstances
    /// should you mint a new Ustr for a potential path from content.  If that's
    /// what you have, use `lookup_file_from_str` if it's a one-off, or use
    /// `existing_ustr` if you will be using the path multiple times.
    ///
    /// The general concern is to avoid interning a bunch of incorrect query
    /// strings.
    pub fn lookup_file_from_ustr(&self, path_ustr: &Ustr) -> Option<&ConcisePerFileInfo<Ustr>> {
        self.concise_per_file.get(path_ustr)
    }

    /// File lookup when we don't have a Ustr already available; this is
    /// the appropriate call-site to use if you have a web-sourced potential
    /// path string which could be wrong (and therefore should not be interned).
    pub fn lookup_file_from_str(&self, path: &str) -> Option<&ConcisePerFileInfo<Ustr>> {
        if let Some(path_ustr) = existing_ustr(path) {
            self.concise_per_file.get(&path_ustr)
        } else {
            None
        }
    }

    /// Search the list of files by applying a regexp to the paths.
    pub fn search_files(
        &self,
        pathre: &str,
        include_dirs: bool,
        limit: usize,
    ) -> Result<FileMatches> {
        let re_path = Regex::new(pathre)?;
        let mut matches: Vec<FileMatch> = self
            .concise_per_file
            .iter()
            .filter(|v| {
                if !include_dirs && v.1.is_dir {
                    false
                } else {
                    re_path.is_match(v.0)
                }
            })
            .map(|v| FileMatch {
                path: *v.0,
                concise: v.1.clone(),
            })
            .take(limit)
            .collect();
        matches.sort_unstable_by(|a, b| natural_lexical_cmp(&a.path, &b.path));
        Ok(FileMatches {
            file_matches: matches,
        })
    }
}

/// Information about expected failures/problems for specific web platform
/// tests.
#[derive(Clone, Debug, Serialize, Deserialize)]

pub struct WPTExpectationInfo {
    /// The condition strings and related bugs that disable this test in its
    /// entirety.
    pub disabling_conditions: Vec<(String, String)>,
    /// The number of `_subtests` that were disabled or conditionally disabled
    pub disabled_subtests_count: i64,
}

/// Information from `test-info-all-tests.json` which knows about files that the
/// test manifests know about.
#[derive(Clone, Debug, Serialize, Deserialize)]
pub struct TestInfo {
    pub failed_runs: i64,
    pub skip_if: Option<String>,
    pub skipped_runs: i64,
    pub total_run_time_secs: f64,
    /// "total runs" less "skipped runs"
    pub unskipped_runs: i64,
    /// For web platform tests with expected failures/problems, the info about
    /// that.  Tests that are expected to succeed will have None here.
    pub wpt_expectation_info: Option<WPTExpectationInfo>,
}

/// Per-file info derived from the concise and detailed info for a given file.
/// Everything in here is optional data, but this structure will be available
/// for every file to simplify control-flow.
pub struct PerFileInfo {
    pub bugzilla_component: Option<(String, String)>,
    pub test_info: Option<TestInfo>,
    pub coverage: Option<Vec<i32>>,
}

pub fn get_concise_file_info<'a>(
    all_concise_info: &'a Value,
    path: &str,
) -> Option<&'a Map<String, Value>> {
    let mut cur_obj = all_concise_info.get("root")?.as_object()?;

    for path_component in path.split('/') {
        // The current node must be a directory, get its contents.
        let dir_obj = cur_obj.get("contents")?.as_object()?;
        // And now find the next node inside the components
        cur_obj = dir_obj.get(path_component)?.as_object()?;
    }

    Some(cur_obj)
}

pub fn read_detailed_file_info(path: &str, index_path: &str) -> Option<DetailedPerFileInfo> {
    let json_fname = format!("{}/detailed-per-file-info/{}", index_path, path);
    let json_file = File::open(json_fname).ok()?;
    let mut reader = BufReader::new(&json_file);
    from_reader(&mut reader).ok()
}

```

## tools/src/file_format/identifiers.rs
```
extern crate memmap;

use self::memmap::Mmap;
use std::collections::HashMap;
use std::fs::File;
use std::io::BufRead;

use std::str;
use std::sync::Arc;
use ustr::{ustr, Ustr};

use serde::{Deserialize, Serialize};
use serde_json::to_string;

use super::config::Config;

fn uppercase(s: &[u8]) -> Vec<u8> {
    s.iter().map(u8::to_ascii_uppercase).collect()
}

#[derive(Clone, Debug)]
pub struct IdentMap {
    mmap: Arc<Mmap>,
}

#[derive(Serialize, Deserialize)]
pub struct IdentResult {
    pub id: Ustr,
    pub symbol: Ustr,
}

// XXX commented out like the callsite; they can probably both be removed
/*
// TODO: switch to https://crates.io/crates/cpp_demangle which is probably what
// pernosco uses (based on khuey being an owner) and so for consistency purposes
// is probably the right call.
fn demangle_name(name: &str) -> String {
    let output = Command::new("c++filt")
        .arg("--no-params")
        .arg(name)
        .output();
    match output {
        Err(_) => name.to_string(),
        Ok(output) => {
            if !output.status.success() {
                return name.to_string();
            }
            String::from_utf8(output.stdout)
                .unwrap_or(name.to_string())
                .trim()
                .to_string()
        }
    }
}
*/

impl IdentMap {
    pub fn new(filename: &str) -> Option<IdentMap> {
        let file = match File::open(filename) {
            Ok(file) => file,
            Err(e) => {
                warn!("Failed to open {}: {:?}", filename, e);
                return None;
            }
        };
        unsafe {
            match Mmap::map(&file) {
                Ok(mmap) => Some(IdentMap {
                    mmap: Arc::new(mmap),
                }),
                Err(e) => {
                    warn!("Failed to mmap {}: {:?}", filename, e);
                    None
                }
            }
        }
    }

    pub fn load(config: &Config) -> HashMap<String, IdentMap> {
        let mut result = HashMap::new();
        for (tree_name, tree_config) in &config.trees {
            println!("Loading identifiers {}", tree_name);
            let filename = format!("{}/identifiers", tree_config.paths.index_path);
            if let Some(map) = IdentMap::new(&filename) {
                result.insert(tree_name.clone(), map);
            }
        }
        result
    }

    fn get_line(&self, pos: usize) -> &[u8] {
        let mut pos = pos;
        let bytes = self.mmap.as_ref();
        if bytes[pos] == b'\n' {
            pos -= 1;
        }

        let mut start = pos;
        let mut end = pos;

        while start > 0 && bytes[start - 1] != b'\n' {
            start -= 1;
        }

        let size = bytes.len();
        while end < size && bytes[end] != b'\n' {
            end += 1;
        }

        &bytes[start..end]
    }

    fn bisect(&self, needle: &[u8], upper_bound: bool) -> usize {
        let mut needle = uppercase(needle);
        if upper_bound {
            needle.push(b'~');
        }

        let mut first = 0;
        let mut count = self.mmap.len();

        while count > 0 {
            let step = count / 2;
            let pos = first + step;

            let line = self.get_line(pos);
            let line_upper = uppercase(line);
            if line_upper < needle || (upper_bound && line_upper == needle) {
                first = pos + 1;
                count -= step + 1;
            } else {
                count = step;
            }
        }

        first
    }

    pub fn lookup(
        &self,
        needle: &str,
        exact_match: bool,
        ignore_case: bool,
        max_results: usize,
    ) -> Vec<IdentResult> {
        let bytes = self.mmap.as_ref();

        let start = self.bisect(needle.as_bytes(), false);
        let end = self.bisect(needle.as_bytes(), true);

        let mut result = vec![];
        let slice = &bytes[start..end];

        for line in slice.lines() {
            let line = line.unwrap();
            let (id, symbol) = match line.rsplit_once(' ') {
                Some((id, symbol)) => (id.to_string(), symbol),
                None => continue,
            };

            // We only need to worry about suffix-related cases if the needle is
            // shorter than the identifier.
            if needle.len() < id.len() {
                let suffix = &id[needle.len()..];
                if exact_match || suffix.contains(':') || suffix.contains('.') {
                    continue;
                }
            }
            if !ignore_case && !id.starts_with(needle) {
                continue;
            }

            // Note: I've commented out our use of demangling because this is
            // arguably a legacy concept in the face of our having structured
            // data available for all the cases where demangling would succeed.
            /*
            let demangled = demangle_name(&symbol);
            if demangled != symbol {
                id = demangled;
            }
            */

            result.push(IdentResult {
                id: ustr(&id),
                symbol: ustr(symbol),
            });
            if result.len() == max_results {
                break;
            }
        }

        result
    }

    pub fn lookup_json(
        &self,
        needle: &str,
        complete: bool,
        fold_case: bool,
        max_results: usize,
    ) -> String {
        let results = self.lookup(needle, complete, fold_case, max_results);
        to_string(&results).unwrap()
    }
}

```

## tools/src/file_format/doc_trees.rs
```
use serde_json::from_reader;
use std::collections::HashMap;
use std::fs::File;

#[derive(Debug)]
pub struct DocTrees {
    data: HashMap<String, String>,
}

impl DocTrees {
    fn new(data: HashMap<String, String>) -> Self {
        Self { data }
    }

    pub fn new_empty() -> Self {
        Self {
            data: HashMap::new(),
        }
    }

    pub fn find(&self, src_path: &str) -> Option<String> {
        for (target_prefix, src_prefix) in &self.data {
            match src_path.strip_prefix(src_prefix) {
                Some(inner_path) => {
                    let no_ext_inner_path = match inner_path.strip_suffix(".md") {
                        Some(s) => s,
                        None => match inner_path.strip_suffix(".rst") {
                            Some(s) => s,
                            None => {
                                return None;
                            }
                        },
                    };
                    return Some(target_prefix.to_owned() + no_ext_inner_path + ".html");
                }
                None => {}
            }
        }

        None
    }
}

pub fn read_doc_trees(filename: &String) -> DocTrees {
    let file = match File::open(filename) {
        Ok(f) => f,
        Err(_) => {
            info!("Error trying to open doc trees file [{}]", filename);
            return DocTrees::new_empty();
        }
    };

    let data: HashMap<String, String> = match from_reader(file) {
        Ok(result) => result,
        Err(_) => {
            info!("Error trying to read doc trees file [{}]", filename);
            return DocTrees::new_empty();
        }
    };

    DocTrees::new(data)
}

```

## tools/src/file_format/mod.rs
```
pub mod analysis;
pub mod ontology_pointer_kind;

#[cfg(not(target_arch = "wasm32"))]
pub mod history;

#[cfg(not(target_arch = "wasm32"))]
pub mod analysis_manglings;
#[cfg(not(target_arch = "wasm32"))]
pub mod config;
#[cfg(not(target_arch = "wasm32"))]
pub mod coverage;
#[cfg(not(target_arch = "wasm32"))]
pub mod crossref_converter;
#[cfg(not(target_arch = "wasm32"))]
pub mod crossref_lookup;
#[cfg(not(target_arch = "wasm32"))]
pub mod doc_trees;
#[cfg(not(target_arch = "wasm32"))]
pub mod globbing_file_list;
#[cfg(not(target_arch = "wasm32"))]
pub mod identifiers;
#[cfg(not(target_arch = "wasm32"))]
pub mod merger;
#[cfg(not(target_arch = "wasm32"))]
pub mod ontology_mapping;
#[cfg(not(target_arch = "wasm32"))]
pub mod per_file_info;
#[cfg(not(target_arch = "wasm32"))]
pub mod repo_data_ingestion;
#[cfg(not(target_arch = "wasm32"))]
pub mod url_map;

```

## tools/src/file_format/repo_data_ingestion.rs
```
use std::collections::btree_map::Entry;
use std::collections::BTreeMap;
use std::fs;
use std::fs::File;
use std::io::BufWriter;
use std::path::Path;

use liquid::Template;
use regex::Regex;
use serde::{Deserialize, Serialize};
use serde_json::{from_str, from_value, json, to_writer, Map, Value};
use ustr::{ustr, Ustr};

use crate::describe::describe_file;
use crate::languages::select_formatting;
use crate::templating::builder::build_and_parse;

use super::config::TreeConfig;
use super::coverage::interpolate_coverage;
use super::globbing_file_list::GlobbingFileList;

#[derive(Deserialize)]
pub struct RepoIngestionConfig {
    #[serde(default)]
    pub textfile: BTreeMap<String, TextFileConfig>,
    #[serde(default)]
    pub jsonfile: BTreeMap<String, JsonFileConfig>,
    #[serde(default)]
    pub pathkind: BTreeMap<Ustr, PathKindConfig>,
}

#[derive(Deserialize)]
pub struct PathKindConfig {
    pub name: Ustr,
    #[serde(default)]
    pub default: bool,
    /// The order in which heuristics will be greedily applied.
    pub decision_order: u32,
    /// The order in which results will be displayed.
    pub sort_order: u32,
    #[serde(default)]
    pub heuristics: PathKindHeuristics,
}

/// The heuristics mechanism is a way of classifying a file into a pathkind
/// based on its path.  This is secondary to any explicit mappings received via
/// explicit lists of files from the textfile/jsonfile mechanisms which will
/// clobber the value computed by these heuristics.
#[derive(Default, Deserialize)]
pub struct PathKindHeuristics {
    #[serde(default)]
    pub dir_names: Vec<String>,
    #[serde(default)]
    pub dir_prefixes: Vec<String>,
    #[serde(default)]
    pub dir_suffixes: Vec<String>,
    #[serde(default)]
    pub path_prefixes: Vec<String>,
}

impl PathKindHeuristics {
    pub fn file_matches<'a, I>(&self, file: &str, dir_segments: I) -> bool
    where
        I: Iterator<Item = &'a str>,
    {
        for prefix in &self.path_prefixes {
            if file.starts_with(prefix) {
                return true;
            }
        }

        for dir in dir_segments {
            if self.dir_names.iter().any(|x| x == dir) {
                return true;
            }

            for dir_prefix in &self.dir_prefixes {
                if dir.starts_with(dir_prefix) {
                    return true;
                }
            }

            for dir_suffix in &self.dir_suffixes {
                if dir.ends_with(dir_suffix) {
                    return true;
                }
            }
        }

        false
    }
}

/// Describes a location a file might be found in a directory tree known to the
/// configuration.
///
/// Supported roots:
/// - `config_repo`: The root of the config repo, this is a checkout of
///   mozsearch-mozilla for searchfox.org.  The tree/repo name does not enter
///   into this
/// - `files`: The tree's source directory with checked out file state, which is
///   usually also its git directory.
/// - `index`: The root of the indexing tree.  This should only be used for
///   files located explicitly in the root.  Use one of the other options
///   instead of specifying a subdirectory.
/// - `mozsearch`: The root of the mozsearch checkout.
/// - `objdir`: The tree's objdir directory.
#[derive(Deserialize)]
pub struct SourceDescriptor {
    pub root: String,
    pub file: String,
}

/// Very limited support for processing text files.  The intent here is to
/// support `.eslintignore` and `.gitignore` style lists as well as very simple
/// "mark all these files as needing data review" in-tree mechanisms that would
/// otherwise potentially require building new `mach` infrastructure (for
/// mozilla-central)
///
#[derive(Deserialize)]
pub struct TextFileConfig {
    pub source: Vec<SourceDescriptor>,
    /// One of the following file formats and associated semantics:
    /// - `file-glob-list`: Passes the list of all files in the repo against the
    ///   `filter_input_ext` extension if present to pre-filter, and then checks
    ///   each remaining file against the contents of the file using
    ///   `.gitignore` semantics where `!` can be used to provide for
    ///   exclusions.
    pub format: String,
    /// Filters the file list that we check against the "file-list" format.
    pub filter_input_ext: Option<Vec<Ustr>>,
    /// A tag to apply to the file if it matched the file list.
    pub apply_tag: Option<Ustr>,
    pub remove_tag: Option<Ustr>,
}

#[derive(Deserialize)]
pub struct JsonFileConfig {
    pub source: Vec<SourceDescriptor>,
    pub ingestion: FileIngestion,
    #[serde(default)]
    pub concise: ConciseIngestion,
    #[serde(default)]
    pub detailed: DetailedIngestion,
}

#[derive(Default, Deserialize)]
pub struct ConciseIngestion {
    pub path_kind: Option<JsonEvalNodeIngestion>,
    pub bugzilla_component: Option<JsonEvalNodeIngestion>,
    pub subsystem: Option<JsonEvalNodeIngestion>,
    #[serde(default)]
    pub info: JsonEvalDictIngestion,
}

#[derive(Default, Deserialize)]
pub struct DetailedIngestion {
    pub coverage_lines: Option<JsonEvalNodeIngestion>,
    #[serde(default)]
    pub info: JsonEvalDictIngestion,
}

pub struct ProbeConfig {
    path: Option<Regex>,
}

impl ProbeConfig {
    pub fn new_from_env() -> Self {
        let path = if let Ok(probe_path) = std::env::var("PROBE_PATH") {
            if let Ok(re_path) = Regex::new(&probe_path) {
                Some(re_path)
            } else {
                None
            }
        } else {
            None
        };

        Self { path }
    }

    pub fn should_probe_path(&self, path: &str) -> bool {
        if let Some(path_regex) = &self.path {
            return path_regex.is_match(path);
        }
        false
    }
}

pub struct EvalContext<'a> {
    obj: liquid::Object,
    probe: &'a ProbeConfig,
}

/// Defines an object dictionary's contents.
#[derive(Default, Deserialize)]
pub struct JsonEvalDictIngestion {
    #[serde(flatten)]
    pub extra: BTreeMap<String, JsonEvalNodeIngestion>,
}

impl JsonEvalDictIngestion {
    pub fn eval(
        &mut self,
        ctx: &EvalContext,
        probing: bool,
        input_val: &Value,
        existing_output_value: Value,
    ) -> Value {
        let _obj_entered = if probing {
            let span = Some(trace_span!("dict_eval").entered());
            trace!(existing = ?existing_output_value);
            span
        } else {
            None
        };
        let mut mix_into = match existing_output_value {
            Value::Object(obj) => obj,
            _ => Map::new(),
        };
        for (key, value_ingest) in &mut self.extra {
            if probing {
                trace!(key = %key);
            }
            let existing = mix_into.remove(key).unwrap_or(Value::Null);
            let evaled = value_ingest.eval(ctx, probing, input_val, existing);

            if !evaled.is_null() {
                if probing {
                    trace!(key, val = ?evaled, "inserting non-null key/value");
                }
                mix_into.insert(key.clone(), evaled);
            } else if probing {
                trace!(key, val = ?evaled, "not inserting null value");
            }
        }
        Value::Object(mix_into)
    }

    pub fn is_empty(&self) -> bool {
        self.extra.is_empty()
    }
}

/// Defines a mapping transform over arrays only for now.
#[derive(Deserialize)]
pub struct JsonEvalMapIngestion {
    /// Offset to start from.
    pub first_index: usize,
    /// Transform to perform for each value in the array.
    pub each: JsonEvalNodeIngestion,
}

impl JsonEvalMapIngestion {
    pub fn eval(&mut self, ctx: &EvalContext, probing: bool, input_val: Value) -> Value {
        let _obj_entered = if probing {
            Some(trace_span!("map_eval").entered())
        } else {
            None
        };
        match input_val {
            Value::Array(arr) => Value::Array(
                arr.into_iter()
                    .skip(self.first_index)
                    .map(|v| self.each.eval(ctx, probing, &v, Value::Null))
                    .collect(),
            ),
            _ => Value::Null,
        }
    }
}

/// Defines a mechanism for evaluating a JSON input value and returning a JSON
/// output value, possibly via mutating an existing object dictionary that
/// already exists in the "slot" where this value will be stored.
///
/// Intended to be populated by TOML deserialization.
#[derive(Deserialize)]
pub struct JsonEvalNodeIngestion {
    /// Perform a JSON pointer value of the input value, replacing the input
    /// value for the purposes of processing any subsequent properties.
    /// An empty string returns the value itself.  See
    /// https://docs.rs/serde_json/latest/serde_json/value/enum.Value.html#method.pointer
    /// for more info.
    pub pointer: Option<String>,
    /// Perform a mapping transform over an array.  Evaluated prior to
    /// aggregation.
    pub map: Option<Box<JsonEvalMapIngestion>>,
    /// In the event of a null value, perform this evaluation instead.  This is
    /// being introduced to deal with WPT MANIFEST.json entries which optimize
    /// by omitting test id paths which are the same as the test file path.
    ///
    /// Introducing this does raise the question of whether we should just be
    /// adding support for something like skylark here rather than adding
    /// another specialized case.  I think adding this is appropriate based on
    /// my understanding of the domain, and would probably argue that we should
    /// probably push anything that would resemble embedded scripting upstream
    /// as a step that should be run separate from core searchfox logic.  We're
    /// not doing that in this case because I think an appropriate jq script for
    /// this would still be pretty confusing/complex compared to this
    /// incremental enhancement.
    pub null_fallback: Option<Box<JsonEvalNodeIngestion>>,
    /// Perform some kind of trivial computation on the pointer result from
    /// above.  Current options are:
    /// - "length": Assume we're given an array and get its length.
    pub aggregation: Option<String>,
    /// When present, indicates that the return value should be an object
    /// dictionary, and that the contained key/value definitions should be mixed
    /// in to any already existing object in this slot.
    pub object: Option<JsonEvalDictIngestion>,
    /// When present, indicates that the return value should be a string that
    /// is created by evaluating the payload as a liquid template and evaluating
    /// it with the input value exposed as `value`
    pub liquid: Option<String>,
    #[serde(skip)]
    pub liquid_cache: Option<Template>,
}

impl JsonEvalNodeIngestion {
    pub fn eval(
        &mut self,
        ctx: &EvalContext,
        probing: bool,
        input_val: &Value,
        existing_output_value: Value,
    ) -> Value {
        let _eval_entered = if probing {
            Some(trace_span!("node_eval").entered())
        } else {
            None
        };
        let mut traversed = match &self.pointer {
            Some(traversal) => match input_val.pointer(traversal) {
                Some(val) => {
                    if probing {
                        trace!(traversal, val = ?val, "traversed");
                    }
                    val.clone()
                }
                None => Value::Null,
            },
            None => input_val.clone(),
        };
        if traversed.is_null() {
            if let Some(null_fallback) = &mut self.null_fallback {
                traversed = null_fallback.eval(ctx, probing, &traversed, Value::Null);
                if probing {
                    trace!(val = ?traversed, "null_fallback");
                }
            }
        }
        if let Some(mapper) = &mut self.map {
            traversed = mapper.eval(ctx, probing, traversed);
            if probing {
                trace!(val = ?traversed, "mapped");
            }
        }
        if let Some(aggr) = &self.aggregation {
            traversed = match aggr.as_str() {
                "length" => match traversed {
                    Value::Array(arr) => json!(arr.len()),
                    Value::Object(obj) => json!(obj.len()),
                    _ => Value::Null,
                },
                _ => traversed,
            };
            if probing {
                trace!(val = ?traversed, "length");
            }
        }
        if let Some(object_ingest) = &mut self.object {
            object_ingest.eval(ctx, probing, &traversed, existing_output_value)
        } else if let Some(liquid_str) = &self.liquid {
            let template = self
                .liquid_cache
                .get_or_insert_with(|| build_and_parse(liquid_str));
            let globals = liquid::object!({
                "value": traversed,
                "context": ctx.obj,
            });
            let rendered = template.render(&globals).unwrap();
            if probing {
                trace!(val = rendered, "rendered");
            }
            Value::String(rendered)
        } else {
            traversed
        }
    }
}

#[derive(Deserialize)]
pub struct FileIngestion {
    root: String,
    nesting: String,
    nesting_key: Option<String>,
    partitioned_by: Option<String>,
    #[serde(default)]
    path_prefix: String,
    filename_key: Option<String>,
    value_lookup: Option<String>,
}

pub struct RepoIngestion {
    pub config: RepoIngestionConfig,
    pub state: IngestionState,
}

#[derive(Clone, Debug, Deserialize, Serialize)]
pub struct ConcisePerFileInfo<T: Ord> {
    pub path_kind: T,
    pub is_dir: bool,
    pub file_size: u64,
    pub bugzilla_component: Option<(T, T)>,
    pub subsystem: Option<T>,
    pub tags: Vec<T>,
    pub description: Option<String>,
    pub info: Value,
}

impl ConcisePerFileInfo<Ustr> {
    fn default_is_dir(is_dir: bool) -> Self {
        ConcisePerFileInfo {
            path_kind: ustr(""),
            is_dir,
            file_size: 0,
            bugzilla_component: None,
            subsystem: None,
            tags: vec![],
            description: None,
            info: json!({}),
        }
    }
}

#[derive(Deserialize, Serialize)]
pub struct DetailedPerFileInfo {
    pub is_dir: bool,
    /// Coverage data; mozilla-central absolutely exceeds i32 regularly.
    pub coverage_lines: Option<Vec<i64>>,
    pub info: Value,
}

impl DetailedPerFileInfo {
    fn default_is_dir(is_dir: bool) -> Self {
        DetailedPerFileInfo {
            is_dir,
            coverage_lines: None,
            info: json!({}),
        }
    }
}

pub struct IngestionState {
    pub concise_per_file: BTreeMap<Ustr, ConcisePerFileInfo<Ustr>>,
    pub detailed_per_file: BTreeMap<Ustr, DetailedPerFileInfo>,
}

fn write_json_to_file<T: Serialize>(val: &T, path: &str) -> Option<()> {
    let file = File::create(path).ok()?;
    let writer = BufWriter::new(file);
    to_writer(writer, val).ok()?;
    Some(())
}

impl IngestionState {
    /// Call the helper function with the concise and detailed storages for the
    /// given path, creating the entries if they do not exist.
    pub fn with_file_info<F>(&mut self, path: &Ustr, is_dir: bool, f: F)
    where
        F: FnOnce(&mut ConcisePerFileInfo<Ustr>, &mut DetailedPerFileInfo),
    {
        let concise_storage = self
            .concise_per_file
            .entry(*path)
            .or_insert_with(|| ConcisePerFileInfo::default_is_dir(is_dir));

        let detailed_storage = self
            .detailed_per_file
            .entry(*path)
            .or_insert_with(|| DetailedPerFileInfo::default_is_dir(is_dir));

        f(concise_storage, detailed_storage);
    }

    pub fn write_out_concise_file_info(&self, index_path: &str) {
        let output_fname = format!("{}/concise-per-file-info.json", index_path);
        write_json_to_file(&self.concise_per_file, &output_fname);
    }

    pub fn write_out_and_drop_detailed_file_info(&mut self, index_path: &str) {
        for (path, detailed_info) in &self.detailed_per_file {
            let detailed_file_info_fname = if detailed_info.is_dir {
                // We flatten the directories because we already need to do name
                // transforming so we leverage the invariant that there should
                // never be consecutive slashes to normalize them to "_" after
                // doubling existing "_"s.
                format!(
                    "{}/detailed-per-dir-info/{}",
                    index_path,
                    path.replace("_", "__").replace("/", "_")
                )
            } else {
                format!("{}/detailed-per-file-info/{}", index_path, path)
            };

            // We haven't actually bothered to create this directory tree anywhere,
            // and we expect to be sparsely populating it, so just do the mkdir -p
            // ourself here.
            let detailed_path = std::path::Path::new(&detailed_file_info_fname);
            let parent_path = match detailed_path.parent() {
                Some(p) => p,
                None => {
                    warn!("Unable to derive parent of {}", detailed_file_info_fname);
                    continue;
                }
            };
            if let Err(e) = std::fs::create_dir_all(parent_path) {
                warn!(
                    "Problem creating parent of {}: {}",
                    detailed_file_info_fname, e
                );
                continue;
            }

            write_json_to_file(detailed_info, &detailed_file_info_fname);
        }
        self.detailed_per_file.clear();
    }
}

impl RepoIngestion {
    pub fn new(config_str: &str) -> Result<RepoIngestion, String> {
        let config: RepoIngestionConfig =
            toml::from_str(config_str).map_err(|err| err.to_string())?;

        Ok(RepoIngestion {
            config,
            state: IngestionState {
                concise_per_file: BTreeMap::new(),
                detailed_per_file: BTreeMap::new(),
            },
        })
    }

    /// Process the file list of all files (not just files with analysis data)
    /// and apply both path kind heuristics based on the path as well as loading
    /// the file contents to perform trivial processing like having our
    /// `describe_file` mechanism try and derive a snippet.
    ///
    /// The describe mechanism previously happened during the output-file stage.
    pub fn ingest_file_list_and_apply_heuristics(
        &mut self,
        files: &Vec<Ustr>,
        tree_config: &TreeConfig,
    ) {
        let mut ordered_path_kinds: Vec<&PathKindConfig> = self.config.pathkind.values().collect();
        ordered_path_kinds.sort_unstable_by_key(|x| x.decision_order);
        let default_pk = ordered_path_kinds[0].name;

        for file_path in files {
            // split in reverse order so we can skip the filename itself.
            let segments = file_path.rsplit("/").skip(1);
            let mut use_path_kind = default_pk;
            for pk_config in &ordered_path_kinds {
                if pk_config
                    .heuristics
                    .file_matches(file_path, segments.clone())
                {
                    use_path_kind = pk_config.name;
                    break;
                }
            }

            let raw_file_path = tree_config.find_source_file(file_path);
            let path_wrapper = Path::new(&raw_file_path);
            let metadata = match fs::symlink_metadata(path_wrapper) {
                Ok(m) => m,
                Err(e) => {
                    if tree_config.should_ignore_missing_file(&raw_file_path) {
                        info!("Problem gathering metadata for {}: {}", raw_file_path, e);
                    } else {
                        warn!("Problem gathering metadata for {}: {}", raw_file_path, e);
                    }
                    continue;
                }
            };
            let file_size = metadata.len();

            let description = match fs::read_to_string(&raw_file_path) {
                Ok(contents) => {
                    let format = select_formatting(&raw_file_path);
                    let maybe_description = describe_file(&contents, path_wrapper, &format);
                    if let Some(ref description) = maybe_description {
                        // We currently want to output
                        let description_fname =
                            format!("{}/description/{}", tree_config.paths.index_path, file_path);
                        let description_file = match File::create(&description_fname) {
                            Ok(df) => df,
                            Err(e) => {
                                warn!(
                                    "Problem creating description file {}: {}",
                                    description_fname, e
                                );
                                continue;
                            }
                        };
                        let desc_writer = BufWriter::new(description_file);
                        let file_description = json!({
                            "description": description,
                        });
                        to_writer(desc_writer, &file_description).unwrap();
                    }

                    maybe_description
                }
                Err(_) => None,
            };

            self.state.with_file_info(file_path, false, |pfi, _dfi| {
                pfi.path_kind = use_path_kind;
                pfi.description = description;
                pfi.file_size = file_size;
            });
        }
    }

    pub fn ingest_dir_list(&mut self, dirs: &Vec<Ustr>) {
        for dir_path in dirs {
            self.state.with_file_info(dir_path, true, |_cfi, _dfi| {});
        }
    }

    pub fn ingest_files<F>(&mut self, maybe_read_file: F) -> Result<(), String>
    where
        F: Fn(&str, &str) -> Result<Option<String>, &'static str>,
    {
        let probe_config = ProbeConfig::new_from_env();

        let find_file = |descriptors: &Vec<SourceDescriptor>| -> Result<Option<String>, String> {
            for desc in descriptors {
                match maybe_read_file(&desc.root, &desc.file) {
                    Ok(Some(contents)) => {
                        return Ok(Some(contents));
                    }
                    Ok(None) => {}
                    Err(_) => {
                        return Err(format!(
                            "Problem reading '{}' from root '{}'",
                            desc.file, desc.root
                        ));
                    }
                }
            }

            Ok(None)
        };

        // ### Text Files
        let mut textfile_sources = vec![];
        for (name, config) in &self.config.textfile {
            if let Some(contents) = find_file(&config.source)? {
                textfile_sources.push((name.clone(), contents));
            }
        }
        for (name, contents) in textfile_sources {
            self.ingest_textfile_data(&name, contents, &probe_config)?;
        }

        // ### JSON Files
        let mut jsonfile_sources = vec![];
        for (name, config) in &self.config.jsonfile {
            if let Some(str_contents) = find_file(&config.source)? {
                let val: Value = match from_str(&str_contents) {
                    Err(e) => {
                        return Err(format!("JSON parsing problem for '{}': {:}", name, e));
                    }
                    Ok(v) => v,
                };
                jsonfile_sources.push((name.clone(), val));
            }
        }
        for (name, mut value) in jsonfile_sources {
            self.ingest_jsonfile_data(&name, &mut value, &probe_config)?;
        }

        Ok(())
    }

    pub fn ingest_textfile_data(
        &mut self,
        name: &str,
        file_contents: String,
        probe_config: &ProbeConfig,
    ) -> Result<(), String> {
        let config = match self.config.textfile.get(name) {
            Some(config) => config,
            None => {
                return Err(format!("No config for {}", name));
            }
        };
        info!(
            "Processing text file: {} using format {}",
            name, &config.format
        );

        match config.format.as_str() {
            "file-list" => {
                let globber = GlobbingFileList::new(file_contents);

                for (path, concise) in &mut self.state.concise_per_file {
                    let probing = probe_config.should_probe_path(path);

                    // Apply path extension pre-filter.
                    if let Some(exts) = &config.filter_input_ext {
                        if let Some(idx) = path.rfind('.') {
                            let ext = ustr(&path[idx + 1..]);
                            if exts.iter().any(|x| x == &ext) {
                                // good!
                            } else {
                                continue;
                            }
                        } else {
                            continue;
                        }
                    }

                    // Now apply the glob filter.
                    if !globber.is_match(path) {
                        if probing {
                            trace!("'{}' did not match", path);
                        }
                        continue;
                    } else if probing {
                        trace!("'{}' matched", path);
                    }

                    // It matches if we're here.
                    if let Some(tag) = config.apply_tag {
                        match concise.tags.binary_search(&tag) {
                            Ok(_) => {} // nothing to do, tag already present
                            Err(pos) => {
                                concise.tags.insert(pos, tag);
                            }
                        }
                    }
                    if let Some(tag) = config.remove_tag {
                        match concise.tags.binary_search(&tag) {
                            Ok(pos) => {
                                concise.tags.remove(pos);
                            }
                            Err(_pos) => {} // Nothing to do, tag is not present.
                        }
                    }
                }
                Ok(())
            }
            x => Err(format!("Unsupported file format '{}'", x)),
        }
    }

    /// Destructively ingest the given value for the given filename if we have a configuration
    /// entry for it.  The filename should just be the basename, without any dirname.
    ///
    /// Note: We can probably move this to just take a
    ///
    /// `input_val` is a `&mut Value` which we will use `take()` on, mutating
    /// the JSON in place and then consuming the data-structure to the extent
    /// possible to reduce allocations.  But practically speaking this class is
    /// going to tend to be a bit clone-heavy because of the generic nature of
    /// the concise/detailed data traversals which, although they could `take()`
    /// things, currently clone them because we don't actually care about the
    /// waste enough to impose a "you can only consume each piece of data once"
    /// restriction on the impl.  That seems like the kind of thing that would
    /// be super annoying for people.
    pub fn ingest_jsonfile_data(
        &mut self,
        name: &str,
        input_val: &mut Value,
        probe_config: &ProbeConfig,
    ) -> Result<(), String> {
        info!("Processing JSON file: {}", name);
        let config = match self.config.jsonfile.get_mut(name) {
            Some(config) => config,
            None => {
                return Err(format!("No config for {}", name));
            }
        };

        let mut lookups = None;
        if let Some(value_lookup) = &config.ingestion.value_lookup {
            match input_val.pointer_mut(value_lookup) {
                Some(Value::Object(obj)) => {
                    lookups = Some(obj.clone());
                }
                _ => {
                    return Err(format!("Unable to locate value lookup '{}'", value_lookup));
                }
            }
        }

        let mut root = match input_val.pointer_mut(&config.ingestion.root) {
            Some(v) => v.take(),
            None => {
                return Err(format!(
                    "Unable to find root of '{}'",
                    config.ingestion.root
                ));
            }
        };

        match config.ingestion.nesting.as_str() {
            // Used by:
            // - bugzilla mapping, uses the lookup
            // - wpt MANIFEST.json files, uses the
            "hierarchical-dict-dirs-are-dicts-files-are-values" => {
                let path_prefix = config.ingestion.path_prefix.clone();
                if let Some(_partition_key) = config.ingestion.partitioned_by.clone() {
                    if let Value::Object(obj) = root {
                        for (_, partitioned_root) in obj {
                            self.state.recurse_dir_dict_with_lookup(
                                config,
                                probe_config,
                                &lookups,
                                &path_prefix,
                                partitioned_root,
                            )?;
                        }
                    }
                    Ok(())
                } else {
                    self.state.recurse_dir_dict_with_lookup(
                        config,
                        probe_config,
                        &lookups,
                        &path_prefix,
                        root,
                    )
                }
            }
            // code coverage mapping
            "hierarchical-dict-explicit-key" => {
                if let Some(children_key) = &config.ingestion.nesting_key.clone() {
                    let path_prefix = config.ingestion.path_prefix.clone();
                    self.state.recurse_nested_explicit_children(
                        config,
                        probe_config,
                        children_key,
                        &path_prefix,
                        root.take(),
                    )
                } else {
                    Err(format!(
                        "nesting_key required for {}",
                        config.ingestion.nesting
                    ))
                }
            }
            "boring-dict-of-arrays" => {
                // for the path_prefix, normalize a trailing "/" onto it for
                // consistency with the path_so_far-based mechanisms.
                if let (Some(path_key), path_prefix, Value::Object(root_obj)) = (
                    &config.ingestion.nesting_key.clone(),
                    if config.ingestion.path_prefix.is_empty() {
                        "".to_string()
                    } else {
                        format!("{}/", config.ingestion.path_prefix)
                    },
                    root,
                ) {
                    // The serde Map wrapper lacks `into_values` so we destructure.
                    for (_, result_array_val) in root_obj.into_iter() {
                        if let Value::Array(result_array) = result_array_val {
                            for val in result_array {
                                if let Some(Value::String(path)) = val.get(path_key) {
                                    self.state.eval_file_values(
                                        config,
                                        probe_config,
                                        false,
                                        &ustr(&format!("{}{}", &path_prefix, path)),
                                        false,
                                        false,
                                        &val,
                                    );
                                }
                            }
                        }
                    }
                    Ok(())
                } else {
                    Err(format!(
                        "nesting_key required for {}",
                        config.ingestion.nesting
                    ))
                }
            }
            "flat-dir-dict-files-are-keys" => {
                if let (Some(children_key), filename_key, Value::Object(root_obj)) = (
                    &config.ingestion.nesting_key.clone(),
                    config.ingestion.filename_key.clone(),
                    root.take(),
                ) {
                    // If there's a path_prefix, normalize a trailing "/" onto it
                    // for consistency with our path_so_far-based mechanisms.
                    let use_path_prefix = if config.ingestion.path_prefix.is_empty() {
                        "".to_string()
                    } else {
                        format!("{}/", config.ingestion.path_prefix)
                    };
                    for (dir_path, dir_obj) in root_obj.into_iter() {
                        if let Some(Value::Object(file_list_obj)) = dir_obj.get(children_key) {
                            // note: I'm skipping the take() step here because lazy.
                            for (filename, file_contents) in file_list_obj {
                                let use_filename = match &filename_key {
                                    Some(key) => {
                                        if let Some(Value::String(s)) = file_contents.get(key) {
                                            s
                                        } else {
                                            continue;
                                        }
                                    }
                                    _ => filename,
                                };
                                let path =
                                    format!("{}{}/{}", use_path_prefix, dir_path, use_filename);
                                self.state.eval_file_values(
                                    config,
                                    probe_config,
                                    false,
                                    &ustr(&path),
                                    false,
                                    false,
                                    file_contents,
                                );
                            }
                        }
                    }
                    Ok(())
                } else {
                    Err(format!(
                        "nesting_key required for {}",
                        config.ingestion.nesting
                    ))
                }
            }
            _ => Err(format!(
                "no such nesting strategy: {}",
                config.ingestion.nesting
            )),
        }
    }
}

impl IngestionState {
    #[allow(clippy::too_many_arguments)]
    pub fn eval_file_values(
        &mut self,
        config: &mut JsonFileConfig,
        probe_config: &ProbeConfig,
        parent_probing: bool,
        path: &Ustr,
        create_if_does_not_exist: bool,
        is_dir: bool,
        file_val: &Value,
    ) {
        let ctx = EvalContext {
            obj: liquid::object!({
                "path": path,
            }),
            probe: probe_config,
        };

        let probing = parent_probing || ctx.probe.should_probe_path(path);

        let concise_entry = self.concise_per_file.entry(*path);
        if let Entry::Vacant(_) = &concise_entry {
            if !create_if_does_not_exist {
                return;
            }
        }
        let concise_storage =
            concise_entry.or_insert_with(|| ConcisePerFileInfo::default_is_dir(is_dir));
        if let Some(ingestion) = &mut config.concise.path_kind {
            let evaled = ingestion.eval(&ctx, probing, file_val, Value::Null);
            if !evaled.is_null() {
                concise_storage.path_kind = from_value(evaled).unwrap();
            }
        }
        if let Some(ingestion) = &mut config.concise.bugzilla_component {
            let evaled = ingestion.eval(&ctx, probing, file_val, Value::Null);
            if !evaled.is_null() {
                concise_storage.bugzilla_component = Some(from_value(evaled).unwrap());
            }
        }
        if let Some(ingestion) = &mut config.concise.subsystem {
            let evaled = ingestion.eval(&ctx, probing, file_val, Value::Null);
            if !evaled.is_null() {
                concise_storage.subsystem = Some(from_value(evaled).unwrap());
            }
        }

        concise_storage.info =
            config
                .concise
                .info
                .eval(&ctx, probing, file_val, concise_storage.info.take());

        let detailed_storage = self
            .detailed_per_file
            .entry(*path)
            .or_insert_with(|| DetailedPerFileInfo::default_is_dir(is_dir));

        if let Some(ingestion) = &mut config.detailed.coverage_lines {
            let evaled = ingestion.eval(&ctx, probing, file_val, Value::Null);
            if !evaled.is_null() {
                match from_value(evaled) {
                    Ok(converted) => {
                        detailed_storage.coverage_lines = Some(interpolate_coverage(converted));
                    }
                    Err(e) => {
                        warn!(
                            "Weirdness {} on evaluating: {}",
                            e,
                            serde_json::to_string(file_val).unwrap()
                        );
                    }
                }
            }
        }
        detailed_storage.info =
            config
                .detailed
                .info
                .eval(&ctx, probing, file_val, detailed_storage.info.take());
    }

    pub fn recurse_dir_dict_with_lookup(
        &mut self,
        config: &mut JsonFileConfig,
        probe_config: &ProbeConfig,
        lookups: &Option<Map<String, Value>>,
        path_so_far: &str,
        cur: Value,
    ) -> Result<(), String> {
        if let Value::Object(obj) = cur {
            for (filename, value) in obj {
                let path = if path_so_far.is_empty() {
                    filename.clone()
                } else {
                    format!("{}/{}", path_so_far, filename)
                };
                if value.is_object() {
                    self.recurse_dir_dict_with_lookup(config, probe_config, lookups, &path, value)?;
                } else if let Some(lookup_values) = lookups {
                    let lookup_key = match value {
                        Value::String(s) => s,
                        Value::Number(n) => format!("{}", n),
                        _ => "".to_string(),
                    };
                    if let Some(looked_up_value) = lookup_values.get(&lookup_key) {
                        self.eval_file_values(
                            config,
                            probe_config,
                            false,
                            &ustr(&path),
                            false,
                            false,
                            looked_up_value,
                        );
                    }
                } else {
                    self.eval_file_values(
                        config,
                        probe_config,
                        false,
                        &ustr(&path),
                        false,
                        false,
                        &value,
                    );
                }
            }
            Ok(())
        } else {
            Err(format!("expected Object at path '{}'", path_so_far))
        }
    }

    pub fn recurse_nested_explicit_children(
        &mut self,
        config: &mut JsonFileConfig,
        probe_config: &ProbeConfig,
        children_key: &str,
        path_so_far: &str,
        mut cur: Value,
    ) -> Result<(), String> {
        // We only want to evalute for leaf nodes because we do not currently
        // want to create derived files for directories.  In the future we may
        // want to do so, in which case we will need to explicitly brand the
        // entries as directories and ensure they get mangled to not collide
        // with their own directory.
        //
        // Currently the `children_key` is only for directories and there is no
        // other way to currently distinguish in the coverage file format, so we
        // key off of this.
        if let Some(v) = cur.get_mut(children_key) {
            if let Value::Object(obj) = v.take() {
                for (filename, value) in obj {
                    let path = if path_so_far.is_empty() {
                        filename.clone()
                    } else {
                        format!("{}/{}", path_so_far, filename)
                    };
                    self.recurse_nested_explicit_children(
                        config,
                        probe_config,
                        children_key,
                        &path,
                        value,
                    )?;
                }
            }
        } else if !path_so_far.is_empty() {
            // (path_so_far would only be empty in this case for a completely
            // empty file, but there's no need to risk doing buggy stuff in that
            // case.)
            self.eval_file_values(
                config,
                probe_config,
                false,
                &ustr(path_so_far),
                false,
                false,
                &cur,
            );
        }

        Ok(())
    }
}

```

## tools/src/tokenize.rs
```
use std::cell::Cell;

use crate::languages::LanguageSpec;

#[derive(Clone, Debug, PartialEq)]
pub enum TokenKind {
    PlainText,
    Punctuation,
    Identifier(Option<String>),
    StringLiteral,
    Comment,
    RegularExpressionLiteral,
    TagName,
    TagAttrName,
    EndTagName,
    Newline,
}

#[derive(Debug)]
pub struct Token {
    pub start: usize,
    pub end: usize,
    pub kind: TokenKind,
}

fn is_whitespace(ch: char) -> bool {
    ch == ' ' || ch == '\t' || ch == '\n' || ch == '\r'
}

pub fn tokenize_css(string: &str) -> Vec<Token> {
    fn tokenize_css_block(input: &mut cssparser::Parser, raw_input: &str, tokens: &mut Vec<Token>) {
        use cssparser::Token::*;
        let reserved = crate::languages::SYN_RESERVED_CLASS;
        let mut start = input.position().byte_index();
        while let Ok(token) = input.next_including_whitespace_and_comments().cloned() {
            let mut has_block = false;
            let kind = match token {
                Ident(name) => {
                    // Poor heuristic to try to find property names.
                    let state = input.state();
                    let is_custom_property = name.starts_with("--");
                    let colon_and_space_follows =
                        matches!(input.next_including_whitespace_and_comments(), Ok(&Colon))
                            && matches!(
                                input.next_including_whitespace_and_comments(),
                                Ok(&WhiteSpace(..))
                            );
                    input.reset(&state);
                    TokenKind::Identifier(if !is_custom_property && colon_and_space_follows {
                        Some(reserved.into())
                    } else {
                        None
                    })
                }
                AtKeyword(..) => TokenKind::Identifier(Some(reserved.into())),
                IDHash(..) | Hash(..) => TokenKind::Identifier(None),
                QuotedString(..) => TokenKind::StringLiteral,
                Colon | Semicolon | Comma | IncludeMatch | DashMatch | PrefixMatch
                | SuffixMatch | SubstringMatch | CloseParenthesis | CloseSquareBracket
                | CloseCurlyBracket | Delim(..) => TokenKind::Punctuation,
                BadUrl(..)
                | BadString(..)
                | UnquotedUrl(..)
                | Number { .. }
                | Percentage { .. }
                | Dimension { .. }
                | WhiteSpace(..) => TokenKind::PlainText,
                CDO | CDC | Comment(..) => TokenKind::Comment,
                Function(..) => {
                    has_block = true;
                    TokenKind::Identifier(None)
                }
                ParenthesisBlock | SquareBracketBlock | CurlyBracketBlock => {
                    has_block = true;
                    TokenKind::Punctuation
                }
            };

            fn push_tokens(
                raw_input: &str,
                start: usize,
                end: usize,
                kind: &TokenKind,
                tokens: &mut Vec<Token>,
            ) {
                if start == end {
                    return;
                }
                // tokens shouldn't span across lines
                let mut span_start = start;
                for span in raw_input[start..end].split('\n') {
                    let span_end = span_start + span.len();
                    if span_start != span_end {
                        tokens.push(Token {
                            start: span_start,
                            end: span_end,
                            kind: kind.clone(),
                        });
                    }
                    let newline_needed = span_start + span.len() != end;
                    span_start = span_end;
                    if newline_needed {
                        tokens.push(Token {
                            start: span_start,
                            end: span_start + 1,
                            kind: TokenKind::Newline,
                        });
                        span_start += 1;
                    }
                }
            }

            if has_block {
                let mut block_start = start;
                let mut block_end = start;
                let mut block_tokens = vec![];
                let _: Result<(), cssparser::ParseError<()>> = input.parse_nested_block(|input| {
                    block_start = input.position().byte_index();
                    tokenize_css_block(input, raw_input, &mut block_tokens);
                    block_end = input.position().byte_index();
                    Ok(())
                });
                push_tokens(raw_input, start, block_start, &kind, tokens);
                tokens.extend(block_tokens.into_iter());
                let end = input.position().byte_index();
                push_tokens(raw_input, block_end, end, &kind, tokens);
                start = end;
            } else {
                let end = input.position().byte_index();
                push_tokens(raw_input, start, end, &kind, tokens);
                start = end;
            }
        }
    }

    let mut input = cssparser::ParserInput::new(string);
    let mut input = cssparser::Parser::new(&mut input);
    let mut tokens = vec![];

    tokenize_css_block(&mut input, string, &mut tokens);

    tokens
}

pub fn tokenize_plain(string: &str) -> Vec<Token> {
    let lines = string.split('\n');
    let mut tokens = Vec::new();
    let mut start = 0;
    for line in lines {
        if !line.is_empty() {
            tokens.push(Token {
                start,
                end: start + line.len(),
                kind: TokenKind::PlainText,
            });
        }
        start += line.len();
        if start == string.len() {
            break;
        }
        tokens.push(Token {
            start,
            end: start + 1,
            kind: TokenKind::Newline,
        });
        start += 1;
    }
    tokens
}

pub fn tokenize_static_prefs(string: &str) -> Vec<Token> {
    let mut tokens = Vec::new();

    let chars: Vec<(usize, char)> = string.char_indices().collect();
    let cur_pos = Cell::new(0);

    let get_char = || {
        let p = cur_pos.get();
        if p == chars.len() {
            debug!("Attempted read past end");
            return (p, '\0');
        }
        cur_pos.set(p + 1);
        chars[p]
    };

    let peek_char = || {
        if cur_pos.get() == chars.len() {
            return '\0';
        }

        let (_, ch) = chars[cur_pos.get()];
        ch
    };

    while cur_pos.get() < chars.len() {
        let (start, mut ch) = get_char();

        match ch {
            '\n' => {
                tokens.push(Token {
                    start,
                    end: start + 1,
                    kind: TokenKind::Newline,
                });
            }
            '"' => {
                let end;
                loop {
                    ch = peek_char();
                    match ch {
                        '"' | '\0' => {
                            end = get_char().0;
                            break;
                        }
                        _ => {
                            get_char();
                        }
                    }
                }

                tokens.push(Token {
                    start,
                    end: end + 1,
                    kind: TokenKind::StringLiteral,
                });
            }
            '#' => {
                let mut end = start;
                loop {
                    ch = peek_char();
                    match ch {
                        '\n' | '\0' => {
                            break;
                        }
                        _ => {
                            end = get_char().0;
                        }
                    }
                }

                tokens.push(Token {
                    start,
                    end: end + 1,
                    kind: TokenKind::Comment,
                });
            }
            'A'..='Z' | 'a'..='z' => {
                // Treat the entire preference name as single identifier.
                let mut end = start;
                loop {
                    ch = peek_char();
                    match ch {
                        'A'..='Z' | 'a'..='z' | '0'..='9' | '-' | '_' | '.' => {
                            end = get_char().0;
                        }
                        _ => {
                            break;
                        }
                    }
                }

                tokens.push(Token {
                    start,
                    end: end + 1,
                    kind: TokenKind::Identifier(None),
                });
            }
            _ => {
                let mut end = start;
                loop {
                    ch = peek_char();
                    match ch {
                        '\n' | '"' | '#' | 'A'..='Z' | 'a'..='z' | '\0' => {
                            break;
                        }
                        _ => {
                            end = get_char().0;
                        }
                    }
                }

                tokens.push(Token {
                    start,
                    end: end + 1,
                    kind: TokenKind::PlainText,
                });
            }
        }
    }
    tokens
}

pub fn tokenize_c_like(string: &str, spec: &LanguageSpec) -> Vec<Token> {
    let is_ident = |ch: char| -> bool {
        (ch == '_')
            || ch.is_alphabetic()
            || ch.is_ascii_digit()
            || (ch == '#' && spec.hash_identifier)
    };

    let mut tokens = Vec::new();

    let chars: Vec<(usize, char)> = string.char_indices().collect();
    let mut backtick_nesting: Vec<Cell<u32>> = Vec::new();
    let cur_pos = Cell::new(0);

    let mut next_token_maybe_regexp_literal = true;

    let get_char = || {
        let p = cur_pos.get();
        // Defense in depth bailing if we would otherwise throw on chars[p]
        // below.
        if p == chars.len() {
            // We return '!' in peek cases past the end for reasons that aren't
            // immediately clear.  I've gone with a nul here because this seems
            // less likely to result in the state machine advancing into a
            // state that doesn't correspond with reality, but wouldn't be
            // surprised if this turns out to have its own problems.
            debug!("Attempted read past end");
            return (p, '\0');
        }
        cur_pos.set(p + 1);
        chars[p]
    };

    let peek_char = || {
        if cur_pos.get() == chars.len() {
            return '!';
        }

        let (_, ch) = chars[cur_pos.get()];
        ch
    };

    let peek_char2 = || {
        if cur_pos.get() + 1 >= chars.len() {
            return '!';
        }

        let (_, ch) = chars[cur_pos.get() + 1];
        ch
    };

    let peek_isnot = |c| {
        if cur_pos.get() == chars.len() {
            return false;
        }

        let (_, ch) = chars[cur_pos.get()];
        ch != c
    };

    let peek_pos = || {
        if cur_pos.get() == chars.len() {
            return string.len();
        }

        let (i, _) = chars[cur_pos.get()];
        i
    };

    // Breaks the current token across a newline to keep line numbering
    // correct. Returns the new value for `start` on the new line.
    let push_newline = |start: usize, tokens: &mut Vec<_>, cur_tok_kind: TokenKind| -> usize {
        let nl = peek_pos() - 1;
        if start != nl {
            tokens.push(Token {
                start,
                end: nl,
                kind: cur_tok_kind,
            });
        }
        tokens.push(Token {
            start: nl,
            end: peek_pos(),
            kind: TokenKind::Newline,
        });
        nl + 1
    };

    'token_loop: while cur_pos.get() < chars.len() {
        let (start, mut ch) = get_char();
        let mut continue_backtick = false;
        if let Some(braces) = backtick_nesting.last_mut() {
            match ch {
                '{' => {
                    braces.set(braces.get() + 1);
                }
                '}' => {
                    braces.set(braces.get() - 1);
                    continue_backtick = braces.get() == 0;
                }
                _ => {}
            }
        }

        // If continue_backtick is true, then backtick_nesting.last() is
        // non-None and 0, so pop it off the stack before continuing.
        if continue_backtick {
            backtick_nesting.pop();
        }

        // Pre-process rust byte strings here. To do so, scan ahead a little:
        // - If the next character is not a quote or an r, this is an
        //   identifier. No action is taken.
        // - If the next character is a quote, we have a byte string.
        // - If the next character is an r, check the following character:
        //   - If that character is a # or a quote, then this is a raw byte
        //     string literal.
        // In the cases where we don't have a byte string, we do nothing.
        // Otherwise, consume the 'b', but leave `start` alone. This way, 'ch'
        // will point to the proper character to consume this token (either
        // the 'r' for a raw string literal or a quote for a byte string).
        if spec.rust_tweaks && ch == 'b' {
            match (peek_char(), peek_char2()) {
                ('\'', _) | ('"', _) | ('r', '"') | ('r', '#') => {
                    let (_, next) = get_char();
                    ch = next;
                }
                _ => {}
            }
        }

        if spec.rust_tweaks && ch == 'r' && (peek_char() == '#' || peek_char() == '"') {
            // Rust raw string literals.
            // Consume 0 or more #s.
            let mut nhashes = 0;
            loop {
                let (_, ch) = get_char();
                if ch == '"' {
                    break;
                } else if ch != '#' || peek_pos() == string.len() {
                    // Not actually a (valid) Rust raw string literal. We can
                    // run into this in macro inputs or rust files that are
                    // intentionally syntactically invalid (as can happen in the
                    // rust tree).
                    //
                    // Just treat it as plain text and move on, which is
                    // better than crashing
                    tokens.push(Token {
                        start,
                        end: peek_pos(),
                        kind: TokenKind::PlainText,
                    });
                    continue 'token_loop;
                }
                nhashes += 1;
            }

            let mut start = start;
            'rust_raw_string: loop {
                if peek_pos() == string.len() {
                    debug!("Unterminated raw string");
                    return tokens;
                }

                let (_, next) = get_char();
                if next == '\n' {
                    // Tokens shouldn't span across lines.
                    start = push_newline(start, &mut tokens, TokenKind::StringLiteral);
                }
                if next != '"' {
                    continue;
                }

                // Consume nhashes #s.
                for _ in 0..nhashes {
                    if peek_char() != '#' {
                        continue 'rust_raw_string;
                    }
                    get_char();
                }

                break;
            }
            tokens.push(Token {
                start,
                end: peek_pos(),
                kind: TokenKind::StringLiteral,
            });
        } else if ch == 'R' && peek_char() == '"' {
            // Handle raw literals per
            // <http://en.cppreference.com/w/cpp/language/string_literal>.
            get_char();

            // Read the delimiter.
            let paren;
            loop {
                let (idx, c) = get_char();
                if c == '(' {
                    paren = idx;
                    break;
                }

                if peek_pos() == string.len() {
                    debug!("Expecting '(' after raw string literal");
                    return tokens;
                }
            }

            let delimiter = &string[start + 2..paren];

            let mut start = start;
            'raw_string: loop {
                if peek_pos() == string.len() {
                    debug!("Unterminated raw string");
                    return tokens;
                }

                let (_, next) = get_char();
                if next == '\n' {
                    // Tokens shouldn't span across lines.
                    start = push_newline(start, &mut tokens, TokenKind::StringLiteral);
                }
                if next != ')' {
                    continue;
                }

                // Find the delimiter.
                for c in delimiter.chars() {
                    if c != peek_char() {
                        continue 'raw_string;
                    }
                    get_char();
                }

                // Is this the end quote?
                if peek_char() != '"' {
                    continue;
                }
                get_char();

                // Done!
                break;
            }

            tokens.push(Token {
                start,
                end: peek_pos(),
                kind: TokenKind::StringLiteral,
            });
            next_token_maybe_regexp_literal = false;
        } else if is_ident(ch) {
            let cxx14_number = spec.cxx14_digit_separators && ch.is_ascii_digit();
            while is_ident(peek_char()) || (cxx14_number && peek_char() == '\'') {
                get_char();
            }

            let word = string[start..peek_pos()].to_string();
            let class = spec.reserved_words.get(&word).cloned();

            tokens.push(Token {
                start,
                end: peek_pos(),
                kind: TokenKind::Identifier(class),
            });
            // NOTE: yield and await can also be followed by RegExp, but
            //       only in generator or async context.
            next_token_maybe_regexp_literal = word == "return";
        } else if ch == '\n' {
            tokens.push(Token {
                start,
                end: peek_pos(),
                kind: TokenKind::Newline,
            });
        } else if ch == ' ' || ch == '\t' || ch == '\r' {
            // Skip it.
        } else if ch == '#' && spec.hash_comment {
            loop {
                if peek_pos() == string.len() {
                    tokens.push(Token {
                        start,
                        end: peek_pos(),
                        kind: TokenKind::Comment,
                    });
                    return tokens;
                }

                let (_, next) = get_char();
                if next == '\n' {
                    break;
                }
            }
            let nl = peek_pos() - 1;
            tokens.push(Token {
                start,
                end: nl,
                kind: TokenKind::Comment,
            });
            tokens.push(Token {
                start: nl,
                end: peek_pos(),
                kind: TokenKind::Newline,
            });
        } else if ch == '#' && spec.c_preprocessor {
            while peek_char() == ' ' || peek_char() == '\t' {
                get_char();
            }

            let id_start = peek_pos();
            while is_ident(peek_char()) {
                get_char();
            }

            let word = "#".to_owned() + &string[id_start..peek_pos()];
            let class = spec.reserved_words.get(&word).cloned();

            tokens.push(Token {
                start,
                end: peek_pos(),
                kind: TokenKind::Identifier(class),
            });
            next_token_maybe_regexp_literal = false;
        } else if ch == '/' && spec.c_style_comments {
            let ch = peek_char();
            if ch == '*' {
                let mut nesting = 1;
                let mut start = start;
                get_char();
                loop {
                    if peek_pos() == string.len() {
                        debug!("Unterminated /* comment");
                        return tokens;
                    }

                    let (_, next) = get_char();
                    if next == '*' && peek_char() == '/' {
                        if nesting == 1 {
                            break;
                        }
                        get_char();
                        nesting -= 1;
                    } else if next == '\n' {
                        // Tokens shouldn't span across lines.
                        start = push_newline(start, &mut tokens, TokenKind::Comment);
                    } else if spec.rust_tweaks && next == '/' && peek_char() == '*' {
                        get_char();
                        nesting += 1;
                    }
                }
                get_char();
                tokens.push(Token {
                    start,
                    end: peek_pos(),
                    kind: TokenKind::Comment,
                });
            } else if ch == '/' {
                get_char();
                loop {
                    if peek_pos() == string.len() {
                        tokens.push(Token {
                            start,
                            end: peek_pos(),
                            kind: TokenKind::Comment,
                        });
                        return tokens;
                    }

                    let (_, next) = get_char();
                    if next == '\n' {
                        break;
                    }
                }
                let nl = peek_pos() - 1;
                tokens.push(Token {
                    start,
                    end: nl,
                    kind: TokenKind::Comment,
                });
                tokens.push(Token {
                    start: nl,
                    end: peek_pos(),
                    kind: TokenKind::Newline,
                });
            } else if next_token_maybe_regexp_literal && spec.regexp_literals {
                loop {
                    if cur_pos.get() == chars.len() {
                        debug!("Invalid regexp literal");
                        return tokenize_plain(string);
                    }
                    let (_, next) = get_char();
                    if next == '/' {
                        break;
                    } else if next == '[' {
                        while peek_isnot(']') {
                            get_char();
                        }
                    } else if next == '\\' && peek_isnot('\n') {
                        get_char();
                    } else if next == '\n' {
                        debug!("Invalid regexp literal");
                        return tokenize_plain(string);
                    }
                }
                tokens.push(Token {
                    start,
                    end: peek_pos(),
                    kind: TokenKind::RegularExpressionLiteral,
                });
                next_token_maybe_regexp_literal = true;
            } else {
                tokens.push(Token {
                    start,
                    end: peek_pos(),
                    kind: TokenKind::Punctuation,
                });
                next_token_maybe_regexp_literal = true;
            }
        } else if continue_backtick || (ch == '`' && spec.backtick_strings) {
            let mut start = start;
            loop {
                if peek_pos() == string.len() {
                    debug!("Unterminated backtick string");
                    return tokens;
                }

                let (_, next) = get_char();
                if next == '`' {
                    break;
                } else if next == '\n' {
                    // Tokens shouldn't span across lines.
                    start = push_newline(start, &mut tokens, TokenKind::StringLiteral);
                } else if next == '\\' && peek_isnot('\n') {
                    get_char();
                } else if next == '$' && peek_char() == '{' {
                    // A template! Note that we're in a template and start
                    // counting unconsumed { and } tokens. When we find the
                    // last close brace that isn't part of a string or regexp,
                    // we'll come back in here to finish this template string.
                    get_char(); // Skip '{'.
                    backtick_nesting.push(Cell::new(1));
                    break;
                }
            }
            if peek_pos() != start {
                tokens.push(Token {
                    start,
                    end: peek_pos(),
                    kind: TokenKind::StringLiteral,
                });
            }
            next_token_maybe_regexp_literal = true;
        } else if ch == '\'' || ch == '"' {
            let need_triple = spec.triple_quote_literals && peek_char() == ch && peek_char2() == ch;
            if need_triple {
                get_char();
                get_char();
            }

            // In Rust, ' could be the start of a byte literal *or* a
            // Lisp-like atom. Check for that here (but be careful for
            // something like '\n').
            if spec.rust_tweaks && ch == '\'' && peek_char() != '\\' && peek_char2() != '\'' {
                // Push the lonely quote.
                tokens.push(Token {
                    start,
                    end: start + 1,
                    kind: TokenKind::Punctuation,
                });
                loop {
                    if peek_pos() == string.len() {
                        break;
                    }

                    if !is_ident(peek_char()) {
                        break;
                    }
                    get_char();
                }

                // Push the rest of the label.
                tokens.push(Token {
                    start: start + 1,
                    end: peek_pos(),
                    kind: TokenKind::Identifier(None),
                });
                continue;
            }

            let mut start = start;
            loop {
                if peek_pos() == string.len() {
                    debug!("Unterminated quote");
                    return tokens;
                }

                let (_, next) = get_char();
                if next == ch && (!need_triple || (peek_char() == ch && peek_char2() == ch)) {
                    if need_triple {
                        get_char();
                        get_char();
                    }
                    break;
                } else if next == '\n' {
                    // Tokens shouldn't span across lines.
                    start = push_newline(start, &mut tokens, TokenKind::StringLiteral);
                } else if next == '\\' && peek_isnot('\n') {
                    get_char();
                }
            }
            tokens.push(Token {
                start,
                end: peek_pos(),
                kind: TokenKind::StringLiteral,
            });
            next_token_maybe_regexp_literal = false;
        } else {
            tokens.push(Token {
                start,
                end: peek_pos(),
                kind: TokenKind::Punctuation,
            });

            // Horrible hack to treat '/' in (1+2)/3 as division and not regexp literal.
            //
            // NOTE: This doesn't cover the following differences:
            //
            //   ++ vs +
            //   -- vs -
            //     a++ /foo/g;                    // This should be Div
            //     a + /(x*)/.exec(s)[1].length;  // This should be RegExp
            //
            //   } for block vs object
            //     {}/foo/g;                      // This should be RegExp
            //     x = {}/foo/g;                  // This should be Div
            //
            // See test_regexp_js and test_not_regexp_js for examples.
            let s = string[start..peek_pos()].to_string();
            next_token_maybe_regexp_literal = s == "="
                || s == "("
                || s == "["
                || s == "{"
                || s == "}"
                || s == ":"
                || s == ";"
                || s == "&"
                || s == "|"
                || s == "!"
                || s == ","
                || s == "?"
                || s == ">"
                || s == "<";
        }
    }

    tokens
}

pub fn tokenize_tag_like(string: &str, script_spec: &LanguageSpec) -> Vec<Token> {
    fn is_ident(ch: char) -> bool {
        ch == '.'
            || ch == '_'
            || ch == '-'
            || ch == ':'
            || ch.is_alphabetic()
            || ch.is_ascii_digit()
    }

    let mut tokens = Vec::new();

    let chars: Vec<(usize, char)> = string.char_indices().collect();
    let cur_pos = Cell::new(0);

    fn punctuation_kind(ch: char) -> TokenKind {
        if ch == '\n' {
            TokenKind::Newline
        } else {
            TokenKind::Punctuation
        }
    }

    let get_char = || {
        let p = cur_pos.get();
        cur_pos.set(p + 1);
        chars[p]
    };

    let peek_ahead = |s: &str| {
        let p = cur_pos.get();
        if p + s.len() > chars.len() {
            return false;
        }
        let sub = &chars[p..p + s.len()];
        let sub = sub.iter().map(|&(_, ch)| ch).collect::<String>();
        sub == s
    };

    let peek_pos = || {
        if cur_pos.get() == chars.len() {
            return string.len();
        }

        let (i, _) = chars[cur_pos.get()];
        i
    };

    #[derive(Debug)]
    enum TagState {
        TagNone(usize),
        TagStart(usize),
        TagId(usize),
        TagAfterId,
        TagAttrName(usize),
        TagBeforeEq,
        TagAttrEq,
        TagAttrValue(char, usize),
        TagAttrBareValue(usize),
        EndTagId(usize),
        EndStartTag(usize),
        EndTagDone,
        TagCDATA(usize),
        TagComment(usize),
        TagPI(usize),
        Doctype(bool),
    }

    let mut tag_state = TagState::TagNone(0);

    let mut in_script_tag = false;
    let mut in_style_tag = false;
    let mut cur_line = 1;
    while cur_pos.get() < chars.len() {
        let (start, ch) = get_char();

        //println!("t {} {:?}", ch, tag_state);

        if ch == '\n' {
            cur_line += 1;
        }

        match tag_state {
            TagState::TagNone(plain_start) => {
                let skip = (in_script_tag && !peek_ahead("/script"))
                    || (in_style_tag && !peek_ahead("/style"));
                if ch == '<' && !skip {
                    if plain_start < start {
                        tokens.push(Token {
                            start: plain_start,
                            end: start,
                            kind: TokenKind::PlainText,
                        });
                    }

                    if peek_ahead("!--") {
                        tag_state = TagState::TagComment(start);
                    } else if peek_ahead("![CDATA[") {
                        let _ = get_char(); // !
                        let _ = get_char(); // [
                        let _ = get_char(); // C
                        let _ = get_char(); // D
                        let _ = get_char(); // A
                        let _ = get_char(); // T
                        let _ = get_char(); // A
                        let _ = get_char(); // [
                        tokens.push(Token {
                            start,
                            end: peek_pos(),
                            kind: TokenKind::Punctuation,
                        });
                        tag_state = TagState::TagCDATA(peek_pos());
                    } else if peek_ahead("!DOCTYPE") || peek_ahead("!doctype") {
                        tokens.push(Token {
                            start,
                            end: peek_pos(),
                            kind: TokenKind::Punctuation,
                        });
                        tag_state = TagState::Doctype(false);
                    } else if peek_ahead("?") {
                        tag_state = TagState::TagPI(start);
                    } else {
                        tag_state = TagState::TagStart(start);
                    }
                } else if ch == '\n' {
                    if plain_start < start {
                        tokens.push(Token {
                            start: plain_start,
                            end: start,
                            kind: TokenKind::PlainText,
                        });
                    }
                    tokens.push(Token {
                        start,
                        end: peek_pos(),
                        kind: TokenKind::Newline,
                    });
                    tag_state = TagState::TagNone(peek_pos());
                }
            }
            TagState::TagStart(open) => {
                if is_ident(ch) {
                    tokens.push(Token {
                        start: open,
                        end: start,
                        kind: TokenKind::Punctuation,
                    });
                    tag_state = TagState::TagId(start);
                } else if ch == '/' {
                    tokens.push(Token {
                        start: open,
                        end: peek_pos(),
                        kind: TokenKind::Punctuation,
                    });
                    tag_state = TagState::EndTagId(peek_pos());
                } else {
                    debug!("Error type 1 (line {})", cur_line);
                    return tokenize_plain(string);
                }
            }
            TagState::TagId(id_start) => {
                if !is_ident(ch) {
                    tokens.push(Token {
                        start: id_start,
                        end: start,
                        kind: TokenKind::TagName,
                    });

                    let word = string[id_start..start].to_string();
                    in_script_tag = word == "script";
                    in_style_tag = word == "style";

                    if ch == '/' {
                        tag_state = TagState::EndStartTag(start);
                    } else {
                        tokens.push(Token {
                            start,
                            end: peek_pos(),
                            kind: punctuation_kind(ch),
                        });
                        if ch == '>' {
                            tag_state = TagState::TagNone(peek_pos());
                        } else if is_whitespace(ch) {
                            tag_state = TagState::TagAfterId;
                        } else {
                            debug!("Error type 2 (line {})", cur_line);
                            return tokenize_plain(string);
                        }
                    }
                }
            }
            TagState::TagAfterId => {
                if is_ident(ch) {
                    tag_state = TagState::TagAttrName(start);
                } else if ch == '>' {
                    tag_state = TagState::TagNone(peek_pos());
                    tokens.push(Token {
                        start,
                        end: peek_pos(),
                        kind: punctuation_kind(ch),
                    });
                } else if ch == '/' {
                    tag_state = TagState::EndStartTag(start);
                } else if is_whitespace(ch) {
                    tokens.push(Token {
                        start,
                        end: peek_pos(),
                        kind: punctuation_kind(ch),
                    });
                }
            }
            TagState::TagAttrName(id_start) => {
                if !is_ident(ch) {
                    tokens.push(Token {
                        start: id_start,
                        end: start,
                        kind: TokenKind::TagAttrName,
                    });
                    if ch == '/' {
                        tag_state = TagState::EndStartTag(start);
                    } else {
                        tokens.push(Token {
                            start,
                            end: peek_pos(),
                            kind: punctuation_kind(ch),
                        });
                        if ch == '>' {
                            tag_state = TagState::TagNone(peek_pos());
                        } else if ch == '=' {
                            tag_state = TagState::TagAttrEq;
                        } else if is_whitespace(ch) {
                            tag_state = TagState::TagBeforeEq;
                        } else {
                            debug!("Error type 3 (line {})", cur_line);
                            return tokenize_plain(string);
                        }
                    }
                }
            }
            TagState::TagBeforeEq => {
                if ch == '=' {
                    tag_state = TagState::TagAttrEq;
                    tokens.push(Token {
                        start,
                        end: peek_pos(),
                        kind: punctuation_kind(ch),
                    });
                } else if ch == '>' {
                    tag_state = TagState::TagNone(peek_pos());
                    tokens.push(Token {
                        start,
                        end: peek_pos(),
                        kind: punctuation_kind(ch),
                    });
                } else if ch == '/' {
                    tag_state = TagState::EndStartTag(start);
                } else if is_whitespace(ch) {
                    tokens.push(Token {
                        start,
                        end: peek_pos(),
                        kind: punctuation_kind(ch),
                    });
                }
            }
            TagState::TagAttrEq => {
                if ch == '"' || ch == '\'' {
                    tag_state = TagState::TagAttrValue(ch, peek_pos());
                    tokens.push(Token {
                        start,
                        end: peek_pos(),
                        kind: TokenKind::Punctuation,
                    });
                } else if is_whitespace(ch) {
                    tokens.push(Token {
                        start,
                        end: peek_pos(),
                        kind: punctuation_kind(ch),
                    });
                } else {
                    tag_state = TagState::TagAttrBareValue(start);
                }
            }
            TagState::TagAttrValue(end_ch, attr_start) => {
                if ch == end_ch {
                    tag_state = TagState::TagAfterId;
                    tokens.push(Token {
                        start: attr_start,
                        end: start,
                        kind: TokenKind::StringLiteral,
                    });
                    tokens.push(Token {
                        start,
                        end: peek_pos(),
                        kind: TokenKind::Punctuation,
                    });
                } else if ch == '\n' {
                    tokens.push(Token {
                        start: attr_start,
                        end: start,
                        kind: TokenKind::StringLiteral,
                    });
                    tokens.push(Token {
                        start,
                        end: peek_pos(),
                        kind: TokenKind::Newline,
                    });
                    tag_state = TagState::TagAttrValue(end_ch, peek_pos());
                }
            }
            TagState::TagAttrBareValue(attr_start) => {
                if is_whitespace(ch) || ch == '>' || (ch == '/' && peek_ahead(">")) {
                    tokens.push(Token {
                        start: attr_start,
                        end: start,
                        kind: TokenKind::StringLiteral,
                    });
                    if ch == '>' {
                        tag_state = TagState::TagNone(peek_pos());
                        tokens.push(Token {
                            start,
                            end: peek_pos(),
                            kind: punctuation_kind(ch),
                        });
                    } else if ch == '/' {
                        tag_state = TagState::EndStartTag(start);
                    } else if is_whitespace(ch) {
                        tag_state = TagState::TagAfterId;
                        tokens.push(Token {
                            start,
                            end: peek_pos(),
                            kind: punctuation_kind(ch),
                        });
                    }
                }
            }
            TagState::EndTagId(id_start) => {
                if !is_ident(ch) {
                    in_script_tag = false;
                    in_style_tag = false;
                    tokens.push(Token {
                        start: id_start,
                        end: start,
                        kind: TokenKind::EndTagName,
                    });
                    tokens.push(Token {
                        start,
                        end: peek_pos(),
                        kind: punctuation_kind(ch),
                    });
                    if ch == '>' {
                        tag_state = TagState::TagNone(peek_pos());
                    } else if is_whitespace(ch) {
                        tag_state = TagState::EndTagDone;
                    } else {
                        debug!("Error type 4 (line {})", cur_line);
                        return tokenize_plain(string);
                    }
                }
            }
            TagState::EndTagDone => {
                if ch == '>' {
                    tag_state = TagState::TagNone(peek_pos());
                } else if !is_whitespace(ch) {
                    debug!("Error type 5 (line {})", cur_line);
                    return tokenize_plain(string);
                }
                tokens.push(Token {
                    start,
                    end: peek_pos(),
                    kind: punctuation_kind(ch),
                });
            }
            TagState::EndStartTag(slash) => {
                if ch == '>' {
                    in_script_tag = false;
                    in_style_tag = false;
                    tag_state = TagState::TagNone(peek_pos());
                    tokens.push(Token {
                        start: slash,
                        end: peek_pos(),
                        kind: punctuation_kind(ch),
                    });
                }
            }
            TagState::TagCDATA(cdata_start) => {
                if ch == ']' && peek_ahead("]>") {
                    let _ = get_char();
                    let _ = get_char();

                    if cdata_start < peek_pos() {
                        tokens.push(Token {
                            start: cdata_start,
                            end: peek_pos(),
                            kind: TokenKind::PlainText,
                        });
                    }
                    tag_state = TagState::TagNone(peek_pos());
                } else if ch == '\n' {
                    if cdata_start < start {
                        tokens.push(Token {
                            start: cdata_start,
                            end: start,
                            kind: TokenKind::PlainText,
                        });
                    }
                    tokens.push(Token {
                        start,
                        end: peek_pos(),
                        kind: TokenKind::Newline,
                    });
                    tag_state = TagState::TagCDATA(peek_pos());
                }
            }
            TagState::TagComment(comment_start) => {
                if ch == '-' && peek_ahead("->") {
                    let _ = get_char();
                    let _ = get_char();
                    tokens.push(Token {
                        start: comment_start,
                        end: peek_pos(),
                        kind: TokenKind::Comment,
                    });
                    tag_state = TagState::TagNone(peek_pos());
                } else if ch == '\n' {
                    tokens.push(Token {
                        start: comment_start,
                        end: start,
                        kind: TokenKind::Comment,
                    });
                    tokens.push(Token {
                        start,
                        end: peek_pos(),
                        kind: TokenKind::Newline,
                    });
                    tag_state = TagState::TagComment(peek_pos());
                }
            }
            TagState::TagPI(comment_start) => {
                if ch == '>' {
                    tokens.push(Token {
                        start: comment_start,
                        end: peek_pos(),
                        kind: TokenKind::Comment,
                    });
                    tag_state = TagState::TagNone(peek_pos());
                } else if ch == '\n' {
                    tokens.push(Token {
                        start: comment_start,
                        end: start,
                        kind: TokenKind::Comment,
                    });
                    tokens.push(Token {
                        start,
                        end: peek_pos(),
                        kind: TokenKind::Newline,
                    });
                    tag_state = TagState::TagPI(peek_pos());
                }
            }
            TagState::Doctype(in_bracket) => {
                if ch == '[' && !in_bracket {
                    tag_state = TagState::Doctype(true);
                } else if ch == ']' && in_bracket {
                    tag_state = TagState::Doctype(false);
                } else if ch == '>' && !in_bracket {
                    tag_state = TagState::TagNone(start);
                } else if ch == '<' {
                    tokens.push(Token {
                        start,
                        end: peek_pos(),
                        kind: TokenKind::Punctuation,
                    });
                } else if ch == '\n' {
                    tokens.push(Token {
                        start,
                        end: peek_pos(),
                        kind: TokenKind::Newline,
                    });
                }
            }
        }
    }

    if let TagState::TagNone(plain_start) = tag_state {
        tokens.push(Token {
            start: plain_start,
            end: string.len(),
            kind: TokenKind::PlainText,
        });
    }

    fn peek(tag_stack: &[&str], index: usize, check: &str) -> bool {
        if tag_stack.len() <= index {
            return false;
        }
        tag_stack.get(tag_stack.len() - index - 1) == Some(&check)
    }

    let mut result = Vec::new();
    let mut script = String::new();
    let mut style = String::new();
    let mut tag_stack = Vec::new();
    let mut in_script = false;
    let mut script_start = 0;
    let mut in_style = false;
    let mut style_start = 0;
    let mut literal_is_id = false;
    let mut literal_is_js = false;
    let mut literal_is_css = false;
    for token in tokens {
        match token.kind {
            TokenKind::TagName | TokenKind::EndTagName => {
                let tag_name = &string[token.start..token.end];
                if token.kind == TokenKind::TagName {
                    tag_stack.push(tag_name);
                } else {
                    tag_stack.pop();
                }
                result.push(token);
            }
            TokenKind::TagAttrName => {
                let attr_name = &string[token.start..token.end];
                if (peek(&tag_stack, 0, "field")
                    || peek(&tag_stack, 0, "property")
                    || peek(&tag_stack, 0, "method")
                    || peek(&tag_stack, 0, "parameter"))
                    && attr_name == "name"
                {
                    literal_is_id = true;
                }

                if attr_name.starts_with("on") {
                    literal_is_js = true;
                }
                if attr_name == "style" {
                    literal_is_css = true;
                }

                result.push(token);
            }
            TokenKind::PlainText | TokenKind::Newline | TokenKind::Comment => {
                let text = &string[token.start..token.end];
                if in_script {
                    script.push_str(text);
                } else if in_style {
                    style.push_str(text);
                } else {
                    result.push(token);
                }
            }
            TokenKind::StringLiteral => {
                if literal_is_id {
                    literal_is_id = false;
                    result.push(Token {
                        start: token.start,
                        end: token.end,
                        kind: TokenKind::Identifier(None),
                    });
                } else if literal_is_js {
                    literal_is_js = false;

                    let script_start = token.start;
                    let script = &string[token.start..token.end];
                    let script_toks = tokenize_c_like(script, script_spec);
                    let script_toks = script_toks.into_iter().map(|t| Token {
                        start: t.start + script_start,
                        end: t.end + script_start,
                        kind: t.kind,
                    });
                    result.extend(script_toks);
                } else if literal_is_css {
                    literal_is_css = false;

                    let css_start = token.start;
                    let css = &string[token.start..token.end];
                    let css_toks = tokenize_css(css);
                    let css_toks = css_toks.into_iter().map(|t| Token {
                        start: t.start + css_start,
                        end: t.end + css_start,
                        kind: t.kind,
                    });
                    result.extend(css_toks);
                } else {
                    result.push(token);
                }
            }
            TokenKind::Punctuation => {
                let punc = &string[token.start..token.end];

                if punc == "/>" {
                    tag_stack.pop();
                }

                let starting_script = peek(&tag_stack, 0, "script")
                    || peek(&tag_stack, 0, "constructor")
                    || peek(&tag_stack, 0, "destructor")
                    || peek(&tag_stack, 0, "handler")
                    || peek(&tag_stack, 0, "field")
                    || (peek(&tag_stack, 1, "method") && peek(&tag_stack, 0, "body"))
                    || (peek(&tag_stack, 1, "property") && peek(&tag_stack, 0, "getter"))
                    || (peek(&tag_stack, 1, "property") && peek(&tag_stack, 0, "setter"))
                    || false;

                let starting_style = peek(&tag_stack, 0, "style");

                if starting_script && punc == ">" {
                    in_script = true;
                    script_start = token.end;
                    result.push(token);
                } else if in_script && punc == "</" {
                    let script_toks = tokenize_c_like(&script, script_spec);
                    let script_toks = script_toks.into_iter().map(|t| Token {
                        start: t.start + script_start,
                        end: t.end + script_start,
                        kind: t.kind,
                    });
                    result.extend(script_toks);

                    script = String::new();

                    in_script = false;
                    result.push(token);
                } else if in_script {
                    script.push_str(punc);
                } else if starting_style && punc == ">" {
                    in_style = true;
                    style_start = token.end;
                    result.push(token);
                } else if in_style && punc == "</" {
                    let style_toks = tokenize_css(&style);
                    let style_toks = style_toks.into_iter().map(|t| Token {
                        start: t.start + style_start,
                        end: t.end + style_start,
                        kind: t.kind,
                    });
                    result.extend(style_toks);

                    style = String::new();

                    in_style = false;
                    result.push(token);
                } else if in_style {
                    style.push_str(punc);
                } else {
                    result.push(token);
                }
            }
            _ => {
                assert!(!in_script);
                assert!(!in_style);
                result.push(token);
            }
        }
    }

    if in_script {
        let script_toks = tokenize_c_like(&script, script_spec);
        let script_toks = script_toks.into_iter().map(|t| Token {
            start: t.start + script_start,
            end: t.end + script_start,
            kind: t.kind,
        });
        result.extend(script_toks);
    }
    if in_style {
        let style_toks = tokenize_css(&style);
        let style_toks = style_toks.into_iter().map(|t| Token {
            start: t.start + style_start,
            end: t.end + style_start,
            kind: t.kind,
        });
        result.extend(style_toks);
    }

    result
}

#[cfg(test)]
mod tests {
    use super::*;
    use crate::languages::*;

    fn token_matches(start: usize, end: usize, kind: TokenKind, rhs: &Token) {
        assert_eq!(start, rhs.start, "start matches");
        assert_eq!(end, rhs.end, "end matches");
        assert_eq!(kind, rhs.kind, "kind matches");
    }

    fn check_tokens(s: &str, expected: &[(&str, TokenKind)], spec: &LanguageSpec) {
        let toks = tokenize_c_like(s, spec);
        check_tokens_match(s, &toks, expected);
    }

    fn check_tokens_match(s: &str, toks: &[Token], expected: &[(&str, TokenKind)]) {
        assert_eq!(
            toks.len(),
            expected.len(),
            "should have the same number of tokens"
        );

        for (a, b) in toks.iter().zip(expected.iter()) {
            assert_eq!(&s[a.start..a.end], b.0, "token strings should match");
            assert_eq!(a.kind, b.1, "token types should match");
        }
    }

    fn check_css_tokens(s: &str, expected: &[(&str, TokenKind)]) {
        let toks = tokenize_css(s);
        println!("{:#?}", toks);
        check_tokens_match(s, &toks, expected);
    }

    #[test]
    fn test_raw_strings_cpp() {
        let spec = match select_formatting("test.cpp") {
            FormatAs::FormatCLike(spec) => spec,
            _ => panic!("wrong spec"),
        };

        // NB: Expects `R"...";` (note the trailing semicolon to ensure we don't
        // simply grab the whole string as a single token.
        let check_simple = |s: &str| {
            let simple_raw = String::from(s);
            token_matches(
                0,
                simple_raw.len() - 1,
                TokenKind::StringLiteral,
                tokenize_c_like(&simple_raw, spec).first().unwrap(),
            );
        };

        check_simple(r##"R"(hello)";"##);
        check_simple(r##"R"foo(hel"lo)foo";"##);
        check_simple(r##"R"foo(hello)fooo)foo";"##);
        check_simple(r##"R"124.foo(hello)fooo)124.foo";"##);
        check_simple(r##"R"1234567890123456(just long enough)1234567890123456";"##);

        let check_empty = |s: &str| {
            let simple_raw = String::from(s);
            assert!(tokenize_c_like(&simple_raw, spec).is_empty());
        };

        check_empty(r##"R"foo(unterminated string literal)""##);
        check_empty(r##"R"foo"##); // unterminated sentinel
    }

    #[test]
    fn test_template_strings_js() {
        let spec = match select_formatting("test.js") {
            FormatAs::FormatCLike(spec) => spec,
            _ => {
                panic!("wrong spec");
            }
        };

        let check = |s: &str, expected: &[(&str, TokenKind)]| {
            check_tokens(s, expected, spec);
        };

        check(
            r##"`Hello, world`"##,
            &[("`Hello, world`", TokenKind::StringLiteral)],
        );
        check(
            r##"`Hello ${'w' + 'orld'}`"##,
            &[
                ("`Hello ${", TokenKind::StringLiteral),
                ("'w'", TokenKind::StringLiteral),
                ("+", TokenKind::Punctuation),
                ("'orld'", TokenKind::StringLiteral),
                ("}`", TokenKind::StringLiteral),
            ],
        );
        check(
            r##"`Hello ${`${w}` + 'orld'}`"##,
            &vec![
                ("`Hello ${", TokenKind::StringLiteral),
                ("`${", TokenKind::StringLiteral),
                ("w", TokenKind::Identifier(None)),
                ("}`", TokenKind::StringLiteral),
                ("+", TokenKind::Punctuation),
                ("'orld'", TokenKind::StringLiteral),
                ("}`", TokenKind::StringLiteral),
            ],
        );
        check(
            r##"`${() => { 'no}op' } + 'oop'}`"##,
            &vec![
                ("`${", TokenKind::StringLiteral),
                ("(", TokenKind::Punctuation),
                (")", TokenKind::Punctuation),
                ("=", TokenKind::Punctuation),
                (">", TokenKind::Punctuation),
                ("{", TokenKind::Punctuation),
                ("'no}op'", TokenKind::StringLiteral),
                ("}", TokenKind::Punctuation),
                ("+", TokenKind::Punctuation),
                ("'oop'", TokenKind::StringLiteral),
                ("}`", TokenKind::StringLiteral),
            ],
        );
    }

    #[test]
    fn test_regexp_js() {
        let spec = match select_formatting("test.js") {
            FormatAs::FormatCLike(spec) => spec,
            _ => {
                panic!("wrong spec");
            }
        };

        let check = |s: &str, expected: &[(&str, TokenKind)]| {
            check_tokens(s, expected, spec);
        };

        check("/foo/", &[("/foo/", TokenKind::RegularExpressionLiteral)]);
        check(
            "v = /foo/;",
            &[
                ("v", TokenKind::Identifier(None)),
                ("=", TokenKind::Punctuation),
                ("/foo/", TokenKind::RegularExpressionLiteral),
                (";", TokenKind::Punctuation),
            ],
        );
        check(
            "(/foo/);",
            &[
                ("(", TokenKind::Punctuation),
                ("/foo/", TokenKind::RegularExpressionLiteral),
                (")", TokenKind::Punctuation),
                (";", TokenKind::Punctuation),
            ],
        );
        check(
            "[/foo/];",
            &[
                ("[", TokenKind::Punctuation),
                ("/foo/", TokenKind::RegularExpressionLiteral),
                ("]", TokenKind::Punctuation),
                (";", TokenKind::Punctuation),
            ],
        );
        check(
            "{/foo/;}",
            &[
                ("{", TokenKind::Punctuation),
                ("/foo/", TokenKind::RegularExpressionLiteral),
                (";", TokenKind::Punctuation),
                ("}", TokenKind::Punctuation),
            ],
        );
        check(
            "({ p: /foo/ });",
            &vec![
                ("(", TokenKind::Punctuation),
                ("{", TokenKind::Punctuation),
                ("p", TokenKind::Identifier(None)),
                (":", TokenKind::Punctuation),
                ("/foo/", TokenKind::RegularExpressionLiteral),
                ("}", TokenKind::Punctuation),
                (")", TokenKind::Punctuation),
                (";", TokenKind::Punctuation),
            ],
        );
        check(
            "v = a && /foo/;",
            &vec![
                ("v", TokenKind::Identifier(None)),
                ("=", TokenKind::Punctuation),
                ("a", TokenKind::Identifier(None)),
                ("&", TokenKind::Punctuation),
                ("&", TokenKind::Punctuation),
                ("/foo/", TokenKind::RegularExpressionLiteral),
                (";", TokenKind::Punctuation),
            ],
        );
        check(
            "v = a || /foo/;",
            &vec![
                ("v", TokenKind::Identifier(None)),
                ("=", TokenKind::Punctuation),
                ("a", TokenKind::Identifier(None)),
                ("|", TokenKind::Punctuation),
                ("|", TokenKind::Punctuation),
                ("/foo/", TokenKind::RegularExpressionLiteral),
                (";", TokenKind::Punctuation),
            ],
        );
        check(
            "!/foo/.test(x);",
            &vec![
                ("!", TokenKind::Punctuation),
                ("/foo/", TokenKind::RegularExpressionLiteral),
                (".", TokenKind::Punctuation),
                ("test", TokenKind::Identifier(None)),
                ("(", TokenKind::Punctuation),
                ("x", TokenKind::Identifier(None)),
                (")", TokenKind::Punctuation),
                (";", TokenKind::Punctuation),
            ],
        );
        check(
            "[a, /foo/];",
            &vec![
                ("[", TokenKind::Punctuation),
                ("a", TokenKind::Identifier(None)),
                (",", TokenKind::Punctuation),
                ("/foo/", TokenKind::RegularExpressionLiteral),
                ("]", TokenKind::Punctuation),
                (";", TokenKind::Punctuation),
            ],
        );
        check(
            "v = a ? /foo/ : b;",
            &vec![
                ("v", TokenKind::Identifier(None)),
                ("=", TokenKind::Punctuation),
                ("a", TokenKind::Identifier(None)),
                ("?", TokenKind::Punctuation),
                ("/foo/", TokenKind::RegularExpressionLiteral),
                (":", TokenKind::Punctuation),
                ("b", TokenKind::Identifier(None)),
                (";", TokenKind::Punctuation),
            ],
        );
        check(
            "v = a ? b : /foo/;",
            &vec![
                ("v", TokenKind::Identifier(None)),
                ("=", TokenKind::Punctuation),
                ("a", TokenKind::Identifier(None)),
                ("?", TokenKind::Punctuation),
                ("b", TokenKind::Identifier(None)),
                (":", TokenKind::Punctuation),
                ("/foo/", TokenKind::RegularExpressionLiteral),
                (";", TokenKind::Punctuation),
            ],
        );
        check(
            "v = a ?? /foo/;",
            &vec![
                ("v", TokenKind::Identifier(None)),
                ("=", TokenKind::Punctuation),
                ("a", TokenKind::Identifier(None)),
                ("?", TokenKind::Punctuation),
                ("?", TokenKind::Punctuation),
                ("/foo/", TokenKind::RegularExpressionLiteral),
                (";", TokenKind::Punctuation),
            ],
        );
        check(
            "a => /foo/;",
            &[
                ("a", TokenKind::Identifier(None)),
                ("=", TokenKind::Punctuation),
                (">", TokenKind::Punctuation),
                ("/foo/", TokenKind::RegularExpressionLiteral),
                (";", TokenKind::Punctuation),
            ],
        );
        check(
            "a < /foo/.exec(s).length;",
            &vec![
                ("a", TokenKind::Identifier(None)),
                ("<", TokenKind::Punctuation),
                ("/foo/", TokenKind::RegularExpressionLiteral),
                (".", TokenKind::Punctuation),
                ("exec", TokenKind::Identifier(None)),
                ("(", TokenKind::Punctuation),
                ("s", TokenKind::Identifier(None)),
                (")", TokenKind::Punctuation),
                (".", TokenKind::Punctuation),
                ("length", TokenKind::Identifier(None)),
                (";", TokenKind::Punctuation),
            ],
        );
        check(
            "{}/foo/",
            &[
                ("{", TokenKind::Punctuation),
                ("}", TokenKind::Punctuation),
                ("/foo/", TokenKind::RegularExpressionLiteral),
            ],
        );
        check(
            "{}\n/foo/",
            &[
                ("{", TokenKind::Punctuation),
                ("}", TokenKind::Punctuation),
                ("\n", TokenKind::Newline),
                ("/foo/", TokenKind::RegularExpressionLiteral),
            ],
        );
        check(
            ";/foo/",
            &[
                (";", TokenKind::Punctuation),
                ("/foo/", TokenKind::RegularExpressionLiteral),
            ],
        );
        check(
            "x / /foo/.source.length",
            &vec![
                ("x", TokenKind::Identifier(None)),
                ("/", TokenKind::Punctuation),
                ("/foo/", TokenKind::RegularExpressionLiteral),
                (".", TokenKind::Punctuation),
                ("source", TokenKind::Identifier(None)),
                (".", TokenKind::Punctuation),
                ("length", TokenKind::Identifier(None)),
            ],
        );
        check(
            "x => { return /foo/; }",
            &vec![
                ("x", TokenKind::Identifier(None)),
                ("=", TokenKind::Punctuation),
                (">", TokenKind::Punctuation),
                ("{", TokenKind::Punctuation),
                (
                    "return",
                    TokenKind::Identifier(Some("class=\"syn_reserved\" ".to_string())),
                ),
                ("/foo/", TokenKind::RegularExpressionLiteral),
                (";", TokenKind::Punctuation),
                ("}", TokenKind::Punctuation),
            ],
        );
    }

    #[test]
    fn test_not_regexp_js() {
        let spec = match select_formatting("test.js") {
            FormatAs::FormatCLike(spec) => spec,
            _ => {
                panic!("wrong spec");
            }
        };

        let check = |s: &str, expected: &[(&str, TokenKind)]| {
            check_tokens(s, expected, spec);
        };
        check(
            "a/foo/g",
            &[
                ("a", TokenKind::Identifier(None)),
                ("/", TokenKind::Punctuation),
                ("foo", TokenKind::Identifier(None)),
                ("/", TokenKind::Punctuation),
                ("g", TokenKind::Identifier(None)),
            ],
        );
        check(
            "a++/foo/g",
            &vec![
                ("a", TokenKind::Identifier(None)),
                ("+", TokenKind::Punctuation),
                ("+", TokenKind::Punctuation),
                ("/", TokenKind::Punctuation),
                ("foo", TokenKind::Identifier(None)),
                ("/", TokenKind::Punctuation),
                ("g", TokenKind::Identifier(None)),
            ],
        );
        check(
            "1./foo/g",
            &vec![
                ("1", TokenKind::Identifier(None)),
                (".", TokenKind::Punctuation),
                ("/", TokenKind::Punctuation),
                ("foo", TokenKind::Identifier(None)),
                ("/", TokenKind::Punctuation),
                ("g", TokenKind::Identifier(None)),
            ],
        );
        check(
            "'1'/foo/g",
            &[
                ("'1'", TokenKind::StringLiteral),
                ("/", TokenKind::Punctuation),
                ("foo", TokenKind::Identifier(None)),
                ("/", TokenKind::Punctuation),
                ("g", TokenKind::Identifier(None)),
            ],
        );
        check(
            "\"1\"/foo/g",
            &[
                ("\"1\"", TokenKind::StringLiteral),
                ("/", TokenKind::Punctuation),
                ("foo", TokenKind::Identifier(None)),
                ("/", TokenKind::Punctuation),
                ("g", TokenKind::Identifier(None)),
            ],
        );
        check(
            "(a + b)/foo/g",
            &vec![
                ("(", TokenKind::Punctuation),
                ("a", TokenKind::Identifier(None)),
                ("+", TokenKind::Punctuation),
                ("b", TokenKind::Identifier(None)),
                (")", TokenKind::Punctuation),
                ("/", TokenKind::Punctuation),
                ("foo", TokenKind::Identifier(None)),
                ("/", TokenKind::Punctuation),
                ("g", TokenKind::Identifier(None)),
            ],
        );
        check(
            "[1]/foo/g",
            &vec![
                ("[", TokenKind::Punctuation),
                ("1", TokenKind::Identifier(None)),
                ("]", TokenKind::Punctuation),
                ("/", TokenKind::Punctuation),
                ("foo", TokenKind::Identifier(None)),
                ("/", TokenKind::Punctuation),
                ("g", TokenKind::Identifier(None)),
            ],
        );
    }

    #[test]
    fn check_newlines() {
        let js_spec = match select_formatting("test.js") {
            FormatAs::FormatCLike(spec) => spec,
            _ => {
                panic!("wrong spec");
            }
        };
        let cpp_spec = match select_formatting("test.cpp") {
            FormatAs::FormatCLike(spec) => spec,
            _ => {
                panic!("wrong spec");
            }
        };

        // C++ raw literal with a newline:
        check_tokens(
            "R\"(foo\nbar)\"",
            &[
                ("R\"(foo", TokenKind::StringLiteral),
                ("\n", TokenKind::Newline),
                ("bar)\"", TokenKind::StringLiteral),
            ],
            cpp_spec,
        );
        check_tokens(
            "/* one /* line\nanother line */",
            &[
                ("/* one /* line", TokenKind::Comment),
                ("\n", TokenKind::Newline),
                ("another line */", TokenKind::Comment),
            ],
            cpp_spec,
        );
        check_tokens(
            "`Hello ${world\n}\nanother line`",
            &vec![
                ("`Hello ${", TokenKind::StringLiteral),
                ("world", TokenKind::Identifier(None)),
                ("\n", TokenKind::Newline),
                ("}", TokenKind::StringLiteral),
                ("\n", TokenKind::Newline),
                ("another line`", TokenKind::StringLiteral),
            ],
            js_spec,
        );
    }

    #[test]
    fn check_rust_stuff() {
        let rust_spec = match select_formatting("test.rs") {
            FormatAs::FormatCLike(spec) => spec,
            _ => {
                panic!("wrong spec");
            }
        };

        // Rust byte strings
        check_tokens(
            r##"b'a' b"bbb""##,
            &[
                ("b'a'", TokenKind::StringLiteral),
                (r#"b"bbb""#, TokenKind::StringLiteral),
            ],
            rust_spec,
        );

        // Rust labels
        check_tokens(
            "&'static",
            &[
                ("&", TokenKind::Punctuation),
                ("'", TokenKind::Punctuation),
                ("static", TokenKind::Identifier(None)),
            ],
            rust_spec,
        );
        check_tokens(
            "&'static ",
            &[
                ("&", TokenKind::Punctuation),
                ("'", TokenKind::Punctuation),
                ("static", TokenKind::Identifier(None)),
            ],
            rust_spec,
        );
        check_tokens(
            "'label: while",
            &[
                ("'", TokenKind::Punctuation),
                ("label", TokenKind::Identifier(None)),
                (":", TokenKind::Punctuation),
                (
                    "while",
                    TokenKind::Identifier(Some(String::from("class=\"syn_reserved\" "))),
                ),
            ],
            rust_spec,
        );
        check_tokens(
            "'\\n' while",
            &[
                ("'\\n'", TokenKind::StringLiteral),
                (
                    "while",
                    TokenKind::Identifier(Some(String::from("class=\"syn_reserved\" "))),
                ),
            ],
            rust_spec,
        );
        check_tokens(
            "'b' while",
            &[
                ("'b'", TokenKind::StringLiteral),
                (
                    "while",
                    TokenKind::Identifier(Some(String::from("class=\"syn_reserved\" "))),
                ),
            ],
            rust_spec,
        );

        // Rust raw strings
        check_tokens(
            r##"r#"hello"world"#"##,
            &[(r##"r#"hello"world"#"##, TokenKind::StringLiteral)],
            rust_spec,
        );
        check_tokens(
            r#"r"hello world""#,
            &[(r#"r"hello world""#, TokenKind::StringLiteral)],
            rust_spec,
        );
        check_tokens(
            r###"r##"hello world"# there"##"###,
            &[(
                r###"r##"hello world"# there"##"###,
                TokenKind::StringLiteral,
            )],
            rust_spec,
        );
        check_tokens(
            "br\"hello world\"",
            &[("br\"hello world\"", TokenKind::StringLiteral)],
            rust_spec,
        );
        check_tokens(
            "br#\"hello world \" there\"#",
            &[("br#\"hello world \" there\"#", TokenKind::StringLiteral)],
            rust_spec,
        );

        // Rust nested comments
        check_tokens(
            "/* hello world */",
            &[("/* hello world */", TokenKind::Comment)],
            rust_spec,
        );
        check_tokens(
            "/* hello /* world */ there */",
            &[("/* hello /* world */ there */", TokenKind::Comment)],
            rust_spec,
        );
        check_tokens("/*/**/*/", &[("/*/**/*/", TokenKind::Comment)], rust_spec);

        // Rust numbers
        // NB: This result is a little unexpected, but it's fine since we
        // don't actually need to generate code. The resulting output should
        // look the same as though we actually parsed `1.5`.
        check_tokens(
            "1.5",
            &[
                ("1", TokenKind::Identifier(None)),
                (".", TokenKind::Punctuation),
                ("5", TokenKind::Identifier(None)),
            ],
            rust_spec,
        );
    }

    #[test]
    fn test_preproc_cpp() {
        let cpp_spec = match select_formatting("test.cpp") {
            FormatAs::FormatCLike(spec) => spec,
            _ => panic!("wrong spec"),
        };

        check_tokens(
            "#define",
            &[(
                "#define",
                TokenKind::Identifier(Some("class=\"syn_reserved\" ".to_string())),
            )],
            cpp_spec,
        );

        check_tokens(
            "#  \t  \t  define",
            &[(
                "#  \t  \t  define",
                TokenKind::Identifier(Some("class=\"syn_reserved\" ".to_string())),
            )],
            cpp_spec,
        );
    }

    #[test]
    fn test_css() {
        check_css_tokens(
            ".foo { bar: baz}\n#bar{}",
            &[
                (".", TokenKind::Punctuation),
                ("foo", TokenKind::Identifier(None)),
                (" ", TokenKind::PlainText),
                ("{", TokenKind::Punctuation),
                (" ", TokenKind::PlainText),
                (
                    "bar",
                    TokenKind::Identifier(Some(crate::languages::SYN_RESERVED_CLASS.into())),
                ),
                (":", TokenKind::Punctuation),
                (" ", TokenKind::PlainText),
                ("baz", TokenKind::Identifier(None)),
                ("}", TokenKind::Punctuation),
                ("\n", TokenKind::Newline),
                ("#bar", TokenKind::Identifier(None)),
                ("{", TokenKind::Punctuation),
                ("}", TokenKind::Punctuation),
            ],
        );
    }
}

```

## tools/src/cmd_pipeline/cmd_search_text.rs
```
use async_trait::async_trait;
use clap::Args;

use super::{
    interface::{PipelineCommand, PipelineValues},
    transforms::path_glob_transform,
};

use crate::abstract_server::{AbstractServer, ErrorDetails, ErrorLayer, Result, ServerError};

/// Perform a fulltext search against our livegrep/codesearch server over gRPC.
/// This is local-only at this time.
#[derive(Debug, Args)]
pub struct SearchText {
    /// Text to search for; this will be regexp escaped.
    #[clap(value_parser)]
    text: Option<String>,

    /// Search for a regular expression.  This can't be used if `text` is used.
    #[clap(long, value_parser)]
    re: Option<String>,

    /// Constrain matching path patterns with a non-regexp path constraint that
    /// will be escaped into a regexp.
    #[clap(long, value_parser)]
    path: Option<String>,

    /// Constrain matching path patterns with a regexp.
    #[clap(long, value_parser)]
    pathre: Option<String>,

    /// Should this be case-sensitive?  By default we are case-insensitive.
    #[clap(short, long, value_parser)]
    case_sensitive: bool,

    #[clap(short, long, value_parser, default_value = "0")]
    limit: usize,
}

#[derive(Debug)]
pub struct SearchTextCommand {
    pub args: SearchText,
}

#[async_trait]
impl PipelineCommand for SearchTextCommand {
    async fn execute(
        &self,
        server: &(dyn AbstractServer + Send + Sync),
        _input: PipelineValues,
    ) -> Result<PipelineValues> {
        let re_pattern = if let Some(re) = &self.args.re {
            re.clone()
        } else if let Some(text) = &self.args.text {
            regex::escape(text)
        } else {
            return Err(ServerError::StickyProblem(ErrorDetails {
                layer: ErrorLayer::BadInput,
                message: "Missing search text or `re` pattern!".to_string(),
            }));
        };

        let pathre_pattern = if let Some(pathre) = &self.args.pathre {
            pathre.clone()
        } else if let Some(path) = &self.args.path {
            path_glob_transform(path)
        } else {
            "".to_string()
        };

        let matches = server
            .search_text(
                &re_pattern,
                !self.args.case_sensitive,
                &pathre_pattern,
                self.args.limit,
            )
            .await?;

        Ok(PipelineValues::TextMatches(matches))
    }
}

```

## tools/src/cmd_pipeline/cmd_webtest.rs
```
use std::collections::HashMap;
use std::fs;
use std::io::Write;
use std::thread;
use std::time::{Duration, Instant};

use async_trait::async_trait;
use clap::Args;
use fantoccini::{Client, ClientBuilder};
use termcolor::{Color, ColorChoice, ColorSpec, StandardStream, WriteColor};

use super::interface::{PipelineCommand, PipelineValues};

use crate::abstract_server::{AbstractServer, ErrorDetails, ErrorLayer, Result, ServerError};

/// Runs the specified
#[derive(Debug, Args)]
pub struct Webtest {
    filter: Option<String>,
}

#[derive(Debug)]
pub struct WebtestCommand {
    pub args: Webtest,
}

fn print_log(ty: &str, msg: String) {
    let mut stderr = StandardStream::stderr(ColorChoice::Always);

    let mut spec = ColorSpec::new();

    let color = match ty {
        "INFO" => Color::Blue,
        "DEBUG" => Color::Black,
        "PASS" => Color::Green,
        "FAIL" => Color::Red,
        "STACK" => Color::Red,
        "TEST_START" => Color::Yellow,
        "TEST_END" => Color::Yellow,
        _ => Color::Cyan,
    };
    spec.set_fg(Some(color));

    if ty == "DEBUG" {
        spec.set_dimmed(true);
    }

    stderr.set_color(&spec).unwrap();
    write!(&mut stderr, "{}", ty).unwrap();

    stderr.reset().unwrap();
    writeln!(&mut stderr, " - {}", msg).unwrap();
}

fn println_color(color: Color, msg: &str) {
    let mut stderr = StandardStream::stderr(ColorChoice::Always);
    stderr
        .set_color(ColorSpec::new().set_fg(Some(color)))
        .unwrap();
    writeln!(&mut stderr, "{}", msg).unwrap();
    stderr.reset().unwrap();
}

fn println_bold(msg: &str) {
    let mut stderr = StandardStream::stderr(ColorChoice::Always);
    stderr.set_color(ColorSpec::new().set_bold(true)).unwrap();
    writeln!(&mut stderr, "{}", msg).unwrap();
    stderr.reset().unwrap();
}

type TestResult<T> = std::result::Result<T, String>;

impl WebtestCommand {
    async fn setup_webdriver_and_run_tests(&self) -> TestResult<bool> {
        let mut caps = serde_json::map::Map::new();
        let opts = serde_json::json!({ "args": ["--headless"] });
        caps.insert("moz:firefoxOptions".to_string(), opts);
        let client = ClientBuilder::native()
            .capabilities(caps)
            .connect("http://localhost:4444")
            .await
            .map_err(|e| format!("{:?}", e))?;

        let result = self.run_tests(&client).await;

        client.close().await.map_err(|e| format!("{:?}", e))?;

        let passed = result.map_err(|e| format!("{:?}", e))?;

        Ok(passed)
    }

    async fn run_tests(
        &self,
        client: &Client,
    ) -> std::result::Result<bool, fantoccini::error::CmdError> {
        let entire_start = Instant::now();
        let mut test_count = 0;
        let mut subtest_count = 0;
        let mut failed_tests = vec![];
        let mut failed_log = HashMap::new();

        let files = fs::read_dir("tests/webtest/").unwrap();
        for file in files {
            if file.is_err() {
                continue;
            }

            let file = file.unwrap();

            let name = file.file_name().clone().into_string().unwrap();
            if !name.starts_with("test_") {
                continue;
            }
            if !name.ends_with(".js") {
                continue;
            }

            let path = file.path().to_str().unwrap().to_string();

            if let Some(filter) = &self.args.filter {
                eprintln!("Filter: {} on {}", filter, path);
                if !path.contains(filter) {
                    continue;
                }
            }

            let url = "http://localhost/tests/webtest/webtest.html";
            print_log("INFO", format!("Navigate to {}", url));
            client.goto(url).await?;

            print_log("INFO", format!("Loading {}", path));
            client
                .execute(
                    "window.TestHarness.loadTest(...arguments);",
                    vec![serde_json::json!(path)],
                )
                .await?;

            let start = Instant::now();
            let mut failed = false;

            // TODO: Add special log command to increase the timeout.
            let timeout = 30 * 1000;

            'test_loop: loop {
                let log_value = client
                    .execute("return window.TestHarness.getNewLogs();", vec![])
                    .await?;
                let log: Vec<(String, String)> = serde_json::value::from_value(log_value)?;
                for (ty, msg) in log {
                    if ty == "SUBTEST" {
                        subtest_count += 1;
                        continue;
                    }

                    print_log(ty.as_str(), msg.clone());

                    if ty == "FAIL" {
                        failed = true;
                    }
                    if ty == "FAIL" || ty == "STACK" {
                        if !failed_log.contains_key(&path) {
                            failed_log.insert(path.clone(), vec![]);
                        }
                        failed_log
                            .get_mut(&path)
                            .unwrap()
                            .push((ty.clone(), msg.clone()));
                    }
                    if ty == "TEST_END" {
                        break 'test_loop;
                    }
                }
                let elapsed_time = start.elapsed();
                if elapsed_time > Duration::from_millis(timeout) {
                    failed = true;
                    print_log("FAIL", format!("{} | Test timed out", path));
                    break 'test_loop;
                }

                thread::sleep(Duration::from_millis(100));
            }

            if failed {
                failed_tests.push(path.clone());

                let filename = format!("/tmp/screen-{}.png", name);
                print_log("INFO", format!("Saving screenshot to {}", filename));
                let data = client.screenshot().await?;
                fs::write(filename, data)?;
            }

            test_count += 1;
        }

        let elapsed_time = entire_start.elapsed();
        eprintln!();
        println_color(Color::Yellow, "Overall Summary");
        println_color(Color::Yellow, "===============");
        eprintln!(
            "Ran {} tests and {} subtests in {:.3}s.",
            test_count,
            subtest_count,
            elapsed_time.as_millis() as f64 / 1000.0
        );
        eprintln!("Passed: {} tests", test_count - failed_tests.len());
        eprintln!("Failed: {} tests", failed_tests.len());
        eprintln!();

        let passed = failed_tests.is_empty();

        if !failed_tests.is_empty() {
            println_color(Color::Yellow, "Unexpected Results");
            println_color(Color::Yellow, "------------------");
            for path in failed_tests {
                println_bold(path.as_str());
                if let Some(log) = failed_log.get(&path) {
                    for (ty, msg) in log {
                        print_log(ty.as_str(), msg.clone());
                    }
                }
            }
        } else {
            eprintln!("OK");
        }

        Ok(passed)
    }
}

#[async_trait]
impl PipelineCommand for WebtestCommand {
    async fn execute(
        &self,
        _server: &(dyn AbstractServer + Send + Sync),
        _input: PipelineValues,
    ) -> Result<PipelineValues> {
        let passed = self.setup_webdriver_and_run_tests().await.map_err(|e| {
            ServerError::TransientProblem(ErrorDetails {
                layer: ErrorLayer::ConfigLayer,
                message: e,
            })
        })?;

        if !passed {
            return Err(ServerError::TransientProblem(ErrorDetails {
                layer: ErrorLayer::ConfigLayer,
                message: "Test failed".to_string(),
            }));
        }

        Ok(PipelineValues::Void)
    }
}

```

## tools/src/cmd_pipeline/cmd_graph.rs
```
use std::collections::HashSet;
use std::iter::FromIterator;

use async_trait::async_trait;
use clap::{Args, ValueEnum};
use dot_generator::*;
use dot_structures::*;
use regex::{Captures, Regex};
use serde_json::{json, Value};

use graphviz_rust::cmd::{CommandArg, Format, Layout};
use graphviz_rust::exec;
use graphviz_rust::printer::{DotPrinter, PrinterContext};

use super::interface::{
    GraphResultsBundle, PipelineCommand, PipelineValues, RenderedGraph, TextFile,
};
use super::symbol_graph::{
    DerivedSymbolInfo, HierarchicalRenderState, HierarchyDefaultSummarizePolicy, HierarchyPolicies,
};

use crate::abstract_server::{AbstractServer, Result};

#[derive(Clone, Debug, PartialEq, ValueEnum)]
pub enum GraphFormat {
    // JSON format, useful for when GraphMode is Hier.
    Json,
    // Raw dot syntax without any layout performed.
    RawDot,
    #[allow(clippy::upper_case_acronyms)]
    SVG,
    #[allow(clippy::upper_case_acronyms)]
    PNG,
    // Dot with layout information.
    Dot,
    // Transformed SVG accompanied by symbol metadata in a JSON structure.
    Mozsearch,
}

#[derive(Clone, Debug, PartialEq, ValueEnum)]
pub enum GraphHierarchy {
    /// No hierarchy, everything is a node, there are no clusters.
    Flat,
    /// Derive hierarchy from the pretty identifier hierarchy exclusively.
    Pretty,
    /// Derive hierarchy from the subsystem and class structure, skipping
    /// explicit C++ namespaces.
    Subsystem,
    /// Derive hierarchy from the full file paths containing definitions, noting
    /// that this inherently may fragment a C++ class so that the class is
    /// defined in a header file and many of its methods are defined in a cpp
    /// file.
    File,
    /// Derive hierarchy from the directories that contain the files symbols
    /// are defined in, but ignoring the actual filename.  This should keep C++
    /// classes and their methods together.
    /// TODO: Make sure we always map installed headers back to their origin
    /// location.
    Dir,
}

#[derive(Clone, Debug, PartialEq, ValueEnum)]
pub enum GraphLayout {
    /// Default use of the dot engine.
    Dot,
    /// Use neato
    Neato,
    /// Use fdp
    Fdp,
}

/// Render a received graph into a dot, svg, or json-wrapped-svg which also
/// includes embedded crossref information.
#[derive(Debug, Args)]
pub struct Graph {
    #[clap(long, value_parser, value_enum, default_value = "svg")]
    pub format: GraphFormat,

    #[clap(long, value_parser, value_enum, default_value = "pretty")]
    pub hier: GraphHierarchy,

    #[clap(long, value_parser, value_enum, default_value = "dot")]
    pub layout: GraphLayout,

    /// Enable debug mode which currently means forcing the format to be Json.
    /// This is currently structured this way because this is intended to be
    /// used as a flag translated by `query_core.toml` and we avoid problems
    /// where the default format argument created by the pipelien fights a user
    /// controlled value.  But it also makes sense to have this debug flag be
    /// something explicit and for the explicit use of the query syntax.
    #[clap(long, value_parser)]
    pub debug: bool,

    /// When to summarize clusters in hierarchical diagrams.
    #[clap(long, value_parser, value_enum, default_value = "none")]
    pub summarize: HierarchyDefaultSummarizePolicy,

    /// Force summarize clusters with the given pretty identifiers.  This
    /// overrrides the defaults provided by "summarize".
    #[clap(long, value_parser)]
    pub collapse: Vec<String>,

    /// Force expand clusters with the given pretty identifiers.  This
    /// overrides the defaults provided by "summarize".
    #[clap(long, value_parser)]
    pub expand: Vec<String>,

    /// How many (pointer-like) fields should have a class have before we group
    /// its fields by subsystem.
    #[clap(long, value_parser = clap::value_parser!(u32).range(0..=1024), default_value = "6")]
    pub group_fields_at: u32,

    #[clap(long, value_parser)]
    pub colorize_callees: Vec<String>,
}

/// ## Graph Implementation Thoughts / Rationale ##
///
/// ### Latency, Pre-Computation, and Interaction ###
///
/// #### Pre-Graph Status Quo ####
///
/// Current searchfox UX is that while search may take a few seconds (the first
/// time the query is experienced; we do cache), when they arrive, you'll have
/// all the results you're going to get unless you continue typing.  There's no
/// async trickle-in.
///
/// While there can be upsides to async data retrieval, this primarily makes
/// sense for cases where the data being asynchronously populated is reliably
/// known to be at the end of the current results list.  Asynchronous retrieval
/// that leads to visual and interaction instability can be frustrating,
/// especially when it's not clear if the results have stabilized.
///
/// One thing we haven't done yet in the normal searchfox UI (but did experiment
/// with in the fancy branch) is to allow iterative (faceted) filtering of the
/// displayed results.  There has only been the ability to collapse sections of
/// results.  But we could do more with this.
///
/// #### Application to Graphing ####
///
/// When building the graph, we will potentially gather information about edges
/// to nodes that don't make the initial cut for presentation.  But rather than
/// discarding them, we'll keep them around in the dataset that we serve up so
/// that the collapsed clusters can be interactively expanded or additional data
/// (ex: on fields accessed) can be provided in a detail display when clicking
/// on nodes, etc.
///
/// Using IndexedDB as an example of what this means, from the fancy branch we
/// already know that edges can potentially fall into the following groups:
/// - In-module function calls.  This covers both "boring" getters/setters and
///   assertions that don't express interseting control flow, as well as more
///   significant helper modules that potentially in turn call other non-boring
///   methods.
/// - Cross-module function calls to non-core-infrastructure modules.  In IDB
///   this would mean Quota Manager and mozStorage are both modules that
///   involve core application-domain logic.
/// - Cross-module function calls to "boring" core-infrastructure modules.  For
///   example, the fancy branch elides all calls to smart pointers and XPCOM
///   string classes because these usually are not interesting on their own and
///   the field is instead more interesting.  Note that the fancy branch ended
///   up filtering to only in-module edges eventually, which meant that this
///   additional filtering was somewhat mooted and potentially was not
///   sufficient as it would not have prevented data structure spam, etc.
///
/// As noted above, the fancy branch prototype found that limiting calls to the
/// same module as determined by source path provided a reasonable filtering,
/// but it's quite possible that the interesting bits are in fact happening in
/// other modules.  So, at least as long as a work limit isn't it, we could
/// traverse into the other modules but make a choice at presentation time to
/// collapse those edges by having clustered by module and simpifying the edges
/// so that they go to a single node representation of the cluster that can be
/// clicked on to be expanded.
///
/// The expansion can be handled by using existing JS code (built on graphviz
/// compiled to WASM) that can animate a transition between the different
/// rendered graph states.
///
/// Note that this is currently an end state of the proposal at
/// https://bugzilla.mozilla.org/show_bug.cgi?id=1749232 and we won't be
/// implementing this initially, but this will inform how the graph is modeled.
/// That said, it's quite possible most of this logic will be implemented as a
/// graph transformation pass that clusters nodes.  The initial transitive
/// traversal might instead be focused on a work limit heuristic based on rough
/// order-of-magnitude weight adjustments.
///
#[derive(Debug)]
pub struct GraphCommand {
    pub args: Graph,
}

/// Convert tunneled symbol identifiers in the SVG into `data-symbols`
/// attributes.  Specific conversions:
/// - `<title>` tags holding the node identifiers.
/// - `<a xlink...>` tags holding links resulting from HTML labels.
///
/// Currently our general behavior is to drop anything that has an identifier
/// that starts with "SYN_" and to assume that everything else is a valid
/// symbol.  The original fancy branch prototype instead generated completely
/// synthetic identifiers in all cases which were added to a map so that
/// data-symbols could be looked up in the map.  I think there's something to be
/// said for for having the identifiers be more self-descriptive as we're
/// currently doing, but arguably it probably makes more sense to generate a
/// mangled pretty identifier with compensation made for any collisions.
///
/// TODO: As proposed above, potentially move towards allocating identifiers
/// with a lookup map.
fn transform_svg(svg: &str) -> String {
    lazy_static! {
        static ref RE_TITLE: Regex = Regex::new(">\n<title>([^<]+)</title>").unwrap();
        static ref RE_XLINK: Regex =
            Regex::new(r#"<a xlink:href="([^"]+)" xlink:title="[^"]+">"#).unwrap();
    }
    let titled = RE_TITLE.replace_all(svg, |caps: &Captures| {
        let captured = caps.get(1).unwrap().as_str();
        // Do not transform the `g` title of "g" to data-symbols.  Although
        // maybe we should be providing it a better title?  Although maybe
        // a straight-up heading explaining the graph is even better, as I
        // think this is where we're going to want to put the dual UI.
        if captured == "g" || captured.starts_with("SYN_") {
            ">".to_string()
        } else {
            format!(
                " data-symbols=\"{}\">",
                urlencoding::decode(captured).unwrap_or_default()
            )
        }
    });
    RE_XLINK
        .replace_all(&titled, |caps: &Captures| {
            let captured = caps.get(1).unwrap().as_str();
            if captured.starts_with("SYN_") {
                "<g>".to_string()
            } else {
                format!(
                    "<g data-symbols=\"{}\">",
                    urlencoding::decode(captured).unwrap_or_default()
                )
            }
        })
        .replace("</a>", "</g>")
}

#[async_trait]
impl PipelineCommand for GraphCommand {
    async fn execute(
        &self,
        server: &(dyn AbstractServer + Send + Sync),
        input: PipelineValues,
    ) -> Result<PipelineValues> {
        let mut graphs = match input {
            PipelineValues::SymbolGraphCollection(sgc) => sgc,
            // TODO: Figure out a better way to handle a nonsensical pipeline
            // configuration / usage.
            _ => {
                return Ok(PipelineValues::Void);
            }
        };

        let decorate_node = |node: &mut Node, sym_info: &DerivedSymbolInfo| {
            for (i, colorize) in self.args.colorize_callees.iter().enumerate() {
                if let Some(Value::Array(arr)) = sym_info.crossref_info.get("callees") {
                    for callee in arr {
                        if let Some(Value::String(pretty)) = callee.get("pretty") {
                            if pretty.ends_with(colorize) {
                                node.attributes.push(attr!("colorscheme", "pastel28"));
                                node.attributes.push(attr!("style", "filled"));
                                node.attributes.push(attr!("fillcolor", i + 1));
                            }
                        }
                    }
                }
            }
        };

        let (dot_graph, render_state) = match &self.args.hier {
            GraphHierarchy::Flat => (
                graphs.graph_to_graphviz(graphs.graphs.len() - 1, decorate_node),
                HierarchicalRenderState::new(),
            ),
            hier_mode => {
                let policies = HierarchyPolicies {
                    grouping: hier_mode.clone(),
                    summarize: self.args.summarize.clone(),
                    force_summarize_pretties: HashSet::from_iter(
                        self.args.collapse.iter().cloned(),
                    ),
                    force_expand_pretties: HashSet::from_iter(self.args.expand.iter().cloned()),
                    group_fields_at: self.args.group_fields_at,
                    use_port_dirs: false,
                };
                graphs
                    .derive_hierarchical_graph(&policies, graphs.graphs.len() - 1, server)
                    .await?;
                //return Ok(PipelineValues::SymbolGraphCollection(graphs));
                graphs.hierarchical_graph_to_graphviz(
                    &policies,
                    graphs.hierarchical_graphs.len() - 1,
                    &self.args.layout,
                )
            }
        };
        if self.args.format == GraphFormat::RawDot {
            let raw_dot_str = dot_graph.print(&mut PrinterContext::default());
            return Ok(PipelineValues::TextFile(TextFile {
                mime_type: "text/x-dot".to_string(),
                contents: raw_dot_str,
            }));
        }

        // Currently our debug mode is to just force ourselves to render the graph
        // as JSON.
        let use_format = match (self.args.debug, &self.args.format) {
            (true, _) => GraphFormat::Json,
            (_, format) => format.clone(),
        };

        let (format, mime_type) = match &use_format {
            GraphFormat::SVG | GraphFormat::Mozsearch => (Format::Svg, "image/svg+xml".to_string()),
            GraphFormat::PNG => (Format::Png, "image/png".to_string()),
            _ => (Format::Dot, "text/x-dot".to_string()),
        };
        let mut exec_commands = vec![CommandArg::Format(format)];
        exec_commands.push(match self.args.layout {
            GraphLayout::Dot => CommandArg::Layout(Layout::Dot),
            GraphLayout::Neato => CommandArg::Layout(Layout::Neato),
            GraphLayout::Fdp => CommandArg::Layout(Layout::Fdp),
        });
        let graph_contents = exec(dot_graph, &mut PrinterContext::default(), exec_commands)?;
        match use_format {
            GraphFormat::Json => Ok(PipelineValues::SymbolGraphCollection(graphs)),
            GraphFormat::Mozsearch => Ok(PipelineValues::GraphResultsBundle(GraphResultsBundle {
                graphs: vec![RenderedGraph {
                    graph: transform_svg(&graph_contents),
                    extra: json!({
                        "nodes": render_state.svg_node_extra,
                        "edges": render_state.svg_edge_extra,
                    }),
                }],
                symbols: graphs.node_set.symbols_meta_to_jumpref_json_destructive(),
                overloads_hit: graphs.overloads_hit,
            })),
            _ => Ok(PipelineValues::TextFile(TextFile {
                mime_type,
                contents: graph_contents,
            })),
        }
    }
}

```

## tools/src/cmd_pipeline/cmd_render.rs
```
use async_trait::async_trait;
use clap::Args;

use super::interface::{PipelineCommand, PipelineValues};
use crate::{
    abstract_server::{
        AbstractServer, ErrorDetails, ErrorLayer, Result, SearchfoxIndexRoot, ServerError,
    },
    file_utils::write_file_ensuring_parent_dir,
    templating::builder::{
        build_and_parse_help_index, build_and_parse_search_template, build_and_parse_settings,
    },
};

/// Render a single template, potentially processing pipeline input.
#[derive(Debug, Args)]
pub struct Render {
    /// Preconfigured rendering task.  This could be an enum or sub-command, but
    /// for now we're just going for strings.
    #[clap(value_parser)]
    task: String,
}

/// General operation:
/// - We take a `BatchGroups` as input.
/// - We iterate over each batch group and pass it to the liquid template
///   associated with this task.
/// - We expand the path template associated with the task and write it out.
#[derive(Debug)]
pub struct RenderCommand {
    pub args: Render,
}

#[async_trait]
impl PipelineCommand for RenderCommand {
    async fn execute(
        &self,
        server: &(dyn AbstractServer + Send + Sync),
        _input: PipelineValues,
    ) -> Result<PipelineValues> {
        let tree_info = server.tree_info()?;

        match self.args.task.as_str() {
            "search-template" => {
                let template = build_and_parse_search_template();

                let liquid_globals = liquid::object!({
                    "tree": tree_info.name,
                    // the header always needs this
                    "query": "",
                });
                let rendered = match template.render(&liquid_globals) {
                    Ok(r) => r,
                    Err(e) => {
                        return Err(ServerError::StickyProblem(ErrorDetails {
                            layer: ErrorLayer::ConfigLayer,
                            message: format!("Template problems: {}", e),
                        }));
                    }
                };
                let output_path =
                    server.translate_path(SearchfoxIndexRoot::IndexTemplates, "search.html")?;
                write_file_ensuring_parent_dir(&output_path, &rendered)?;
                Ok(PipelineValues::Void)
            }
            "help" => {
                let template = build_and_parse_help_index();

                let content_path =
                    server.translate_path(SearchfoxIndexRoot::ConfigRepo, "help.html")?;
                let content = std::fs::read_to_string(content_path)?;

                let liquid_globals = liquid::object!({
                    "tree": tree_info.name,
                    // the header always needs this
                    "query": "",
                    "content": content,
                });
                let rendered = match template.render(&liquid_globals) {
                    Ok(r) => r,
                    Err(e) => {
                        return Err(ServerError::StickyProblem(ErrorDetails {
                            layer: ErrorLayer::ConfigLayer,
                            message: format!("Template problems: {}", e),
                        }));
                    }
                };
                let output_path =
                    server.translate_path(SearchfoxIndexRoot::IndexTemplates, "help.html")?;
                write_file_ensuring_parent_dir(&output_path, &rendered)?;
                Ok(PipelineValues::Void)
            }
            "settings" => {
                let template = build_and_parse_settings();

                let liquid_globals = liquid::object!({
                    "tree": tree_info.name,
                    // the header always needs this
                    "query": "",
                });
                let rendered = match template.render(&liquid_globals) {
                    Ok(r) => r,
                    Err(e) => {
                        return Err(ServerError::StickyProblem(ErrorDetails {
                            layer: ErrorLayer::ConfigLayer,
                            message: format!("Template problems: {}", e),
                        }));
                    }
                };
                let output_path =
                    server.translate_path(SearchfoxIndexRoot::IndexPages, "settings.html")?;
                write_file_ensuring_parent_dir(&output_path, &rendered)?;
                Ok(PipelineValues::Void)
            }
            unknown => Err(ServerError::StickyProblem(ErrorDetails {
                layer: ErrorLayer::ConfigLayer,
                message: format!("Unknown task type: {}", unknown),
            })),
        }
    }
}

```

## tools/src/cmd_pipeline/cmd_crossref_expand.rs
```
use std::collections::{HashSet, VecDeque};

use async_trait::async_trait;
use clap::Args;
use serde_json::Value;
use tracing::trace;
use ustr::ustr;

use super::interface::{
    OverloadInfo, OverloadKind, PipelineCommand, PipelineValues, SymbolCrossrefInfo,
    SymbolCrossrefInfoList, SymbolMetaFlags, SymbolRelation,
};

use crate::abstract_server::{AbstractServer, ErrorDetails, ErrorLayer, Result, ServerError};

/// Given a set of symbol crossref data, expand the set via relevant semantic
/// relationships like override set membership.  This is fundamentally entwined
/// with what information we want to present in search results and how we want
/// to present it.
#[derive(Debug, Args)]
pub struct CrossrefExpand {
    #[clap(long, value_parser, default_value = "100")]
    pub subclass_local_limit: u32,
    #[clap(long, value_parser, default_value = "400")]
    pub subclass_global_limit: u32,

    #[clap(long, value_parser, default_value = "100")]
    pub override_local_limit: u32,
    #[clap(long, value_parser, default_value = "400")]
    pub override_global_limit: u32,
}

/// Crosseref expansion exists to help us:
/// 1. Provide relevant context to symbol results, like class hierarchies.
/// 2. Let us unify results that could be said to be conceptually equivalent but
///    which logistically involve distinct symbols.
///
/// Before the introduction of "structured" records, we unified results by
/// leveraging the ability to associate multiple symbols with a single
/// identifier.  Clever langage-specific analyses could do things like tie the
/// JS symbol and C++ getter and setter symbols to the identifier for an IDL
/// file, or associate all C++ overrides with every layer of their C++ class
/// ancestry.  This was generally amazing with the one downside that if you
/// searched for a specific sub-class's override by identifier that in turn was
/// overridden by its own sub-classes, you would also get its cousins' overrides
/// with no way in the UI to ignore the cousins.  One would have to arrive at
/// a symbol search that listed the unified list of symbols as would be produced
/// by the "search" context menu option and remove the symbols that were not of
/// interest.
///
/// Post-structured refactoring, an intentional decision has been made to have
/// mozsearch understand the semantic relationships explicitly so that analyses
/// don't have to do hacky things which destroy information and potentially
/// require other layers to have to undo or work-around cleverness, especially
/// since it doesn't scale across multiple langauge analyzers trying to be
/// clever in their own ways.  (These changes also allow for other meaningful
/// optimizations like replacing the jumps/searches ANALYSIS_DATA table with
/// crossref data and massively eliminating data duplication.)
///
/// Currently we still exist in a hybrid scenario, where overrides must be
/// manually traversed but IPC/IDL edges currently still involve the identifiers
/// lookup mechanism unifying things.  Our initial focus in this implementation
/// will be on the override set because this is where we've regressed some use
/// cases and we have tentative plans for faceting and diagramming.
#[derive(Debug)]
pub struct CrossrefExpandCommand {
    pub args: CrossrefExpand,
}

struct LimitGroup {
    kind: OverloadKind,
    local_limit: u32,
    // Hitting the global limit is a multi-step process, so we have to keep a
    // running tally.
    global_count: u32,
    global_limit: u32,
}

#[async_trait]
impl PipelineCommand for CrossrefExpandCommand {
    async fn execute(
        &self,
        server: &(dyn AbstractServer + Send + Sync),
        input: PipelineValues,
    ) -> Result<PipelineValues> {
        let source_crossrefs = match input {
            PipelineValues::SymbolCrossrefInfoList(scil) => scil,
            _ => {
                return Err(ServerError::StickyProblem(ErrorDetails {
                    layer: ErrorLayer::ConfigLayer,
                    message: "crossref-expand needs a CrossrefInfoList".to_string(),
                }));
            }
        };

        // Our approach here is derived from that of `traverse` because of the
        // inherent conceptual overlap and the implementation necessity to
        // ensure we only process a given symbol at most once.  Things are
        // simpler here, though, because we're just dealing with set membership
        // here, not edges (which can be erroneously elided if naively
        // supressing vertices without being aware of the tupling).
        //
        // That said, when our input set involves multiple symbols that exist
        // within the same connected groups but have different identifiers, we
        // will likely run into problems with the single `SymbolRelation` we use
        // to label each symbol.  We do take care to list distances in the enum,
        // but even a min() heuristic is potentially going to look weird.

        let mut to_traverse = VecDeque::new();
        let mut considered = HashSet::new();

        for info in source_crossrefs.symbol_crossref_infos {
            considered.insert(info.symbol);
            to_traverse.push_back((
                info.symbol,
                info.relation.clone(),
                info.quality.clone(),
                Some(info),
            ));
        }

        let mut expanded = vec![];

        // Running tallies for our limits.
        let mut override_limits = LimitGroup {
            kind: OverloadKind::Overrides,
            local_limit: self.args.override_local_limit,
            global_count: 0,
            global_limit: self.args.override_global_limit,
        };
        let mut subclass_limits = LimitGroup {
            kind: OverloadKind::Subclasses,
            local_limit: self.args.subclass_local_limit,
            global_count: 0,
            global_limit: self.args.subclass_global_limit,
        };

        while let Some((symbol, relation, quality, maybe_info)) = to_traverse.pop_front() {
            let mut info = match maybe_info {
                Some(existing) => existing,
                None => {
                    let fresh_info = server.crossref_lookup(&symbol, false).await?;
                    SymbolCrossrefInfo {
                        symbol,
                        crossref_info: fresh_info,
                        relation: relation.clone(),
                        quality,
                        overloads_hit: vec![],
                        flags: SymbolMetaFlags::default(),
                    }
                }
            };

            // Given a JSON pointer to a an array, if present, process it, transforming
            // each array value using `xfunc` to extract the symbol.  `use_relation`
            // specifies the resulting relationship that should be associated
            // with the extracted symbol.  `use_limits` is the `LimitGroup` to
            // adjust and apply.
            let mut proc_ptr =
                |ptr: &str,
                 xfunc: &dyn Fn(&Value) -> &Value,
                 use_relation: SymbolRelation,
                 use_limits: Option<&mut LimitGroup>| {
                    if let Some(Value::Array(arr)) = info.crossref_info.pointer(ptr) {
                        if let Some(limits) = use_limits {
                            if limits.local_limit > 0 && arr.len() as u32 > limits.local_limit {
                                info.overloads_hit.push(OverloadInfo {
                                    kind: limits.kind.clone(),
                                    // We're explicitly hanging off a symbol, so we don't need to
                                    // encode any other symbol here.
                                    sym: None,
                                    exist: arr.len() as u32,
                                    included: 0,
                                    local_limit: limits.local_limit,
                                    global_limit: 0,
                                });
                                return;
                            }
                            if limits.global_limit > 0
                                && limits.global_count + arr.len() as u32 > limits.global_limit
                            {
                                info.overloads_hit.push(OverloadInfo {
                                    kind: limits.kind.clone(),
                                    // We're explicitly hanging off a symbol, so we don't need to
                                    // encode any other symbol here.
                                    sym: None,
                                    exist: arr.len() as u32,
                                    included: 0,
                                    local_limit: 0,
                                    global_limit: limits.global_limit,
                                });
                                return;
                            }
                            limits.global_count += arr.len() as u32;
                        }
                        trace!(edge = ptr, count = arr.len(), "considering");
                        for wrapped in arr.iter() {
                            if let Value::String(sym) = xfunc(wrapped) {
                                let usym = ustr(sym);
                                if considered.insert(usym) {
                                    to_traverse.push_back((
                                        usym,
                                        use_relation.clone(),
                                        info.quality.clone(),
                                        None,
                                    ));
                                }
                            }
                        }
                    }
                };

            // Build up our traversal lists based on the relation for the node
            // we're processing.  Our general approach is:
            // - `Queried` nodes are the roots of our search and all edges are
            //   possible.
            // - As we move down into descendant nodes we only continue to move
            //   in this direction because upward movement would be
            //   backtracking.
            // - As we move upward into ancestor nodes, there are new downward
            //   edges to consider, and we do consider these and label them
            //   cousins.  We rely on `considered` to avoid creating loops.
            // - We apply local and global (within this command) limits to avoid
            //   performing traversals of edge sets that are too large.  This is
            //   primarily about not trying to show all the subclasses of
            //   nsISupports or all the overrides of nsISupports::AddRef
            //   automatically without the user explicitly indicating that's
            //   what they want.
            match &relation {
                SymbolRelation::Queried => {
                    proc_ptr(
                        "/meta/overridenBy",
                        &|x| x,
                        SymbolRelation::OverrideOf(symbol, 1),
                        Some(&mut override_limits),
                    );
                    proc_ptr(
                        "/meta/overrides",
                        &|x| &x["sym"],
                        SymbolRelation::OverriddenBy(symbol, 1),
                        None,
                    );
                    proc_ptr(
                        "/meta/subclasses",
                        &|x| x,
                        SymbolRelation::SubclassOf(symbol, 1),
                        Some(&mut subclass_limits),
                    );
                    proc_ptr(
                        "/meta/supers",
                        &|x| &x["sym"],
                        SymbolRelation::SuperclassOf(symbol, 1),
                        None,
                    );
                }
                SymbolRelation::OverriddenBy(root_sym, dist) => {
                    proc_ptr(
                        "/meta/overrides",
                        &|x| &x["sym"],
                        SymbolRelation::OverriddenBy(*root_sym, dist + 1),
                        None,
                    );
                    proc_ptr(
                        "/meta/overridenBy",
                        &|x| x,
                        SymbolRelation::CousinOverrideOf(*root_sym, dist + 1),
                        Some(&mut override_limits),
                    );
                }
                SymbolRelation::OverrideOf(root_sym, dist) => {
                    proc_ptr(
                        "/meta/overridenBy",
                        &|x| x,
                        SymbolRelation::OverrideOf(*root_sym, dist + 1),
                        Some(&mut override_limits),
                    );
                }
                SymbolRelation::CousinOverrideOf(root_sym, dist) => {
                    proc_ptr(
                        "/meta/overridenBy",
                        &|x| x,
                        SymbolRelation::CousinOverrideOf(*root_sym, dist + 1),
                        Some(&mut override_limits),
                    );
                }
                SymbolRelation::SubclassOf(root_sym, dist) => {
                    proc_ptr(
                        "/meta/subclasses",
                        &|x| x,
                        SymbolRelation::SubclassOf(*root_sym, dist + 1),
                        Some(&mut subclass_limits),
                    );
                }
                SymbolRelation::SuperclassOf(root_sym, dist) => {
                    proc_ptr(
                        "/meta/supers",
                        &|x| &x["sym"],
                        SymbolRelation::SuperclassOf(*root_sym, dist + 1),
                        None,
                    );
                    proc_ptr(
                        "/meta/subclasses",
                        &|x| x,
                        SymbolRelation::CousinClassOf(*root_sym, dist + 1),
                        Some(&mut subclass_limits),
                    );
                }
                SymbolRelation::CousinClassOf(root_sym, dist) => {
                    proc_ptr(
                        "/meta/subclasses",
                        &|x| x,
                        SymbolRelation::CousinClassOf(*root_sym, dist + 1),
                        Some(&mut subclass_limits),
                    );
                }
            }

            expanded.push(info);
        }

        Ok(PipelineValues::SymbolCrossrefInfoList(
            SymbolCrossrefInfoList {
                symbol_crossref_infos: expanded,
                unknown_symbols: vec![],
            },
        ))
    }
}

```

## tools/src/cmd_pipeline/cmd_augment_results.rs
```
use std::{cell::Cell, collections::HashMap, rc::Rc};

use async_trait::async_trait;
use clap::Parser;
use lol_html::{element, HtmlRewriter, Settings};
use ustr::UstrMap;

use super::interface::{PipelineCommand, PipelineValues};
use crate::abstract_server::{
    AbstractServer, ErrorDetails, ErrorLayer, HtmlFileRoot, Result, ServerError,
};

/// Augment a FlattenedResultsBundle by scraping the rendered HTML output files
/// for lines of interest plus any context, plus applying any predicates that
/// run against data baked into the output file (like coverage data or history
/// data).
///
/// It's possible the performance for this will be horrible, although it may be
/// possible to throw some of the following at this model:
/// - Having the HTML line extraction run as multiple parallel tasks.
/// - Increasing VM RAM to allow for more caching.
/// - Switching to a VM with local SSD for better I/O.
/// - Pre-computing line offsets (in the uncompresed HTML, noting that we do
///   gzip the HTML and probably still want to seek through that) so that we can
///   reduce the amount of HTML parsing required (even if we still have to seek
///   through a lot).
/// - Create some alternate on-disk representation for the rendered HTML files
///   that's intentionally distinct from the pre-gzipped files.  This could be
///   a different compression format and/or sharded files (maybe with duplicate
///   data at the start/end so we never have to span shards normally).
///
/// Alternately, we might:
/// - Run a post-file-rendering phase and effectively re-compute the crossref
///   database with HTML baked in and some number of extra lines of context?
/// - Have the crossref database include some extra context and have tokenizer
///   state included at the first line point so the tokenizer can do a bare
///   bones syntax highlighting.
#[derive(Debug, Parser)]
pub struct AugmentResults {
    /// Lines of context before a hit.
    #[clap(short, long, value_parser, default_value = "0")]
    before: u32,

    /// Lines of context after a hit.
    #[clap(short, long, value_parser, default_value = "0")]
    after: u32,
}

#[derive(Debug)]
pub struct AugmentResultsCommand {
    pub args: AugmentResults,
}

#[async_trait]
impl PipelineCommand for AugmentResultsCommand {
    async fn execute(
        &self,
        server: &(dyn AbstractServer + Send + Sync),
        input: PipelineValues,
    ) -> Result<PipelineValues> {
        let mut results = match input {
            PipelineValues::FlattenedResultsBundle(frb) => frb,
            _ => {
                return Err(ServerError::StickyProblem(ErrorDetails {
                    layer: ErrorLayer::ConfigLayer,
                    message: "augment-resultst needs a FlattenedResultsBundle".to_string(),
                }));
            }
        };

        // ### Build up map of HTML lines
        //
        // This bit could potentially run in parallel.  We're not going to yet
        // because:
        // - Wanna see how bad it is.
        // - Don't wanna have this experimental thing steal all the resources
        //   from the non-experimental router.py and web-server.rs yet!

        let mut path_line_contents: UstrMap<HashMap<u32, String>> = UstrMap::default();

        for (path, lines_to_show) in
            results.compute_path_line_sets(self.args.before, self.args.after)
        {
            // XXX Doing this as a single string received in a lump is fine for
            // our testing use-case, but this may need to be reconsidered in
            // production.  Or maybe production really wants the performance?
            // Production certainly should have the RAM for our known worst
            // case scenarios.
            let html_str = server
                .fetch_html(HtmlFileRoot::FormattedFile, &path)
                .await?;

            let file_lines = path_line_contents.entry(path).or_default();

            // ### HTML Extraction: What We Want
            //
            // We want the full line container which looks like:
            // - div id="line-N" class="source-line-with-number" role="row"
            //   - div role="cell"
            //     - div class="cov-strip cov-uncovered cov-known"
            //   - div role="cell"
            //     - div class="blame-strip c2" data-blame="..."
            //   - div role="cell" class="line-number" data-line-number="N"
            //   - code role="cell" class="source-line"
            //     - ex: span class="sync_comment"
            //     - ex: span class="syn_def syn_type" data-symbols="..." data-i
            //
            // ### HTML Extraction Low Level Details
            //
            // Until https://github.com/cloudflare/lol-html/issues/40 or
            // the spin-off https://github.com/cloudflare/lol-html/issues/78
            // are implemented, lol_html doesn't explicitly provide a way to
            // derive the value of an element.
            //
            // So we attempt a hack where we use a custom output sink that is
            // kept aware of where we are in the file.  The good news is that
            // since lol_html is oriented around minimal memory allocation, we
            // can generally control when flushes happen.

            let mut writing_line: u32 = 0;
            let cur_line = Cell::new(0u32);
            let want_cur_line = Cell::new(false);
            let suppressing = Rc::new(Cell::new(false));
            let nesting_suppress = suppressing.clone();

            let mut buf = vec![];

            let mut rewrite = HtmlRewriter::new(
                Settings {
                    element_content_handlers: vec![
                        element!(r#"div.nesting-container"#, move |el| {
                            nesting_suppress.set(true);
                            let end_suppress = nesting_suppress.clone();
                            el.on_end_tag(move |_end| {
                                end_suppress.set(true);
                                Ok(())
                            })?;
                            Ok(())
                        }),
                        element!(r#"div.source-line-with-number"#, |el| {
                            suppressing.set(false);
                            if let Some(id_str) = el.get_attribute("id") {
                                let id_parts: Vec<&str> = id_str.split("-").collect();
                                if id_parts.len() == 2 && id_parts[0] == "line" {
                                    let lno = id_parts[1].parse().unwrap_or(0);
                                    cur_line.set(lno);
                                    want_cur_line.set(lines_to_show.contains(&lno));
                                }
                            }

                            Ok(())
                        }),
                    ],
                    ..Settings::default()
                },
                |c: &[u8]| {
                    if suppressing.get() {
                        return;
                    }

                    // We were actively writing and potentially have some
                    // buffer.
                    if writing_line > 0 {
                        // We're done writing; flush!
                        if cur_line.get() != writing_line {
                            file_lines
                                .insert(writing_line, String::from_utf8_lossy(&buf).to_string());
                            writing_line = 0;
                            buf.clear();
                        }
                        // We're still writing!
                        else {
                            // Write into the buffer and then leave, because we
                            // don't need to consider switching into writing, as
                            // we're still here.
                            buf.extend_from_slice(c);
                            return;
                        }
                    }
                    // We either closed out writing or weren't writing.  But now
                    // we need to see if we should be writing!
                    if cur_line.get() > 0 && want_cur_line.get() {
                        writing_line = cur_line.get();
                        buf.extend_from_slice(c);
                    }
                    // Otherwise, this wasn't interesting.
                },
            );

            rewrite.write(html_str.as_bytes()).unwrap();
            rewrite.end().unwrap();
        }

        // ## Ingest the new lines.
        results.ingest_html_lines(&path_line_contents, self.args.before, self.args.after);

        Ok(PipelineValues::FlattenedResultsBundle(results))
    }
}

```

## tools/src/cmd_pipeline/cmd_filter_analysis.rs
```
use async_trait::async_trait;
use clap::Args;
use tokio_stream::StreamExt;

use super::interface::{
    JsonRecords, PipelineCommand, PipelineValues, RecordType, SymbolicQueryOpts,
};
use crate::{
    abstract_server::{AbstractServer, Result},
    cmd_pipeline::interface::JsonRecordsByFile,
};

/// Filter the contents of a single analysis file.
#[derive(Debug, Args)]
pub struct FilterAnalysis {
    /// Tree-relative analysis file path
    #[clap(value_parser)]
    file: String,

    #[clap(long, short, value_parser, value_enum)]
    record_type: Option<Vec<RecordType>>,

    #[clap(long, short, value_parser)]
    kind: Option<String>,

    #[clap(flatten)]
    query_opts: SymbolicQueryOpts,
}

#[derive(Debug)]
pub struct FilterAnalysisCommand {
    pub args: FilterAnalysis,
}

/// ### Implementation Note
/// Filtering is currently performed via generic JSON rather than the strongly
/// typed `analysis.rs` types, but this pre-dates the change to using serde-json
/// and it probably makes sense to switch to using the raw types.
#[async_trait]
impl PipelineCommand for FilterAnalysisCommand {
    async fn execute(
        &self,
        server: &(dyn AbstractServer + Send + Sync),
        _input: PipelineValues,
    ) -> Result<PipelineValues> {
        let mut filtered = server.fetch_raw_analysis(&self.args.file).await?;

        // ## Filter by record type
        if let Some(record_types) = &self.args.record_type {
            filtered = Box::pin(filtered.filter(move |val| {
                // Record type is currently indicated via boolean presence of
                // "source", "target", or "structured" so check for the
                // stringified version of the enum value.
                for rt in record_types {
                    if val.get(rt.name()).is_some() {
                        return true;
                    }
                }
                false
            }));
        }

        // ## Filter by kind
        if let Some(kind) = &self.args.kind {
            // kind varies by record type:
            // - target: "kind" is a single valued attribute
            // - source: kind is baked into the comma-delimited "syntax"
            filtered = Box::pin(filtered.filter(move |val| {
                match (val["source"].is_number(), val["target"].is_number()) {
                    // source: consult "syntax"
                    (true, _) => match val["syntax"].as_str() {
                        None => false,
                        Some(actual) => actual.split(",").any(|k| k == kind),
                    },
                    // target: consult "kind"
                    (false, true) => match val["kind"].as_str() {
                        None => false,
                        Some(actual) => actual == kind,
                    },
                    _ => false,
                }
            }));
        }

        // ## Filter by symbol
        if let Some(symbol) = &self.args.query_opts.symbol {
            // "sym" is optionally
            filtered = Box::pin(filtered.filter(move |val| match val["sym"].as_str() {
                None => false,
                Some(actual) => actual.split(",").any(|s| s == symbol),
            }));
        }

        // ## Filter by symbol prefix
        if let Some(symbol_prefix) = &self.args.query_opts.symbol_prefix {
            // "sym" is optionally
            filtered = Box::pin(filtered.filter(move |val| match val["sym"].as_str() {
                None => false,
                Some(actual) => actual.split(",").any(|s| s.starts_with(symbol_prefix)),
            }));
        }

        // ## Filter by identifier
        if let Some(identifier) = &self.args.query_opts.identifier {
            filtered = Box::pin(filtered.filter(move |val| {
                match val["pretty"].as_str() {
                    None => false,
                    // source records have a space-delimited prefix that we want
                    // to skip; by using split/last we handle it being optional.
                    Some(actual) => actual.split(" ").last().unwrap_or("") == identifier,
                }
            }));
        }

        Ok(PipelineValues::JsonRecords(JsonRecords {
            by_file: vec![JsonRecordsByFile {
                file: self.args.file.clone(),
                records: filtered.collect().await,
            }],
        }))
    }
}

```

## tools/src/cmd_pipeline/cmd_format_symbols.rs
```
use std::cmp::Ordering;
use std::collections::{HashMap, HashSet, VecDeque};
use std::hash::{DefaultHasher, Hash, Hasher};

use async_trait::async_trait;
use clap::{Args, ValueEnum};
use itertools::Itertools;
use serde_json::{from_str, Value};

use super::{
    interface::{
        PipelineCommand, PipelineValues, SymbolCrossrefInfo, SymbolTreeTable,
        SymbolTreeTableAlignmentAndSize, SymbolTreeTableField, SymbolTreeTableFieldOffsetAndSize,
        SymbolTreeTableFieldType, SymbolTreeTableItem, SymbolTreeTableList, SymbolTreeTableNode,
    },
    symbol_graph::{DerivedSymbolInfo, SymbolGraphNodeId},
};

use crate::file_format::analysis::{
    AnalysisStructured, StructuredBitPositionInfo, StructuredFieldInfo,
};

use crate::abstract_server::{AbstractServer, ErrorDetails, ErrorLayer, Result, ServerError};

#[derive(Clone, Debug, PartialEq, ValueEnum)]
pub enum SymbolFormatMode {
    FieldLayout,
    // - class-field-use-matrix: table for each class, look up all its methods and all its
    //   fields, then filter the method "calls" to the fields.
    // - caller-matrix: look up a class, get all its methods.  look up all of
    //   the callers of all of those methods.  group them by their class.
    //   - row depth 0 is subsystem
    //   - row depth 1 is class or file if no class
    //   - row depth 2 is method/function
    //   - columns are the methods on the class, probably alphabetical.
    //     - columns could maybe have an upsell to the arg-matrix?
    //   - cells are a count.
    // - arg-matrix:
    //   - like caller-matrix but only for a single matrix and the columns are
    //     the args.
}

/// Given a list of symbol crossref infos, produce a SymbolTreeTable for display
/// purposes.
#[derive(Debug, Args)]
pub struct FormatSymbols {
    #[clap(long, value_parser, value_enum, default_value = "field-layout")]
    pub mode: SymbolFormatMode,

    #[clap(long, value_parser)]
    pub show_cols: Option<String>,

    #[clap(long, value_parser)]
    pub hide_cols: Option<String>,
}

#[derive(Debug)]
pub struct FormatSymbolsCommand {
    pub args: FormatSymbols,
}

#[derive(Clone, Copy, Eq, Hash, Ord, PartialEq, PartialOrd)]
struct PlatformId(u32);

impl PlatformId {
    fn all() -> Self {
        Self(0)
    }
}

#[derive(Clone, Copy, Eq, Hash, PartialEq)]
struct PlatformGroupId(u32);

type ClassId = SymbolGraphNodeId;
type FieldId = SymbolGraphNodeId;

// The identifier for the specific class in the layout.
// Single class can get multiple TraversalId if the class
// appears multiple times in the hierarchy.
#[derive(Clone, Copy, Eq, Hash, PartialEq)]
struct TraversalId(u32);

// A struct to represent single field and hole before the field,
// for specific platform.
#[derive(Clone, Eq, Hash, PartialEq)]
struct Field {
    class_id: ClassId,
    class_traversal_id: TraversalId,
    class_end_offset: Option<u32>,
    field_id: Option<FieldId>,
    field_type_syms: Option<String>,
    type_pretty: String,
    pretty: String,
    def_path: String,
    start_lineno: u64,
    end_lineno: u64,
    hole_bytes: Option<u32>,
    hole_after_base: bool,
    end_padding_bytes: Option<u32>,
    offset_bytes: u32,
    bit_positions: Option<StructuredBitPositionInfo>,
    size_bytes: Option<u32>,
}

impl Field {
    #[allow(clippy::too_many_arguments)]
    fn new(
        class_id: ClassId,
        class_traversal_id: TraversalId,
        class_offset: u32,
        class_size: Option<u32>,
        field_id: FieldId,
        field_type_syms: String,
        struct_def_path: &Option<String>,
        identifier_lineno: u64,
        info: &StructuredFieldInfo,
    ) -> Self {
        let (def_path, start_lineno, end_lineno) = Self::parse_path_and_line_range(
            info.line_range.to_string(),
            struct_def_path,
            identifier_lineno,
        );

        Self {
            class_id,
            class_traversal_id,
            class_end_offset: class_size.map(|size| class_offset + size),
            field_id: Some(field_id),
            field_type_syms: Some(field_type_syms),
            type_pretty: info.type_pretty.to_string(),
            pretty: info.pretty.to_string(),
            def_path,
            start_lineno,
            end_lineno,
            hole_bytes: None,
            hole_after_base: false,
            end_padding_bytes: None,
            offset_bytes: class_offset + info.offset_bytes,
            bit_positions: info.bit_positions.clone(),
            size_bytes: info.size_bytes,
        }
    }

    fn parse_path_and_line_range(
        s: String,
        struct_def_path: &Option<String>,
        identifier_lineno: u64,
    ) -> (String, u64, u64) {
        match s.split_once("#") {
            Some((path, range)) => {
                let def_path = if path.is_empty() {
                    // If the field is defined in the same file as struct itself,
                    // the path part is omitted.
                    struct_def_path.clone().unwrap_or("".to_string())
                } else {
                    path.to_string()
                };

                match range.split_once("-") {
                    Some((start, end)) => (
                        def_path,
                        start.parse().unwrap_or(identifier_lineno),
                        end.parse().unwrap_or(identifier_lineno),
                    ),
                    None => (
                        def_path,
                        range.parse().unwrap_or(identifier_lineno),
                        range.parse().unwrap_or(identifier_lineno),
                    ),
                }
            }
            None => (
                struct_def_path.clone().unwrap_or("".to_string()),
                identifier_lineno,
                identifier_lineno,
            ),
        }
    }

    fn new_vtable(
        class_id: ClassId,
        class_traversal_id: TraversalId,
        class_offset: u32,
        class_size: u32,
        size_bytes: u32,
    ) -> Self {
        Self {
            class_id,
            class_traversal_id,
            class_end_offset: Some(class_offset + class_size),
            field_id: None,
            field_type_syms: None,
            type_pretty: "".to_string(),
            pretty: "(vtable)".to_string(),
            def_path: "".to_string(),
            start_lineno: 0,
            end_lineno: 0,
            hole_bytes: None,
            hole_after_base: false,
            end_padding_bytes: None,
            offset_bytes: class_offset,
            bit_positions: None,
            size_bytes: Some(size_bytes),
        }
    }
}

// A container for fields, with pre-calculated hash of fields.
struct FieldsWithHash {
    fields: Vec<Field>,
    hash: u64,
}

impl FieldsWithHash {
    fn new_with_field(field: Field) -> Self {
        Self {
            fields: vec![field],
            hash: 0,
        }
    }

    fn sort(&mut self) {
        self.fields.sort_by(|a, b| {
            let byte_result = a.offset_bytes.cmp(&b.offset_bytes);
            if byte_result != Ordering::Equal {
                return byte_result;
            }

            match (&a.bit_positions, &b.bit_positions) {
                (Some(a_pos), Some(b_pos)) => a_pos.begin.cmp(&b_pos.begin),
                _ => byte_result,
            }
        });
    }

    fn calculate_holes(&mut self) {
        let mut last_end_offset = 0;
        let mut last_index = 0;

        let len = self.fields.len();

        for index in 0..len {
            if self.fields[index].offset_bytes > last_end_offset {
                if index != last_index
                    && self.fields[last_index].class_traversal_id
                        != self.fields[index].class_traversal_id
                {
                    if let Some(end_offset) = &self.fields[last_index].class_end_offset.clone() {
                        if last_end_offset < *end_offset {
                            self.fields[last_index].end_padding_bytes =
                                Some(end_offset - last_end_offset);
                        }
                        last_end_offset = *end_offset;
                    }

                    self.fields[index].hole_after_base = true;
                }

                if self.fields[index].offset_bytes > last_end_offset {
                    self.fields[index].hole_bytes =
                        Some(self.fields[index].offset_bytes - last_end_offset);
                }
            }

            last_index = index;

            if let Some(pos) = &self.fields[index].bit_positions {
                let end = self.fields[index].offset_bytes + (pos.begin + pos.width + 7) / 8;
                if end > last_end_offset {
                    last_end_offset = end;
                }
                continue;
            }

            if let Some(size) = &self.fields[index].size_bytes {
                last_end_offset = self.fields[index].offset_bytes + size;
            }
        }

        if !self.fields.is_empty() {
            if let Some(end_offset) = &self.fields[last_index].class_end_offset {
                if last_end_offset < *end_offset {
                    self.fields[last_index].end_padding_bytes = Some(end_offset - last_end_offset);
                }
            }
        }
    }

    fn calculate_hash(&mut self) {
        let mut hasher = DefaultHasher::new();
        self.fields.hash(&mut hasher);
        self.hash = hasher.finish();
    }
}

struct FieldListItem {
    def_paths: String,
    average_lineno: u64,
    average_bit_offset: u64,
    group_bits: u64,
    field_variants: Vec<Option<Field>>,
}

struct AlignmentAndSize {
    alignment: Option<u32>,
    size: u32,
}

impl AlignmentAndSize {
    fn new(alignment: Option<u32>, size: u32) -> Self {
        Self { alignment, size }
    }
}

// A struct to represent single class, with
// fields per each platform group.
struct Class {
    id: ClassId,
    name: String,
    alignment_and_size: HashMap<PlatformId, AlignmentAndSize>,
    fields: HashMap<Option<FieldId>, HashMap<PlatformGroupId, Field>>,
    merged_fields: Vec<Vec<Option<Field>>>,
}

impl Class {
    fn new(id: ClassId, name: String) -> Self {
        Self {
            id,
            name,
            alignment_and_size: HashMap::new(),
            fields: HashMap::new(),
            merged_fields: vec![],
        }
    }

    fn add_field(&mut self, group_id: PlatformGroupId, field: Field) {
        let field_id = field.field_id.clone();

        if let Some(field_variants_map) = self.fields.get_mut(&field_id) {
            field_variants_map.insert(group_id, field);
            return;
        }

        let mut field_variants_map = HashMap::new();
        field_variants_map.insert(group_id, field);
        self.fields.insert(field_id, field_variants_map);
    }

    fn finish_populating(&mut self, groups: &Vec<(PlatformGroupId, Vec<PlatformId>)>) {
        // Sort the fields based on:
        //   * Line number
        //   * Average bit offset of the field
        //   * Integer encoding of the groups where the field exists

        let mut field_list = vec![];

        for field_variants_map in self.fields.values() {
            let mut group_bits: u64 = 0;
            let mut total_lineno: u64 = 0;
            let mut total_bit_offset: u64 = 0;
            let mut field_count: u64 = 0;
            let mut field_def_paths = vec![];

            let mut field_variants = vec![];
            for (group_id, _) in groups {
                match field_variants_map.get(group_id) {
                    Some(field) => {
                        total_lineno += field.start_lineno;
                        total_bit_offset += (field.offset_bytes as u64) * 8;
                        if let Some(pos) = &field.bit_positions {
                            total_bit_offset += pos.begin as u64;
                        }
                        group_bits |= 1 << group_id.0;

                        field_count += 1;

                        field_variants.push(Some(field.clone()));

                        if !field_def_paths.contains(&field.def_path) {
                            field_def_paths.push(field.def_path.clone());
                        }
                    }
                    None => {
                        field_variants.push(None);
                    }
                }
            }

            let average_lineno = total_lineno / field_count;
            let average_bit_offset = total_bit_offset / field_count;

            field_list.push(FieldListItem {
                def_paths: field_def_paths.join(","),
                average_lineno,
                average_bit_offset,
                group_bits,
                field_variants,
            })
        }

        field_list.sort_by(|a, b| {
            if a.def_paths == b.def_paths {
                // If the fields are defined in the same file,
                // Sort them by line number, to sort
                // fields which exists only in certain platform
                // in right order.
                //
                // If the field comes from different file, for example when
                // `#include` is used, just ignore the line number and
                // compare with offset.
                let result = a.average_lineno.cmp(&b.average_lineno);
                if result != Ordering::Equal {
                    return result;
                }
            }

            let result = a.average_bit_offset.cmp(&b.average_bit_offset);
            if result != Ordering::Equal {
                return result;
            }

            let result = a.group_bits.cmp(&b.group_bits);
            if result != Ordering::Equal {
                return result;
            }

            Ordering::Equal
        });

        self.merged_fields = field_list
            .into_iter()
            .map(|item| item.field_variants)
            .collect();
    }
}

// Collect all platforms appeared in the analysis.
struct PlatformMap {
    platform_id_to_name: Vec<String>,

    // The temporary data structure to calculate platform ID.
    platform_name_to_id: HashMap<String, PlatformId>,
}

impl PlatformMap {
    fn new() -> Self {
        Self {
            platform_id_to_name: vec![],
            platform_name_to_id: HashMap::new(),
        }
    }

    fn add(&mut self, platform: String) -> PlatformId {
        if let Some(platform_id) = self.platform_name_to_id.get(&platform) {
            return *platform_id;
        }

        let platform_id = PlatformId(self.platform_name_to_id.len() as u32);
        self.platform_id_to_name.push(platform.clone());
        self.platform_name_to_id.insert(platform, platform_id);

        platform_id
    }

    fn finish_populating(&mut self) {
        if self.is_empty() {
            let id = self.add("All platforms".to_string());
            assert!(id == PlatformId::all());
        }
    }

    fn is_empty(&self) -> bool {
        self.platform_name_to_id.is_empty()
    }

    fn get(&self, platform: String) -> PlatformId {
        *self.platform_name_to_id.get(&platform).unwrap()
    }

    fn platform_ids(&self) -> Vec<PlatformId> {
        self.platform_id_to_name
            .iter()
            .enumerate()
            .map(|(i, _)| PlatformId(i as u32))
            .collect()
    }

    fn get_name(&self, platform_id: &PlatformId) -> String {
        self.platform_id_to_name[platform_id.0 as usize].clone()
    }
}

fn platform_name_to_order(name: &str) -> u32 {
    if name.starts_with("win") {
        return 0;
    }
    if name.starts_with("macosx") {
        return 1;
    }
    if name.starts_with("linux") {
        return 2;
    }
    if name.starts_with("android") {
        return 3;
    }
    if name.starts_with("ios") {
        return 4;
    }
    5
}

// Struct to hold the list of fields for the entire class hierarchy
// per platform, and calculate the hole between them.
struct FieldsPerPlatform {
    fields_per_platform: HashMap<PlatformId, FieldsWithHash>,
}

impl FieldsPerPlatform {
    fn new() -> Self {
        Self {
            fields_per_platform: HashMap::new(),
        }
    }

    fn add_field(&mut self, platform_id: &PlatformId, field: Field) {
        if let Some(fields) = self.fields_per_platform.get_mut(platform_id) {
            fields.fields.push(field);
            return;
        }

        self.fields_per_platform
            .insert(*platform_id, FieldsWithHash::new_with_field(field));
    }

    // Once all fields are populated, process them for further operation.
    fn finish_populating(&mut self, has_unsupported_multiple_inheritance: bool) {
        for (_, fields) in self.fields_per_platform.iter_mut() {
            fields.sort();
            if !has_unsupported_multiple_inheritance {
                fields.calculate_holes();
            }
            fields.calculate_hash();
        }
    }

    fn group_platforms(
        &self,
        platform_map: &PlatformMap,
    ) -> Vec<(PlatformGroupId, Vec<PlatformId>)> {
        if self.fields_per_platform.is_empty() {
            // If all fields are platform-agnostic, simply return them.
            return vec![(PlatformGroupId(0), platform_map.platform_ids())];
        }

        // Group platforms by fields.
        let mut groups: Vec<(u64, Vec<PlatformId>)> = vec![];

        let mut platform_ids = platform_map.platform_ids();

        // Make the order consistent as much as possible across classes.
        platform_ids.sort_by(|a, b| {
            let a_name = platform_map.get_name(a);
            let b_name = platform_map.get_name(b);

            let a_order = platform_name_to_order(&a_name);
            let b_order = platform_name_to_order(&b_name);

            let result = a_order.cmp(&b_order);
            if result != Ordering::Equal {
                return result;
            }

            a_name.cmp(&b_name)
        });

        'next_platform: for platform_id in &platform_ids {
            if let Some(fields) = self.fields_per_platform.get(platform_id) {
                for (hash, platforms) in &mut groups {
                    if fields.hash == *hash {
                        let existing = &self.fields_per_platform.get(&platforms[0]).unwrap().fields;
                        if fields.fields == *existing {
                            platforms.push(*platform_id);
                            continue 'next_platform;
                        }
                    }
                }

                groups.push((fields.hash, vec![*platform_id]));
            }
        }

        groups
            .into_iter()
            .enumerate()
            .map(|(i, (_, platforms))| (PlatformGroupId(i as u32), platforms))
            .collect()
    }

    fn get_fields_for_platforms<'a>(&'a self, platform_ids: &[PlatformId]) -> Option<&'a [Field]> {
        let platform_id = &platform_ids[0];
        self.fields_per_platform
            .get(platform_id)
            .map(|fields| fields.fields.as_slice())
    }
}

// A structure to represent the next item for the class hierarchy traversal.
struct TraversalItem {
    // The next class's ID.
    class_id: ClassId,

    // The offset for the class per platform.
    // Platforms not included in this map are not used for this traversal.
    offset_map: HashMap<PlatformId, u32>,
}

impl TraversalItem {
    fn new(class_id: ClassId) -> Self {
        Self {
            class_id,
            offset_map: HashMap::new(),
        }
    }

    fn add_offset(&mut self, platform_id: PlatformId, offset: u32) {
        self.offset_map.insert(platform_id, offset);
    }

    fn get_offset(&self, platform_id: &PlatformId) -> u32 {
        match self.offset_map.get(platform_id) {
            Some(offset) => *offset,
            None => 0,
        }
    }

    fn is_enabled(&self, platform_id: &PlatformId) -> bool {
        self.offset_map.contains_key(platform_id)
    }

    fn platforms(&self) -> Vec<PlatformId> {
        let mut result = vec![];
        for platform_id in self.offset_map.keys() {
            result.push(*platform_id);
        }
        result
    }
}

struct SupersMap {
    super_ids: Vec<ClassId>,
    supers: HashMap<ClassId, HashMap<PlatformId, u32>>,
}

impl SupersMap {
    fn new() -> Self {
        Self {
            super_ids: vec![],
            supers: HashMap::new(),
        }
    }

    fn add(&mut self, class_id: ClassId, platform_id: PlatformId, offset: u32) {
        if let Some(item) = self.supers.get_mut(&class_id) {
            item.insert(platform_id, offset);
            return;
        }

        let mut item = HashMap::new();
        item.insert(platform_id, offset);
        self.super_ids.push(class_id.clone());
        self.supers.insert(class_id, item);
    }

    fn into_traversal_items(self) -> Vec<TraversalItem> {
        let mut result = vec![];

        for class_id in self.super_ids {
            let offset_map = self.supers.get(&class_id).unwrap();
            let mut item = TraversalItem::new(class_id);
            for (platform_id, offset) in offset_map {
                item.add_offset(*platform_id, *offset);
            }
            result.push(item);
        }

        result
    }
}

struct ClassMap {
    // All processed classes.
    class_map: HashMap<TraversalId, Class>,

    // The list of classes, in the traverse order.
    class_list: Vec<TraversalId>,

    // All platforms appeared inside the analysis.
    platform_map: PlatformMap,

    // Platforms grouped by the field layout.
    groups: Vec<(PlatformGroupId, Vec<PlatformId>)>,

    // Formatted lines of each file referred from fields.
    file_lines: HashMap<String, Vec<String>>,

    has_unsupported_multiple_inheritance: bool,

    root_class_id: Option<ClassId>,
    stt: SymbolTreeTable,
}

impl ClassMap {
    fn new() -> Self {
        Self {
            class_map: HashMap::new(),
            class_list: vec![],
            platform_map: PlatformMap::new(),
            groups: vec![],
            file_lines: HashMap::new(),
            has_unsupported_multiple_inheritance: false,
            root_class_id: None,
            stt: SymbolTreeTable::new(),
        }
    }

    async fn populate(
        &mut self,
        nom_sym_info: SymbolCrossrefInfo,
        server: &(dyn AbstractServer + Send + Sync),
    ) -> Result<()> {
        let root_sym_id = self.populate_platform_map(nom_sym_info, server).await?;

        self.root_class_id = Some(root_sym_id.clone());

        let mut fields_per_platform = FieldsPerPlatform::new();

        let mut root_item = TraversalItem::new(root_sym_id);
        for platform_id in self.platform_map.platform_ids() {
            root_item.add_offset(platform_id, 0);
        }

        let mut pending_items = VecDeque::new();
        pending_items.push_back(root_item);

        let mut traversal_index = 0;

        let mut has_multiple_inheritance = false;
        let mut has_non_zero_super_offset = false;

        while let Some(item) = pending_items.pop_front() {
            let class_id = item.class_id.clone();

            let sym_info = self.stt.node_set.get(&class_id);
            let depth = sym_info.depth;
            let Some(structured) = Self::get_struct_structured(sym_info) else {
                continue;
            };
            let struct_def_path = sym_info.get_def_path().cloned();

            let mut cls = Class::new(class_id.clone(), structured.pretty.to_string());

            let traversal_id = TraversalId(traversal_index);

            traversal_index += 1;

            self.class_list.push(traversal_id);

            let mut supers = SupersMap::new();

            for (maybe_platform, s) in structured.per_platform() {
                let mut maybe_platform_id: Option<PlatformId> = None;

                if let Some(platform) = maybe_platform {
                    let platform_id = self.platform_map.get(platform.clone());
                    if !item.is_enabled(&platform_id) {
                        continue;
                    }
                    maybe_platform_id = Some(platform_id);
                }

                let class_alignment = s.alignment_bytes;

                if let Some(class_size) = s.size_bytes {
                    if let Some(platform_id) = &maybe_platform_id {
                        cls.alignment_and_size.insert(
                            platform_id.clone(),
                            AlignmentAndSize::new(class_alignment, class_size),
                        );

                        if let Some(vtable_size_bytes) = &s.own_vf_ptr_bytes {
                            let offset = item.get_offset(platform_id);
                            let field = Field::new_vtable(
                                class_id.clone(),
                                traversal_id,
                                offset,
                                class_size,
                                *vtable_size_bytes,
                            );
                            fields_per_platform.add_field(platform_id, field.clone());
                        }
                    } else {
                        for platform_id in item.platforms() {
                            cls.alignment_and_size.insert(
                                platform_id,
                                AlignmentAndSize::new(class_alignment, class_size),
                            );

                            if let Some(vtable_size_bytes) = &s.own_vf_ptr_bytes {
                                let offset = item.get_offset(&platform_id);
                                let field = Field::new_vtable(
                                    class_id.clone(),
                                    traversal_id,
                                    offset,
                                    class_size,
                                    *vtable_size_bytes,
                                );
                                fields_per_platform.add_field(&platform_id, field.clone());
                            }
                        }
                    }
                }

                if s.supers.len() > 1 {
                    has_multiple_inheritance = true;
                }

                for super_info in &s.supers {
                    let (super_id, _) = self
                        .stt
                        .node_set
                        .ensure_symbol(&super_info.sym, server, depth + 1)
                        .await?;

                    if super_info.offset_bytes > 0 {
                        has_non_zero_super_offset = true;
                    }

                    if let Some(platform_id) = &maybe_platform_id {
                        let offset = item.get_offset(platform_id);
                        supers.add(
                            super_id.clone(),
                            *platform_id,
                            offset + super_info.offset_bytes,
                        );
                    } else {
                        for platform_id in item.platforms() {
                            let offset = item.get_offset(&platform_id);
                            supers.add(
                                super_id.clone(),
                                platform_id,
                                offset + super_info.offset_bytes,
                            );
                        }
                    }
                }

                for field in s.fields.clone() {
                    let (field_id, field_lineno) = {
                        let (field_id, field_info) = self
                            .stt
                            .node_set
                            .ensure_symbol(&field.sym, server, depth + 1)
                            .await?;

                        (field_id, field_info.get_def_lno())
                    };

                    let mut field_type_syms_vec = vec![];
                    let mut field_type_syms_set = HashSet::new();

                    // Add field type to the jumprefs, but we don't use the
                    // returned info.
                    if !field.type_sym.is_empty() {
                        let _ = self
                            .stt
                            .node_set
                            .ensure_symbol(&field.type_sym, server, depth + 1)
                            .await?;

                        field_type_syms_vec.push(field.type_sym.to_string());
                        field_type_syms_set.insert(field.type_sym);
                    }
                    for info in &field.pointer_info {
                        if field_type_syms_set.contains(&info.sym) {
                            continue;
                        }

                        let _ = self
                            .stt
                            .node_set
                            .ensure_symbol(&info.sym, server, depth + 1)
                            .await?;

                        field_type_syms_vec.push(info.sym.to_string());
                        field_type_syms_set.insert(info.sym);
                    }

                    let field_type_syms = field_type_syms_vec.iter().join(",");

                    if let Some(platform_id) = &maybe_platform_id {
                        let offset = item.get_offset(platform_id);
                        let field = Field::new(
                            class_id.clone(),
                            traversal_id,
                            offset,
                            s.size_bytes,
                            field_id.clone(),
                            field_type_syms,
                            &struct_def_path,
                            field_lineno,
                            &field,
                        );
                        self.populate_file_lines(&field.def_path, server).await?;
                        fields_per_platform.add_field(platform_id, field.clone());
                    } else {
                        for platform_id in item.platforms() {
                            let offset = item.get_offset(&platform_id);
                            let field = Field::new(
                                class_id.clone(),
                                traversal_id,
                                offset,
                                s.size_bytes,
                                field_id.clone(),
                                field_type_syms.clone(),
                                &struct_def_path,
                                field_lineno,
                                &field,
                            );
                            self.populate_file_lines(&field.def_path, server).await?;
                            fields_per_platform.add_field(&platform_id, field.clone());
                        }
                    }
                }
            }

            self.class_map.insert(traversal_id, cls);

            for super_item in supers.into_traversal_items() {
                pending_items.push_back(super_item);
            }
        }

        self.has_unsupported_multiple_inheritance =
            has_multiple_inheritance && !has_non_zero_super_offset;

        fields_per_platform.finish_populating(self.has_unsupported_multiple_inheritance);

        self.groups = fields_per_platform.group_platforms(&self.platform_map);

        for (group_id, platforms) in &self.groups {
            if let Some(fields) = fields_per_platform.get_fields_for_platforms(platforms) {
                for field in fields {
                    let cls = self.class_map.get_mut(&field.class_traversal_id).unwrap();
                    cls.add_field(*group_id, field.clone());
                }
            }
        }

        for cls in self.class_map.values_mut() {
            cls.finish_populating(&self.groups);
        }

        Ok(())
    }

    async fn populate_file_lines(
        &mut self,
        path: &String,
        server: &(dyn AbstractServer + Send + Sync),
    ) -> Result<()> {
        if path.is_empty() {
            return Ok(());
        }

        if self.file_lines.contains_key(path) {
            return Ok(());
        }

        let result = server.fetch_formatted_lines(path).await;
        if result.is_err() {
            return Ok(());
        }

        let (lines, sym_json) = result.unwrap();

        let syms: serde_json::Result<HashMap<String, Value>> = from_str(&sym_json);
        if let Ok(syms) = syms {
            for (sym, info) in syms {
                self.stt.extra_syms.insert(sym, info);
            }
        }

        self.file_lines.insert(path.clone(), lines);

        Ok(())
    }

    fn get_struct_structured(sym_info: &DerivedSymbolInfo) -> Option<AnalysisStructured> {
        let structured = sym_info.get_structured()?;

        // See clang TagTypeKind.
        // https://clang.llvm.org/doxygen/namespaceclang.html#a9237bdb3cf715b9bff8bcb3172635548
        if structured.kind != "struct"
            && structured.kind != "__interface"
            && structured.kind != "union"
            && structured.kind != "class"
            && structured.kind != "enum"
        {
            return None;
        }

        Some(structured)
    }

    async fn populate_platform_map(
        &mut self,
        nom_sym_info: SymbolCrossrefInfo,
        server: &(dyn AbstractServer + Send + Sync),
    ) -> Result<SymbolGraphNodeId> {
        let (root_sym_id, _) = self.stt.node_set.add_symbol(DerivedSymbolInfo::new(
            nom_sym_info.symbol,
            nom_sym_info.crossref_info,
            0,
        ));

        let mut pending_ids = VecDeque::new();
        pending_ids.push_back(root_sym_id.clone());

        while let Some(class_id) = pending_ids.pop_front() {
            let sym_info = self.stt.node_set.get(&class_id);
            let depth = sym_info.depth;
            let Some(structured) = Self::get_struct_structured(sym_info) else {
                continue;
            };

            for super_info in &structured.supers {
                let (super_id, _) = self
                    .stt
                    .node_set
                    .ensure_symbol(&super_info.sym, server, depth + 1)
                    .await?;
                pending_ids.push_back(super_id.clone());
            }

            for (maybe_platform, _) in structured.per_platform() {
                if let Some(platform) = maybe_platform {
                    self.platform_map.add(platform.clone());
                }
            }
        }

        self.platform_map.finish_populating();

        Ok(root_sym_id)
    }

    fn generate_tables(mut self, tables: &mut Vec<SymbolTreeTable>) {
        for (_, platforms) in &self.groups {
            let label = platforms
                .iter()
                .map(|platform_id| self.platform_map.get_name(platform_id))
                .join(" ")
                .to_owned();

            self.stt.platforms.push(label);
        }

        for traversal_id in &self.class_list {
            let cls = self.class_map.get(traversal_id).unwrap();

            let is_root = cls.id == self.root_class_id.as_ref().unwrap().clone();

            let mut node_alignment_and_size = vec![];

            if is_root {
                for (_, platforms) in &self.groups {
                    let platform_id = platforms[0];

                    let (alignment, size) = match cls.alignment_and_size.get(&platform_id) {
                        Some(AlignmentAndSize { alignment, size }) => {
                            if let Some(alignment) = alignment {
                                (format!("align({})", alignment), format!("{}", size))
                            } else {
                                ("".to_string(), format!("{}", size))
                            }
                        }
                        None => ("".to_string(), "?".to_string()),
                    };

                    node_alignment_and_size
                        .push(SymbolTreeTableAlignmentAndSize::new(alignment, size));
                }
            }

            let mut class_node = SymbolTreeTableNode::new(
                cls.name.clone(),
                self.stt.node_set.get(&cls.id).symbol.to_string(),
                !is_root,
                node_alignment_and_size,
            );

            if self.has_unsupported_multiple_inheritance && is_root {
                class_node.items.push(
                    SymbolTreeTableItem::Warning(
                        "(This class has multiple inheritance but the offset is not found in the analysis file. The field offsets in base classes can be wrong, and holes/paddings are not calculated)".to_string()
                    )
                );
            }

            let field_prefix = format!("{}::", cls.name);

            for field_variants in &cls.merged_fields {
                let mut has_hole = false;
                for maybe_field in field_variants {
                    if let Some(field) = &maybe_field {
                        if field.hole_bytes.is_some() {
                            has_hole = true;
                            break;
                        }
                    }
                }

                if has_hole {
                    let mut holes = vec![];

                    for maybe_field in field_variants {
                        match maybe_field {
                            Some(field) => {
                                let hole_bytes = field.hole_bytes.unwrap_or(0);
                                if hole_bytes == 0 {
                                    holes.push(None);
                                    continue;
                                }

                                holes.push(Some(format!(
                                    "{} byte{} hole{}",
                                    hole_bytes,
                                    if hole_bytes > 1 { "s" } else { "" },
                                    if field.hole_after_base {
                                        " after base class"
                                    } else {
                                        ""
                                    }
                                )));
                            }
                            None => {
                                if maybe_field.is_none() {
                                    holes.push(None);
                                }
                            }
                        }
                    }

                    class_node.items.push(SymbolTreeTableItem::Hole(holes));
                }

                let mut field_name = "".to_string();
                let mut field_symbols = "".to_string();

                if let Some(field) = field_variants.iter().flatten().next() {
                    let pretty = field.pretty.clone();
                    field_name = pretty.replace(&field_prefix, "");

                    if let Some(field_id) = &field.field_id {
                        field_symbols = self.stt.node_set.get(field_id).symbol.to_string();
                    }
                }

                let mut field_item = SymbolTreeTableField::new(field_name, field_symbols);

                let mut type_label_set = HashSet::new();
                let mut path_and_range_set = HashSet::new();

                for maybe_field in field_variants {
                    match maybe_field {
                        Some(field) => {
                            if !type_label_set.contains(&field.type_pretty) {
                                type_label_set.insert(field.type_pretty.clone());

                                field_item.types.push(SymbolTreeTableFieldType::new(
                                    field.type_pretty.clone(),
                                    match &field.field_type_syms {
                                        Some(type_syms) => type_syms.clone(),
                                        None => "".to_string(),
                                    },
                                ));
                            }

                            let key =
                                (field.def_path.clone(), field.start_lineno, field.end_lineno);
                            if !path_and_range_set.contains(&key) {
                                path_and_range_set.insert(key);

                                if let Some(lines) = self.file_lines.get(&field.def_path) {
                                    for lineno in field.start_lineno..=field.end_lineno {
                                        if lineno == 0 {
                                            continue;
                                        }
                                        let index = lineno as usize - 1;
                                        if let Some(line) = lines.get(index) {
                                            field_item.lines.push(line.clone());
                                        }
                                    }
                                }
                            }

                            if let Some(pos) = &field.bit_positions {
                                field_item.offset_and_size.push(Some(
                                    SymbolTreeTableFieldOffsetAndSize::new(
                                        format!(
                                            "@ {:#x} + {} bit{}",
                                            field.offset_bytes,
                                            pos.begin,
                                            if pos.begin > 1 { "s" } else { "" }
                                        ),
                                        format!(
                                            "{} bit{}",
                                            pos.width,
                                            if pos.width > 1 { "s" } else { "" }
                                        ),
                                    ),
                                ))
                            } else {
                                field_item.offset_and_size.push(Some(
                                    SymbolTreeTableFieldOffsetAndSize::new(
                                        format!("@ {:#x}", field.offset_bytes,),
                                        format!("{}", field.size_bytes.unwrap_or(0),),
                                    ),
                                ));
                            }
                        }
                        None => {
                            field_item.offset_and_size.push(None);
                        }
                    }
                }

                class_node
                    .items
                    .push(SymbolTreeTableItem::Field(field_item));

                let mut has_end_padding = false;
                for maybe_field in field_variants {
                    if let Some(field) = &maybe_field {
                        if field.end_padding_bytes.is_some() {
                            has_end_padding = true;
                            break;
                        }
                    }
                }

                if has_end_padding {
                    let mut end_paddings = vec![];

                    for maybe_field in field_variants {
                        match maybe_field {
                            Some(field) => {
                                let end_padding_bytes = field.end_padding_bytes.unwrap_or(0);
                                if end_padding_bytes == 0 {
                                    end_paddings.push(None);
                                    continue;
                                }

                                end_paddings.push(Some(format!(
                                    "{} byte{} padding",
                                    end_padding_bytes,
                                    if end_padding_bytes > 1 { "s" } else { "" }
                                )));
                            }
                            None => {
                                if maybe_field.is_none() {
                                    end_paddings.push(None);
                                }
                            }
                        }
                    }

                    class_node
                        .items
                        .push(SymbolTreeTableItem::EndPadding(end_paddings));
                }
            }

            self.stt.rows.push(class_node);
        }

        tables.push(self.stt);
    }
}

#[async_trait]
impl PipelineCommand for FormatSymbolsCommand {
    async fn execute(
        &self,
        server: &(dyn AbstractServer + Send + Sync),
        input: PipelineValues,
    ) -> Result<PipelineValues> {
        let cil = match input {
            PipelineValues::SymbolCrossrefInfoList(cil) => cil,
            _ => {
                return Err(ServerError::StickyProblem(ErrorDetails {
                    layer: ErrorLayer::ConfigLayer,
                    message: "format-symbols needs a CrossrefInfoList".to_string(),
                }));
            }
        };

        match self.args.mode {
            SymbolFormatMode::FieldLayout => {
                let mut tables = vec![];

                for nom_sym_info in cil.symbol_crossref_infos {
                    let mut map = ClassMap::new();
                    map.populate(nom_sym_info, server).await?;
                    map.generate_tables(&mut tables);
                }

                let mut class_names = vec![];
                if let Some(cols) = &self.args.show_cols {
                    for col in cols.split(",") {
                        if col == "type" {
                            class_names.push(format!("show-{}", col));
                        }
                    }
                }
                if let Some(cols) = &self.args.hide_cols {
                    for col in cols.split(",") {
                        if col == "line" || col == "name" {
                            class_names.push(format!("hide-{}", col));
                        }
                    }
                }

                let class_name = if class_names.is_empty() {
                    None
                } else {
                    Some(class_names.join(" "))
                };

                Ok(PipelineValues::SymbolTreeTableList(SymbolTreeTableList {
                    tables,
                    class_name,
                }))
            }
        }
    }
}

```

## tools/src/cmd_pipeline/cmd_fuse_crossrefs.rs
```
use async_trait::async_trait;
use clap::Args;

use super::interface::{
    PipelineJunctionCommand, PipelineValues, SymbolCrossrefInfoList, SymbolMetaFlags,
};

use crate::abstract_server::{AbstractServer, ErrorDetails, ErrorLayer, Result, ServerError};

/// Experimental junction enabling multiple query chains that output
/// crossref-lookup to be unified into a single SymbolCrossrefInfoList but with
/// different annotatins/metadata.
///
/// The driving use-case right now is calls-between-source/calls-between-target
/// where the crossref-lookups support exploding a class into its methods.  This
/// is very experimental as it seems quite possible that an approach that's more
/// explicitly aware of hierarchy and/or could leverage some pair-wise
/// precomputations could be useful.
#[derive(Debug, Args)]
pub struct FuseCrossrefs {}

#[allow(dead_code)]
#[derive(Debug)]
pub struct FuseCrossrefsCommand {
    pub args: FuseCrossrefs,
}

#[async_trait]
impl PipelineJunctionCommand for FuseCrossrefsCommand {
    async fn execute(
        &self,
        _server: &(dyn AbstractServer + Send + Sync),
        input: Vec<(String, PipelineValues)>,
    ) -> Result<PipelineValues> {
        let mut fused_crossref = vec![];
        let mut fused_unknown = vec![];

        // We currently don't care about the name of the input because we only
        // match by type, but one could imagine a scenario in which they serve
        // as labels we want to propagate.
        for (name, pipe_value) in input {
            let add_flags = match name.as_ref() {
                "source" => SymbolMetaFlags::Source,
                "target" => SymbolMetaFlags::Target,
                _ => SymbolMetaFlags::default(),
            };
            match pipe_value {
                PipelineValues::SymbolCrossrefInfoList(mut scil) => {
                    for mut info in scil.symbol_crossref_infos {
                        info.flags |= add_flags;
                        fused_crossref.push(info);
                    }
                    fused_unknown.append(&mut scil.unknown_symbols);
                }
                _ => {
                    return Err(ServerError::StickyProblem(ErrorDetails {
                        layer: ErrorLayer::ConfigLayer,
                        message: "fuse-crossrefs got something weird".to_string(),
                    }));
                }
            }
        }

        Ok(PipelineValues::SymbolCrossrefInfoList(
            SymbolCrossrefInfoList {
                symbol_crossref_infos: fused_crossref,
                unknown_symbols: fused_unknown,
            },
        ))
    }
}

```

## tools/src/cmd_pipeline/cmd_search_identifiers.rs
```
use async_trait::async_trait;
use clap::Args;

use super::interface::{
    PipelineCommand, PipelineValues, SymbolList, SymbolQuality, SymbolWithContext,
};

use crate::abstract_server::{AbstractServer, Result};

/// Return the crossref data for one or more symbols received via pipeline or as
/// explicit arguments.
#[derive(Debug, Args)]
pub struct SearchIdentifiers {
    /// Explicit identifiers to search.
    #[clap(value_parser)]
    identifiers: Vec<String>,

    /// Should this be an exact-match?  By default we do a prefix search.
    #[clap(short, long, value_parser)]
    exact_match: bool,

    /// Should this be case-sensitive?  By default we are case-insensitive.
    #[clap(short, long, value_parser)]
    case_sensitive: bool,

    /// Should this only match types as indicated by the `T_` convention?
    /// Currently a hack and this should instead be handled by having the
    /// crossref-lookup do the filtering.
    #[clap(long, value_parser)]
    types_only: bool,

    /// Minimum identifier length to search for.  The default of 3 is derived
    /// from router.py's `is_trivial_search` heuristic requiring a length of 3,
    /// although it was only required along one axis.
    #[clap(long, value_parser, default_value = "3")]
    min_length: usize,

    #[clap(short, long, value_parser, default_value = "1000")]
    limit: usize,
}

#[derive(Debug)]
pub struct SearchIdentifiersCommand {
    pub args: SearchIdentifiers,
}

#[async_trait]
impl PipelineCommand for SearchIdentifiersCommand {
    async fn execute(
        &self,
        server: &(dyn AbstractServer + Send + Sync),
        input: PipelineValues,
    ) -> Result<PipelineValues> {
        let identifier_list: Vec<String> = match input {
            PipelineValues::IdentifierList(il) => il
                .identifiers
                .into_iter()
                .map(|id| id.to_string())
                .collect(),
            // Right now we're assuming that we're the first command in the
            // pipeline so that we would have no inputs if someone wants to use
            // arguments...
            PipelineValues::Void => self.args.identifiers.clone(),
            // TODO: Figure out a better way to handle a nonsensical pipeline
            // configuration / usage.
            _ => {
                return Ok(PipelineValues::Void);
            }
        };

        let mut symbols: Vec<SymbolWithContext> = vec![];
        for id in identifier_list {
            // Skip any identifiers that are shorter than our minimum length.
            if id.len() < self.args.min_length {
                continue;
            }

            for (sym, from_ident) in server
                .search_identifiers(
                    &id,
                    self.args.exact_match,
                    !self.args.case_sensitive,
                    self.args.limit,
                )
                .await?
            {
                if self.args.types_only && !sym.starts_with("T_") {
                    continue;
                }

                let quality = match (
                    &self.args.exact_match,
                    id.as_str() == from_ident.as_str(),
                    &id,
                    &from_ident,
                ) {
                    (true, _, _, _) => SymbolQuality::ExplicitIdentifier,
                    (false, true, _, _) => SymbolQuality::ExactIdentifier,
                    (_, _, searched, result) => SymbolQuality::IdentifierPrefix(
                        searched.len() as u32,
                        (result.len() - searched.len()) as u32,
                    ),
                };
                symbols.push(SymbolWithContext {
                    symbol: sym,
                    quality,
                    from_identifier: Some(from_ident),
                });
            }
        }

        Ok(PipelineValues::SymbolList(SymbolList { symbols }))
    }
}

```

## tools/src/cmd_pipeline/cmd_search.rs
```
use async_trait::async_trait;
use clap::Args;
use json_structural_diff::JsonDiff;
use serde_json::{json, Map, Value};

use super::interface::{JsonValue, PipelineCommand, PipelineValues};
use crate::abstract_server::{AbstractServer, Result};

/// Run a traditional searchfox search against the web server.  This will turn
/// into a no-op when run against a local index at this time, but in the future
/// may be able to spin up the necessary pieces.
#[derive(Debug, Args)]
pub struct Search {
    /// Query string
    #[clap(value_parser)]
    query: String,

    /// Diff the results of this query against the previous command's output,
    /// producing diff output.
    #[clap(short, long, value_parser)]
    diff: bool,

    /// Normalize "bounds" out of existence because they can differ when using
    /// different query strings.
    #[clap(short, long, value_parser)]
    normalize: bool,

    /// Convert arrays of "path"-keyed objects to a dict whose keys are the "path"
    /// values and the value is still the same value, including "path".  This is
    /// meant to make the diff option more friendly in cases where ordering is not
    /// a concern.
    #[clap(short, long, value_parser)]
    dictify: bool,
}

#[derive(Debug)]
pub struct SearchCommand {
    pub args: Search,
}

/// Recursively transforms JSON values, removing any "bounds" value it finds
/// while recursing into any object or array values it finds.
fn normalize_bounds(val: &mut Value) {
    match val {
        Value::Object(o) => {
            o.remove(&"bounds".to_string());

            for v in o.values_mut() {
                normalize_bounds(v);
            }
        }
        Value::Array(a) => {
            for entry in a {
                normalize_bounds(entry);
            }
        }
        _ => {}
    };
}

fn dictify(val: &mut Value) -> Option<Value> {
    match val {
        Value::Object(o) => {
            for v in o.values_mut() {
                if let Some(replacement) = dictify(v) {
                    *v = replacement;
                }
            }
        }
        Value::Array(a) => {
            // Check if the first entry exists and is an object and has a "path".
            if a.iter().any(|entry| entry.get("path").is_some()) {
                // We're going to create a new Value and return that, as this is a
                // transform where we are changing our container; all other transforms
                // are happening within the same existing outer container.
                let mut obj = Map::new();
                for entry in a {
                    if let Some(Value::String(path)) = entry.get("path") {
                        obj.insert(path.clone(), entry.take());
                    }
                }
                return Some(Value::Object(obj));
            }

            for entry in a {
                if let Some(replacement) = dictify(entry) {
                    *entry = replacement;
                }
            }
        }
        _ => {}
    };

    None
}

fn dictify_root(mut val: Value) -> Value {
    if let Some(replacement) = dictify(&mut val) {
        return replacement;
    }
    val
}

#[async_trait]
impl PipelineCommand for SearchCommand {
    async fn execute(
        &self,
        server: &(dyn AbstractServer + Send + Sync),
        input: PipelineValues,
    ) -> Result<PipelineValues> {
        let mut value = server.perform_query(&self.args.query).await?;

        if self.args.diff {
            let mut input_json = match input {
                PipelineValues::JsonValue(j) => j.value,
                _ => json!({}),
            };

            if self.args.normalize {
                normalize_bounds(&mut input_json);
                normalize_bounds(&mut value);
            }

            if self.args.dictify {
                input_json = dictify_root(input_json);
                value = dictify_root(value);
            }

            let json_diff = JsonDiff::diff(&input_json, &value, false);
            value = json_diff.diff.unwrap_or_else(|| json!({}));
        }

        Ok(PipelineValues::JsonValue(JsonValue { value }))
    }
}

```

## tools/src/cmd_pipeline/cmd_show_html.rs
```
use std::{cell::Cell, rc::Rc};

use async_trait::async_trait;
use clap::Args;
use lol_html::{element, HtmlRewriter, Settings};

use super::interface::{JsonRecords, PipelineCommand, PipelineValues};
use crate::{
    abstract_server::{AbstractServer, HtmlFileRoot, Result},
    cmd_pipeline::interface::{HtmlExcerpts, HtmlExcerptsByFile},
};

/// Output the HTML lines corresponding to the JSON records received via input.
///
/// There's also likely a use-case to process an HTML file as a root where we
/// then filter lines based on "data-symbols".  It's not immediately clear if
/// this command should also handle that or not.  There's also the question of
/// whether symbol filtering would only excerpt the span associated with the
/// symbol or would walk up to the enclosing line.  The answers should probably
/// be driven by command-line use-cases; in particular, the experience of
/// evolving a more targeted query.  Having to modify a command up-stream should
/// be considered undesirable.
#[derive(Debug, Args)]
pub struct ShowHtml {}

#[allow(dead_code)]
#[derive(Debug)]
pub struct ShowHtmlCommand {
    pub args: ShowHtml,
}

#[async_trait]
impl PipelineCommand for ShowHtmlCommand {
    async fn execute(
        &self,
        server: &(dyn AbstractServer + Send + Sync),
        input: PipelineValues,
    ) -> Result<PipelineValues> {
        let jr = match input {
            PipelineValues::JsonRecords(jr) => jr,
            _ => JsonRecords { by_file: vec![] },
        };

        let mut html_by_file = vec![];

        for fr in jr.by_file {
            // ## For each file!
            let lines_to_show = fr.line_set();

            // XXX Doing this as a single string received in a lump is fine for
            // our testing use-case, but this may need to be reconsidered in
            // production.  Or maybe production really wants the performance?
            // Production certainly should have the RAM for our known worst
            // case scenarios.
            let html_str = server
                .fetch_html(HtmlFileRoot::FormattedFile, &fr.file)
                .await?;

            let mut file_excerpts = vec![];

            // ### HTML Extraction: What We Want
            //
            // We want the full line container which looks like:
            // - div id="line-N" class="source-line-with-number" role="row"
            //   - div role="cell"
            //     - div class="cov-strip cov-uncovered cov-known"
            //   - div role="cell"
            //     - div class="blame-strip c2" data-blame="..."
            //   - div role="cell" class="line-number" data-line-number="N"
            //   - code role="cell" class="source-line"
            //     - ex: span class="sync_comment"
            //     - ex: span class="syn_def syn_type" data-symbols="..." data-i
            //
            // ### HTML Extraction Low Level Details
            //
            // Until https://github.com/cloudflare/lol-html/issues/40 or
            // the spin-off https://github.com/cloudflare/lol-html/issues/78
            // are implemented, lol_html doesn't explicitly provide a way to
            // derive the value of an element.
            //
            // So we attempt a hack where we use a custom output sink that is
            // kept aware of where we are in the file.  The good news is that
            // since lol_html is oriented around minimal memory allocation, we
            // can generally control when flushes happen.

            let mut writing_line: u32 = 0;
            let cur_line = Cell::new(0u32);
            let want_cur_line = Cell::new(false);
            let suppressing = Rc::new(Cell::new(false));
            let nesting_suppress = suppressing.clone();

            let mut buf = vec![];

            let mut rewrite = HtmlRewriter::new(
                Settings {
                    element_content_handlers: vec![
                        element!(r#"div.nesting-container"#, move |el| {
                            nesting_suppress.set(true);
                            let end_suppress = nesting_suppress.clone();
                            el.on_end_tag(move |_end| {
                                end_suppress.set(true);
                                Ok(())
                            })?;
                            Ok(())
                        }),
                        element!(r#"div.source-line-with-number"#, |el| {
                            suppressing.set(false);
                            if let Some(id_str) = el.get_attribute("id") {
                                let id_parts: Vec<&str> = id_str.split("-").collect();
                                if id_parts.len() == 2 && id_parts[0] == "line" {
                                    let lno = id_parts[1].parse().unwrap_or(0);
                                    cur_line.set(lno);
                                    want_cur_line.set(lines_to_show.contains(&lno));
                                }
                            }

                            Ok(())
                        }),
                    ],
                    ..Settings::default()
                },
                |c: &[u8]| {
                    if suppressing.get() {
                        return;
                    }

                    // We were actively writing and potentially have some
                    // buffer.
                    if writing_line > 0 {
                        // We're done writing; flush!
                        if cur_line.get() != writing_line {
                            writing_line = 0;
                            file_excerpts.push(String::from_utf8_lossy(&buf).to_string());
                            buf.clear();
                        }
                        // We're still writing!
                        else {
                            // Write into the buffer and then leave, because we
                            // don't need to consider switching into writing, as
                            // we're still here.
                            buf.extend_from_slice(c);
                            return;
                        }
                    }
                    // We either closed out writing or weren't writing.  But now
                    // we need to see if we should be writing!
                    if cur_line.get() > 0 && want_cur_line.get() {
                        writing_line = cur_line.get();
                        buf.extend_from_slice(c);
                    }
                    // Otherwise, this wasn't interesting.
                },
            );

            rewrite.write(html_str.as_bytes()).unwrap();
            rewrite.end().unwrap();

            html_by_file.push(HtmlExcerptsByFile {
                file: fr.file.clone(),
                excerpts: file_excerpts,
            });
        }

        Ok(PipelineValues::HtmlExcerpts(HtmlExcerpts {
            by_file: html_by_file,
        }))
    }
}

```

## tools/src/cmd_pipeline/symbol_graph.rs
```
use std::collections::{BTreeMap, BTreeSet, HashMap, HashSet};

use clap::ValueEnum;
use dot_generator::*;
use dot_structures::*;
use graphviz_rust::printer::{DotPrinter, PrinterContext};
use itertools::Itertools;
use petgraph::{
    algo::all_simple_paths,
    graph::{DefaultIx, NodeIndex},
    Directed, Graph as PetGraph,
};
use serde::ser::SerializeStruct;
use serde::{Serialize, Serializer};
use serde_json::{from_value, json, to_value, Value};
use tracing::trace;
use ustr::{ustr, Ustr, UstrMap};

use crate::{
    abstract_server::{AbstractServer, ErrorDetails, ErrorLayer, Result, ServerError},
    file_format::{
        analysis::AnalysisStructured, analysis_manglings::split_pretty,
        crossref_converter::convert_crossref_value_to_sym_info_rep,
        ontology_mapping::label_to_badge_info,
    },
};

pub use crate::symbol_graph_edge_kind::EdgeKind;

use super::{
    cmd_graph::{GraphHierarchy, GraphLayout},
    interface::OverloadInfo,
};

/**
Graph abstraction for symbols built on top of petgraph.

### Motivation / Implementation Rationale

Conceptually, we want our graphs to operate in terms of searchfox symbols
where the symbol names are the identifiers and we associate a bunch of
information with the symbol.  In the JS fancy branch we were able to easily
implement a (naive, unoptimized) graph with strings as keys.  However,
petgraph is not architected to be used directly in this way.  Graph supports
using arbitrary values but operates in terms of the `NodeIndex<Ix>` values
returned by `add_node`.  GraphMap does exist and allows adding edges
directly by using the nodes directly (or rather, their "weights"), but
requires the weights to implement `Copy`, which is not the case for String.
Additionally, https://timothy.hobbs.cz/rust-play/petgraph-internals.html
indicates GraphMap has worse performance characteristics.

To this end, we implement wrappers around Petgraph that let us operate in
a more ergonomic fashion.  We structure our wrappers to support the creation
of multiple graphs backed by a shared pool of symbol information,
recognizing that:
- petgraph's `Graph` doesn't really like having nodes/edges removed (which
  is why `StableGraph` exists), favoring a graph that is incrementally built
  in an append-only fashion and then used immediately thereafter.
- For debugging and to make it easier for people to understand how searchfox
  works here, it's desirable to be able to visualize the various graph
  states that are produced in the process of the algorithms.  Which means
  that an approach where we take graphs as immutable inputs and produce new
  immutable graphs as output works for us.
- This probably works out better with rust's ownership model?

For a more sophisticated and elegant approach to things like this, it's
worth considering the approach used by cargo-guppy at
https://github.com/facebookincubator/cargo-guppy/tree/main/guppy/src/graph
which is built using custom index classes and other sophisticated things
that I (:asuth) likely won't understand until after this implementation
is working.

### Structs and their relationships

- SymbolGraphNodeSet holds the collection of symbols, which consists of a
  vector of the per-symbol crossref information wrapped into a
  DerivedSymbolInfo which provides us a location to put optionally caching
  getters for facts about the symbol that can be internally derived from
  just the symbol's crossref information.
- SymbolGraphNodeId is a u32 identifier for the symbol which is what we use
  as the node weight in the graphs.  The identifier is just the index of the
  DerivedSymbolInfo in its containing vec.
- NamedSymbolGraph wraps the underlying Graph and provides manipulation
  methods that operate using SymbolGraphNodeId values as nodes that can be
  used to describe edges.  This should gain metadata fields
- SymbolGraphCollection bundles a SymbolGraphNodeSet with all of the
  NamedSymbolGraph instances that depend on the node set and are appropriate
  to surface through the pipeline as results or interesting intermediary
  states for debugging.
*/
pub fn make_safe_port_id(dubious_id: &str) -> String {
    dubious_id.replace(['<', '>', ':', '"'], "_")
}

/// A symbol and its cross-reference information plus caching helpers.
#[derive(Clone)]
pub struct DerivedSymbolInfo {
    pub symbol: Ustr,
    pub crossref_info: Value,
    pub badges: Vec<SymbolBadge>,
    /// For symbols that are fields with pointer_infos, we set the effective
    /// subsystem to be the subsystem of the first pointer_info payload.  We
    /// use this as a first attempt at grouping fields, but it might make sense
    /// instead to store the SymbolNodeId of the first target here instead.
    pub effective_subsystem: Option<Ustr>,
    pub depth: u32,
}

#[derive(Clone, Eq, Ord, PartialEq, PartialOrd)]
pub struct SymbolBadge {
    // Priority for us to mess with the ordering of badges.
    pub pri: i32,
    pub label: Ustr,
    // XXX this doesn't work yet; we'll need to tunnel the extra metadata about
    // these through the cell's id.
    pub source_jump: Option<String>,
}

pub fn semantic_kind_is_callable(semantic_kind: &str) -> bool {
    match semantic_kind {
        "function" => true,
        "method" => true,
        // XXX this is to enable visualizing include deps using "calls-to"; this
        // makes some sense, but really it does imply that this should be a
        // pairwise function if we're going to overload "calls-to" like this.
        // It potentially seems reasonable to do this because it would be a bit
        // pedantic to demand people manually pick the right term; our whole
        // reason for calls-to/calls-from is mainly because "uses" is just so
        // ambiguous.  Also, this is really about avoiding "calls-from" showing
        // fields being referenced when we're talking about control-flow.  That
        // might suggest an orthogonal diagram setting or something.
        "file" => true,
        _ => false,
    }
}

// TODO: evaluate the type of kinds we now allow thanks to SCIP; we may need to
// expand this match branch or just normalize more in SCIP indexing.
pub fn semantic_kind_is_class(semantic_kind: &str) -> bool {
    match semantic_kind {
        "class" => true,
        // Gecko has many structs that are basically classes; also, for our
        // purposes in general, anything with fields is a class.
        "struct" => true,
        _ => false,
    }
}

impl DerivedSymbolInfo {
    pub fn is_callable(&self) -> bool {
        match self.crossref_info.pointer("/meta/kind") {
            Some(Value::String(sem_kind)) => semantic_kind_is_callable(sem_kind),
            _ => false,
        }
    }

    pub fn is_class(&self) -> bool {
        match self.crossref_info.pointer("/meta/kind") {
            Some(Value::String(sem_kind)) => semantic_kind_is_class(sem_kind),
            _ => false,
        }
    }

    /// Provide the structured rep of this symbol if it has one.  If we use this
    /// a lot we should potentially consider using interior mutability to cache
    /// this or have performed the conversion eagerly upon creation.
    pub fn get_structured(&self) -> Option<AnalysisStructured> {
        match self.crossref_info.get("meta") {
            Some(v) => from_value(v.clone()).ok(),
            _ => None,
        }
    }

    /// For hierarchy purposes we want to skip over namespace or namespace-like
    /// symbols in certain modes of operation, this centralizes the heuristic
    /// for that.  Right now this is C++ specific.
    ///
    /// TODO: handle rust and other types from SCIP; as noted in scip-indexer
    /// we probably need to do work there too.  Definitely a case for some unit
    /// tests.
    pub fn is_namespace(&self) -> bool {
        self.symbol.starts_with("NS_")
    }

    pub fn get_pretty(&self) -> Ustr {
        match self.crossref_info.pointer("/meta/pretty") {
            Some(Value::String(pretty)) => ustr(pretty),
            _ => self.symbol,
        }
    }

    pub fn get_binding_slot_sym(&self, kind: &str) -> Option<Ustr> {
        if let Some(Value::Array(slots)) = self.crossref_info.pointer("/meta/bindingSlots") {
            for slot in slots {
                if let Some(Value::String(slot_kind)) = slot.get("slotKind") {
                    if slot_kind.as_str() != kind {
                        continue;
                    }
                    if let Some(Value::String(sym)) = slot.get("sym") {
                        return Some(ustr(sym));
                    }
                    break;
                }
            }
        }
        None
    }

    pub fn get_subsystem(&self) -> Option<Ustr> {
        match self.crossref_info.pointer("/meta/subsystem") {
            Some(Value::String(subsystem)) => Some(ustr(subsystem)),
            _ => None,
        }
    }

    /// Find the path that contains a "def" record for this symbol.  (There
    /// should ideally only be a single definition path, even if we might have
    /// multiple line hits at the path because of #ifdefs.)
    pub fn get_def_path(&self) -> Option<&String> {
        match self.crossref_info.pointer("/defs/0/path") {
            Some(Value::String(path)) => Some(path),
            _ => None,
        }
    }

    /// If this symbol has a definition, return the definition's line number.
    /// This is intended to assist with lexically ordering fields within a
    /// structure/class.
    pub fn get_def_lno(&self) -> u64 {
        match self.crossref_info.pointer("/defs/0/lines/0/lno") {
            Some(Value::Number(lno)) => lno.as_u64().unwrap_or(0),
            _ => 0,
        }
    }

    // Potentially reduce our memory usage by dropping our uses and calls fields
    // if they are present, as they won't be used for jumpref production.
    pub fn reduce_memory_usage_by_dropping_non_jumpref_info(&mut self) {
        if let Some(obj) = self.crossref_info.as_object_mut() {
            obj.remove("uses");
            obj.remove("callees");
        }
    }
}

impl DerivedSymbolInfo {
    pub fn new(symbol: Ustr, crossref_info: Value, depth: u32) -> Self {
        DerivedSymbolInfo {
            symbol,
            crossref_info,
            badges: vec![],
            effective_subsystem: None,
            depth,
        }
    }
}

/// A collection of one or more graphs that share a common underlying set of
/// per-symbol information across the graphs.
pub struct SymbolGraphCollection {
    pub node_set: SymbolGraphNodeSet,
    pub edge_set: SymbolGraphEdgeSet,
    pub graphs: Vec<NamedSymbolGraph>,
    pub overloads_hit: Vec<OverloadInfo>,
    pub hierarchical_graphs: Vec<HierarchicalSymbolGraph>,
}

impl Serialize for SymbolGraphCollection {
    fn serialize<S>(&self, serializer: S) -> std::result::Result<S::Ok, S::Error>
    where
        S: Serializer,
    {
        let mut graphs = vec![];
        for i in 0..self.graphs.len() {
            graphs.push(self.graph_to_json(i));
        }

        let mut hierarchical_graphs = vec![];
        for i in 0..self.hierarchical_graphs.len() {
            hierarchical_graphs.push(self.hier_graph_to_json(i));
        }

        let mut sgc = serializer.serialize_struct("SymbolGraphCollection", 2)?;
        sgc.serialize_field(
            "jumprefs",
            &self.node_set.symbols_meta_to_jumpref_json_nomut(),
        )?;
        sgc.serialize_field("graphs", &graphs)?;
        sgc.serialize_field("hierarchicalGraphs", &hierarchical_graphs)?;
        sgc.end()
    }
}

/// Escape double-quotes to safely use a string as an `esc` tagged value.
///
/// Although graphviz-rust's dot-generator has a concept of `esc`, this does not
/// actually perform any escaping of quotes at the current time. Instead, it
/// just wraps the string in double quotes.  But if we fail to escape any double
/// quotes in the value, they will not be escaped and the graphviz parse will
/// fail.
fn escape_quotes(s: &str) -> String {
    // We're using a raw string so this backslash is propagated as a backslash
    // and is not escaping the double-quote.
    s.replace('"', r#"\""#)
}

/// Perform the necessary escaping for `html` tagged value contents that aren't
/// supposed to be HTML.
fn escape_html(s: &str) -> String {
    s.replace("&", "&amp;")
        .replace("<", "&lt;")
        .replace(">", "&gt;")
}

/// Helper for cases where we want a NodeId that's escaped because there currently
/// is no macro support for this.  We automatically call `escape_quotes` to
/// escape any double-quotes that might be in the identifier.
fn escaped_node_id(id: &str) -> NodeId {
    NodeId(Id::Escaped(format!("\"{}\"", escape_quotes(id))), None)
}

impl SymbolGraphCollection {
    /// Convert the graph with the given index to a { nodes, edges } rep where:
    ///
    /// - nodes is a sorted array of symbol strings.
    /// - edges is a sorted array of { from, to } where from/to are symbol
    ///   strings and the sort is over [from, to]
    pub fn graph_to_json(&self, graph_idx: usize) -> Value {
        let graph = match self.graphs.get(graph_idx) {
            Some(g) => g,
            None => return json!({}),
        };

        // I am biasing for code readability over performance.  In particular,
        // note that we need not infer the nodes from the edges, but it's less
        // code this way.
        //
        // XXX currently we're not serializing the edge information here either,
        // but probably should.
        let mut nodes = BTreeSet::new();
        let mut edges = BTreeMap::new();
        for (source_id, target_id, _edge_id) in graph.list_edges() {
            let source_info = self.node_set.get(&source_id);
            nodes.insert(source_info.symbol);
            let source_sym = source_info.symbol;

            let target_info = self.node_set.get(&target_id);
            nodes.insert(target_info.symbol);
            let target_sym = target_info.symbol;

            edges.insert(
                format!("{}-{}", source_sym, target_sym),
                json!({ "from": source_sym, "to": target_sym }),
            );
        }

        json!({
            "nodes": nodes.into_iter().collect::<Vec<Ustr>>(),
            "edges": edges.into_values().collect::<Value>(),
        })
    }

    /// Convert the graph with the given index to a { nodes, edges } rep where:
    ///
    /// - nodes is a sorted array of symbol strings.
    /// - edges is a sorted array of { from, to } where from/to are symbol
    ///   strings and the sort is over [from, to]
    pub fn hier_graph_to_json(&self, graph_idx: usize) -> Value {
        let graph = match self.hierarchical_graphs.get(graph_idx) {
            Some(g) => g,
            None => return json!({}),
        };

        graph.root.to_json(&self.node_set)
    }

    /// Convert the graph with the given index to a graphviz rep.
    pub fn graph_to_graphviz<F>(&self, graph_idx: usize, node_decorate: F) -> Graph
    where
        F: Fn(&mut Node, &DerivedSymbolInfo),
    {
        let mut dot_graph = graph!(
            di id!("g");
            node!("node"; attr!("shape","box"), attr!("fontname", esc "Courier New"), attr!("fontsize", "10"))
        );

        let graph = match self.graphs.get(graph_idx) {
            Some(g) => g,
            None => return dot_graph,
        };

        let mut nodes = BTreeSet::new();
        for (source_id, target_id, _edge_id) in graph.list_edges() {
            let source_info = self.node_set.get(&source_id);
            let source_sym = source_info.symbol;
            if nodes.insert(source_sym) {
                let mut node = node!(esc source_sym.clone(); attr!("label", esc escape_quotes(&source_info.get_pretty())));
                node_decorate(&mut node, source_info);
                dot_graph.add_stmt(stmt!(node));
            }

            let target_info = self.node_set.get(&target_id);
            let target_sym = target_info.symbol;
            if nodes.insert(target_sym) {
                let mut node = node!(esc target_sym.clone(); attr!("label", esc escape_quotes(&target_info.get_pretty())));
                node_decorate(&mut node, target_info);
                dot_graph.add_stmt(stmt!(node));
            }

            // node_id!'s macro_rules currently can't handle an `esc` prefix, so
            // we create the structs via a hand-rolled `escaped_node_id` that
            // replicates what the equivalent macros would do.
            dot_graph.add_stmt(stmt!(
                edge!(escaped_node_id(&source_sym) => escaped_node_id(&target_sym))
            ));
        }

        dot_graph
    }

    pub fn to_json(&self) -> Value {
        to_value(self).unwrap()
    }

    pub async fn derive_hierarchical_graph(
        &mut self,
        policies: &HierarchyPolicies,
        graph_idx: usize,
        server: &(dyn AbstractServer + Send + Sync),
    ) -> Result<()> {
        trace!("derive_hierarchical_graph");
        let graph = match self.graphs.get(graph_idx) {
            Some(g) => g,
            None => {
                return Ok(());
            }
        };

        let mut root = HierarchicalNode {
            segment: HierarchySegment::PrettySegment("".to_string(), ""),
            display_name: "".to_string(),
            symbols: vec![],
            action: None,
            children: BTreeMap::default(),
            edges: vec![],
            descendant_edge_count: 0,
            height: 0,
        };

        let mut checked_pretties = UstrMap::default();
        let mut sym_segments: HashMap<SymbolGraphNodeId, Vec<HierarchySegment>> =
            HashMap::default();

        // ## Populate the hierarchy nodes.
        //
        // We have a few major modes of operation here:
        // - (Flat doesn't count; this method won't be called.)
        // - For GraphHierarchy::Pretty we just split everything by the pretty
        //   and try to lookup the pretty in case there is a symbol associated
        //   with it.  In some cases, we may not have symbol information.
        // - For everything else we want to figure out the topmost non-namespace
        //   symbol.  That is, if we have "foo::bar::OuterClass::InnerClass::method"
        //   then we want to lose "foo::bar::" but retain
        //   "OuterClass::InnerClass::method".  We would then prepend the
        //   subsystem/file/dir.  Our structured information does not currently
        //   provide an explicit relationship between nested classes and their
        //   containing classes, so the pretty splicing is our best option at
        //   this time.
        //
        // Because of the commonality of needing to potentially consider every
        // level of pretty symbol, we run this as a 3-pass approach:
        // 1. We parse the pretty symbols and attempt to perform a symbol lookup
        //    for every piece
        // 2. We branch based on the GraphHierarchy requested to populate the
        //    segments.
        // 3. We process the segments to populate the nodes.
        //
        // Extra complications:
        // - Inner classes (a class defined within another class) are a major
        //   practical problem because visually we would like to represent a
        //   class and its fields as a "record"-type HTML label display, but
        //   if we nest the inner class under the root class, this causes our
        //   heuristics to not fire.  We address this problem by aggregating
        //   the outer class name onto the inner class name.  A class "Foo" in
        //   the namespace "ns" which has an inner class "InnerFoo" would end
        //   up with Foo having pretty segments of ["ns", "Foo"] and InnerFoo
        //   having pretty segments of ["ns", "Foo::Innerfoo"] with no potential
        //   for "Foo::InnerFoo" to be split an additional time.

        // For synthetic nodes / clusters, give them a depth of 13 right now
        // which is current our last depth enum, although we saturate depth
        // visually at 10 right now.
        const SYNTHETIC_DEPTH: u32 = 13;

        for sym_id in graph.list_nodes() {
            let (sym, sym_pretty) = {
                let node_info = self.node_set.get(&sym_id);
                (node_info.symbol.as_str(), node_info.get_pretty().as_str())
            };
            let mut pretty_so_far = "".to_string();
            trace!(sym = %sym_pretty, "processing symbol");
            let (pieces, delim) = split_pretty(sym_pretty, sym);
            let mut pieces_and_syms = vec![];
            for mut piece in pieces {
                trace!(piece = %piece, "processing piece");
                pretty_so_far = if pretty_so_far.is_empty() {
                    piece.clone()
                } else {
                    format!("{}{}{}", pretty_so_far, delim, piece)
                };
                let ustr_so_far = ustr(&pretty_so_far);

                // If this is a partial pretty, then we only need to perform a lookup
                // for it once, but if it's a full pretty then we need to process it
                // because overloads exist (and have the same pretty)!
                if sym_pretty == pretty_so_far || !checked_pretties.contains_key(&ustr_so_far) {
                    // We haven't checked this before, so process it.

                    // inner class handling help
                    // (needs to borrow from node_set, so invoke before ensure_symbol below)
                    let container_is_class = match pieces_and_syms.last() {
                        Some((_, Some(container_sym_id))) => {
                            let container_info = self.node_set.get(container_sym_id);
                            container_info.is_class()
                        }
                        _ => false,
                    };

                    // See if we can find a symbol for this identifier.
                    if sym_pretty == pretty_so_far {
                        trace!(pretty = %pretty_so_far, "reusing known symbol");

                        if container_is_class && self.node_set.get(&sym_id).is_class() {
                            if let Some((container_piece, _)) = pieces_and_syms.pop() {
                                trace!(pretty = %pretty_so_far, "inner class heuristic merging class piece '{}' with container '{}'", container_piece, piece);
                                piece = format!("{}{}{}", container_piece, delim, piece);
                            }
                        }
                        pieces_and_syms.push((piece, Some(sym_id.clone())));
                    } else {
                        // TODO: Either don't set the limit to 1 or provide a better
                        // explanation or some assert on why this is okay.  In
                        // general, since we are taking a fast path on the full pretty
                        // match, we shouldn't be dealing with overloads here, so there
                        // really should only be a single ancestor symbol per pretty.
                        if let Some((match_sym, _)) = server
                            .search_identifiers(&pretty_so_far, true, false, 1)
                            .await?
                            .first()
                        {
                            let (match_sym_id, match_sym_info) = self
                                .node_set
                                .ensure_symbol(match_sym, server, SYNTHETIC_DEPTH)
                                .await?;

                            let needs_pop = if container_is_class && match_sym_info.is_class() {
                                if let Some((container_piece, _)) = pieces_and_syms.pop() {
                                    trace!(pretty = %pretty_so_far, "inner class heuristic merging class piece '{}' with container '{}'", container_piece, piece);
                                    piece = format!("{}{}{}", container_piece, delim, piece);
                                }
                                true
                            } else {
                                false
                            };

                            checked_pretties
                                .insert(ustr_so_far, Some((match_sym_id.clone(), needs_pop)));
                            pieces_and_syms.push((piece, Some(match_sym_id)));
                        } else {
                            trace!(pretty = %pretty_so_far, "failed to locate symbol for identifier");
                            checked_pretties.insert(ustr_so_far, None);
                            pieces_and_syms.push((piece, None));
                        }
                    };
                } else {
                    match checked_pretties.get(&ustr_so_far) {
                        Some(Some((use_sym_id, needs_pop))) => {
                            if *needs_pop {
                                if let Some((container_piece, _)) = pieces_and_syms.pop() {
                                    trace!(pretty = %pretty_so_far, "inner class heuristic merging class piece '{}' with container '{}'", container_piece, piece);
                                    piece = format!("{}{}{}", container_piece, delim, piece);
                                }
                            }
                            pieces_and_syms.push((piece, Some(use_sym_id.clone())));
                        }
                        _ => {
                            pieces_and_syms.push((piece, None));
                        }
                    }
                }
            }

            let first_real_sym = pieces_and_syms
                .iter()
                .position(|(_piece, maybe_sym)| {
                    if let Some(sym) = maybe_sym {
                        let info = self.node_set.get(sym);
                        !info.is_namespace()
                    } else {
                        false
                    }
                })
                .unwrap_or_else(|| pieces_and_syms.len() - 1);
            let segments_and_syms =
                match &policies.grouping {
                    GraphHierarchy::Flat | GraphHierarchy::Pretty => pieces_and_syms
                        .into_iter()
                        .map(|(piece, sym)| (HierarchySegment::PrettySegment(piece, delim), sym))
                        .collect_vec(),
                    GraphHierarchy::Subsystem => {
                        // For subsystem we always use the subsystem of the first
                        // symbol we find now in order to avoid weird cases where
                        // the methods in a symbol are defined in a different cpp
                        // file that is technically part of a different subsystem.
                        //
                        // This is different than the decision we've made for dir/file
                        // where we do allow fragmentation (but where the class pretty
                        // containment will mean that the class shows up in both the
                        // right place and also the wrong place(s)).
                        let subsystem = match &pieces_and_syms[first_real_sym].1 {
                            Some(sym) => self
                                .node_set
                                .get(sym)
                                .get_subsystem()
                                .map(|x| x.as_str())
                                .unwrap_or(""),
                            None => "",
                        };
                        let segs = subsystem.split("/").map(|piece| {
                            (
                                HierarchySegment::PrettySegment(piece.to_string(), "/"),
                                None,
                            )
                        });
                        segs.chain(pieces_and_syms.into_iter().skip(first_real_sym).map(
                            |(piece, sym)| (HierarchySegment::PrettySegment(piece, delim), sym),
                        ))
                        .collect_vec()
                    }
                    grouping @ GraphHierarchy::File | grouping @ GraphHierarchy::Dir => {
                        // We use the most-specific symbol here.
                        let path = match &pieces_and_syms[pieces_and_syms.len() - 1].1 {
                            Some(sym) => self
                                .node_set
                                .get(sym)
                                .get_def_path()
                                .map(|x| x.as_str())
                                .unwrap_or(""),
                            None => "",
                        };
                        let mut segs = path.split("/").collect_vec();
                        if let GraphHierarchy::Dir = grouping {
                            segs.pop();
                        };
                        segs.into_iter()
                            .map(|piece| {
                                (
                                    HierarchySegment::PrettySegment(piece.to_string(), "/"),
                                    None,
                                )
                            })
                            .chain(pieces_and_syms.into_iter().skip(first_real_sym).map(
                                |(piece, sym)| (HierarchySegment::PrettySegment(piece, delim), sym),
                            ))
                            .collect_vec()
                    }
                };

            let mut segments_so_far = vec![];
            for (segment, maybe_sym) in segments_and_syms {
                segments_so_far.push(segment);
                let mut reversed_segments = segments_so_far.clone();
                reversed_segments.reverse();
                if let Some(sym_id) = maybe_sym {
                    //trace!(pretty = %segments_so_far, "placing found symbol");
                    root.place_sym(reversed_segments, sym_id);
                }
            }

            sym_segments.insert(sym_id, segments_so_far);
        }

        // ## Populate the hierarchy edges
        for (from_id, to_id, edge_id) in graph.list_edges() {
            let from_segments = sym_segments.get(&from_id).unwrap();
            let to_segments = sym_segments.get(&to_id).unwrap();

            let mut common_path: Vec<HierarchySegment> = from_segments
                .iter()
                .zip(to_segments.iter())
                .take_while(|(a, b)| a == b)
                .map(|(a, _)| a.clone())
                .collect();

            // If one is an ancestor of the other, then put the edge above the
            // outer ancestor by popping off a segment.  This allows us to use a
            // table where we might otherwise fall back to a cluster.
            if common_path.len() == from_segments.len() || common_path.len() == to_segments.len() {
                common_path.pop();
            }
            common_path.reverse();
            root.place_edge(common_path, from_id, to_id, edge_id);
        }

        self.hierarchical_graphs.push(HierarchicalSymbolGraph {
            name: graph.name.clone(),
            root,
        });

        Ok(())
    }

    /// Convert the graph with the given index to a graphviz rep.
    pub fn hierarchical_graph_to_graphviz(
        &mut self,
        policies: &HierarchyPolicies,
        graph_idx: usize,
        graph_layout: &GraphLayout,
    ) -> (Graph, HierarchicalRenderState) {
        trace!(graph_idx = %graph_idx, "hierarchical_graph_to_graphviz");
        let mut state = HierarchicalRenderState::new();
        let graph = match self.hierarchical_graphs.get_mut(graph_idx) {
            Some(g) => g,
            None => {
                trace!("no such graph");
                return (
                    graph!(
                        di id!("g");
                        node!("node"; attr!("shape","box"), attr!("fontname", esc "Courier New"), attr!("fontsize", "10"))
                    ),
                    state,
                );
            }
        };

        graph
            .root
            .compile(policies, 0, false, &self.node_set, &mut state);

        let mut dot_graph = Graph::DiGraph {
            id: id!("g"),
            strict: false,
            stmts: vec![],
        };

        // XXX I'm trying something here where we add our own styling statements
        // here before rendering the graph and then propagating its statements
        // across.  As noted below, we previously were just having the styling
        // happening in the graph rendering process below; we really need to
        // figure out where this should happen, but this hybrid approach is
        // somewhat reasonable for now.
        if graph_layout == &GraphLayout::Neato {
            dot_graph.add_stmt(stmt!(node!("graph"; attr!("overlap","prism"), attr!("mode","hier"), attr!("sep",esc "+10"))));
        }

        // Note that the root node renders the default style directives.
        let stmts = graph
            .root
            .render(policies, &self.node_set, &self.edge_set, &mut state);
        for stmt in stmts {
            dot_graph.add_stmt(stmt);
        }

        (dot_graph, state)
    }
}

/// A graph whose nodes are symbols from a `SymbolGraphNodeSet`.
pub struct NamedSymbolGraph {
    pub name: String,
    graph: PetGraph<u32, SymbolGraphEdgeId, Directed>,
    /// Maps SymbolGraphNodeId values to NodeIndex values when the node is
    /// present in the graph.  Exclusively used by ensure_node and it's likely
    /// this could be improved to more directly use NodeIndex.
    node_id_to_ix: HashMap<u32, DefaultIx>,
    /// Inverted/reverse map of the above.
    node_ix_to_id: HashMap<DefaultIx, u32>,
}

impl NamedSymbolGraph {
    pub fn new(name: String) -> Self {
        NamedSymbolGraph {
            name,
            graph: PetGraph::new(),
            node_id_to_ix: HashMap::new(),
            node_ix_to_id: HashMap::new(),
        }
    }

    pub fn containts_node(&self, sym_id: SymbolGraphNodeId) -> bool {
        self.node_id_to_ix.contains_key(&sym_id.0)
    }

    pub fn ensure_node(&mut self, sym_id: SymbolGraphNodeId) -> NodeIndex {
        if let Some(idx) = self.node_id_to_ix.get(&sym_id.0) {
            return NodeIndex::new(*idx as usize);
        }

        let idx = self.graph.add_node(sym_id.0).index() as u32;
        self.node_id_to_ix.insert(sym_id.0, idx);
        self.node_ix_to_id.insert(idx, sym_id.0);

        NodeIndex::new(idx as usize)
    }

    pub fn list_nodes(&self) -> Vec<SymbolGraphNodeId> {
        self.graph
            .node_indices()
            .map(|ix| SymbolGraphNodeId(*self.node_ix_to_id.get(&(ix.index() as u32)).unwrap()))
            .collect()
    }

    pub fn ensure_edge(
        &mut self,
        source: SymbolGraphNodeId,
        target: SymbolGraphNodeId,
        edge: SymbolGraphEdgeId,
    ) {
        let source_ix = self.ensure_node(source);
        let target_ix = self.ensure_node(target);
        self.graph.update_edge(source_ix, target_ix, edge);
    }

    pub fn list_edges(&self) -> Vec<(SymbolGraphNodeId, SymbolGraphNodeId, SymbolGraphEdgeId)> {
        let mut id_edges = vec![];
        for edge in self.graph.raw_edges() {
            let source_id = self
                .node_ix_to_id
                .get(&(edge.source().index() as u32))
                .unwrap();
            let target_id = self
                .node_ix_to_id
                .get(&(edge.target().index() as u32))
                .unwrap();
            id_edges.push((
                SymbolGraphNodeId(*source_id),
                SymbolGraphNodeId(*target_id),
                edge.weight.clone(),
            ));
        }
        id_edges
    }

    /// Find all the paths between two nodes; if you have more than one pair of
    /// nodes you probably want to use `all_simple_paths_using_supernodes` which
    /// will induce source and sink supernodes.
    pub fn all_simple_paths(
        &mut self,
        source: SymbolGraphNodeId,
        target: SymbolGraphNodeId,
    ) -> Vec<Vec<(SymbolGraphNodeId, SymbolGraphNodeId, SymbolGraphEdgeId)>> {
        let source_ix = self.ensure_node(source);
        let target_ix = self.ensure_node(target);
        let paths = all_simple_paths(&self.graph, source_ix, target_ix, 0, None);
        let node_paths = paths
            .map(|v: Vec<_>| {
                v.into_iter()
                    .tuple_windows()
                    .map(|(src, tgt)| {
                        let edge_ix = self.graph.find_edge(src, tgt).unwrap();
                        (
                            SymbolGraphNodeId(
                                *self.node_ix_to_id.get(&(src.index() as u32)).unwrap(),
                            ),
                            SymbolGraphNodeId(
                                *self.node_ix_to_id.get(&(tgt.index() as u32)).unwrap(),
                            ),
                            self.graph[edge_ix].clone(),
                        )
                    })
                    .collect()
            })
            .collect();
        node_paths
    }

    /// XXX don't use this, use
    ///
    /// Variant of all_simple_paths that takes source and target sets and
    /// creates supernodes behind the source set and from the target set in
    /// order to potentially improve the net algorithmic complexity.
    ///
    /// Right now this will mutate our current graph and we don't bother
    /// cleaning that up because the expectation is a successor graph will be
    /// created.
    pub fn all_simple_paths_using_supernodes(
        &mut self,
        next_node_id: u32,
        next_edge_id: u32,
        source_nodes: &Vec<SymbolGraphNodeId>,
        target_nodes: &Vec<SymbolGraphNodeId>,
    ) -> Vec<Vec<(SymbolGraphNodeId, SymbolGraphNodeId, SymbolGraphEdgeId)>> {
        let super_source_id = next_node_id;
        let super_target_id = super_source_id + 1;

        let super_source_ix = self.graph.add_node(super_source_id);
        let super_target_ix = self.graph.add_node(super_target_id);

        let synth_source_edge_id = next_edge_id;
        let synth_target_edge_id = synth_source_edge_id + 1;

        // Add edges from the synthetic source supernode to all source nodes
        for source_id in source_nodes {
            let source_ix = self.ensure_node(source_id.clone());
            self.graph.add_edge(
                super_source_ix,
                source_ix,
                SymbolGraphEdgeId(synth_source_edge_id),
            );
        }

        // Add edges from all target nodes to the synthetic target supernode.
        for target_id in target_nodes {
            let target_ix = self.ensure_node(target_id.clone());
            self.graph.add_edge(
                target_ix,
                super_target_ix,
                SymbolGraphEdgeId(synth_target_edge_id),
            );
        }

        trace!(num_nodes=%super_target_id, num_edges=%synth_target_edge_id, "created supernodes, running petgraph all_simple_paths algorithm");

        // Now we get the paths...
        let paths = all_simple_paths(&self.graph, super_source_ix, super_target_ix, 0, None);

        trace!("have iterator");
        let node_paths = paths
            .map(|v: Vec<_>| {
                v.into_iter()
                    // skip the source supernode
                    .dropping(1)
                    // skip the target supernode
                    .dropping_back(1)
                    .tuple_windows()
                    .map(|(src, tgt)| {
                        let edge_ix = self.graph.find_edge(src, tgt).unwrap();
                        (
                            SymbolGraphNodeId(
                                *self.node_ix_to_id.get(&(src.index() as u32)).unwrap(),
                            ),
                            SymbolGraphNodeId(
                                *self.node_ix_to_id.get(&(tgt.index() as u32)).unwrap(),
                            ),
                            self.graph[edge_ix].clone(),
                        )
                    })
                    .collect()
            })
            .collect();
        node_paths
    }
}

/// Helper hierarchy for building graphviz html table labels as used for our
/// graph display.  This is not meant to be generic.
pub struct LabelTable {
    pub rows: Vec<LabelRow>,

    pub columns_needed: u32,
}

pub struct LabelRow {
    // note that currently our "compile" step assumes there's only ever one cell
    pub cells: Vec<LabelCell>,
}

pub struct LabelCell {
    pub id: Option<String>,
    pub bg_color: Option<&'static str>,
    pub contents: String,
    pub badges: Vec<SymbolBadge>,
    pub symbol: String,
    pub port: String,
    pub indent_level: u32,
}

impl LabelTable {
    pub fn compile(&mut self) {
        for row in &self.rows {
            self.columns_needed = std::cmp::max(self.columns_needed, row.compile());
        }
    }

    pub fn render(&self) -> String {
        let mut rows = vec![];
        for row in &self.rows {
            rows.push(row.render(self.columns_needed));
        }
        format!(
            r#"<<table border="0" cellborder="1" cellspacing="0" cellpadding="4">{}</table>>"#,
            rows.join("")
        )
    }
}

impl LabelRow {
    pub fn compile(&self) -> u32 {
        let mut columns = 0;
        for cell in &self.cells {
            columns += cell.indent_level + 1;
        }
        columns
    }

    pub fn render(&self, _column_count: u32) -> String {
        let mut row_pieces = vec![];
        for cell in &self.cells {
            let indent_str = "&nbsp;".repeat(cell.indent_level as usize);
            let maybe_id = match &cell.id {
                Some(idval) => format!("id=\"{}\" ", escape_quotes(idval)),
                None => "".to_string(),
            };
            let maybe_styling = match &cell.bg_color {
                Some(bgcolor) => format!("bgcolor=\"{}\" ", bgcolor),
                None => "".to_string(),
            };
            let badge_reps = cell
                .badges
                .iter()
                .map(|b| format!("<U>{}</U>", escape_html(&b.label)))
                .collect_vec();
            row_pieces.push(format!(
                r#"<td {}{}href="{}" port="{}" align="left">{}{}{}{}</td>"#,
                maybe_id,
                maybe_styling,
                urlencoding::encode(&cell.symbol),
                cell.port,
                indent_str,
                // The contents can explicitly contain HTML and so the populator is responsible to
                // escape as appropriate.
                cell.contents,
                if badge_reps.is_empty() { "" } else { "  " },
                badge_reps.join(""),
            ));
        }
        format!("<tr>{}</tr>", row_pieces.join(""))
    }
}

/// Default policy for when to summarize clusters in the hierarchical diagram;
/// specific overrides can be set in both directions.
#[derive(Clone, Debug, PartialEq, ValueEnum)]
pub enum HierarchyDefaultSummarizePolicy {
    /// Summarize everything so we can just have an overview.
    All,
    /// Summarize nothing; everything is expanded.
    None,
    /// Summarize clusters that don't have a root node in them.
    Other,
}

/// Policies to guide the hierarchy creation and rendering.
pub struct HierarchyPolicies {
    pub grouping: GraphHierarchy,
    pub summarize: HierarchyDefaultSummarizePolicy,
    pub force_summarize_pretties: HashSet<String>,
    pub force_expand_pretties: HashSet<String>,
    pub group_fields_at: u32,
    pub use_port_dirs: bool,
}

pub enum HierarchicalLayoutAction {
    /// Used for the root node; its contents are rendered at the same level as
    /// the parent and the parent is not rendered.
    Flatten,
    /// Collapse the node into its child.  This is used for situations like
    /// namespaces with one one child which is a sub-namespace and there is no
    /// benefit to creating a separate cluster for the parent.
    Collapse,
    /// Be a graphviz cluster.  Used for situations where there are multiple
    /// children that are eitehr conceptually distinct (ex: separate classes) or
    /// where there are simply too many edges directly between its children and
    /// so the use of a table would be very visually busy.
    ///
    /// Currently there is a NodeId for the cluster and one for the placeholder.
    Cluster(String, String),
    /// Be a table and therefore all children are records.  Used for situations
    /// like a class where the children are methods/fields and the containment
    /// relationship makes sense to express as a table and there is no issue
    /// with edges between the children making the diagram too busy.
    ///
    /// Payload is the node id for the node that is/holds the HTML label.
    Table(String),
    /// For children of a Table parent.
    ///
    /// Payload is the port name.
    Record(String),
    /// Just a normal graphviz node, either contained in a cluster or by the
    /// root.
    Node(String),
}

/// A typed hierarchy segment.  While initially we expect all segments to be
/// part of a pretty identifier, in the future this may include:
/// - Process Type (Parent, Content, Network, etc.)
/// - Subsystem / subcomponent / submodule
///
/// XXX for now we're just going to use `PrettySegment` for everything because
/// it containing the delimiter will let us get away with a lot, and we
/// explicitly associate symbols with the `HierarchicalNode` instances, which
/// means we don't need them on the segment here, but it could make sense to
/// revisit.
#[derive(Clone, Eq, PartialEq, PartialOrd, Ord, Debug)]
pub enum HierarchySegment {
    /// The pretty identifier segment and the delimiter that should be used to
    /// join it to its parent.  Note that in some cases like paths we choose to
    /// include the delimeter as part of the segment string and the delimiter
    /// may accordingly be an empty string.
    PrettySegment(String, &'static str),
}

impl HierarchySegment {
    pub fn to_human_readable(&self) -> String {
        match self {
            Self::PrettySegment(p, _) => p.clone(),
        }
    }

    pub fn join_with_str(&self, existing_str: &str) -> String {
        if existing_str.is_empty() {
            return self.to_human_readable();
        }
        match self {
            Self::PrettySegment(piece, delim) => {
                format!("{}{}{}", existing_str, delim, piece)
            }
        }
    }
}

/// A hierarchial graph derived from a NamedSymbolGraph.
pub struct HierarchicalSymbolGraph {
    pub name: String,
    pub root: HierarchicalNode,
}

pub struct HierarchicalNode {
    /// The segment this is named by in the parent node's `children`.
    pub segment: HierarchySegment,
    /// The display name to use for this segment.  This starts out as the
    /// segment or a more human-readable variation of the segment.  This may be
    /// updated in situations like if its parent is given a `Collapse` action.
    ///
    /// This may be empty in the case of the root node.
    pub display_name: String,
    /// The list of symbols associated with with location.  This can happen due
    /// to differences in signatures across platforms resulting in multiple
    /// symbols corresponding to a single pretty identifier, which is what we
    /// want to support here.  This can also happen due to explicitly overloaded
    /// methods, and ideally we would not fully coalesce these cases, but for
    /// now we do.
    ///
    /// TODO: Improve the explicit overloaded method situation.  (There are
    /// thoughts on how to address this elsewhere; maybe the graphing bug?)
    pub symbols: Vec<SymbolGraphNodeId>,
    /// The action to take when rendering this node to a graph.  Initially None,
    /// but set during `compile`, and then later used for rendering (and debug
    /// and test output so we can understand what decisions were taken).
    pub action: Option<HierarchicalLayoutAction>,
    pub children: BTreeMap<HierarchySegment, HierarchicalNode>,
    /// List of the edges for which this node was the common ancestor.  This is
    /// done so that the `compile` step can understand the number of edges
    /// amongst its descendants and avoid creating (too many) edges between
    /// table rows/cells.  It also results in a better organized dot file which
    /// is nice for readability.
    pub edges: Vec<(SymbolGraphNodeId, SymbolGraphNodeId, SymbolGraphEdgeId)>,
    /// How many edges are contained in the descendants of this node (but not
    /// including this node's own edges).
    pub descendant_edge_count: u32,
    /// The maximum descendant depth below this node.  A node with no children
    /// has a height of 0.  A node with a child with no children has a height of
    /// 1.
    pub height: u32,
}

impl HierarchicalNode {
    pub fn to_json(&self, node_set: &SymbolGraphNodeSet) -> Value {
        let symbols: Vec<Ustr> = self
            .symbols
            .iter()
            .map(|id| node_set.get(id).symbol)
            .collect();
        let action = match &self.action {
            None => Value::Null,
            Some(HierarchicalLayoutAction::Flatten) => json!({
                "layoutAction": "flatten",
            }),
            Some(HierarchicalLayoutAction::Collapse) => json!({
                "layoutAction": "collapse",
            }),
            Some(HierarchicalLayoutAction::Cluster(cluster_id, placeholder_id)) => {
                json!({
                    "layoutAction": "cluster",
                    "clusterId": cluster_id,
                    "placeholderId": placeholder_id,
                })
            }
            Some(HierarchicalLayoutAction::Table(node_id)) => {
                json!({
                    "layoutAction": "table",
                    "nodeId": node_id,
                })
            }
            Some(HierarchicalLayoutAction::Record(port_name)) => {
                json!({
                    "layoutAction": "record",
                    "portName": port_name,
                })
            }
            Some(HierarchicalLayoutAction::Node(node_id)) => {
                json!({
                    "layoutAction": "node",
                    "nodeId": node_id,
                })
            }
        };
        let children: Vec<Value> = self
            .children
            .values()
            .map(|kid| kid.to_json(node_set))
            .collect();
        let edges: Vec<Value> = self
            .edges
            .iter()
            .map(|(from_id, to_id, _edge_id)| {
                // TODO: consider propagating the edge info; it's not essential
                // to validating graph correctness right now, but probably
                // should be something the tests should ensure is stable.
                json!({
                    "from": node_set.get(from_id).symbol.clone(),
                    "to": node_set.get(to_id).symbol.clone(),
                })
            })
            .collect();
        json!({
            "segment": self.segment.to_human_readable(),
            "displayName": self.display_name,
            "height": self.height,
            "symbols": symbols,
            "action": action,
            "children": children,
            "edges": edges,
            "descendantEdgeCount": self.descendant_edge_count,
        })
    }

    /// Recursively traverse child nodes, creating them as needed, in order to
    /// place symbols and create hierarchy as a byproduct.
    pub fn place_sym(
        &mut self,
        mut reversed_segments: Vec<HierarchySegment>,
        sym_id: SymbolGraphNodeId,
    ) {
        if let Some(next_seg) = reversed_segments.pop() {
            let kid = self.children.entry(next_seg.clone()).or_insert_with(|| {
                let display_name = next_seg.to_human_readable();
                HierarchicalNode {
                    segment: next_seg,
                    display_name,
                    symbols: vec![],
                    action: None,
                    children: BTreeMap::default(),
                    edges: vec![],
                    descendant_edge_count: 0,
                    height: 0,
                }
            });
            kid.place_sym(reversed_segments, sym_id);
            // The height of the kid may have updated, so potentially update our
            // height.  This will bubble upwards appropriately.
            self.height = core::cmp::max(self.height, kid.height + 1);
        } else if !self.symbols.contains(&sym_id) {
            self.symbols.push(sym_id);
        }
    }

    pub fn place_edge(
        &mut self,
        mut reversed_segments: Vec<HierarchySegment>,
        from_id: SymbolGraphNodeId,
        to_id: SymbolGraphNodeId,
        edge_id: SymbolGraphEdgeId,
    ) {
        if let Some(next_seg) = reversed_segments.pop() {
            if let Some(kid) = self.children.get_mut(&next_seg) {
                // The edge will go in our descendant, so we bump the count.
                self.descendant_edge_count += 1;
                kid.place_edge(reversed_segments, from_id, to_id, edge_id);
            }
        } else {
            // We do not modify the descendant_edge_count because it's our own
            // edge.
            self.edges.push((from_id, to_id, edge_id));
        }
    }

    pub fn compile(
        &mut self,
        policies: &HierarchyPolicies,
        depth: usize,
        has_class_ancestor: bool,
        node_set: &SymbolGraphNodeSet,
        state: &mut HierarchicalRenderState,
    ) {
        let is_root = depth == 0;

        let is_class = if !self.symbols.is_empty() {
            let sym_info = node_set.get(&self.symbols[0]);
            sym_info.is_class()
        } else {
            false
        };
        let be_class = has_class_ancestor || is_class;

        // If the node has only one child and no edges, we can collapse it UNLESS
        // the child is a class, in which case we really don't want to.
        if !is_root && !be_class && self.children.len() == 1 && self.edges.is_empty() {
            let sole_kid = self.children.values_mut().next().unwrap();
            // (not all nodes will have associated symbols)
            let kid_is_class = if let Some(kid_id) = sole_kid.symbols.first() {
                node_set.get(kid_id).is_class()
            } else {
                false
            };

            // The child's needs impact our ability to collapse:
            // - If the kid is a class, don't collapse into it.  (Classes can still
            //   be clusters, but the idea is they should/need to be distinguished
            //   from classes.)
            if !kid_is_class {
                self.action = Some(HierarchicalLayoutAction::Collapse);
                if !self.display_name.is_empty() {
                    // There are 2 potential delimiters in play here, although it's really only
                    // the synthetic root where we don't have a useful delimiter and we need to
                    // favor the kid, but this seems like a reasonable policy that the kid knows
                    // its best delimiter.
                    let delim = match &sole_kid.segment {
                        HierarchySegment::PrettySegment(_, delim) => delim,
                    };
                    sole_kid.display_name =
                        format!("{}{}{}", self.display_name, delim, sole_kid.display_name);
                    self.display_name = "".to_string();
                }
                sole_kid.compile(policies, depth + 1, be_class, node_set, state);
                return;
            }
        }

        let mut be_cluster = false;

        if is_root {
            self.action = Some(HierarchicalLayoutAction::Flatten);
            for kid in self.children.values_mut() {
                kid.compile(policies, depth + 1, be_class, node_set, state);
            }
        } else if !self.edges.is_empty() && !self.children.is_empty() {
            // If there are edges at this level, it does not make sense to be a
            // table because the self-edges end up quite gratuitous.  (The edges
            // vec only contains edges among our immediate children.)
            //
            // The exception is that if the edge is just a self-edge, then
            // there's no need to make this a cluster.  This can happen for
            // overloaded methods where we collapse them to a single graphviz
            // node but one variant of the function calls the other variant.
            be_cluster = true;
        } else if be_class && self.descendant_edge_count < 5 && self.height == 1 {
            // If the number of internal edges are low and we've reached a class AND
            // we have a height of 1 (which implies having children), then we can
            // be a table.
            //
            // In the prototype, this choice was not aware of height and so could
            // result in trying to create a table that could be complicated by the
            // existence of inner classes.  Our introduction of height should
            // eliminate that concern while not precluding use of a table when
            // we are only dealing with classes.  Like it could be nice to have
            // a class and its immediate subclasses shown as a table as long as
            // there aren't methods nested under the sub-class.
            //
            // Note that the prototype never dealt with that more complex table
            // case, it just had a comment noting the weirdness possible.
            let parent_id_str = self.derive_id(node_set, state);
            {
                let port_id_str = make_safe_port_id(&parent_id_str);
                let in_target = if policies.use_port_dirs {
                    node_id!(esc parent_id_str, port!(id!(esc port_id_str), "w"))
                } else {
                    node_id!(esc parent_id_str, port!(id!(esc port_id_str)))
                };
                let out_target = if policies.use_port_dirs {
                    node_id!(esc parent_id_str, port!(id!(esc port_id_str), "e"))
                } else {
                    node_id!(esc parent_id_str, port!(id!(esc port_id_str)))
                };
                state.register_symbol_edge_targets(&self.symbols, in_target, out_target);
            }

            for kid in self.children.values_mut() {
                let kid_id_str = kid.derive_id(node_set, state);
                let port_id_str = make_safe_port_id(&kid_id_str);
                let in_target = if policies.use_port_dirs {
                    node_id!(esc parent_id_str, port!(id!(esc port_id_str), "w"))
                } else {
                    node_id!(esc parent_id_str, port!(id!(esc port_id_str)))
                };
                let out_target = if policies.use_port_dirs {
                    node_id!(esc parent_id_str, port!(id!(esc port_id_str), "e"))
                } else {
                    node_id!(esc parent_id_str, port!(id!(esc port_id_str)))
                };
                state.register_symbol_edge_targets(&kid.symbols, in_target, out_target);
                kid.action = Some(HierarchicalLayoutAction::Record(kid_id_str));
            }
            self.action = Some(HierarchicalLayoutAction::Table(parent_id_str));
        } else if !self.children.is_empty() {
            // If there are kids, we want to be a cluster after all.
            be_cluster = true;
        } else {
            let node_id_str = self.derive_id(node_set, state);
            let id = node_id!(esc escape_quotes(&node_id_str));
            state.register_symbol_edge_targets(&self.symbols, id.clone(), id);
            self.action = Some(HierarchicalLayoutAction::Node(node_id_str));
        }

        if be_cluster {
            let placeholder_id_str = state.issue_new_synthetic_id();
            let cluster_id = self.derive_id(node_set, state);
            let placeholder_id = node_id!(esc placeholder_id_str);

            self.action = Some(HierarchicalLayoutAction::Cluster(
                cluster_id,
                placeholder_id_str,
            ));

            // XXX The use of a placeholder is from the prototype; need to
            // understand and document the approach more.
            state.register_symbol_edge_targets(
                &self.symbols,
                placeholder_id.clone(),
                placeholder_id,
            );

            for kid in self.children.values_mut() {
                kid.compile(policies, depth + 1, be_class, node_set, state);
            }
        }
    }

    /// Normalize situations for nodes which lack a symbol id so that we create
    /// a synthetic id which can be used as a node id and return the string
    /// representation of the symbol, if any.
    ///
    /// Returns the String to use as the node id in the graphviz graph.  For
    /// nodes that have backing symbols, this will be all of the symbols joined
    /// with commas because data-symbols will do the right thing
    /// post-transformation.
    pub fn derive_id(
        &self,
        node_set: &SymbolGraphNodeSet,
        state: &mut HierarchicalRenderState,
    ) -> String {
        if !self.symbols.is_empty() {
            self.symbols
                .iter()
                .map(|sym_id| node_set.get(sym_id).symbol)
                .join(",")
        } else {
            state.issue_new_synthetic_id()
        }
    }

    pub fn render(
        &self,
        policies: &HierarchyPolicies,
        node_set: &SymbolGraphNodeSet,
        edge_set: &SymbolGraphEdgeSet,
        state: &mut HierarchicalRenderState,
    ) -> Vec<Stmt> {
        let action = match &self.action {
            Some(a) => a,
            None => {
                return vec![];
            }
        };

        let mut result = vec![];
        match action {
            // Collapse ends up looking the same as flatten.
            HierarchicalLayoutAction::Flatten => {
                // Provide the default settings here in the root too.

                // concentrate has the advantage of bundling edges together that are going to the
                // same location (although not exhaustively), but has the downside that it can
                // result in wackier looking edges as we end up with more spline segments and each
                // segment can do its own wiggly thing, so concentrated edges can seem to wiggle for
                // no good reason.
                //
                // XXX turned this off because it messes up edge hover highlighting.
                //result.push(stmt!(attr!("concentrate", "true")));

                // Decidedly not better, but interesting!
                // (As discussed extensively on discourse, the LR is a rotation of the TD that
                // does not do well with records.)
                //result.push(stmt!(attr!("rankdir", "lr")));
                //result.push(stmt!(attr!("layout", "osage")));

                // The graph node affects both the root graph and subgraphs/clusters, which is why
                // we don't just set the attributes here at the root level on the root digraph.
                result.push(stmt!(node!("graph";
                    attr!("fontname", esc "Courier New"),
                    attr!("fontsize", "12"),
                    // Didn't notice a difference yet, but may be useful.
                    //attr!("ratio", "compress"),
                    // Currently newrank just ends up making clusters taller with weird whitespace
                    // because it's leaving space for nodes contained in sibling clusters (because
                    // the point of newrank is to allow nodes outside the cluster to impact rank
                    // calculations).  It only makes sense to turn this on when we have a benefit
                    // like being able to use rank=same to line nodes in the cluster up with nodes
                    // outside the cluster or in another cluster.
                    //attr!("newrank", "true"),
                    attr!("compound", "true")
                )));
                result.push(stmt!(node!("node"; attr!("shape","box"), attr!("fontname", esc "Courier New"), attr!("fontsize", "10"))));
                for kid in self.children.values() {
                    result.extend(kid.render(policies, node_set, edge_set, state));
                }
            }
            HierarchicalLayoutAction::Collapse => {
                for kid in self.children.values() {
                    result.extend(kid.render(policies, node_set, edge_set, state));
                }
            }
            HierarchicalLayoutAction::Cluster(cluster_id, placeholder_id) => {
                let mut sg = subgraph!(esc cluster_id; attr!("cluster", "true"), attr!("label", esc escape_quotes(&self.display_name)));
                sg.stmts.push(stmt!(
                    node!(esc placeholder_id; attr!("shape", "point"), attr!("style", "invis"))
                ));

                // Build a rank=same group for all nodes in the cluster that only have edges into
                // them external to the cluster.  We do this in order to make the graph more
                // vertical because this will help nodes with edges into our cluster have a rank
                // that doesn't overlap with our cluster.  At least for nodes that are only
                // public-facing; if the node has internal edges into it, this heuristic won't
                // apply.
                //
                // We achieve this by walking the list of edges inside this cluster and making note
                // of all of the "to" identifiers in a set.  As we walk our list of children, any
                // child without any of its symbols being in that to set gets added to this top
                // group.
                //
                // A limitation of this approach is that we don't distinguish between a node that
                // has external edges to it and a node that has no edges to it at all.  But we only
                // expect that to happen in overview situations, and those probably want additional
                // specializations that we can deal with then.
                //
                // XXX In some cases this may be leading to weird artifacts where nodes that have
                // an edge to nodes that are being floated upward end up with the same rank.  This
                // may be a result of not having an up-to-date graphviz on the live servers,
                // however.  (The original motivation for updating my docker copy was crashes, and
                // I believe I have seen that on the live server, but they have been less common
                // than I was worried about.)
                let mut internal_targets: HashSet<SymbolGraphNodeId> = HashSet::new();
                for (_, to_id, _) in &self.edges {
                    internal_targets.insert(to_id.clone());
                }
                let mut top_nodes = subgraph!(; attr!("rank", "same"), attr!("cluster", "false"), attr!("label", esc ""));

                for kid in self.children.values() {
                    sg.stmts
                        .extend(kid.render(policies, node_set, edge_set, state));

                    if !kid.symbols.iter().any(|id| internal_targets.contains(id)) {
                        if let Some(HierarchicalLayoutAction::Node(kid_id)) = &kid.action {
                            top_nodes.stmts.push(stmt!(node!(esc kid_id)));
                        }
                    }
                }

                // Only push the rank=same group if it will cover at least 2 nodes; we need to
                // account for the attributes we added above too.
                if top_nodes.stmts.len() >= (3 + 2) {
                    sg.stmts.push(stmt!(top_nodes));
                }

                result.push(stmt!(sg));
            }
            HierarchicalLayoutAction::Table(node_id) => {
                let mut table = LabelTable {
                    rows: vec![],
                    columns_needed: 0,
                };

                table.rows.push(LabelRow {
                    cells: vec![LabelCell {
                        id: Some(state.id_for_nodes(&self.symbols)),
                        bg_color: None,
                        contents: format!("<b>{}</b>", escape_html(&self.display_name)),
                        badges: node_set.get_merged_badges_for_symbols(&self.symbols),
                        symbol: node_id.clone(),
                        port: make_safe_port_id(node_id),
                        indent_level: 0,
                    }],
                });

                let grouped_kids = if self.children.len() >= policies.group_fields_at as usize {
                    let mut grouped = self
                        .children
                        .values()
                        .into_group_map_by(|kid| {
                            if let Some(kid_sym_id) = kid.symbols.first() {
                                let kid_info = node_set.get(kid_sym_id);
                                kid_info
                                    .effective_subsystem
                                    .or_else(|| kid_info.get_subsystem())
                            } else {
                                None
                            }
                        })
                        .into_iter()
                        .collect_vec();
                    grouped.sort_by_cached_key(|(g, _)| *g);
                    grouped
                } else {
                    self.children
                        .values()
                        .into_group_map_by(|_| -> Option<Ustr> { None })
                        .into_iter()
                        .collect_vec()
                };

                // Start from our own depth, but it's quite likely that the kids may actually have
                // lower depths.
                let mut min_depth = node_set.get_min_depth_for_symbols(&self.symbols);
                let mut kid_edges = vec![];
                // Only show group headers if there's more than 1 group!
                let use_groups = grouped_kids.len() > 1;
                for (group, ordered_kids) in grouped_kids {
                    if let Some(group_name) = group {
                        if use_groups {
                            table.rows.push(LabelRow {
                                cells: vec![LabelCell {
                                    id: None,
                                    bg_color: Some("#eee"),
                                    contents: format!("<I>{}</I>", escape_html(&group_name)),
                                    badges: vec![],
                                    port: "".to_string(),
                                    symbol: "".to_string(),
                                    indent_level: 0,
                                }],
                            });
                        }
                    }
                    for kid in ordered_kids {
                        if let Some(HierarchicalLayoutAction::Record(kid_id)) = &kid.action {
                            let kid_depth = node_set.get_min_depth_for_symbols(&kid.symbols);
                            if kid_depth < min_depth {
                                min_depth = kid_depth;
                            }
                            let kid_port_name = make_safe_port_id(kid_id);
                            table.rows.push(LabelRow {
                                cells: vec![LabelCell {
                                    id: Some(state.id_for_nodes(&kid.symbols)),
                                    bg_color: None,
                                    contents: escape_html(&kid.display_name),
                                    badges: node_set.get_merged_badges_for_symbols(&kid.symbols),
                                    symbol: kid_id.clone(),
                                    port: kid_port_name,
                                    indent_level: 1,
                                }],
                            });

                            for (from_id, to_id, _edge_id) in &kid.edges {
                                if let Some((from_node, to_node)) =
                                    state.lookup_edge(from_id, to_id)
                                {
                                    kid_edges.push(stmt!(edge!(from_node => to_node)));
                                }
                            }
                        }
                    }
                }

                table.compile();
                let table_html = table.render();

                // We don't put a custom "id" on this because we only want the rows to have our
                // identifiers.
                let node = node!(esc node_id;
                          attr!("shape", "none"),
                          attr!("label", html table_html),
                          attr!("class", esc format!("diagram-depth-{}", min_depth)));
                result.push(stmt!(node));

                result.extend(kid_edges);
            }
            HierarchicalLayoutAction::Record(_) => {
                // Records will be handled by their parent table.
            }
            HierarchicalLayoutAction::Node(node_id) => {
                let badges = node_set.get_merged_badges_for_symbols(&self.symbols);
                let maybe_labels = if !badges.is_empty() {
                    format!(
                        " {}",
                        badges
                            .into_iter()
                            .map(|b| format!("<U>{}</U>", escape_html(&b.label)))
                            .collect_vec()
                            .join("")
                    )
                } else {
                    "".to_string()
                };
                result.push(stmt!(
                    node!(esc node_id;
                          attr!("id", state.id_for_nodes(&self.symbols)),
                          attr!("label", html format!("<{}{}>", escape_html(&self.display_name), maybe_labels)),
                          attr!("class", esc format!("diagram-depth-{}", node_set.get_min_depth_for_symbols(&self.symbols))))
                ));
            }
        }

        let mut emitted_edges = HashSet::new();
        let mut ctx = PrinterContext::default();
        for (from_id, to_id, edge_id) in &self.edges {
            if let Some((from_node, to_node)) = state.lookup_edge(from_id, to_id) {
                // We de-duplicate on what the string rep ends up looking like because the NodeId type
                // and its sub-types don't really want to get put in a set.
                //
                // The need to de-duplicate currently arises from multiple symbols being associated
                // with a single node/pretty identifier due to multiple platforms.  Which is to say
                // that we have already de-duplicated edges on a symbol basis upstream, but only
                // now are we de-duplicating in (pretty) node space.
                //
                // TODO: Do a better job of handling unification of EdgeDetails for these de-duped
                // edges.  Right now we just use the details of the first edge we end up emitting.
                // As long as the de-duplication is just dealing with platform variations, the net
                // result should be the same, but it would be preferable to have explicitly had an
                // edge unification step when doing the hierarchical processing.
                let edge_info = edge_set.get(edge_id);

                let (style, loc, arrow) = match edge_info.kind {
                    EdgeKind::Default => ("solid", "arrowhead", "normal"),
                    EdgeKind::Inheritance => ("solid", "arrowtail", "onormal"),
                    EdgeKind::Implementation => ("dashed", "arrowhead", "onormal"),
                    EdgeKind::Composition => ("solid", "arrowtail", "diamond"),
                    EdgeKind::Aggregation => ("solid", "arrowtail", "odiamond"),
                    EdgeKind::IPC => ("dotted", "arrowhead", "vee"),
                    EdgeKind::CrossLanguage => ("solid", "arrowhead", "lnormal"),
                };

                let mut maybe_edge =
                    edge!(from_node => to_node; attr!("style", style), attr!(loc, arrow));
                // arrowtail only works if dir=back or dir=both
                // XXX eh, make this more efficient maybe.
                if loc == "arrowtail" {
                    maybe_edge.attributes.push(attr!("dir", "back"));
                }
                if emitted_edges.insert(maybe_edge.print(&mut ctx)) {
                    // As per the TODO above, here is us now translating the edge details.  In
                    // theory we could do this above if we expect the data to always be the same,
                    // but it's better to err on the side of not creating multiple edges if we're
                    // wrong about that.
                    maybe_edge
                        .attributes
                        .push(attr!("id", state.id_for_edge(edge_id)));

                    state.set_edge_metadata(from_id, to_id, edge_id, &edge_info.data);

                    result.push(stmt!(maybe_edge));
                }
            }
        }

        result
    }
}

/// Rendering pass shared state.  This object is passed through the recursive
/// call trees for the hierarchical objects which can't have a durable reference
/// to global state because then the object hierarchy wouldn't be a clean tree.
///
/// Of particular importance is the `sym_to_edges` mapping which is used to
/// store a mapping from `SymbolGraphNodeId` (one per underlying symbol) to
/// the graphviz identifiers that should use when drawing an edge into the node
/// and out of the node.  This is necessary for several reasons:
/// - We map multiple symbols onto a single visual node (usually based on
///   pretty identifier), which means a mapping is necessary somewhere.
/// - We use record-style labels and currently have edges enter on the left and
///   exit on the right.
///
/// ### SVG Metadata
///
/// The object also accumulates state that is subsequently used to fix-up the
/// graphviz SVG output to include extra metadata.  Long term, it would likely
/// be preferable to contribute fixes to graphviz upstream to let specific
/// attributes be propagated through SVG layout; from the discourse server
/// discussion I believe there has been interest in generally supporting the
/// propagation of user-defined attributes more generally.
///
/// Note that this is a new second way that we are propagating data to the UI.
/// We currently perform regexp transforms on title and xlink nodes to turn
/// those into "data-symbols" attributes.  This is desirable for legibility of
/// the resulting SVG doc, as these are data payloads.  It would be nice not to
/// need to use regexps for this, but it doesn't look like lol_html
/// intentionally supports SVG (there's very trivial test coverage where it's
/// not really clear what's intended to be supported), and it doesn't seem worth
/// the work to more properly support XML streaming.
///
/// This info is generally more like metadata, and so a JSON sidecar keyed by
/// newly generated node/edge identifiers is workable.  (The dot SVG output
/// layer right now is making identifiers we can't predict, but if we manually
/// assign identifiers, it will use them, and so we use that.)
pub struct HierarchicalRenderState {
    next_synthetic_id: u32,
    /// Maps the SymbolGraphNodeId to (in-edge id, out-edge-id)
    sym_to_edges: HashMap<u32, (NodeId, NodeId)>,
    /// Maps node identifiers to data for hover purposes; value tuple is:
    pub svg_node_extra: BTreeMap<String, SvgNodeExtra>,
    pub svg_edge_extra: BTreeMap<String, SvgEdgeExtra>,
}

#[derive(Default, Serialize)]
pub struct SvgNodeExtra {
    /// List of edge identifiers for edges that should be highlighted with the
    /// in-edge color on hover.
    pub in_edges: Vec<String>,
    /// List of pairs of:
    /// - node identifier for input/source nodes with higlighting
    /// - List of CSS classes to apply to the nodes on hover if specific colors
    ///   should be used.  If the list is empty, a default hover style will be
    ///   used.
    pub in_nodes: Vec<(String, Vec<String>)>,
    /// List of edge identifiers for edges that should be highlighted with the
    /// out-edge color on hover.
    pub out_edges: Vec<String>,
    /// See `in_nodes`.
    pub out_nodes: Vec<(String, Vec<String>)>,
}

#[derive(Serialize)]
pub struct SvgEdgeExtra {
    pub jump: String,
}

impl Default for HierarchicalRenderState {
    fn default() -> Self {
        Self::new()
    }
}

impl HierarchicalRenderState {
    pub fn new() -> Self {
        Self {
            next_synthetic_id: 0,
            sym_to_edges: HashMap::default(),
            svg_node_extra: BTreeMap::default(),
            svg_edge_extra: BTreeMap::default(),
        }
    }

    pub fn issue_new_synthetic_id(&mut self) -> String {
        let use_id = self.next_synthetic_id;
        self.next_synthetic_id += 1;
        format!("SYN_{}", use_id)
    }

    pub fn register_symbol_edge_targets(
        &mut self,
        sym_ids: &Vec<SymbolGraphNodeId>,
        in_target: NodeId,
        out_target: NodeId,
    ) {
        for sym_id in sym_ids {
            self.sym_to_edges
                .insert(sym_id.0, (in_target.clone(), out_target.clone()));
        }
    }

    /// Given an edge defined by symbol node id's, look-up the correct graphviz
    /// out-edge id and graphviz in-edge id, respectively.
    pub fn lookup_edge(
        &self,
        from_id: &SymbolGraphNodeId,
        to_id: &SymbolGraphNodeId,
    ) -> Option<(NodeId, NodeId)> {
        let from_id = match self.sym_to_edges.get(&from_id.0) {
            Some((_, out_target)) => out_target.clone(),
            _ => {
                return None;
            }
        };

        match self.sym_to_edges.get(&to_id.0) {
            Some((in_target, _)) => Some((from_id, in_target.clone())),
            _ => None,
        }
    }

    pub fn id_for_node(&self, node_id: &SymbolGraphNodeId) -> String {
        format!("Gidn{}", node_id.0)
    }

    /// Return a node identifier for the set of nodes.  We just pick the first
    /// one.
    pub fn id_for_nodes(&self, nodes: &[SymbolGraphNodeId]) -> String {
        // XXX there are cases where nodes is empty and the code assumed it
        // would not be.
        if nodes.is_empty() {
            "".to_string()
        } else {
            format!("Gidn{}", nodes[0].0)
        }
    }

    pub fn id_for_edge(&self, edge_id: &SymbolGraphEdgeId) -> String {
        format!("Gide{}", edge_id.0)
    }

    pub fn set_edge_metadata(
        &mut self,
        from_id: &SymbolGraphNodeId,
        to_id: &SymbolGraphNodeId,
        edge_id: &SymbolGraphEdgeId,
        edge_data: &[EdgeDetail],
    ) {
        let from_eid = self.id_for_node(from_id);
        let to_eid = self.id_for_node(to_id);
        let edge_eid = self.id_for_edge(edge_id);

        let mut hover_classes = vec![];
        let mut jump = "".to_string();
        for detail in edge_data {
            match detail {
                EdgeDetail::Jump(jump_detail) => {
                    // If we have multiple jumps, attempt to merge them assuming they're all using
                    // the same source file and so we can just concatenate the lines.  This is
                    // intended to deal with the "uses" case where we process each hit individually
                    // in TraverseCommand without doing any grouping.  The "callees" case will only
                    // ever have a single jump because crossref pre-aggregates.
                    jump = if jump.is_empty() {
                        jump_detail.clone()
                    } else {
                        match jump_detail.rsplit_once('#') {
                            Some((_, lines)) => format!("{},{}", jump, lines),
                            None => jump,
                        }
                    }
                }
                EdgeDetail::HoverClass(class_detail) => {
                    hover_classes.push(class_detail.clone());
                }
            }
        }

        let from_extra = self.svg_node_extra.entry(from_eid.clone()).or_default();
        from_extra.out_edges.push(edge_eid.clone());
        from_extra.out_nodes.push((to_eid.clone(), hover_classes));

        if let Some(extra) = self.svg_edge_extra.get_mut(&edge_eid) {
            extra.jump = jump;
        } else {
            self.svg_edge_extra
                .insert(edge_eid.clone(), SvgEdgeExtra { jump });
        }

        let to_extra = self.svg_node_extra.entry(to_eid).or_default();
        to_extra.in_edges.push(edge_eid);
        to_extra.in_nodes.push((from_eid, vec![]));
    }
}

/// Wrapped u32 identifier for DerivedSymbolInfo nodes in a SymbolGraphNodeSet
/// for type safety.  The values correspond to the index of the node in the
/// `symbol_crossref_infos` vec in `SymbolGraphNodeSet`.
#[derive(Clone, Eq, Hash, PartialEq)]
pub struct SymbolGraphNodeId(u32);

/// Wrapped u32 identifier for EdgeInfo objects in a SymbolGraphNodeSet for type
/// safety.  The values correspond to the index of the edge in the
/// `edge_infos` vec in `SymbolGraphNodeSet`.
#[derive(Clone, Eq, Hash, PartialEq)]
pub struct SymbolGraphEdgeId(u32);

pub struct SymbolGraphNodeSet {
    pub symbol_crossref_infos: Vec<DerivedSymbolInfo>,
    pub symbol_to_index_map: UstrMap<u32>,
}

pub struct SymbolGraphEdgeSet {
    pub edge_infos: Vec<EdgeInfo>,
    edge_lookup: HashMap<(u32, u32), u32>,
}

#[derive(Clone, PartialEq)]
pub enum EdgeDetail {
    /// Provide a source code jump.
    Jump(String),
    /// Hover class for the target node when the source node is hovered.  This
    /// could also be applied to the edge if desired.
    HoverClass(String),
}

/// Information about the edge between two nodes that is something we either
/// want for debugging, layout, or to explicitly present to the user.
pub struct EdgeInfo {
    pub from_id: SymbolGraphNodeId,
    pub to_id: SymbolGraphNodeId,
    pub kind: EdgeKind,
    pub data: Vec<EdgeDetail>,
}

fn make_data_invariant_err() -> ServerError {
    ServerError::StickyProblem(ErrorDetails {
        layer: ErrorLayer::RuntimeInvariantViolation,
        message: "SymbolGraphNodeSet desynchronized".to_string(),
    })
}

impl Default for SymbolGraphNodeSet {
    fn default() -> Self {
        Self::new()
    }
}

impl SymbolGraphNodeSet {
    pub fn new() -> Self {
        Self {
            symbol_crossref_infos: vec![],
            symbol_to_index_map: UstrMap::default(),
        }
    }

    pub fn get(&self, node_id: &SymbolGraphNodeId) -> &DerivedSymbolInfo {
        // It's very much an invariant that only we mint SymbolGraphNodeId's, so
        // the entry should always exist.
        self.symbol_crossref_infos.get(node_id.0 as usize).unwrap()
    }

    pub fn get_mut(&mut self, node_id: &SymbolGraphNodeId) -> &mut DerivedSymbolInfo {
        // It's very much an invariant that only we mint SymbolGraphNodeId's, so
        // the entry should always exist.
        self.symbol_crossref_infos
            .get_mut(node_id.0 as usize)
            .unwrap()
    }

    pub fn get_merged_badges_for_symbols(
        &self,
        nodes: &Vec<SymbolGraphNodeId>,
    ) -> Vec<SymbolBadge> {
        let mut badges: BTreeSet<SymbolBadge> = BTreeSet::default();
        for sym_id in nodes {
            let sym_info = self.get(sym_id);
            for badge in &sym_info.badges {
                badges.insert(badge.clone());
            }
        }
        badges.into_iter().collect()
    }

    pub fn get_min_depth_for_symbols(&self, nodes: &Vec<SymbolGraphNodeId>) -> u32 {
        // Currently 13 is the highest depth we can report.
        let mut min_depth: u32 = 13;
        for sym_id in nodes {
            let sym_info = self.get(sym_id);
            if sym_info.depth < min_depth {
                min_depth = sym_info.depth;
            }
        }
        min_depth
    }

    #[allow(clippy::too_many_arguments)]
    pub fn propagate_paths(
        &self,
        nsgraph: &mut NamedSymbolGraph,
        source_nodes: &Vec<SymbolGraphNodeId>,
        target_nodes: &Vec<SymbolGraphNodeId>,
        edge_set: &SymbolGraphEdgeSet,
        node_soft_limit: u32,
        path_length_limit: u32,
        new_graph: &mut NamedSymbolGraph,
        new_symbol_set: &mut SymbolGraphNodeSet,
        new_edge_set: &mut SymbolGraphEdgeSet,
    ) {
        let super_source_id = self.symbol_crossref_infos.len() as u32;
        let super_target_id = super_source_id + 1;

        let super_source_ix = nsgraph.graph.add_node(super_source_id);
        let super_target_ix = nsgraph.graph.add_node(super_target_id);

        let synth_source_edge_id = edge_set.edge_infos.len() as u32;
        let synth_target_edge_id = synth_source_edge_id + 1;

        let mut suppression = HashSet::new();

        // Add edges from the synthetic source supernode to all source nodes
        for source_id in source_nodes {
            let source_ix = nsgraph.ensure_node(source_id.clone());
            nsgraph.graph.add_edge(
                super_source_ix,
                source_ix,
                SymbolGraphEdgeId(synth_source_edge_id),
            );
        }

        // Add edges from all target nodes to the synthetic target supernode.
        for target_id in target_nodes {
            let target_ix = nsgraph.ensure_node(target_id.clone());
            nsgraph.graph.add_edge(
                target_ix,
                super_target_ix,
                SymbolGraphEdgeId(synth_target_edge_id),
            );
        }

        trace!(num_nodes=%super_target_id, num_edges=%synth_target_edge_id, "created supernodes, running petgraph all_simple_paths algorithm");

        // Now we get the paths...
        let paths = all_simple_paths::<Vec<_>, _>(
            &nsgraph.graph,
            super_source_ix,
            super_target_ix,
            0,
            Some(path_length_limit as usize),
        );

        for path in paths {
            for (src, tgt) in path
                .into_iter() // skip the source supernode
                .dropping(1)
                // skip the target supernode
                .dropping_back(1)
                .tuple_windows()
            {
                let source_ix = src.index() as u32;
                let target_ix = tgt.index() as u32;

                if suppression.insert((source_ix, target_ix)) {
                    let source_id =
                        SymbolGraphNodeId(*nsgraph.node_ix_to_id.get(&source_ix).unwrap());
                    let target_id =
                        SymbolGraphNodeId(*nsgraph.node_ix_to_id.get(&target_ix).unwrap());
                    let edge_ix = nsgraph.graph.find_edge(src, tgt).unwrap();
                    let edge_id = nsgraph.graph[edge_ix].clone();
                    self.propagate_edge(
                        edge_set,
                        &source_id,
                        &target_id,
                        &edge_id,
                        new_graph,
                        new_symbol_set,
                        new_edge_set,
                    );
                }
            }

            if new_symbol_set.symbol_crossref_infos.len() as u32 >= node_soft_limit {
                return;
            }
        }
    }

    /// Given a pair of symbols in the current set, ensure that they exist in
    /// the new node set and propagate the edge and its info in the new graph as well.
    #[allow(clippy::too_many_arguments)]
    pub fn propagate_edge(
        &self,
        edge_set: &SymbolGraphEdgeSet,
        source: &SymbolGraphNodeId,
        target: &SymbolGraphNodeId,
        edge_id: &SymbolGraphEdgeId,
        new_graph: &mut NamedSymbolGraph,
        new_symbol_set: &mut SymbolGraphNodeSet,
        new_edge_set: &mut SymbolGraphEdgeSet,
    ) {
        let new_source_node = self.propagate_sym(source, new_symbol_set);
        let new_target_node = self.propagate_sym(target, new_symbol_set);
        let old_edge_info = &edge_set.edge_infos[edge_id.0 as usize];
        let edge_index = new_edge_set.edge_infos.len();
        new_edge_set.edge_infos.push(EdgeInfo {
            from_id: new_source_node.clone(),
            to_id: new_target_node.clone(),
            kind: old_edge_info.kind.clone(),
            data: old_edge_info.data.clone(),
        });
        let new_edge_id = SymbolGraphEdgeId(edge_index as u32);
        new_graph.ensure_edge(new_source_node, new_target_node, new_edge_id);
    }

    fn propagate_sym(
        &self,
        node_id: &SymbolGraphNodeId,
        new_symbol_set: &mut SymbolGraphNodeSet,
    ) -> SymbolGraphNodeId {
        let info = self.get(node_id);
        match new_symbol_set.symbol_to_index_map.get(&info.symbol) {
            Some(index) => SymbolGraphNodeId(*index),
            None => new_symbol_set.add_symbol(info.clone()).0,
        }
    }

    /// Look-up a symbol returning its id (for graph purposes) and its
    /// DerivedSymbolInfo (for data inspection).
    pub fn lookup_symbol(&self, symbol: &Ustr) -> Option<(SymbolGraphNodeId, &DerivedSymbolInfo)> {
        if let Some(index) = self.symbol_to_index_map.get(symbol) {
            let sym_info = self.symbol_crossref_infos.get(*index as usize);
            sym_info.map(|info| (SymbolGraphNodeId(*index), info))
        } else {
            None
        }
    }

    /// Add a symbol and return the unwrapped data that lookup_symbol would have provided.
    pub fn add_symbol(
        &mut self,
        mut sym_info: DerivedSymbolInfo,
    ) -> (SymbolGraphNodeId, &mut DerivedSymbolInfo) {
        // Propagate any labels from the symbol's structured information into
        // badges set.
        // XXX this is a somewhat weird conflation of presentation logic with
        // more abstract logic.  Also, this on its own isn't entirely sufficient
        // because of how "fields" on a class are a special case.  (In
        // particular, as of writing this, we store information on the class's
        // structured "fields" that we don't store on the structured info for
        // each individual field, and that's weird.  This may or may not change,
        // since there is also some sense in field (meta)data being most
        // useful in the context of the class.)
        if let Some(Value::Array(labels_json)) = sym_info.crossref_info.pointer("/meta/labels") {
            for label in labels_json {
                if let Value::String(label) = label {
                    if let Some((pri, shorter_label)) = label_to_badge_info(label) {
                        sym_info.badges.push(SymbolBadge {
                            pri,
                            label: ustr(shorter_label),
                            source_jump: None,
                        });
                    }
                }
            }
        }
        // Insert the symbol and issue a node id.
        let index = self.symbol_crossref_infos.len();
        let symbol = sym_info.symbol;
        self.symbol_crossref_infos.push(sym_info);
        self.symbol_to_index_map.insert(symbol, index as u32);
        (
            SymbolGraphNodeId(index as u32),
            self.symbol_crossref_infos.get_mut(index).unwrap(),
        )
    }

    /// Check if a symbol is already known and return it if so, otherwise
    /// perform a crossref_lookup and add the symbol.  The caller should provide
    /// the depth that should be associated with the symbol if we need to
    /// perform the lookup; no change will be made to the existing depth if the
    /// symbol is already known.
    pub async fn ensure_symbol<'a>(
        &'a mut self,
        sym: &'a Ustr,
        server: &'a (dyn AbstractServer + Send + Sync),
        depth: u32,
    ) -> Result<(SymbolGraphNodeId, &'a mut DerivedSymbolInfo)> {
        if let Some(index) = self.symbol_to_index_map.get(sym) {
            let sym_info = self
                .symbol_crossref_infos
                .get_mut(*index as usize)
                .ok_or_else(make_data_invariant_err)?;
            return Ok((SymbolGraphNodeId(*index), sym_info));
        }

        let info = server.crossref_lookup(sym, false).await?;
        Ok(self.add_symbol(DerivedSymbolInfo::new(*sym, info, depth)))
    }

    /// Destructively return a sorted Object mapping from symbol identifiers to
    /// their jumpref info.  We sort the symbols for stability for testing
    /// purposes and for human readability reasons.  The destruction is that
    /// the DerivedSymbolInfo's have their `crossref_info` serde_json::Value
    /// instances take()n.
    ///
    /// This method is currently destructive because the
    /// convert_crossref_value_to_sym_info_rep currently is destructive and
    /// because it seems like nothing else currently needs that info.  But it
    /// should be fine to make this optionally non-destructive.
    ///
    /// Okay, now there's a nondestructive version below this that's less
    /// efficient.
    pub fn symbols_meta_to_jumpref_json_destructive(&mut self) -> Value {
        let mut jumprefs = BTreeMap::new();
        for sym_info in self.symbol_crossref_infos.iter_mut() {
            let info = sym_info.crossref_info.take();
            jumprefs.insert(
                sym_info.symbol,
                convert_crossref_value_to_sym_info_rep(info, &sym_info.symbol, None),
            );
        }

        json!(jumprefs)
    }

    /// Nondestructive, less efficient version of `symbols_meta_to_jumpref_json_destructive`.
    pub fn symbols_meta_to_jumpref_json_nomut(&self) -> Value {
        let mut jumprefs = BTreeMap::new();
        for sym_info in self.symbol_crossref_infos.iter() {
            // XXX This is inefficient!
            let info = sym_info.crossref_info.clone();
            jumprefs.insert(
                sym_info.symbol,
                convert_crossref_value_to_sym_info_rep(info, &sym_info.symbol, None),
            );
        }

        json!(jumprefs)
    }
}

impl Default for SymbolGraphEdgeSet {
    fn default() -> Self {
        Self::new()
    }
}

impl SymbolGraphEdgeSet {
    pub fn new() -> Self {
        Self {
            edge_infos: vec![],
            edge_lookup: HashMap::default(),
        }
    }

    pub fn get(&self, edge_id: &SymbolGraphEdgeId) -> &EdgeInfo {
        &self.edge_infos[edge_id.0 as usize]
    }

    pub fn get_mut(&mut self, edge_id: &SymbolGraphEdgeId) -> &mut EdgeInfo {
        &mut self.edge_infos[edge_id.0 as usize]
    }

    /// Merge the provided edge metadata for any existing edge between the
    /// provided symbol, creating the edge metadata if it does not already
    /// exist.  Then calls ensure_edge on the underlying graph using the
    /// underlying SymbolGraphEdgeId.
    ///
    /// Currently we don't return the SymbolGraphEdgeId but we could if callers
    /// needed it.
    pub fn ensure_edge_in_graph(
        &mut self,
        source: SymbolGraphNodeId,
        target: SymbolGraphNodeId,
        kind: EdgeKind,
        data: Vec<EdgeDetail>,
        graph: &mut NamedSymbolGraph,
    ) {
        let edge_id = if let Some(idx) = self.edge_lookup.get(&(source.0, target.0)) {
            let info = self.edge_infos.get_mut(*idx as usize).unwrap();
            for detail in data {
                if !info.data.iter().contains(&detail) {
                    info.data.push(detail);
                }
            }
            SymbolGraphEdgeId(*idx)
        } else {
            let index = self.edge_infos.len();
            self.edge_infos.push(EdgeInfo {
                from_id: source.clone(),
                to_id: target.clone(),
                kind,
                data,
            });
            self.edge_lookup.insert((source.0, target.0), index as u32);
            SymbolGraphEdgeId(index as u32)
        };
        graph.ensure_edge(source, target, edge_id);
    }
}

```

## tools/src/cmd_pipeline/cmd_cat_html.rs
```
use async_trait::async_trait;
use clap::Args;
use lol_html::{
    element, html_content::ContentType, rewrite_str, HtmlRewriter, RewriteStrSettings, Settings,
};
use std::{cell::Cell, rc::Rc};

use super::interface::{PipelineCommand, PipelineValues, TextFile};
use crate::abstract_server::{AbstractServer, HtmlFileRoot, Result};

/// Dump the contents of a HTML file for a (source) file or rendered directory
/// listing from disk in its entirety, applying minimal normalization to
/// compensate for datestamps or specific revision data.
///
/// Intended exclusively for regression testing and in particular to provide
/// coverage for directory listings and semi-generated files like "help.html"
/// and "settings.html" where we want to audit changes to the file in their
/// entirety but we don't want to have N copies of the searchfox HTML super
/// structure.  We would expect to use this command for at most a few source
/// files to validate where the source listing joins to the HTML super structure
/// but would expect to use "show-html" for most "checks" so that they can be
/// much more targeted (than having potentially massive diffs that are
/// constantly including the entirety of a generated page with many irrelevant
/// changes to the specific purpose of the check).
///
/// Differs from show-html which is about excerpting source lines and which has
/// a separate "prod-filter" helper for production "checks".
#[derive(Debug, Args)]
pub struct CatHtml {
    /// Tree-relative source file path or directory.
    #[clap(value_parser)]
    file: String,

    /// Is this a directory's HTML we want (instead of a source file)?
    #[clap(short, long, action)]
    dir: bool,

    /// Is this a template's HTML we want?
    #[clap(short, long, action)]
    template: bool,

    /// Use a CSS selector to limit the returned portion of the document.  This
    /// can be useful to focus a test and make diffs easier to understand.
    #[clap(short, long, value_parser)]
    select: Option<String>,
}

#[derive(Debug)]
pub struct CatHtmlCommand {
    pub args: CatHtml,
}

// HTML normalization of our expected entire HTML files:
// - "This page was generated by Searchfox DATETIME": We wrap the
//   datetime in a spam with class "pretty-date" and attribute with key
//   "data-datetime".  We currently normalize by replacing it with a span
//   `<span>NORMALIZED</span>` which loses the extra attributes but we don't
//   care about that level of fidelity.
fn norm_html_file(s: String) -> String {
    let element_content_handlers = vec![element!(r#"span.pretty-date"#, |el| {
        el.replace("<span>NORMALIZED</span>", ContentType::Html);
        Ok(())
    })];

    rewrite_str(
        &s,
        RewriteStrSettings {
            element_content_handlers,
            ..RewriteStrSettings::default()
        },
    )
    .unwrap()
}

fn extract_html_snippet(html_str: String, selector: &str) -> String {
    let mut excerpts = vec![];

    let suppressing = Rc::new(Cell::new(true));
    let sink_suppressing = suppressing.clone();

    let mut buf = vec![];

    let synthetic_closing = Rc::new(Cell::new(None));
    let sink_closing = synthetic_closing.clone();

    let mut rewrite = HtmlRewriter::new(
        Settings {
            element_content_handlers: vec![element!(selector, move |el| {
                suppressing.set(false);
                let end_suppress = suppressing.clone();
                let end_closing = synthetic_closing.clone();
                el.on_end_tag(move |end| {
                    end_closing.set(Some(format!("</{}>", end.name())));
                    end_suppress.set(true);
                    Ok(())
                })?;
                Ok(())
            })],
            ..Settings::default()
        },
        |c: &[u8]| {
            if sink_suppressing.get() {
                if let Some(closing) = sink_closing.take() {
                    buf.extend_from_slice(closing.as_bytes());
                }
                // Flush if this was apparently a transition from accumulating
                // into our buffer.
                if !buf.is_empty() {
                    excerpts.push(String::from_utf8_lossy(&buf).to_string());
                    buf.clear();
                }
                return;
            }

            buf.extend_from_slice(c);
        },
    );

    rewrite.write(html_str.as_bytes()).unwrap();
    rewrite.end().unwrap();

    excerpts.join("\n")
}

#[async_trait]
impl PipelineCommand for CatHtmlCommand {
    async fn execute(
        &self,
        server: &(dyn AbstractServer + Send + Sync),
        _input: PipelineValues,
    ) -> Result<PipelineValues> {
        let root = if self.args.dir {
            HtmlFileRoot::FormattedDir
        } else if self.args.template {
            HtmlFileRoot::FormattedTemplate
        } else {
            HtmlFileRoot::FormattedFile
        };
        let mut html_str = server.fetch_html(root, &self.args.file).await?;

        if let Some(selector) = &self.args.select {
            html_str = extract_html_snippet(html_str, selector);
        }

        Ok(PipelineValues::TextFile(TextFile {
            mime_type: "text/html".to_string(),
            contents: norm_html_file(html_str),
        }))
    }
}

```

## tools/src/cmd_pipeline/cmd_query.rs
```
use async_trait::async_trait;
use clap::Args;
use serde_json::to_value;

use super::{
    builder::build_pipeline_graph,
    interface::{JsonValue, PipelineCommand, PipelineValues},
};
use crate::{
    abstract_server::{AbstractServer, Result},
    query::chew_query::chew_query,
};

/// Run a new-style `query-parser` `term:value` query parse against the local
/// index.  Remote server is currently a no-op, but when supported the entire
/// query will be run on the server (because we want to test the server).
#[derive(Debug, Args)]
pub struct Query {
    /// Query string
    #[clap(value_parser)]
    query: String,

    /// Output the constructed pipeline instead of running the pipeline.
    #[clap(short, long, value_parser)]
    dump_pipeline: bool,
}

#[derive(Debug)]
pub struct QueryCommand {
    pub args: Query,
}

#[async_trait]
impl PipelineCommand for QueryCommand {
    async fn execute(
        &self,
        server: &(dyn AbstractServer + Send + Sync),
        _input: PipelineValues,
    ) -> Result<PipelineValues> {
        let pipeline_plan = chew_query(&self.args.query)?;

        if self.args.dump_pipeline {
            return Ok(PipelineValues::JsonValue(JsonValue {
                value: to_value(pipeline_plan)?,
            }));
        }

        let graph = build_pipeline_graph(server.clonify(), pipeline_plan)?;

        graph.run(true).await
    }
}

```

## tools/src/cmd_pipeline/cmd_compile_results.rs
```
use std::collections::{BTreeMap, HashSet};

use async_trait::async_trait;
use clap::Args;
use serde_json::{from_value, Value};
use ustr::{ustr, Ustr, UstrMap};

use super::interface::{
    FlattenedKindGroupResults, FlattenedLineSpan, FlattenedPathKindGroupResults,
    FlattenedResultsBundle, FlattenedResultsByFile, PipelineJunctionCommand, PipelineValues,
    PresentationKind, ResultFacetGroup, ResultFacetKind, ResultFacetRoot, SymbolCrossrefInfo,
    SymbolQuality, SymbolRelation,
};

use crate::{
    abstract_server::{
        AbstractServer, ErrorDetails, ErrorLayer, FileMatch, Result, ServerError, TextMatchesByFile,
    },
    file_format::analysis::PathSearchResult,
};

/// Process file, crossref, and fulltext search results into a classic
/// mozsearch mixed results representation consisting of groups of results
/// clustered by "path kind" (normal, test, generated, third party), and then
/// by key/kind precedence (files, IDL, defs, override stuff, super/subclass
/// stuff, assignments, uses, declarations, text matches), noting that
/// precedences will likely change.
#[derive(Debug, Args)]
pub struct CompileResults {
    /// Maximum number of file results to list, truncating at the limit.
    #[clap(short, long, value_parser, default_value = "2000")]
    file_limit: usize,

    /// Maximum number of result lines to limit, truncating at the limit.
    /// Context lines don't impact this limit.
    #[clap(short, long, value_parser, default_value = "2000")]
    line_limit: usize,
}

/// Core result processing logic / helper data-structures most analogous to the
/// router.py `SearchResult` class but with the inputs and outputs retaining a
/// bit more semantic linkage the whole way.
///
/// ### Python SearchResult Relation
///
/// We retain this extra information in order to enable:
/// - Interactive faceting of results related to override sets.  In particular,
///   the ability to rapidly toggle on/off cousin overrides that may not be of
///   interest.
/// - Automatic collapsing of sections that may be useful to have present for
///   completeness but which we believe are likely to not be something the user
///   wants to see.
/// - Better indication of when and where overload situations were hit and so
///   we can generate links that will show the user what was elided by either
///   expanding limits and/or showing the specific elided subset.
///
/// In particular, the python SearchResult has a concept of "qualified" results
/// which is a means of retaining the binding between symbols and the (pretty)
/// identifier that mapped to the symbol in identifier lookup.  SearchResult
/// then uses that information to tuple the "kind" over the pretty identifier.
/// (It's also the case that, historically, pre-structured-analysis landing,
/// things like overrides would destructively be aliased to the same pretty
/// identifier.  We no longer to this; see `CrossrefExpandCommand` for more.)
///
/// For our rust `SearchResults`, we inherently know the "pretty" identifier
/// associated with a symbol from its "meta" `crossref_info` contents.  We also
/// track how we learned about the symbol from a root symbol via its
/// `SymbolRelation`.  Note that for presentation purposes we will continue to
/// do all grouping based on the "pretty" identifier for now, although some day
/// we probably will need to address the existence of overloads better, but
/// right now overload coalescing is an important feature for cross-platform
/// merging where we expect, for example, 32-bit ARM to have different
/// signatures for things, etc.
///
/// ### Overview
///
/// Conceptually we build a [path kind, (kind, identifier), path] ordered
/// hierarchy where the values are line hits and where we flatten the hierarchy
/// by walking it in order.  In router.py, the "qkind" which tupled the kind and
/// identifier relied on an OrderedDict and the sequence in which symbols were
/// added.  The path kind had an explicit order and extraction was done in that
/// order.  paths were sorted and extracted in that order.
#[derive(Default)]
pub struct SearchResults {
    /// Cache mapping observed symbols to their pretty identifiers.  This
    /// depends on us seeing root symbols of relations before their related
    /// symbols, but that's explicitly how things are ordered.
    pub sym_to_pretty: UstrMap<Ustr>,
    /// We retain the meta information for any symbols we include our results
    /// for the benefit of the UI for future use.  We may also end up expanding
    /// the set of symbols-with-meta here as we address the class hierarchy, if
    /// that doesn't end up in a separate output structure.
    pub sym_to_meta: UstrMap<Value>,
    pub path_kind_groups: UstrMap<PathKindGroup>,
    /// Every key_line gets added to this set like `{path}:{key_line}` to
    /// suppress redundant hits on the line (from fulltext matches).
    pub path_line_suppressions: HashSet<String>,
}

#[derive(Default)]
pub struct PathKindGroup {
    pub file_names: Vec<Ustr>,
    pub qual_kind_groups: BTreeMap<QualKindDescriptor, QualKindGroup>,
}

/// Results for a specific kind (definition/use/etc.) for a specific pretty
/// identifier and potentially a set of
#[derive(Clone, PartialEq, PartialOrd, Eq, Ord)]
pub struct QualKindDescriptor {
    pub kind: PresentationKind,
    pub quality: SymbolQuality,
    pub pretty: Ustr,
}

pub struct QualKindGroup {
    pub path_facet: MaybeFacetRoot,
    pub relation_facet: MaybeFacetRoot,
    pub path_hits: BTreeMap<Ustr, FlattenedResultsByFile>,
}

impl QualKindGroup {
    pub fn new() -> Self {
        QualKindGroup {
            path_facet: MaybeFacetRoot::new(ResultFacetKind::PathByPath),
            relation_facet: MaybeFacetRoot::new(ResultFacetKind::SymbolByRelation),
            path_hits: BTreeMap::new(),
        }
    }
}

impl SearchResults {
    /// For each symbol we:
    /// - Figure out what identifier this symbol should be filed under based on
    ///   the `SymbolRelation`, and what "kinds" are applicable for line
    ///   result inclusion.  This helps us determine the `QualKind` to use.
    ///   - Some kinds of relations, like subclass/superclass relationships are
    ///     not intended to have any of their crossref "kinds" used for line
    ///     results, but instead for context which is still a TODO and maybe
    ///     be handled by a different command or a sidecar data structure as
    ///     part of this command.
    /// - Figure out the quality of this identifier based on the `SymbolQuality`
    ///   (which should be the same for all members of the same QualKind).
    /// - Proces the relevant kinds for each symbol, processing each path and
    ///   its associated hits.  Different paths can/will map to different
    ///   pathkinds and this will result in different faceting sets, etc. so the
    ///   processing here
    ///
    pub fn ingest_symbol(&mut self, info: SymbolCrossrefInfo) -> Result<()> {
        lazy_static! {
            static ref SELF: Ustr = ustr("Self");
            static ref OVERRIDDEN_BY: Ustr = ustr("Overriden By");
            static ref OVERRIDES: Ustr = ustr("Overrides");
            static ref COUSIN_OVERRIDES: Ustr = ustr("Cousin Overrides");
        }

        // There are other ways we could get this mapping like always baking the
        // "pretty" into the SymbolRelation or having our crossref infos be in a
        // map, but that complicates ownership issues massively.
        self.sym_to_pretty.insert(info.symbol, info.get_pretty());

        // Skip symbols that are only here for class relationship purposes.
        let (root_sym, relation_facet): (Ustr, &'static Ustr) = match &info.relation {
            SymbolRelation::SubclassOf(_, _)
            | SymbolRelation::SuperclassOf(_, _)
            | SymbolRelation::CousinClassOf(_, _) => {
                return Ok(());
            }
            SymbolRelation::Queried => (info.symbol, &SELF),
            SymbolRelation::OverrideOf(sym, _) => (*sym, &OVERRIDDEN_BY),
            SymbolRelation::OverriddenBy(sym, _) => (*sym, &OVERRIDES),
            SymbolRelation::CousinOverrideOf(sym, _) => (*sym, &COUSIN_OVERRIDES),
        };

        let root_pretty = *self.sym_to_pretty.get(&root_sym).ok_or_else(|| {
            ServerError::StickyProblem(ErrorDetails {
                layer: ErrorLayer::RuntimeInvariantViolation,
                message: format!("no pretty available for root_sym {}", root_sym),
            })
        })?;

        if let Value::Object(obj) = info.crossref_info {
            // This generic traversal is currently somewhat required because we
            // have different shapes for "meta", "callees", and everything else
            // (the kinds).  It could make sense to normalize the schema by not
            // having everything at the top-level.  (If doing that, it might
            // also be worth re-thinking other aspects of storage if it lets us
            // be lazier about parsing, etc.)
            for (kind, val) in obj.into_iter() {
                let pkind = match kind.as_str() {
                    "idl" => PresentationKind::IDL,
                    "defs" => PresentationKind::Definitions,
                    "decls" => PresentationKind::Declarations,
                    "assignments" => PresentationKind::Assignments,
                    "uses" => PresentationKind::Uses,
                    "meta" => {
                        // We save off the meta for this symbol for the UI.
                        self.sym_to_meta.insert(info.symbol, val);
                        continue;
                    }
                    // We expect this to match:
                    // - "callees": This is used only for call-graph stuff and is
                    //   something a human can learn from just looking at the
                    //   contents of a given method/symbol, etc.
                    _ => {
                        continue;
                    }
                };

                let descriptor = QualKindDescriptor {
                    kind: pkind,
                    quality: info.quality.clone(),
                    pretty: root_pretty,
                };

                let path_containers: Vec<PathSearchResult> = from_value(val)?;
                for path_container in path_containers {
                    self.ingest_path_hits(
                        &info.symbol,
                        descriptor.clone(),
                        relation_facet,
                        path_container,
                    );
                }
            }
        }

        Ok(())
    }

    fn ingest_path_hits(
        &mut self,
        sym: &Ustr,
        descriptor: QualKindDescriptor,
        relation_facet: &Ustr,
        path_container: PathSearchResult,
    ) {
        let path_kind_group = self
            .path_kind_groups
            .entry(path_container.path_kind)
            .or_default();
        let qual_kind_group = path_kind_group
            .qual_kind_groups
            .entry(descriptor)
            .or_insert_with(QualKindGroup::new);

        // ### path faceting
        let path_sans_filename = match path_container.path.rfind('/') {
            Some(offset) => ustr(&path_container.path[0..offset + 1]),
            None => ustr(""),
        };
        let mut path_pieces: Vec<Ustr> =
            path_sans_filename.split_inclusive('/').map(ustr).collect();
        // drop the filename portion.
        path_pieces.truncate(path_pieces.len() - 1);
        qual_kind_group
            .path_facet
            .place_item(path_pieces, path_sans_filename);

        // ### symbol relation faceting
        qual_kind_group
            .relation_facet
            .place_item(vec![*relation_facet], *sym);

        // ### line results
        let file_results = qual_kind_group
            .path_hits
            .entry(path_container.path)
            .or_insert_with(|| FlattenedResultsByFile {
                file: path_container.path,
                line_spans: vec![],
            });
        for search_result in path_container.lines {
            // Path-line suppressions exist to avoid redundant fulltext matches
            // showing up, so we don't actually care if there already was
            // another instance of this line already.
            //
            // At least, probably; obviously if it turns out we are ending up
            // with a ton of semantic results on the same line, maybe we need to
            // change the heuristic here to be a Map that suppresses redundant
            // lines for the same symbol on the same line, having the map store
            // the most recently used symbol.  (So we would have a limited
            // memory instead of a growing set.)  Practically speaking, we'd
            // expect this to really only happen in cases like implicit
            // constructors or macros where a ton of actual under-the-hood code
            // gets mapped down to a single token, but in that case we already
            // should have merged all of those redundant same-symbols.
            self.path_line_suppressions
                .insert(format!("{}:{}", path_container.path, search_result.lineno));
            file_results.line_spans.push(FlattenedLineSpan {
                key_line: search_result.lineno,
                line_range: if search_result.peek_range.is_empty() {
                    (search_result.lineno, search_result.lineno)
                } else {
                    (
                        search_result.peek_range.start_lineno,
                        search_result.peek_range.end_lineno,
                    )
                },
                contents: search_result.line,
                context: search_result.context,
                contextsym: search_result.contextsym,
            });
        }
    }

    pub fn ingest_file_match_hits(&mut self, file_matches: Vec<FileMatch>) {
        for file_match in file_matches {
            let path_kind_group = self
                .path_kind_groups
                .entry(file_match.concise.path_kind)
                .or_default();
            path_kind_group.file_names.push(file_match.path);
        }
    }

    pub fn ingest_fulltext_hits(&mut self, matches_by_file: Vec<TextMatchesByFile>) {
        let descriptor = QualKindDescriptor {
            kind: PresentationKind::TextualOccurrences,
            // The quality doesn't matter; there's only one class of text matches.
            quality: SymbolQuality::ExplicitSymbol,
            pretty: ustr(""),
        };

        for file_match in matches_by_file {
            let path = file_match.file;
            let path_kind_group = self
                .path_kind_groups
                .entry(file_match.path_kind)
                .or_default();
            let qual_kind_group = path_kind_group
                .qual_kind_groups
                .entry(descriptor.clone())
                .or_insert_with(QualKindGroup::new);

            // ### line results
            let file_results =
                qual_kind_group
                    .path_hits
                    .entry(path)
                    .or_insert_with(|| FlattenedResultsByFile {
                        file: path,
                        line_spans: vec![],
                    });
            for text_match in file_match.matches {
                if self
                    .path_line_suppressions
                    .insert(format!("{}:{}", path, text_match.line_num))
                {
                    file_results.line_spans.push(FlattenedLineSpan {
                        key_line: text_match.line_num,
                        line_range: (text_match.line_num, text_match.line_num),
                        contents: text_match.line_str,
                        context: ustr(""),
                        contextsym: ustr(""),
                    });
                }
            }
            // The suppressions could mean we don't actually need this path hit,
            // in which case we need to remove the file results.
            //
            // TODO: We should potentially back out the qual_kind_group and
            // path_kind_group here or have the `compile` step notice that the
            // path_hits is empty and so on.
            if file_results.line_spans.is_empty() {
                qual_kind_group.path_hits.remove(&path);
            } else {
                // ### path faceting (now that we know we're keeping the hits)
                let path_sans_filename = match path.rfind('/') {
                    Some(offset) => ustr(&path[0..offset + 1]),
                    None => ustr(""),
                };
                let mut path_pieces: Vec<Ustr> =
                    path_sans_filename.split_inclusive('/').map(ustr).collect();
                // drop the filename portion.
                path_pieces.truncate(path_pieces.len() - 1);
                qual_kind_group
                    .path_facet
                    .place_item(path_pieces, path_sans_filename);
            }
        }
    }

    pub fn compile(self, _file_limit: usize, _line_limit: usize) -> FlattenedResultsBundle {
        let mut path_kind_results = vec![];
        for (path_kind, pk_group) in self.path_kind_groups {
            let mut kind_groups = vec![];
            for (descriptor, qk_group) in pk_group.qual_kind_groups {
                let mut facets = vec![];

                if let Some(facet) = qk_group.relation_facet.compile() {
                    facets.push(facet);
                }
                if let Some(facet) = qk_group.path_facet.compile() {
                    facets.push(facet);
                }

                let mut by_file: Vec<FlattenedResultsByFile> =
                    qk_group.path_hits.into_values().collect();
                // The path_hits within each file are not guaranteed to be sorted,
                // so we sort them now.
                for results in by_file.iter_mut() {
                    results.line_spans.sort_by_key(|x| x.line_range);
                }

                kind_groups.push(FlattenedKindGroupResults {
                    kind: descriptor.kind,
                    pretty: descriptor.pretty,
                    facets,
                    by_file,
                });
            }

            path_kind_results.push(FlattenedPathKindGroupResults {
                path_kind,
                file_names: pk_group.file_names,
                kind_groups,
            });
        }

        FlattenedResultsBundle {
            path_kind_results,
            content_type: "text/plain".to_string(),
        }
    }
}

/// Faceting support logic; the ResultFacetKind bakes in rules.
pub struct MaybeFacetRoot {
    pub kind: ResultFacetKind,
    pub root: MaybeFacetGroup,
}

impl MaybeFacetRoot {
    pub fn new(kind: ResultFacetKind) -> MaybeFacetRoot {
        MaybeFacetRoot {
            kind,
            root: MaybeFacetGroup::default(),
        }
    }

    /// Place the value within a fully built-out hierarchy.  We don't do dynamic
    /// hierarchy creation as things collide; instead we just create it all and
    /// then collapse it out of existence during the `compile` phase.
    pub fn place_item(&mut self, mut pieces: Vec<Ustr>, value: Ustr) {
        pieces.reverse();
        self.root.place_item(pieces, value);
    }

    /// Determine whether there's enough variety that faceting is appropriate,
    /// and if so, return a fully populated `ResultFacetRoot` according to the
    /// rules for this root's `ResultFacetKind`.
    ///
    /// The general algorithm here is:
    /// - Determine if each `MaybeFacetGroup` is "sole" (just one group),
    ///   "clumped" (has multiple sub-groups that meet the clump threshold),
    ///   "clump-able" (has one sub-group that meets the clump threshold and the
    ///   other groups can be clumped into an "Other" catch-all, if allowed)
    ///   or "sparse" (has sub-groups that don't meet the clump threshold).
    /// - A "sole" group that has a "sole" child gets merged with the child.
    ///   This happens for path-based faceting where multiple directory segments
    ///   may be shared in common with no deviation.
    /// -
    pub fn compile(self) -> Option<ResultFacetRoot> {
        if self.root.count == 0 {
            return None;
        }

        let (label, clump_thresh, other) = match self.kind {
            ResultFacetKind::SymbolByRelation => ("Relation".to_string(), 0, None),
            ResultFacetKind::PathByPath => ("Path".to_string(), 3, Some("*".to_string())),
        };
        let (compiled, breadth) = self.root.compile("".to_string(), clump_thresh, other);
        if breadth > 1 {
            Some(ResultFacetRoot {
                label,
                kind: self.kind,
                groups: compiled.nested_groups,
            })
        } else {
            None
        }
    }
}

#[derive(Default)]
pub struct MaybeFacetGroup {
    pub nested_groups: BTreeMap<Ustr, MaybeFacetGroup>,
    pub values: Vec<Ustr>,
    /// Count of the values stored in this group in `values` and any nested
    /// groups.  This value will always be at least 1.
    pub count: u32,
}

impl MaybeFacetGroup {
    pub fn place_item(&mut self, mut reversed_pieces: Vec<Ustr>, value: Ustr) {
        self.count += 1;
        if let Some(next_piece) = reversed_pieces.pop() {
            self.nested_groups
                .entry(next_piece)
                .or_default()
                .place_item(reversed_pieces, value);
        } else {
            self.values.push(value);
        }
    }

    pub fn flatten(mut self) -> Vec<Ustr> {
        for subgroup in self.nested_groups.into_values() {
            let mut sub_flattened = subgroup.flatten();
            self.values.append(&mut sub_flattened);
        }

        self.values
    }

    /// Compiles the current group, returning the compiled result and the
    /// maximum number of nested groups known in the returned sub-tree which
    /// we're going to call the breadth.
    pub fn compile(
        mut self,
        prefix: String,
        clump_thresh: u32,
        other: Option<String>,
    ) -> (ResultFacetGroup, u32) {
        if self.nested_groups.is_empty() {
            // No sub-groups means we are a leaf node and should return as-is.
            return (
                ResultFacetGroup {
                    label: prefix,
                    values: self.values,
                    nested_groups: vec![],
                    count: self.count,
                },
                1,
            );
        } else if self.nested_groups.len() == 1 {
            let (sole_name, sole_group) = self.nested_groups.into_iter().next().unwrap();
            let (mut sole_compiled, breadth) = sole_group.compile(
                prefix.clone() + sole_name.as_str(),
                clump_thresh,
                other.clone(),
            );

            if self.values.is_empty() {
                // Collapse us into the nested group
                return (sole_compiled, breadth);
            }

            // We have values of our own and so we either want to fold the nested
            // group's contents into our own or retain our group and it as a
            // nested group.
            if breadth > 1 {
                // There's a tree somewhere down there, so just nest.
                return (
                    ResultFacetGroup {
                        label: prefix,
                        values: self.values,
                        nested_groups: vec![sole_compiled],
                        count: self.count,
                    },
                    breadth,
                );
            } else {
                // There's no tree below us, so fold its contents into us.  Note
                // that inductively according to this heuristic, we know the
                // sole_compiled will have no nested_groups and instead only
                // values.
                self.values.append(&mut sole_compiled.values);
                return (
                    ResultFacetGroup {
                        label: prefix,
                        values: self.values,
                        nested_groups: vec![],
                        count: self.count,
                    },
                    1,
                );
            }
        }

        // So there must be multiple nested_groups; the question is now how many
        // meet our clump criteria.
        let mut clump_hit_count: u32 = 0;
        let mut clump_miss_count: u32 = 0;

        for group in self.nested_groups.values() {
            if group.count >= clump_thresh {
                clump_hit_count += 1;
            } else {
                clump_miss_count += 1;
            }
        }

        if clump_hit_count >= 2 || (clump_hit_count >= 1 && other.is_some()) {
            let mut nested_groups = vec![];
            let mut breadth: u32;

            // Yes, we're going to materialize this group and some sub-groups.
            if other.is_none() || clump_miss_count == 0 {
                breadth = self.nested_groups.len() as u32;
                // We don't need to worry about building up an "other" group.
                for (name, group) in self.nested_groups {
                    let (sub_compiled, sub_breadth) =
                        group.compile(prefix.clone() + name.as_str(), clump_thresh, other.clone());
                    nested_groups.push(sub_compiled);
                    if sub_breadth > breadth {
                        breadth = sub_breadth;
                    }
                }
            } else {
                let mut other_group = ResultFacetGroup {
                    label: prefix.clone() + other.as_ref().unwrap().as_str(),
                    values: vec![],
                    nested_groups: vec![],
                    count: 0,
                };
                breadth = clump_hit_count + 1;

                for (name, group) in self.nested_groups {
                    if group.count >= clump_thresh {
                        let (sub_compiled, sub_breadth) = group.compile(
                            prefix.clone() + name.as_str(),
                            clump_thresh,
                            other.clone(),
                        );
                        nested_groups.push(sub_compiled);
                        if sub_breadth > breadth {
                            breadth = sub_breadth;
                        }
                    } else {
                        let mut sub_flattened = group.flatten();
                        other_group.values.append(&mut sub_flattened);
                    }
                }
                nested_groups.push(other_group);
            }

            (
                ResultFacetGroup {
                    label: prefix,
                    values: self.values,
                    nested_groups,
                    count: self.count,
                },
                breadth,
            )
        } else {
            // We're not going to materialize this group; just fold everything
            // in to ourselves.
            for subgroup in self.nested_groups.into_values() {
                let mut sub_flattened = subgroup.flatten();
                self.values.append(&mut sub_flattened);
            }

            (
                ResultFacetGroup {
                    label: prefix,
                    values: self.values,
                    nested_groups: vec![],
                    count: self.count,
                },
                1,
            )
        }
    }
}

#[derive(Debug)]
pub struct CompileResultsCommand {
    pub args: CompileResults,
}

#[async_trait]
impl PipelineJunctionCommand for CompileResultsCommand {
    async fn execute(
        &self,
        _server: &(dyn AbstractServer + Send + Sync),
        input: Vec<(String, PipelineValues)>,
    ) -> Result<PipelineValues> {
        let mut results = SearchResults::default();

        // We currently don't care about the name of the input because we only
        // match by type, but one could imagine a scenario in which they serve
        // as labels we want to propagate.
        for (_, pipe_value) in input {
            match pipe_value {
                PipelineValues::FileMatches(fm) => {
                    results.ingest_file_match_hits(fm.file_matches);
                }
                PipelineValues::SymbolCrossrefInfoList(scil) => {
                    for info in scil.symbol_crossref_infos {
                        results.ingest_symbol(info)?;
                    }
                }
                PipelineValues::TextMatches(tm) => {
                    results.ingest_fulltext_hits(tm.by_file);
                }
                _ => {
                    return Err(ServerError::StickyProblem(ErrorDetails {
                        layer: ErrorLayer::ConfigLayer,
                        message: "compile-results got something weird".to_string(),
                    }));
                }
            }
        }

        let results_bundle = results.compile(self.args.file_limit, self.args.line_limit);

        Ok(PipelineValues::FlattenedResultsBundle(results_bundle))
    }
}

```

## tools/src/cmd_pipeline/cmd_jumpref_lookup.rs
```
use async_trait::async_trait;
use clap::Args;

use super::interface::{JsonValue, JsonValueList, PipelineCommand, PipelineValues};

use crate::abstract_server::{AbstractServer, ErrorDetails, ErrorLayer, Result, ServerError};

/// Return the jumpref data for one or more symbols received via pipeline or as
/// explicit arguments and provide it as JSON, specifically as a JsonValueList
/// containing JsonValue instances.  If we end up wanting to do additional
/// processing on jumpref data (like showing the html that it references), the
/// output should probably be given its own type.
#[derive(Debug, Args)]
pub struct JumprefLookup {
    /// Explicit symbols to lookup.
    #[clap(value_parser)]
    symbols: Vec<String>,
}

#[derive(Debug)]
pub struct JumprefLookupCommand {
    pub args: JumprefLookup,
}

#[async_trait]
impl PipelineCommand for JumprefLookupCommand {
    async fn execute(
        &self,
        server: &(dyn AbstractServer + Send + Sync),
        input: PipelineValues,
    ) -> Result<PipelineValues> {
        // Because this pipeline stage can receive symbols from unfiltered user
        // input and we have no reason to believe the `Ustr` interned symbol
        // table contains all potentially known strings, we must operate in
        // String space until we get values back from the crossref lookup!
        let symbol_list: Vec<String> = match input {
            PipelineValues::SymbolList(sl) => sl
                .symbols
                .into_iter()
                .map(|info| info.symbol.to_string())
                .collect(),
            // Right now we're assuming that we're the first command in the
            // pipeline so that we would have no inputs if someone wants to use
            // arguments...
            PipelineValues::Void => self.args.symbols.to_vec(),
            _ => {
                return Err(ServerError::StickyProblem(ErrorDetails {
                    layer: ErrorLayer::ConfigLayer,
                    message: "jumpref-lookup needs a Void or SymbolList".to_string(),
                }));
            }
        };

        let mut jumpref_values = vec![];
        for symbol in symbol_list {
            let info = server.jumpref_lookup(&symbol).await?;
            jumpref_values.push(JsonValue { value: info });
        }

        Ok(PipelineValues::JsonValueList(JsonValueList {
            values: jumpref_values,
        }))
    }
}

```

## tools/src/cmd_pipeline/cmd_tokenize_source.rs
```
use async_trait::async_trait;
use clap::{Args, ValueEnum};

use super::interface::{JsonValue, JsonValueList, PipelineCommand, PipelineValues, TextFile};
use crate::{
    abstract_server::{AbstractServer, ErrorDetails, ErrorLayer, Result, ServerError},
    tree_sitter_support::cst_tokenizer::hypertokenize_source_file,
};

#[derive(Clone, Debug, PartialEq, ValueEnum)]
pub enum TokenizeMode {
    Tokenize,
    Outline,
}

/// Tokenize the given source file using the syntax-token-tree tokenizer.  We
/// do also have the HTML formatting hand-rolled tokenizers, but they aren't
/// hooked up right now.
#[derive(Debug, Args)]
pub struct TokenizeSource {
    /// Tree-relative source file path or directory.
    #[clap(value_parser)]
    file: String,

    #[clap(long, value_enum, default_value_t=TokenizeMode::Tokenize)]
    mode: TokenizeMode,
}

#[derive(Debug)]
pub struct TokenizeSourceCommand {
    pub args: TokenizeSource,
}

#[async_trait]
impl PipelineCommand for TokenizeSourceCommand {
    async fn execute(
        &self,
        server: &(dyn AbstractServer + Send + Sync),
        _input: PipelineValues,
    ) -> Result<PipelineValues> {
        let source_str = server.fetch_raw_source(&self.args.file).await?;

        let token_lines = match hypertokenize_source_file(&self.args.file, &source_str) {
            Ok(content) => content,
            Err(e) => {
                return Err(ServerError::StickyProblem(ErrorDetails {
                    layer: ErrorLayer::DataLayer,
                    message: e,
                }));
            }
        };

        match self.args.mode {
            TokenizeMode::Tokenize => Ok(PipelineValues::TextFile(TextFile {
                mime_type: "text/plain".to_string(),
                contents: token_lines.tokenized.join("\n"),
            })),
            TokenizeMode::Outline => Ok(PipelineValues::JsonValueList(JsonValueList {
                values: token_lines
                    .structure
                    .iter()
                    .map(|fsr| JsonValue {
                        value: serde_json::to_value(fsr).unwrap(),
                    })
                    .collect(),
            })),
        }
    }
}

```

## tools/src/cmd_pipeline/cmd_traverse.rs
```
use std::collections::{HashSet, VecDeque};

use async_trait::async_trait;
use bitflags::bitflags;
use clap::Args;
use serde_json::{from_value, Value};
use tracing::trace;
use ustr::{ustr, Ustr};

use super::{
    interface::{OverloadInfo, OverloadKind, PipelineCommand, PipelineValues, SymbolMetaFlags},
    symbol_graph::{
        DerivedSymbolInfo, NamedSymbolGraph, SymbolBadge, SymbolGraphCollection,
        SymbolGraphEdgeSet, SymbolGraphNodeSet,
    },
};

use crate::{
    abstract_server::{AbstractServer, ErrorDetails, ErrorLayer, Result, ServerError},
    cmd_pipeline::symbol_graph::{EdgeDetail, EdgeKind},
    file_format::{
        analysis::{
            BindingOwnerLang, BindingSlotKind, OntologySlotInfo, OntologySlotKind,
            StructuredBindingSlotInfo, StructuredFieldInfo,
        },
        ontology_mapping::{label_to_badge_info, pointer_kind_to_badge_info},
    },
};

/// Processes piped-in crossref symbol data, recursively traversing the given
/// edges, building up a graph that also holds onto the crossref data for all
/// traversed symbols.
#[derive(Debug, Args)]
pub struct Traverse {
    /// The edge to traverse, currently one of: "uses", "callees", "class",
    /// "inheritance".
    #[clap(long, short, value_parser, default_value = "callees")]
    edge: String,

    /// Maximum traversal depth.  This used to have no limit because traversal
    /// will also be constrained by the applicable node-limit and our breadth
    /// first processing, but depth is now also used to limit the paths-between
    /// maximum path length, so we need to cap it to something that avoids worst
    /// case scenarios.  Honestly, 16 might be too high but should only result
    /// in pathological runtime rather than pathological memory usage.
    ///
    /// The default depth is set to 0 so it can vary if paths-between is enabled
    /// although we use a default depth of 8 for both right now, but it might
    /// make sense to crank paths-between back up to 10.
    #[clap(long, short, value_parser = clap::value_parser!(u32).range(0..=16), default_value = "0")]
    max_depth: u32,

    /// When enabled, the traversal will be performed with the higher
    /// paths-between-node-limit in effect, then the roots of the initial
    /// traversal will be used as pair-wise inputs to the all_simple_paths
    /// petgraph algorithm to derive a new graph which will be constrained to
    /// the normal "node-limit".
    #[clap(long, value_parser)]
    paths_between: bool,

    /// If specified, we will not drop symbol data beyond what is required for
    /// jumpref processing once a symbol has been processed.
    #[clap(long, value_parser)]
    retain_all_symbol_data: bool,

    /// Maximum number of nodes in a resulting graph.  When paths are involved,
    /// we may opt to add the entirety of the path that puts the graph over the
    /// node limit rather than omitting it.
    #[clap(long, value_parser = clap::value_parser!(u32).range(16..=1024), default_value = "384")]
    pub node_limit: u32,
    /// Maximum number of nodes in a graph being built to be processed by
    /// paths-between.
    #[clap(long, value_parser = clap::value_parser!(u32).range(16..=16384), default_value = "8192")]
    pub paths_between_node_limit: u32,
    /// Maximum number of paths to consider for paths-between.  This value has
    /// not been tested but is based on figuring 16 roots is a reasonable number
    /// of roots, and then we could allow growing that by 4.  No clue how this
    /// relates to actual performance.
    #[clap(long, value_parser = clap::value_parser!(u32).range(16..=1024), default_value = "256")]
    pub paths_limit: u32,

    /// If we see "uses" with this many paths with hits, do not process any of
    /// the uses.  This is path-centric because uses are hierarchically
    /// clustered by path right now.
    ///
    /// TODO: Probably have the meta capture the total number of uses so we can
    /// just perform a look-up without this hack.  But this hack works for
    /// experimenting.
    ///
    /// Note: we use a default of 0 because we differentiate the default based on
    /// whether paths-between is in use, but will use any value specified here.
    /// We don't impose a limit because our other node limits apply sufficiently well.
    #[clap(long, value_parser, default_value = "0")]
    pub skip_uses_at_path_count: u32,

    /// Traverse field member uses when the depth has this value or less.
    #[clap(long, value_parser = clap::value_parser!(i32).range(-1..=16), default_value = "-1")]
    pub traverse_field_member_uses: i32,

    /// If we see "field-member-uses" with this many hits, do not process any
    /// of the uses.
    #[clap(long, value_parser, default_value = "24")]
    pub skip_field_member_uses_at_count: u32,
}

#[derive(Debug)]
pub struct TraverseCommand {
    pub args: Traverse,
}

bitflags! {
    #[derive(Clone, Copy)]
    pub struct Traversals: u32 {
        const Super    = 0b00000001;
        const Subclass = 0b00000010;
    }
}

/// ### Theory of Operation
///
/// The crossref database can be thought of as a massive graph.  Each entry in
/// the crossref database is a symbol and also a node.  The crossref entry
/// contains references to other symbol nodes (particularly via the "meta"
/// structured information) as well as code location nodes which also provide
/// symbol nodes by way of their "contextsym".  (In the future we will likely
/// also infer additional graph relationships by looking at function call
/// arguments.)  There are other systems (ex: Kythe) which explicitly
/// represent their data in a graph database/triple-store, but a fundamental
/// searchfox design decision is to use a de-normalized representation and this
/// seems to be holding up for both performance and human comprehension
/// purposes.
///
/// This command is focused on efficiently deriving an interesting, useful, and
/// comprehensible sub-graph of that massive graph.  Although the current state
/// of implementation operates by starting from a set of nodes and enumerating
/// and considering graph edges dynamically, we could imagine that in the future
/// we might use some combination of pre-computation which could involve bulk /
/// batch processing.
///
/// ### Specific traversals
///
/// We potentially traverse all of the following crossref paths:
/// - "calls"
/// - "meta/fields":
/// - "meta/ontologySlots":
/// - "meta/overrides":
/// - "meta/overriddenBy":
/// - "meta/slotOwner":
/// - "uses":
///
#[async_trait]
impl PipelineCommand for TraverseCommand {
    #[allow(clippy::match_like_matches_macro)]
    async fn execute(
        &self,
        server: &(dyn AbstractServer + Send + Sync),
        input: PipelineValues,
    ) -> Result<PipelineValues> {
        let max_depth = match (self.args.max_depth, self.args.paths_between) {
            (0, false) => 8,
            (0, true) => 8,
            (x, _) => x,
        };
        let cil = match input {
            PipelineValues::SymbolCrossrefInfoList(cil) => cil,
            _ => {
                return Err(ServerError::StickyProblem(ErrorDetails {
                    layer: ErrorLayer::ConfigLayer,
                    message: "traverse needs a CrossrefInfoList".to_string(),
                }));
            }
        };

        let mut sym_node_set = SymbolGraphNodeSet::new();
        let mut sym_edge_set = SymbolGraphEdgeSet::new();
        let mut graph = NamedSymbolGraph::new("only".to_string());

        // A to-do list of nodes we have not yet traversed.
        let mut to_traverse = VecDeque::new();
        // Nodes that have been scheduled to be traversed or ruled out.  A node
        // in this set should not be added to `to_traverse`.
        let mut considered = HashSet::new();
        // Root set for paths-between use.
        let mut source_set = vec![];
        let mut target_set = vec![];

        let mut overloads_hit = vec![];

        let all_traversals_valid = Traversals::Super | Traversals::Subclass;

        // Propagate the starting symbols into the graph and queue them up for
        // traversal.
        for info in cil.symbol_crossref_infos {
            let is_source = info.flags.is_empty() || info.flags.contains(SymbolMetaFlags::Source);
            let is_target = info.flags.is_empty() || info.flags.contains(SymbolMetaFlags::Target);

            if is_target {
                to_traverse.push_back((info.symbol, 0, all_traversals_valid));
            }
            considered.insert(info.symbol);

            let (sym_node_id, _info) =
                sym_node_set.add_symbol(DerivedSymbolInfo::new(info.symbol, info.crossref_info, 0));
            // Explicitly put the node in the graph so if we don't find any
            // edges, we still display the node.  This is important for things
            // like "class-diagram" where showing nothing is very confusing.
            graph.ensure_node(sym_node_id.clone());

            // TODO: do something to limit the size of the root-set.  The
            // combinatorial explosion for something like nsGlobalWindowInner is
            // just too silly.  This can added as an overload.
            if is_source {
                source_set.push(sym_node_id.clone());
            }
            if is_target {
                target_set.push(sym_node_id);
            }
        }

        let node_limit = if self.args.paths_between {
            self.args.paths_between_node_limit
        } else {
            self.args.node_limit
        };

        let skip_uses_at_path_count =
            match (self.args.skip_uses_at_path_count, self.args.paths_between) {
                (0, false) => 16u32,
                (0, true) => 96u32,
                (x, _) => x,
            };

        let stop_at_class_label = match self.args.edge.as_str() {
            "callees" => Some("calls-diagram:stop"),
            "class" => Some("class-diagram:stop"),
            "uses" => Some("uses-diagram:stop"),
            _ => None,
        };

        let traverse_callees = match self.args.edge.as_str() {
            "callees" => true,
            _ => false,
        };
        let traverse_fields = match self.args.edge.as_str() {
            "class" => true,
            _ => false,
        };
        // Being able to see the field-member-uses is potentially invaluable,
        // but this really needs additional hueristics to be usable, so only
        // turn this on if explicitly specified for now.
        //
        // TODO: Improve heuristics to allow use of field-member-uses.
        //
        // The general issues:
        // - The fan-out is potentially graph-ruining.  Using an example of
        //   `BlobImpl`, we really do want to show `Blob`
        let traverse_field_member_uses = match self.args.edge.as_str() {
            "class" => self.args.traverse_field_member_uses,
            _ => -1,
        };
        let traverse_overridden_by = match self.args.edge.as_str() {
            "inheritance" => true,
            // For callees, if we have traversed to a specific method, we do
            // care about any further overrides.
            "callees" => true,
            _ => false,
        };
        let traverse_overrides = match self.args.edge.as_str() {
            "inheritance" => true,
            "uses" => true,
            // We intentionally do not traverse upwards here for the callees
            // case; only downwards in the "overridden_by" case above.
            _ => false,
        };
        let traverse_subclasses = match self.args.edge.as_str() {
            "class" => true,
            "inheritance" => true,
            _ => false,
        };
        let traverse_superclasses = match self.args.edge.as_str() {
            "class" => true,
            "inheritance" => true,
            _ => false,
        };
        let traverse_uses = match self.args.edge.as_str() {
            "uses" => true,
            _ => false,
        };

        // General operation:
        // - We pull a node to be traversed off the queue.  This ends up breadth
        //   first.
        // - We check if we already have the crossref info for the symbol and
        //   look it up if not.  There's an asymmetry here between the initial
        //   set of symbols we're traversing from which we already have cached
        //   values for and the new edges we discover, but it's not a concern.
        // - We traverse the list of edges.
        while let Some((sym, depth, cur_traversals)) = to_traverse.pop_front() {
            if sym_node_set.symbol_crossref_infos.len() as u32 >= node_limit {
                trace!(sym = %sym, depth, "stopping because of node limit");
                overloads_hit.push(OverloadInfo {
                    kind: OverloadKind::NodeLimit,
                    sym: Some(sym.to_string()),
                    exist: to_traverse.len() as u32,
                    included: node_limit,
                    local_limit: 0,
                    global_limit: node_limit,
                });
                to_traverse.clear();
                break;
            };

            trace!(sym = %sym, depth, "processing");
            let next_depth = depth + 1;
            // Note that we will regularly end up dropping our `sym_info` borrow
            // as we issue other calls to `ensure_symbol`.  In those cases, we
            // can efficiently re-acquire a reference to our sym_info through
            // `sym_node_set.get` or `sym_node_set.get_mut`.  Because we
            // regularly go async, using `Rc` isn't a great alternative because
            // it's not Send so we would need to step up to `Arc` and we don't
            // really need that.
            let (sym_id, sym_info) = sym_node_set.ensure_symbol(&sym, server, depth).await?;

            if let Some(stop_at_label) = &stop_at_class_label {
                if let Some(labels_json) = sym_info.crossref_info.pointer("/meta/labels").cloned() {
                    let labels: Vec<Ustr> = from_value(labels_json).unwrap();
                    let mut skip_symbol = false;
                    for label in labels {
                        if label.as_str() == *stop_at_label {
                            // Don't process the fields if we see a stop.  This is something
                            // manually specified in ontology-mapping.toml currently.
                            skip_symbol = true;
                        }
                    }
                    // only skip if this isn't the requested symbol (depth == 0)
                    if depth > 0 && skip_symbol {
                        if !self.args.retain_all_symbol_data {
                            sym_info.reduce_memory_usage_by_dropping_non_jumpref_info();
                        }
                        continue;
                    }
                }
            }

            // ## Clone the slotOwner now before engaging in additional borrows.
            let slot_owner = sym_info.crossref_info.pointer("/meta/slotOwner").cloned();

            if traverse_fields {
                // Traverse the fields out of this class
                // Note that depth won't stop us from showing a class's fields,
                // just whether we process the target symbol!
                if let Some(fields_json) = sym_info.crossref_info.pointer("/meta/fields").cloned() {
                    let fields: Vec<StructuredFieldInfo> = from_value(fields_json).unwrap();
                    for field in fields {
                        let mut show_field = !field.labels.is_empty();
                        // Attempt to mark the fields with the subsystem of the field's target class
                        // so that we can group fields by which subsystem they're related to.
                        // Because we propagate subsystems from IDL definitions through to their
                        // bindings, this should also end up working for generated bindings.
                        let mut effective_subsystem = None;

                        let mut targets = vec![];
                        for ptr_info in field.pointer_info {
                            show_field = true;
                            let (target_id, target_info) = sym_node_set
                                .ensure_symbol(&ptr_info.sym, server, next_depth)
                                .await?;
                            if next_depth < max_depth && considered.insert(ptr_info.sym) {
                                trace!(sym = ptr_info.sym.as_str(), "scheduling pointee sym");
                                to_traverse.push_back((
                                    ptr_info.sym,
                                    next_depth,
                                    all_traversals_valid,
                                ));
                            }

                            // In cases where we have multiple pointer_infos for a field, we
                            // arbitrarily picking the first one for now.
                            // XXX For maps, we probably should be favoring the value over the key,
                            // which would suggest we should pick the last va
                            if effective_subsystem.is_none() {
                                effective_subsystem = target_info.get_subsystem();
                            }

                            // XXX consider moving the mapping here to the ontology file
                            // or something that's not source code.
                            let (pri, edge_kind, use_badge, use_class) =
                                pointer_kind_to_badge_info(&ptr_info.kind);
                            targets.push((
                                target_id,
                                pri,
                                edge_kind,
                                use_badge,
                                vec![EdgeDetail::HoverClass(use_class.to_string())],
                            ));
                        }

                        if show_field {
                            let (field_id, field_info) = sym_node_set
                                .ensure_symbol(&field.sym, server, next_depth)
                                .await?;
                            field_info.effective_subsystem = effective_subsystem;
                            for label in field.labels {
                                // XXX like above, consider moving the emoji label mapping here to
                                // the ontology file or elsewhere.
                                if let Some((pri, shorter_label)) = label_to_badge_info(&label) {
                                    field_info.badges.push(SymbolBadge {
                                        pri,
                                        label: ustr(shorter_label),
                                        source_jump: None,
                                    });
                                }
                            }
                            for (tgt_id, pri, edge_kind, ptr_badge, edge_details) in targets {
                                field_info.badges.push(SymbolBadge {
                                    pri,
                                    label: ustr(ptr_badge),
                                    source_jump: None,
                                });
                                sym_edge_set.ensure_edge_in_graph(
                                    field_id.clone(),
                                    tgt_id,
                                    edge_kind,
                                    edge_details,
                                    &mut graph,
                                );
                            }
                        }
                    }
                }
            }

            if depth as i32 <= traverse_field_member_uses && next_depth < max_depth {
                let bad_data = || {
                    ServerError::StickyProblem(ErrorDetails {
                        layer: ErrorLayer::DataLayer,
                        message: format!("Bad edge info in sym {sym} on field-member-uses"),
                    })
                };

                // Find the places where this type is used as a field member.
                //
                // A hack/simplification we do here is just add the class and leave
                // it up to the traversal of the class to generate the field edge
                // for us.  We don't need to worry about weirdness with the depth
                // threshold here because our logic above will always process the
                // class's fields; the field traversal is not a separate step with
                // its own depth addition.
                let sym_info = sym_node_set.get(&sym_id);
                let member_uses_storage = sym_info
                    .crossref_info
                    .pointer("/field-member-uses")
                    .unwrap_or(&Value::Array(vec![]))
                    .clone();
                let member_uses = member_uses_storage.as_array().unwrap();

                if member_uses.len() as u32 >= self.args.skip_field_member_uses_at_count {
                    overloads_hit.push(OverloadInfo {
                        kind: OverloadKind::FieldMemberUses,
                        sym: Some(sym.to_string()),
                        exist: member_uses.len() as u32,
                        included: 0,
                        local_limit: self.args.skip_field_member_uses_at_count,
                        global_limit: 0,
                    });
                } else {
                    for target in member_uses {
                        // fmu is { sym, pretty, fields }
                        // The sym is the class referencing our type.
                        let target_sym_str = target["sym"].as_str().ok_or_else(bad_data)?;
                        let target_sym = ustr(target_sym_str);

                        let (_target_id, target_info) = sym_node_set
                            .ensure_symbol(&target_sym, server, next_depth)
                            .await?;

                        // we already considered depth in the outer condition
                        if considered.insert(target_info.symbol) {
                            trace!(sym = target_sym_str, "scheduling field-member-use");
                            to_traverse.push_back((
                                target_info.symbol,
                                next_depth,
                                all_traversals_valid,
                            ));
                        }
                    }
                }
            }

            // Check whether to traverse a parent binding slot relationship.
            if let Some(val) = slot_owner {
                let slot_owner: StructuredBindingSlotInfo = from_value(val).unwrap();

                // There are a few possibilities with a binding slot.  It can be
                // a binding type that is:
                //
                // IDL-Based:
                //
                // 1. An IPC `Recv` where the "uses" of this method will only be
                //    plumbing that is distracting and should be elided in favor
                //    of showing all `Send` calls instead.
                // 2. An XPIDL-like method implementation that can be called
                //    through either a cross-language glue layer like XPConnect
                //    which requires processing the slots or directly as the
                //    implementation does not have to go through a glue layer
                //    but can be called directly.  In this case, we do want to
                //    process uses directly.
                // 3. Support logic like an `EnablingPref` or `EnablingFunc` and
                //    any use of the symbol is terminal and should not be
                //    (erroneously) treated as somehow triggering the WebIDL
                //    functions which it is enabling for.
                //
                // Cross-language bindings like JNI-wrappers:
                //
                // There's a major divergence here between IDL bindings and cross-language
                // bindings like JNI wrappers.  For IDL, the owner slot is the IDL symbol
                // symbol, but for cross-language wrappers, the owner slot is the
                // implementing language.  (That way, if there are multiple language
                // bindings, they all have a parent of the implementing method, and
                // the implementing method has a list of all of the other-language bindings
                // that reference it.)
                let (should_traverse, skip_remainder, traverse_slot, outbound_edge, edge_kind) =
                    match (slot_owner.props.owner_lang, slot_owner.props.slot_kind) {
                        // Enabling funcs and constants don't count as interesting
                        // uses in either direction; they are support.
                        (
                            _,
                            BindingSlotKind::EnablingPref
                            | BindingSlotKind::EnablingFunc
                            | BindingSlotKind::Const,
                        ) => (false, false, None, false, EdgeKind::Default),
                        // For callees, draw an outbound IPC edge to the "recv" slot via the IDL symbol
                        (_, BindingSlotKind::Send) => (
                            self.args.edge == "callees",
                            true,
                            Some("recv"),
                            true,
                            EdgeKind::IPC,
                        ),
                        // For uses, draw an inbound IPC edge from the "send" slot via the IDL symbol
                        (_, BindingSlotKind::Recv) => (
                            self.args.edge == "uses",
                            true,
                            Some("send"),
                            false,
                            EdgeKind::IPC,
                        ),
                        // For IDL bindings, we want an upward (inbound) edge from the IDL symbol
                        (BindingOwnerLang::Idl, _) => {
                            (true, false, None, false, EdgeKind::Implementation)
                        }
                        // Cross-language binding class relationships are weird because
                        // the bindings inside are bidirectional, so let's ignore them.
                        (_, BindingSlotKind::Class) => {
                            (false, false, None, false, EdgeKind::Default)
                        }
                        // This leaves us with cross-language bindings where the slot owner is always the
                        // implementation symbol so slotOwner is always aligned with "callees"; the "uses"
                        // edges hammen when processing bindingSlots.
                        (_, _) => (
                            self.args.edge == "callees",
                            true,
                            None,
                            true,
                            EdgeKind::CrossLanguage,
                        ),
                    };
                if should_traverse {
                    let (owner_id, owner_info) = sym_node_set
                        .ensure_symbol(&slot_owner.sym, server, next_depth)
                        .await?;

                    // Handle the case where we need to traverse a slot
                    if let Some(other_slot) = traverse_slot {
                        if let Some(other_sym) = owner_info.get_binding_slot_sym(other_slot) {
                            let (other_id, other_info) = sym_node_set
                                .ensure_symbol(&other_sym, server, next_depth)
                                .await?;
                            if outbound_edge {
                                sym_edge_set.ensure_edge_in_graph(
                                    sym_id.clone(),
                                    other_id,
                                    edge_kind,
                                    vec![],
                                    &mut graph,
                                );
                            } else {
                                sym_edge_set.ensure_edge_in_graph(
                                    other_id,
                                    sym_id.clone(),
                                    edge_kind,
                                    vec![],
                                    &mut graph,
                                );
                            }
                            if next_depth < max_depth && considered.insert(other_info.symbol) {
                                trace!(
                                    sym = other_info.symbol.as_str(),
                                    "scheduling traversed binding slot sym"
                                );
                                to_traverse.push_back((
                                    other_info.symbol,
                                    next_depth,
                                    all_traversals_valid,
                                ));
                            }
                        }
                        continue;
                    } else {
                        if outbound_edge {
                            sym_edge_set.ensure_edge_in_graph(
                                sym_id.clone(),
                                owner_id,
                                edge_kind,
                                vec![],
                                &mut graph,
                            );
                        } else {
                            sym_edge_set.ensure_edge_in_graph(
                                owner_id,
                                sym_id.clone(),
                                edge_kind,
                                vec![],
                                &mut graph,
                            );
                        }
                        if next_depth < max_depth && considered.insert(owner_info.symbol) {
                            trace!(
                                sym = owner_info.symbol.as_str(),
                                "scheduling owner binding slot sym"
                            );
                            to_traverse.push_back((
                                owner_info.symbol,
                                next_depth,
                                all_traversals_valid,
                            ));
                        }
                    }

                    if skip_remainder {
                        // XXX we should potentially be using reduce_memory_usage_by_dropping_non_jumpref_info
                        continue;
                    }
                }
            }

            // Process this symbol's binding slots.  As noted at the `slot_owner` traversal, there's
            // a major different between IDL bindings and cross-language bindings like Java/Kotlin
            // JNI.  There are also some traversals that are pointless for us to consider right now
            // like for XPIDL where in theory XPConnect allows calls between C++ and JS but we don't
            // have the necessary analysis data to handle that.
            let sym_info = sym_node_set.get(&sym_id);
            if let Some(Value::Array(slots)) = sym_info
                .crossref_info
                .pointer("/meta/bindingSlots")
                .cloned()
            {
                let mut skip_after_slots = false;
                for slot_val in slots {
                    let slot: StructuredBindingSlotInfo = from_value(slot_val).unwrap();
                    let (should_traverse, skip_other_edges, outbound_edge, edge_kind) =
                        match (slot.props.owner_lang, slot.props.slot_kind) {
                            // Don't bother with IDL bindings; all the relevant traversals involve a
                            // slotOwner at this time.
                            (BindingOwnerLang::Idl, _) => (false, false, false, EdgeKind::Default),
                            // Cross-language binding class relationships are weird because
                            // the bindings inside are bidirectional, so let's ignore them.
                            (_, BindingSlotKind::Class) => (false, false, false, EdgeKind::Default),
                            // For cross-language wrappers the implementing language is the slotOwner
                            // so the binding slots are edges to the binding.  That is, the slotOwner
                            // constitutes a "uses" edge and the slots constitute a "callees" edge.
                            (_, _) => (
                                self.args.edge == "uses",
                                true,
                                false,
                                EdgeKind::CrossLanguage,
                            ),
                        };
                    if should_traverse {
                        // Skipping is conditional on the decision to traverse.
                        if skip_other_edges {
                            skip_after_slots = true;
                        }
                        let (rel_id, _) = sym_node_set
                            .ensure_symbol(&slot.sym, server, next_depth)
                            .await?;
                        if outbound_edge {
                            sym_edge_set.ensure_edge_in_graph(
                                sym_id.clone(),
                                rel_id,
                                edge_kind,
                                vec![],
                                &mut graph,
                            );
                        } else {
                            sym_edge_set.ensure_edge_in_graph(
                                rel_id,
                                sym_id.clone(),
                                edge_kind,
                                vec![],
                                &mut graph,
                            );
                        }
                        if next_depth < max_depth && considered.insert(slot.sym) {
                            trace!(sym = slot.sym.as_str(), "scheduling bind slot sym");
                            to_traverse.push_back((slot.sym, next_depth, all_traversals_valid));
                        }
                    }
                }
                if skip_after_slots {
                    // XXX we should potentially be using reduce_memory_usage_by_dropping_non_jumpref_info
                    continue;
                }
            }

            // Check whether we have any ontology shortcuts to handle.
            let sym_info = sym_node_set.get(&sym_id);
            if let Some(Value::Array(slots)) = sym_info
                .crossref_info
                .pointer("/meta/ontologySlots")
                .cloned()
            {
                let mut keep_going = true;
                for slot_val in slots {
                    let slot: OntologySlotInfo = from_value(slot_val).unwrap();
                    let (should_traverse, upwards) = match slot.slot_kind {
                        OntologySlotKind::RunnableConstructor => (self.args.edge == "uses", true),
                        OntologySlotKind::RunnableMethod => (self.args.edge == "callees", false),
                    };
                    if should_traverse {
                        for rel_sym in slot.syms {
                            let (rel_id, _) = sym_node_set
                                .ensure_symbol(&rel_sym, server, next_depth)
                                .await?;
                            if upwards {
                                sym_edge_set.ensure_edge_in_graph(
                                    rel_id,
                                    sym_id.clone(),
                                    EdgeKind::Default,
                                    vec![],
                                    &mut graph,
                                );
                            } else {
                                sym_edge_set.ensure_edge_in_graph(
                                    sym_id.clone(),
                                    rel_id,
                                    EdgeKind::Default,
                                    vec![],
                                    &mut graph,
                                );
                            }
                            if next_depth < max_depth && considered.insert(rel_sym) {
                                trace!(sym = rel_sym.as_str(), "scheduling ontology sym");
                                to_traverse.push_back((rel_sym, next_depth, all_traversals_valid));
                            }
                        }
                        // For the case of runnables the override hierarchy is arguably a
                        // distraction from the fundamental control flow going on.
                        //
                        // TODO: Evaluate whether avoiding walking up the override edges is helpful
                        // as implemented here.
                        keep_going = false;
                    }
                }
                if !keep_going {
                    continue;
                }
            }

            if traverse_subclasses && cur_traversals.contains(Traversals::Subclass) {
                let bad_data = || {
                    ServerError::StickyProblem(ErrorDetails {
                        layer: ErrorLayer::DataLayer,
                        message: format!("Bad edge info in sym {sym} on meta subclasses"),
                    })
                };

                let sym_info = sym_node_set.get(&sym_id);
                let overrides = sym_info
                    .crossref_info
                    .pointer("/meta/subclasses")
                    .unwrap_or(&Value::Array(vec![]))
                    .clone();

                for target in overrides.as_array().unwrap() {
                    // subclasses are just the raw symbol, not an object dict.
                    let target_sym_str = target.as_str().ok_or_else(bad_data)?;
                    let target_sym = ustr(target_sym_str);

                    let (target_id, target_info) = sym_node_set
                        .ensure_symbol(&target_sym, server, next_depth)
                        .await?;

                    sym_edge_set.ensure_edge_in_graph(
                        sym_id.clone(),
                        target_id,
                        EdgeKind::Inheritance,
                        vec![],
                        &mut graph,
                    );

                    if next_depth < max_depth && considered.insert(target_info.symbol) {
                        trace!(sym = target_sym_str, "scheduling subclass");
                        // If we're going in the subclass direction continue only going in the subclass
                        // direction; don't change direction and perform superclass traversals.
                        // XXX we should potentially be tying this into "considered" somehow.
                        to_traverse.push_back((
                            target_info.symbol,
                            next_depth,
                            Traversals::Subclass,
                        ));
                    }
                }
            }

            if traverse_superclasses && cur_traversals.contains(Traversals::Super) {
                let bad_data = || {
                    ServerError::StickyProblem(ErrorDetails {
                        layer: ErrorLayer::DataLayer,
                        message: format!("Bad edge info in sym {sym} on meta superclasses"),
                    })
                };

                let sym_info = sym_node_set.get(&sym_id);
                let overrides = sym_info
                    .crossref_info
                    .pointer("/meta/supers")
                    .unwrap_or(&Value::Array(vec![]))
                    .clone();

                'target: for target in overrides.as_array().unwrap() {
                    // overrides is { sym, pretty, props }
                    let target_sym_str = target["sym"].as_str().ok_or_else(bad_data)?;
                    let target_sym = ustr(target_sym_str);

                    let (target_id, target_info) = sym_node_set
                        .ensure_symbol(&target_sym, server, next_depth)
                        .await?;

                    if let Some(Value::Array(labels_json)) =
                        target_info.crossref_info.pointer("/meta/labels").cloned()
                    {
                        for label in labels_json {
                            if let Value::String(label) = label {
                                if let Some(badge_label) =
                                    label.strip_prefix("class-diagram:elide-and-badge:")
                                {
                                    let sym_info = sym_node_set.get_mut(&sym_id);
                                    sym_info.badges.push(SymbolBadge {
                                        pri: 50,
                                        label: ustr(badge_label),
                                        source_jump: None,
                                    });
                                    continue 'target;
                                }
                            }
                        }
                    }

                    sym_edge_set.ensure_edge_in_graph(
                        target_id,
                        sym_id.clone(),
                        EdgeKind::Inheritance,
                        vec![],
                        &mut graph,
                    );

                    if next_depth < max_depth && considered.insert(target_info.symbol) {
                        trace!(sym = target_sym_str, "scheduling super");
                        // If we're going in the superclass direction, continue only going in the
                        // superclass direction; don't allow going back down subclasses!
                        // XXX we should potentially be tying this into "considered" somehow
                        to_traverse.push_back((target_info.symbol, next_depth, Traversals::Super));
                    }
                }
            }

            if traverse_overrides {
                let bad_data = || {
                    ServerError::StickyProblem(ErrorDetails {
                        layer: ErrorLayer::DataLayer,
                        message: format!("Bad edge info in sym {sym} on meta overrides"),
                    })
                };

                let sym_info = sym_node_set.get(&sym_id);
                let overrides = sym_info
                    .crossref_info
                    .pointer("/meta/overrides")
                    .unwrap_or(&Value::Array(vec![]))
                    .clone();

                for target in overrides.as_array().unwrap() {
                    // overrides is { sym, pretty }
                    let target_sym_str = target["sym"].as_str().ok_or_else(bad_data)?;
                    let target_sym = ustr(target_sym_str);

                    let (target_id, target_info) = sym_node_set
                        .ensure_symbol(&target_sym, server, next_depth)
                        .await?;

                    if considered.insert(target_info.symbol) {
                        // As a quasi-hack, only add this edge if we didn't
                        // already queue the class for consideration to avoid
                        // getting this edge twice thanks to the reciprocal
                        // relationship we will see when considering it.
                        //
                        // This is only necessary because this is a case
                        // where we are doing bi-directional traversal
                        // because overrides are an equivalence class from
                        // our perspective (right now, before actually
                        // checking the definition of equivalence class. ;)
                        sym_edge_set.ensure_edge_in_graph(
                            target_id,
                            sym_id.clone(),
                            EdgeKind::Inheritance,
                            vec![],
                            &mut graph,
                        );
                        if next_depth < max_depth {
                            trace!(sym = target_sym_str, "scheduling overrides");
                            to_traverse.push_back((
                                target_info.symbol,
                                next_depth,
                                all_traversals_valid,
                            ));
                        }
                    }
                }
            }

            if traverse_overridden_by {
                let bad_data = || {
                    ServerError::StickyProblem(ErrorDetails {
                        layer: ErrorLayer::DataLayer,
                        message: format!("Bad edge info in sym {sym} on meta overriddenBy"),
                    })
                };

                let sym_info = sym_node_set.get(&sym_id);
                let overridden_by = sym_info
                    .crossref_info
                    .pointer("/meta/overriddenBy")
                    .unwrap_or(&Value::Array(vec![]))
                    .clone();

                for target in overridden_by.as_array().unwrap() {
                    // overriddenBy is just a bare symbol name currently
                    let target_sym_str = target.as_str().ok_or_else(bad_data)?;
                    let target_sym = ustr(target_sym_str);

                    let (target_id, target_info) = sym_node_set
                        .ensure_symbol(&target_sym, server, next_depth)
                        .await?;

                    if considered.insert(target_info.symbol) {
                        // Same rationale on avoiding a duplicate edge.
                        sym_edge_set.ensure_edge_in_graph(
                            sym_id.clone(),
                            target_id,
                            EdgeKind::Inheritance,
                            vec![],
                            &mut graph,
                        );
                        if next_depth < max_depth {
                            trace!(sym = target_sym_str, "scheduling overridenBy");
                            to_traverse.push_back((
                                target_info.symbol,
                                next_depth,
                                all_traversals_valid,
                            ));
                        }
                    }
                }
            }

            if traverse_callees {
                let bad_data = || {
                    ServerError::StickyProblem(ErrorDetails {
                        layer: ErrorLayer::DataLayer,
                        message: format!("Bad edge info in sym {sym} on callees"),
                    })
                };

                let sym_info = sym_node_set.get_mut(&sym_id);
                let callees = match (
                    self.args.retain_all_symbol_data,
                    sym_info.crossref_info.get_mut("callees"),
                ) {
                    (true, Some(v)) => match v.clone() {
                        Value::Array(arr) => arr,
                        _ => vec![],
                    },
                    (false, Some(v)) => match v.take() {
                        Value::Array(arr) => arr,
                        _ => vec![],
                    },
                    _ => vec![],
                };

                // Callees are synthetically derived from crossref and is a
                // flat list of { kind, pretty, sym }.  This differs from
                // most other edges which are path hit-lists.
                for target in callees {
                    let target_sym_str = target["sym"].as_str().ok_or_else(bad_data)?;
                    let target_sym = ustr(target_sym_str);
                    //let target_kind = target["kind"].as_str().ok_or_else(bad_data)?;

                    let mut edge_info = vec![];
                    // The jump is precomputed by the crossref process when
                    // deriving the "callees" kindmap entry.
                    if let Some(Value::String(jump)) = target.get("jump") {
                        edge_info.push(EdgeDetail::Jump(jump.clone()));
                    }

                    let (target_id, target_info) = sym_node_set
                        .ensure_symbol(&target_sym, server, next_depth)
                        .await?;

                    if target_info.is_callable() {
                        sym_edge_set.ensure_edge_in_graph(
                            sym_id.clone(),
                            target_id,
                            EdgeKind::Default,
                            edge_info,
                            &mut graph,
                        );
                        if next_depth < max_depth && considered.insert(target_info.symbol) {
                            trace!(sym = target_sym_str, "scheduling callees");
                            to_traverse.push_back((
                                target_info.symbol,
                                next_depth,
                                all_traversals_valid,
                            ));
                        }
                    }
                }
            }

            if traverse_uses {
                let bad_data = || {
                    ServerError::StickyProblem(ErrorDetails {
                        layer: ErrorLayer::DataLayer,
                        message: format!("Bad edge info in sym {sym} on callees"),
                    })
                };

                let sym_info = sym_node_set.get_mut(&sym_id);
                let uses = match (
                    self.args.retain_all_symbol_data,
                    sym_info.crossref_info.get_mut("uses"),
                ) {
                    (true, Some(v)) => match v.clone() {
                        Value::Array(arr) => arr,
                        _ => vec![],
                    },
                    (false, Some(v)) => match v.take() {
                        Value::Array(arr) => arr,
                        _ => vec![],
                    },
                    _ => vec![],
                };
                // we just took the uses, but drop the callees too.
                if !self.args.retain_all_symbol_data {
                    sym_info.reduce_memory_usage_by_dropping_non_jumpref_info();
                }

                // Do not process the uses if there are more paths than our skip limit.
                if uses.len() as u32 >= skip_uses_at_path_count {
                    overloads_hit.push(OverloadInfo {
                        kind: OverloadKind::UsesPaths,
                        sym: Some(sym.to_string()),
                        exist: uses.len() as u32,
                        included: 0,
                        local_limit: skip_uses_at_path_count,
                        global_limit: 0,
                    });
                    continue;
                }

                // We may see a use edge multiple times so we want to suppress it,
                // but we don't want to use `considered` for this because that would
                // hide cycles in the graph!
                let mut use_considered = HashSet::new();

                let mut line_hits: u32 = 0;

                // Uses are path-hitlists and each array item has the form
                // { path, lines: [ { context, contextsym }] } eliding some
                // of the hit fields.  We really just care about the
                // contextsym.
                for path_hits in uses {
                    let path = path_hits["path"].as_str().ok_or_else(bad_data)?;
                    let hits = path_hits["lines"].as_array().ok_or_else(bad_data)?;
                    // For now we're just going to use the path limit for this too.
                    //
                    // The specific scenario driving this is the "abort" method
                    // which ends up called an immense number of times inside of
                    // mfbt/Assertions.h because of assertion macros where each
                    // hit technically has a different contextsym
                    //
                    // First, handle this specific path breaking things for us as
                    // a local limit.  Then add the line count and check if the
                    // global limit has been hit.
                    if hits.len() as u32 >= skip_uses_at_path_count {
                        overloads_hit.push(OverloadInfo {
                            kind: OverloadKind::UsesLines,
                            sym: Some(sym.to_string()),
                            exist: hits.len() as u32,
                            included: 0,
                            local_limit: skip_uses_at_path_count,
                            global_limit: 0,
                        });
                        break;
                    }
                    line_hits += hits.len() as u32;
                    if line_hits >= skip_uses_at_path_count {
                        overloads_hit.push(OverloadInfo {
                            kind: OverloadKind::UsesLines,
                            sym: Some(sym.to_string()),
                            exist: line_hits,
                            included: line_hits - (hits.len() as u32),
                            local_limit: 0,
                            // Note we're reporting this as a global limit to
                            // differentiate from the above case.
                            global_limit: skip_uses_at_path_count,
                        });
                        break;
                    }
                    for source in hits {
                        let source_sym_str = source["contextsym"].as_str().unwrap_or("");
                        //let source_kind = source["kind"].as_str().ok_or_else(bad_data)?;

                        if source_sym_str.is_empty() {
                            continue;
                        }
                        let source_sym = ustr(source_sym_str);

                        let (source_id, source_info) = sym_node_set
                            .ensure_symbol(&source_sym, server, next_depth)
                            .await?;

                        if source_info.is_callable() {
                            // We call this even if our check below determines we've already created
                            // and traversed this edge because we want to merge in edge detail
                            // information.
                            let jump = format!("{}#{}", path, source["lno"].as_u64().unwrap_or(0));
                            sym_edge_set.ensure_edge_in_graph(
                                source_id,
                                sym_id.clone(),
                                EdgeKind::Default,
                                vec![EdgeDetail::Jump(jump)],
                                &mut graph,
                            );
                            // Only traverse the edge once.
                            if use_considered.insert(source_info.symbol)
                                && next_depth < max_depth
                                && considered.insert(source_info.symbol)
                            {
                                trace!(sym = source_sym_str, "scheduling uses");
                                to_traverse.push_back((
                                    source_info.symbol,
                                    next_depth,
                                    all_traversals_valid,
                                ));
                            }
                        }
                    }
                }
            } else if !self.args.retain_all_symbol_data {
                let sym_info = sym_node_set.get_mut(&sym_id);
                sym_info.reduce_memory_usage_by_dropping_non_jumpref_info();
            }
        }

        // ## Paths Between
        let graph_coll = if self.args.paths_between {
            // In this case, we don't want our original node set because we
            // expect it to have an order of magnitude more data than we want
            // in the result set.  So we build a new node set and graph.
            let mut paths_node_set = SymbolGraphNodeSet::new();
            let mut paths_edge_set = SymbolGraphEdgeSet::new();
            let mut paths_graph = NamedSymbolGraph::new("paths".to_string());

            trace!("performing path propagation");
            sym_node_set.propagate_paths(
                &mut graph,
                &source_set,
                &target_set,
                &sym_edge_set,
                // We've relaxed our paths-between node limit and would like to keep it that way,
                // but we definitely need to limit the resulting size of the graph, so we still need
                // to have a node limit, so we use the non-paths-between node limit (which can be
                // raised) for that.
                self.args.node_limit,
                // There's no point considering paths longer than the max depth.
                max_depth,
                &mut paths_graph,
                &mut paths_node_set,
                &mut paths_edge_set,
            );
            if paths_node_set.symbol_crossref_infos.len() as u32 >= self.args.node_limit {
                overloads_hit.push(OverloadInfo {
                    kind: OverloadKind::NodeLimit,
                    sym: None,
                    // We don't know how many there might have been as we did a soft limit
                    // stop while propagating, so say 0 for exist but how many
                    // we included.
                    exist: 0,
                    included: paths_node_set.symbol_crossref_infos.len() as u32,
                    local_limit: 0,
                    global_limit: self.args.node_limit,
                });
            }

            SymbolGraphCollection {
                node_set: paths_node_set,
                edge_set: paths_edge_set,
                graphs: vec![paths_graph],
                overloads_hit,
                hierarchical_graphs: vec![],
            }
        } else {
            SymbolGraphCollection {
                node_set: sym_node_set,
                edge_set: sym_edge_set,
                graphs: vec![graph],
                overloads_hit,
                hierarchical_graphs: vec![],
            }
        };

        Ok(PipelineValues::SymbolGraphCollection(graph_coll))
    }
}

```

## tools/src/cmd_pipeline/cmd_batch_render.rs
```
use async_trait::async_trait;
use clap::Args;

use super::interface::{PipelineCommand, PipelineValues};
use crate::{
    abstract_server::{
        AbstractServer, ErrorDetails, ErrorLayer, Result, SearchfoxIndexRoot, ServerError,
    },
    file_utils::write_file_ensuring_parent_dir,
    templating::builder::build_and_parse_dir_listing,
};

#[derive(Debug, Args)]
pub struct BatchRender {
    /// Preconfigured rendering task.  This could be an enum or sub-command, but
    /// for now we're just going for strings.
    #[clap(value_parser)]
    task: String,
}

/// General operation:
/// - We take a `BatchGroups` as input.
/// - We iterate over each batch group and pass it to the liquid template
///   associated with this task.
/// - We expand the path template associated with the task and write it out.
#[derive(Debug)]
pub struct BatchRenderCommand {
    pub args: BatchRender,
}

#[async_trait]
impl PipelineCommand for BatchRenderCommand {
    async fn execute(
        &self,
        server: &(dyn AbstractServer + Send + Sync),
        input: PipelineValues,
    ) -> Result<PipelineValues> {
        let batch_groups = match input {
            PipelineValues::BatchGroups(bg) => bg,
            _ => {
                return Err(ServerError::StickyProblem(ErrorDetails {
                    layer: ErrorLayer::ConfigLayer,
                    message: "batch-render needs BatchGroups".to_string(),
                }));
            }
        };

        match self.args.task.as_str() {
            "dir" => {
                let template = build_and_parse_dir_listing();
                let tree_info = server.tree_info()?;
                for item in batch_groups.groups {
                    if let PipelineValues::FileMatches(fm) = item.value {
                        let liquid_globals = liquid::object!({
                            "tree": tree_info.name,
                            // the header always needs this
                            "query": "",
                            "path": item.name,
                            "files": fm.file_matches,
                        });
                        let rendered = match template.render(&liquid_globals) {
                            Ok(r) => r,
                            Err(e) => {
                                return Err(ServerError::StickyProblem(ErrorDetails {
                                    layer: ErrorLayer::ConfigLayer,
                                    message: format!("Template problems: {}", e),
                                }));
                            }
                        };
                        let output_path = server.translate_path(
                            SearchfoxIndexRoot::UncompressedDirectoryListing,
                            &item.name,
                        )?;
                        write_file_ensuring_parent_dir(&output_path, &rendered)?;
                    }
                }
                Ok(PipelineValues::Void)
            }
            unknown => Err(ServerError::StickyProblem(ErrorDetails {
                layer: ErrorLayer::ConfigLayer,
                message: format!("Unknown task type: {}", unknown),
            })),
        }
    }
}

```

## tools/src/cmd_pipeline/parser.rs
```
use clap::{Parser, Subcommand, ValueEnum};

use super::cmd_augment_results::AugmentResults;
use super::cmd_batch_render::BatchRender;
use super::cmd_cat_html::CatHtml;
use super::cmd_compile_results::CompileResults;
use super::cmd_crossref_expand::CrossrefExpand;
use super::cmd_crossref_lookup::CrossrefLookup;
use super::cmd_filter_analysis::FilterAnalysis;
use super::cmd_format_symbols::FormatSymbols;
use super::cmd_fuse_crossrefs::FuseCrossrefs;
use super::cmd_graph::Graph;
use super::cmd_jumpref_lookup::JumprefLookup;
use super::cmd_merge_analyses::MergeAnalyses;
use super::cmd_prod_filter::ProductionFilter;
use super::cmd_query::Query;
use super::cmd_render::Render;
use super::cmd_search::Search;
use super::cmd_search_files::SearchFiles;
use super::cmd_search_identifiers::SearchIdentifiers;
use super::cmd_search_text::SearchText;
use super::cmd_show_html::ShowHtml;
use super::cmd_tokenize_source::TokenizeSource;
use super::cmd_traverse::Traverse;
use super::cmd_webtest::Webtest;

#[derive(Clone, Debug, PartialEq, ValueEnum)]
pub enum OutputFormat {
    // Pretty-printed JSON.
    Pretty,
    // Un-pretty-printed JSON.
    Concise,
}

#[derive(Debug, Parser)]
pub struct ToolOpts {
    /// URL of the server to query or the path to the root of the index tree if
    /// using local data.
    #[clap(
        long,
        value_parser,
        default_value = "https://searchfox.org/",
        env = "SEARCHFOX_SERVER"
    )]
    pub server: String,

    /// The name of the indexed tree to use.
    #[clap(
        long,
        value_parser,
        default_value = "mozilla-central",
        env = "SEARCHFOX_TREE"
    )]
    pub tree: String,

    #[clap(long, short, value_parser, value_enum, default_value = "concise")]
    pub output_format: OutputFormat,

    #[clap(subcommand)]
    pub cmd: Command,
}

#[derive(Debug, Subcommand)]
pub enum Command {
    AugmentResults(AugmentResults),
    BatchRender(BatchRender),
    CatHtml(CatHtml),
    CrossrefExpand(CrossrefExpand),
    CrossrefLookup(CrossrefLookup),
    FilterAnalysis(FilterAnalysis),
    FormatSymbols(FormatSymbols),
    Graph(Graph),
    JumprefLookup(JumprefLookup),
    MergeAnalyses(MergeAnalyses),
    ProductionFilter(ProductionFilter),
    Query(Query),
    Render(Render),
    Search(Search),
    SearchFiles(SearchFiles),
    SearchIdentifiers(SearchIdentifiers),
    SearchText(SearchText),
    ShowHtml(ShowHtml),
    TokenizeSource(TokenizeSource),
    Traverse(Traverse),
    Webtest(Webtest),
}

#[derive(Debug, Parser)]
pub struct JunctionOpts {
    #[structopt(subcommand)]
    pub cmd: JunctionCommand,
}

#[derive(Debug, Subcommand)]
pub enum JunctionCommand {
    CompileResults(CompileResults),
    FuseCrossrefs(FuseCrossrefs),
}

```

## tools/src/cmd_pipeline/interface.rs
```
use async_trait::async_trait;
use bitflags::bitflags;
use clap::{Args, ValueEnum};
use serde::{ser::SerializeStruct, Serialize, Serializer};
use serde_json::{json, to_string_pretty, Value};
use std::{
    cmp::Ordering,
    collections::{BTreeMap, HashMap, HashSet},
    fmt::Debug,
};
use tracing::{trace, trace_span, Instrument};
use ustr::{ustr, Ustr, UstrMap};

pub use crate::abstract_server::{AbstractServer, Result};
use crate::{
    abstract_server::{FileMatches, TextMatches},
    file_format::crossref_converter::convert_crossref_value_to_sym_info_rep,
};

use super::symbol_graph::{SymbolGraphCollection, SymbolGraphNodeSet};

#[derive(Clone, Debug, PartialEq, ValueEnum)]
pub enum RecordType {
    Source,
    Target,
    Structured,
}

impl RecordType {
    pub fn name(&self) -> &'static str {
        match self {
            RecordType::Source => "source",
            RecordType::Target => "target",
            RecordType::Structured => "structured",
        }
    }
}

#[derive(Debug, Args)]
pub struct SymbolicQueryOpts {
    /// Exact symbol match
    #[clap(short, value_parser)]
    pub symbol: Option<String>,

    /// Prefix symbol match
    #[clap(long, value_parser)]
    pub symbol_prefix: Option<String>,

    /// Exact identifier match
    #[clap(short, value_parser)]
    pub identifier: Option<String>,
}

#[derive(Serialize)]
pub struct BatchGroups {
    pub groups: Vec<BatchGroupItem>,
}

#[derive(Serialize)]
pub struct BatchGroupItem {
    pub name: String,
    pub value: PipelineValues,
}

/// Hierarchical table whose rows may be optionally associated with symbols.
pub struct SymbolTreeTable {
    pub node_set: SymbolGraphNodeSet,
    pub platforms: Vec<String>,
    pub rows: Vec<SymbolTreeTableNode>,

    /// Symbols to put into SYM_INFO, in addition to node_set.
    pub extra_syms: HashMap<String, Value>,
}

#[derive(Serialize)]
pub struct SymbolTreeTableList {
    pub tables: Vec<SymbolTreeTable>,
    #[serde(rename = "className")]
    pub class_name: Option<String>,
}

impl SymbolTreeTableList {
    pub fn unioned_node_sets_as_jumprefs(&self) -> Value {
        let mut jumprefs = BTreeMap::new();
        for table in &self.tables {
            for sym_info in table.node_set.symbol_crossref_infos.iter() {
                let info = sym_info.crossref_info.clone();
                jumprefs.insert(
                    sym_info.symbol,
                    convert_crossref_value_to_sym_info_rep(info, &sym_info.symbol, None),
                );
            }
            for (sym, info) in &table.extra_syms {
                jumprefs.insert(ustr(sym.as_str()), info.clone());
            }
        }

        json!(jumprefs)
    }
}

#[derive(Serialize)]
pub struct TextWithSymbol {
    // Text to go within the `span` tag; this will be escaped.
    pub text: String,
    // The `data-symbols` attribute of the `span`.
    pub symbols: String,
}

#[derive(Serialize)]
pub struct BasicLink {
    // Text to go within the `a` tag; this will be escaped.
    pub text: String,
    // This is not intended to be a fully valid href yet; the containing enum
    // helps contextualize what the base of the URL should be.  This will be
    // URL encoded to prevent escaping from attributes but not subject to text
    // escaping.
    pub link: String,
}

/// Very basic markup initially being introduced for SymbolTreeTable cells with
/// the expectation this will serialize into a much more terse JSON rep than the
/// structure might imply.
///
/// It's an explicit goal here to avoid generating HTML in our pipeline because:
/// - This simplifies thinking/worrying about escaping.
/// - It's significantly more pleasant to review prettified JSON snapshots of
///   this rather than the resulting HTML, but this should still provide us with
///   sufficient fidelity for review purposes in most cases, minimizing the
///   number of HTML snapshots we need.
/// - The decoupling could potentially be useful for people writing editor
///   plugins or maybe trying an alternate presentation on searchfox results,
///   etc.  This is not remotely a primary goal, but it does feel like a
///   benefit.  (For example, an editor plugin would probably like that the
///   SourceLink is tree-relative rather than a full absolute URL that has to
///   transformed, etc.)
#[derive(Serialize)]
pub enum BasicMarkup {
    Heading(String),
    // This is just text, it doesn't need to go in a tag at all.  It will get
    // escaped.
    Text(String),
    ItalicText(String),
    Newline,
    // This is text that should link to a query endpoint.  For now we just point
    // it at the "default" config, but one might imagine that we might propagate
    // the current config in use through to any subsequent links.  We might also
    // mark these links up with extra metadata so that we can transform them on
    // the client side so that a user's preferred config can perform an
    // override.
    QueryLink(BasicLink),
    SourceLink(BasicLink),
    // These are a variant of Heading and Text where the text has data-symbols.
    SymbolHeading(TextWithSymbol),
    SymbolText(TextWithSymbol),
}

#[derive(Serialize)]
pub struct SymbolTreeTableAlignmentAndSize {
    pub alignment: String,
    pub size: String,
}

impl SymbolTreeTableAlignmentAndSize {
    pub fn new(alignment: String, size: String) -> Self {
        Self { alignment, size }
    }
}

#[derive(Serialize)]
pub struct SymbolTreeTableNode {
    pub name: String,
    pub symbols: String,
    #[serde(rename = "isBaseClass")]
    pub is_base_class: bool,
    #[serde(rename = "alignmentAndSize")]
    pub alignment_and_size: Vec<SymbolTreeTableAlignmentAndSize>,
    pub items: Vec<SymbolTreeTableItem>,
}

impl SymbolTreeTableNode {
    pub fn new(
        name: String,
        symbols: String,
        is_base_class: bool,
        alignment_and_size: Vec<SymbolTreeTableAlignmentAndSize>,
    ) -> Self {
        Self {
            name,
            symbols,
            is_base_class,
            alignment_and_size,
            items: vec![],
        }
    }
}

#[derive(Serialize)]
pub enum SymbolTreeTableItem {
    Field(SymbolTreeTableField),
    Hole(Vec<Option<String>>),
    EndPadding(Vec<Option<String>>),
    Warning(String),
}

#[derive(Serialize)]
pub struct SymbolTreeTableField {
    pub name: String,
    pub symbols: String,
    pub types: Vec<SymbolTreeTableFieldType>,
    pub lines: Vec<String>,
    #[serde(rename = "offsetAndSize")]
    pub offset_and_size: Vec<Option<SymbolTreeTableFieldOffsetAndSize>>,
}

impl SymbolTreeTableField {
    pub fn new(name: String, symbols: String) -> Self {
        Self {
            name,
            symbols,
            types: vec![],
            lines: vec![],
            offset_and_size: vec![],
        }
    }
}

#[derive(Serialize)]
pub struct SymbolTreeTableFieldType {
    pub name: String,
    pub symbols: String,
}

impl SymbolTreeTableFieldType {
    pub fn new(name: String, symbols: String) -> Self {
        Self { name, symbols }
    }
}

#[derive(Serialize)]
pub struct SymbolTreeTableFieldOffsetAndSize {
    pub offset: String,
    pub size: String,
}

impl SymbolTreeTableFieldOffsetAndSize {
    pub fn new(offset: String, size: String) -> Self {
        Self { offset, size }
    }
}

impl Default for SymbolTreeTable {
    fn default() -> Self {
        Self::new()
    }
}

impl SymbolTreeTable {
    pub fn new() -> Self {
        Self {
            node_set: SymbolGraphNodeSet::new(),
            platforms: vec![],
            rows: vec![],
            extra_syms: HashMap::new(),
        }
    }
}

/// Custom serializer so that the node_set information can be expressed on the
/// serialization of the nodes as a stringified symbol that can be looked up
/// rather than an integer identifier.  This makes the test snapshots more
/// useful and stable as well as letting any symbol lookup table be
/// transparently unioned with other similar maps.
impl Serialize for SymbolTreeTable {
    fn serialize<S>(&self, serializer: S) -> std::result::Result<S::Ok, S::Error>
    where
        S: Serializer,
    {
        let mut stt = serializer.serialize_struct("SymbolTreeTable", 2)?;
        stt.serialize_field(
            "jumprefs",
            &self.node_set.symbols_meta_to_jumpref_json_nomut(),
        )?;
        stt.serialize_field("platforms", &self.platforms)?;
        stt.serialize_field("rows", &self.rows)?;
        stt.end()
    }
}

/// The input and output of each pipeline segment
#[derive(Serialize)]
pub enum PipelineValues {
    IdentifierList(IdentifierList),
    SymbolList(SymbolList),
    SymbolCrossrefInfoList(SymbolCrossrefInfoList),
    SymbolGraphCollection(SymbolGraphCollection),
    JsonValue(JsonValue),
    JsonValueList(JsonValueList),
    JsonRecords(JsonRecords),
    FileMatches(FileMatches),
    TextMatches(TextMatches),
    HtmlExcerpts(HtmlExcerpts),
    FlattenedResultsBundle(FlattenedResultsBundle),
    GraphResultsBundle(GraphResultsBundle),
    TextFile(TextFile),
    BatchGroups(BatchGroups),
    SymbolTreeTableList(SymbolTreeTableList),
    Void,
}

/// A list of (searchfox) identifiers.
#[derive(Serialize)]
pub struct IdentifierList {
    pub identifiers: Vec<Ustr>,
}

#[derive(Serialize)]
pub struct SymbolWithContext {
    pub symbol: Ustr,
    pub quality: SymbolQuality,
    pub from_identifier: Option<Ustr>,
}

/// A list of (searchfox) symbols.
#[derive(Serialize)]
pub struct SymbolList {
    pub symbols: Vec<SymbolWithContext>,
}

/// Metadata about how we got to this symbol from the root query.  Intended to
/// help in clustering and/or results ordering.
#[derive(Clone, Serialize)]
pub enum SymbolRelation {
    /// The symbol was directly queried for.
    Queried,
    /// This symbol is an override of the payload symbol (and was added via that
    /// symbol by following the "overriddenBy" downward edges).  The u32 is the
    /// distance.
    OverrideOf(Ustr, u32),
    /// This symbol was overridden by the payload symbol (and was added via that
    /// symbol by following the "overrides" upward edges).  The u32 is the
    /// distance.
    OverriddenBy(Ustr, u32),
    /// This symbol is in the same root override set of the payload symbol (and
    /// was added by following that symbol's "overrides" upward edges and then
    /// "overriddenBy" downward edges), but is a cousin rather than an ancestor
    /// or descendant in the graph.  The u32 is the number of steps to get to
    /// the common ancestor.
    CousinOverrideOf(Ustr, u32),
    /// This symbol is a subclass of the payload symbol (and was added via that
    /// symbol by following the "subclasses" downward edges).  The u32 is the
    /// distance.
    SubclassOf(Ustr, u32),
    /// This symbol is a superclass of the payload symbol (and was added via
    /// that symbol by following the "supers" upward edges).  The u32 is the
    /// distance.
    SuperclassOf(Ustr, u32),
    /// This symbol is a cousin class of the payload symbol (and was added via
    /// that symbol by following the "supers" upward edges and then "subclasses"
    /// downward edges) with a distance indicating the number of steps to get to
    /// the common ancestor.
    CousinClassOf(Ustr, u32),
}

/// Metadata about how likely we think it is that the user was actually looking
/// for this symbol; primarily intended to capture whether or not we got to this
/// symbol by prefix search on an identifier and how much was guessed so that we
/// can scale any speculative effort appropriately, especially during
/// incremental search.
#[derive(Clone, PartialEq, Eq, Serialize)]
pub enum SymbolQuality {
    /// The symbol was explicitly specified and not the result of identifier
    /// lookup.
    ExplicitSymbol,
    /// The identifier was explicitly specified without prefix search enabled.
    ExplicitIdentifier,
    /// We did identifier search and the identifier was an exact match, but this
    /// was done in a context where we prefix search is also performed and
    /// expected.  The difference from `ExplicitIdentifier` here is that it can
    /// make sense to be more limited in automatically expanding the scope of
    /// results.
    ExactIdentifier,
    /// We did identifier search and the prefix matched; the values are how many
    /// characters matched and how many additional characters are in the
    /// identifier beyond the match point.  The latter number should always be
    /// at least 1, as 0 would make this `ExactIdentifier`.
    IdentifierPrefix(u32, u32),
}

impl SymbolQuality {
    /// Compute a quality rank where lower values are higher quality / closer to
    /// what the user typed.
    pub fn numeric_rank(&self) -> u32 {
        match self {
            SymbolQuality::ExplicitSymbol => 0,
            SymbolQuality::ExplicitIdentifier => 1,
            SymbolQuality::ExactIdentifier => 2,
            SymbolQuality::IdentifierPrefix(_matched, extra) => 2 + extra,
        }
    }
}

impl PartialOrd for SymbolQuality {
    fn partial_cmp(&self, other: &Self) -> Option<Ordering> {
        Some(self.cmp(other))
    }
}

impl Ord for SymbolQuality {
    fn cmp(&self, other: &Self) -> Ordering {
        let self_rank = self.numeric_rank();
        let other_rank = other.numeric_rank();
        self_rank.cmp(&other_rank)
    }
}

#[derive(Clone, Serialize)]
pub enum OverloadKind {
    /// There's just too many overrides!  This would happen for
    /// nsISupports::AddRef for example.
    Overrides,
    /// There's just too many subclasses!  This would happen for nsISupports for
    /// example.
    Subclasses,
    /// There's just too many uses!  This happens for mozilla::Runnable::Run for
    /// example when following the "uses" edges in traverse (and no other
    /// heuristics prevent considering the symbol when travelling up the
    /// override edge from the subclass).
    ///
    /// The Paths variant here captures that we gave up based on the number of
    /// paths which is a temporary thing (and we should change to Uses when
    /// correcting).
    UsesPaths,
    /// Uses limit but based on lines.
    UsesLines,
    FieldMemberUses,
    NodeLimit,
}

/// Information about overloads encountered when processing some aspect of a
/// symbol.  We've had a history of being unclear when limits are hit, so the
/// goal here is to be able to explicitly convey when we're hitting limits and
/// ideally to make it possible for the UI to generate links that can help the
/// user take an informed action to re-run with the limit bypassed.  (Our
/// concern is not so much abuse as much as it is about helping provide
/// consistently fast results as a user types a query and that the user opts in
/// to multi-second results rather than stumbling upon them.)
///
/// This is not currently intended to be used for `compile-results`, but could
/// perhaps be adapted for that.
#[derive(Clone, Serialize)]
pub struct OverloadInfo {
    pub kind: OverloadKind,
    /// The symbol, if any, this overload is associated with beyond the owner
    /// of this record.  That is, if this overload is hanging off a symbol's
    /// data structure (ex: SymbolCrossrefInfo), this field will be None unless
    /// we are characterizing and edge to another symbol.  But if this overload
    /// is hanging off a generic container, we may include a symbol name for
    /// additional context.  It's possible we might report the same overload in
    /// both places, even!  Overloads are intended as a diagnostic and for
    /// human comprehension to know the results have been truncated; we don't
    /// have to have a perfectly efficient data structure.
    pub sym: Option<String>,
    /// How many results do we think exist?
    pub exist: u32,
    /// How many results did we include before giving up?  This can be zero or
    /// otherwise less than the `limit`.
    pub included: u32,
    /// If this was a limit on this specific piece of data, what was the limit?
    /// 0 means there was no local limit hit (not that there was no limit).
    pub local_limit: u32,
    /// If this was a limit across multiple pieces of data, what was the limit?
    /// 0 means there was no global limit hit (not that there was no limit).
    pub global_limit: u32,
}

bitflags! {
    /// Experimental/hacky set of flags to enable a single pipeline to hold a
    /// heterogeneous mixture of symbols and where these flags are what makes
    /// the difference.  There is no unifying concept for the flags; it's fine
    /// to pile random semantics into this.
    ///
    /// Being introduced explicitly for calls-between-{source,target} with the
    /// fuse-crossrefs junction merging normal crossref-lookup outputs at the
    /// junction and setting these flags.  This works with the existing
    /// search-identifiers/crossref-lookup sequence flow, and especially if we
    /// added more filtering to the resulting pipeline, but arguably the search/
    /// lookup could be a single op which could allow "crossref-lookup" to
    /// be run repeatedly in sequence with each step adding new infos with new
    /// flags.
    #[derive(Clone, Copy, Default, Serialize)]
    pub struct SymbolMetaFlags: u32 {
        /// Mark a symbol as an interesting point for calls to start in a
        /// calls-between diagram.
        const Source = 0b00000001;
        /// Mark a symbol as an interesting point for calls to end up in a calls
        /// between diagram.
        const Target = 0b00000010;
    }
}
/// A symbol and its cross-reference information.
#[derive(Serialize)]
pub struct SymbolCrossrefInfo {
    pub symbol: Ustr,
    pub crossref_info: Value,
    pub relation: SymbolRelation,
    pub quality: SymbolQuality,
    /// Any overloads encountered when processing this symbol.
    pub overloads_hit: Vec<OverloadInfo>,
    #[serde(rename = "type", skip_serializing_if = "SymbolMetaFlags::is_empty")]
    pub flags: SymbolMetaFlags,
}

impl SymbolCrossrefInfo {
    /// Return the pretty identifier for this symbol from its "meta" "pretty"
    /// field, falling back to the symbol name if we don't have a pretty name.
    pub fn get_pretty(&self) -> Ustr {
        if let Some(Value::String(s)) = self.crossref_info.pointer("/meta/pretty") {
            ustr(s)
        } else {
            self.symbol
        }
    }

    pub fn get_method_symbols(&self) -> Option<Vec<Ustr>> {
        if let Some(Value::Array(arr)) = self.crossref_info.pointer("/meta/methods") {
            if arr.is_empty() {
                return None;
            }
            Some(
                arr.iter()
                    .map(|v| ustr(v["sym"].as_str().unwrap_or("")))
                    .collect(),
            )
        } else {
            None
        }
    }
}

/// A list of `SymbolCrossrefInfo`s plus a list of any unknown symbols provided
/// to the input.
#[derive(Serialize)]
pub struct SymbolCrossrefInfoList {
    pub symbol_crossref_infos: Vec<SymbolCrossrefInfo>,
    pub unknown_symbols: Vec<String>,
}

/// router.py-style mozsearch compiled results that has top-level path-kind
/// (normal/test/generated) result clusters, where each cluster has file names /
/// paths and line hits grouped by symbol-with-kind and by file name/path
/// beneath that.
///
/// Line results can contain raw source text or HTML-rendered excerpts if
/// augmented by the `show-html` command.
#[derive(Serialize)]
pub struct FlattenedResultsBundle {
    pub path_kind_results: Vec<FlattenedPathKindGroupResults>,
    pub content_type: String,
}

impl FlattenedResultsBundle {
    pub fn compute_path_line_sets(&self, before: u32, after: u32) -> UstrMap<HashSet<u32>> {
        let mut path_line_sets = UstrMap::default();
        for path_kind_group in &self.path_kind_results {
            path_kind_group.accumulate_path_line_sets(&mut path_line_sets, before, after);
        }
        path_line_sets
    }

    pub fn ingest_html_lines(
        &mut self,
        path_line_contents: &UstrMap<HashMap<u32, String>>,
        before: u32,
        after: u32,
    ) {
        self.content_type = "text/html".to_string();
        for path_kind_group in &mut self.path_kind_results {
            path_kind_group.ingest_html_lines(path_line_contents, before, after);
        }
    }
}

#[derive(Serialize)]
pub struct FlattenedPathKindGroupResults {
    pub path_kind: Ustr,
    pub file_names: Vec<Ustr>,
    pub kind_groups: Vec<FlattenedKindGroupResults>,
}

impl FlattenedPathKindGroupResults {
    pub fn accumulate_path_line_sets(
        &self,
        path_line_sets: &mut UstrMap<HashSet<u32>>,
        before: u32,
        after: u32,
    ) {
        for kind_group in &self.kind_groups {
            kind_group.accumulate_path_line_sets(path_line_sets, before, after);
        }
    }

    pub fn ingest_html_lines(
        &mut self,
        path_line_contents: &UstrMap<HashMap<u32, String>>,
        before: u32,
        after: u32,
    ) {
        for kind_group in &mut self.kind_groups {
            kind_group.ingest_html_lines(path_line_contents, before, after);
        }
    }
}

#[derive(Serialize)]
pub enum ResultFacetKind {
    /// We're faceting based on the relationship of symbols to the root symbol.
    SymbolByRelation,
    /// We're faceting based on the path of the definition for the symbol.
    PathByPath,
}

/// A context-sensitive facet for results.  Facets are only created when
/// multiple usefully sized groups would exist for the facet.  If there would
/// only be a single group, or there would be N groups for N results, then the
/// facet would not be useful and will not be emitted.
#[derive(Serialize)]
pub struct ResultFacetRoot {
    /// Terse human-readable explanation of the facet for UI display.
    pub label: String,
    pub kind: ResultFacetKind,
    pub groups: Vec<ResultFacetGroup>,
}

/// Hierarchical faceting group that gets nested inside a `ResultFacetRoot`.
#[derive(Serialize)]
pub struct ResultFacetGroup {
    /// Terse human-readable explanation of the facet for UI display.
    pub label: String,
    pub values: Vec<Ustr>,
    pub nested_groups: Vec<ResultFacetGroup>,
    /// The number of hits for this group, inclusive of nested groups.  This
    /// value should be equal to the sum of all of the nested_groups' counts if
    /// there are any nested groups.
    pub count: u32,
}

#[derive(Clone, PartialEq, PartialOrd, Eq, Ord, Serialize)]
pub enum PresentationKind {
    // We don't give "Files" a kind because they don't look like path hit-lists.
    IDL,
    Definitions,
    Declarations,
    Assignments,
    Uses,
    // We do give textual occurrences a kind because they are path hit-lists.
    TextualOccurrences,
}

#[derive(Serialize)]
pub struct FlattenedKindGroupResults {
    pub kind: PresentationKind,
    pub pretty: Ustr,
    pub facets: Vec<ResultFacetRoot>,
    pub by_file: Vec<FlattenedResultsByFile>,
}

impl FlattenedKindGroupResults {
    pub fn accumulate_path_line_sets(
        &self,
        path_line_sets: &mut UstrMap<HashSet<u32>>,
        before: u32,
        after: u32,
    ) {
        for by_file in &self.by_file {
            by_file.accumulate_path_line_sets(path_line_sets, before, after);
        }
    }

    pub fn ingest_html_lines(
        &mut self,
        path_line_contents: &UstrMap<HashMap<u32, String>>,
        before: u32,
        after: u32,
    ) {
        for by_file in &mut self.by_file {
            by_file.ingest_html_lines(path_line_contents, before, after);
        }
    }
}

#[derive(Serialize)]
pub struct FlattenedResultsByFile {
    pub file: Ustr,
    pub line_spans: Vec<FlattenedLineSpan>,
}

impl FlattenedResultsByFile {
    pub fn accumulate_path_line_sets(
        &self,
        path_line_sets: &mut UstrMap<HashSet<u32>>,
        before: u32,
        after: u32,
    ) {
        let line_set = path_line_sets.entry(self.file).or_default();
        for span in &self.line_spans {
            let range = span.expand_range_in_isolation(before, after);
            for line in range.0..=range.1 {
                line_set.insert(line);
            }
        }
    }

    pub fn ingest_html_lines(
        &mut self,
        path_line_contents: &UstrMap<HashMap<u32, String>>,
        before: u32,
        after: u32,
    ) {
        if let Some(file_contents) = path_line_contents.get(&self.file) {
            let mut highest_line: u32 = 0;
            for i_span in 0..self.line_spans.len() {
                let (mut this_start, mut this_end) =
                    self.line_spans[i_span].expand_range_in_isolation(before, after);
                // adjust to avoid overlapping the prior span.
                if this_start <= highest_line {
                    this_start = highest_line + 1;
                }
                // avoid bumping into the next span if there is one
                if i_span < self.line_spans.len() - 1 {
                    let next_start = self.line_spans[i_span + 1].line_range.0;
                    if this_end >= next_start {
                        this_end = next_start - 1;
                    }
                }

                let mut lines = vec![];
                for line in this_start..=this_end {
                    if let Some(content) = file_contents.get(&line) {
                        lines.push(content.as_str());
                    }
                }
                // this_end was aspirational; we may have run out of lines,
                // so use the length.
                self.line_spans[i_span].line_range =
                    (this_start, this_start + (lines.len() - 1) as u32);
                self.line_spans[i_span].contents = lines.join("\n");

                highest_line = this_end;
            }
        }
    }
}

/// Represents a range of lines in a file.
#[derive(Serialize)]
pub struct FlattenedLineSpan {
    /// Canonical line number for this span of lines; the one that should be
    /// highlighted and the key term should be found in. 1-based line numbers.
    pub key_line: u32,
    /// The range of lines the core content results should include; when there's
    /// a block comment preceding something or if the statement/expression spans
    /// multiple lines, this could potentially be larger than just the key_line.
    pub line_range: (u32, u32),
    /// When the FlattenedResultsBundle has a `content_type` of "text/plain"
    /// this is expected to just be the single line of plaintext included in the
    /// `crossref` database.  When the type is "text/html" this is expected to
    /// be the formatted HTML output mutated into place by `ingest_html_lines`
    /// as provided by `cmd_augment_results.rs` and in that case before/after
    /// lines of context may be provided here but not incorporated into the
    /// `line_range` above.
    pub contents: String,
    // context and contextsym are normalized to empty upstream of here instead
    // of being `Option<String>` so we just maintain that for now.
    pub context: Ustr,
    pub contextsym: Ustr,
}

impl FlattenedLineSpan {
    /// Expand the range by before/after, ensuring we don't go below line 1 for
    /// the start, and ignoring the fact that we potentially will expand into
    /// adjacent spans.
    pub fn expand_range_in_isolation(&self, before: u32, after: u32) -> (u32, u32) {
        let start = std::cmp::max(1, self.line_range.0 as i64 - before as i64) as u32;
        let end = self.line_range.1 + after;
        (start, end)
    }
}

/// Rendered graphs and associated metadata.
#[derive(Serialize)]
pub struct GraphResultsBundle {
    pub graphs: Vec<RenderedGraph>,
    pub overloads_hit: Vec<OverloadInfo>,
    pub symbols: Value,
}

#[derive(Serialize)]
pub struct RenderedGraph {
    pub graph: String,
    pub extra: Value,
}

/// JSON records are raw analysis records from a single file (for now)
#[derive(Serialize)]
pub struct JsonRecordsByFile {
    pub file: String,
    pub records: Vec<Value>,
}

impl JsonRecordsByFile {
    /// Return the set of lines covered by the records in this structure.
    ///
    /// A HashSet is returned for ease of consumption even though it would
    /// almost certainly be more efficient to return a vec that the caller
    /// caller can consume in concert with a parallel traversal of (ex) the
    /// generated HTML for the given file.
    pub fn line_set(&self) -> HashSet<u32> {
        let mut line_set = HashSet::new();
        for value in &self.records {
            if let Some(loc) = value["loc"].as_str() {
                let lno = loc.split(":").next().unwrap_or("0").parse().unwrap_or(0);
                line_set.insert(lno);
            }
        }

        line_set
    }
}

/// A single JSON value, usually expected to be from a search query.
///
/// It might make sense to add a type-indicating value or origin of the JSON,
/// but for now this will only be from the query.
#[derive(Serialize)]
pub struct JsonValue {
    pub value: Value,
}

/// Multiple JSON values, as wrapped into a JsonValue.  While any values stored
/// here can obviously just be placed into a JSON array instead, the intent is
/// to avoid confusing aggregation for transport like we're doing here versus
/// what an endpoint that's not aggregating would return.
///
/// This does mean that the JSON serialization of this struct will look a little
/// awkward, but this will make it easier if we start labeling the JsonValue
/// values with their source/etc.
#[derive(Serialize)]
pub struct JsonValueList {
    pub values: Vec<JsonValue>,
}

/// JSON Analysis Records grouped by (source) file.
#[derive(Serialize)]
pub struct JsonRecords {
    pub by_file: Vec<JsonRecordsByFile>,
}

#[derive(Serialize)]
pub struct HtmlExcerptsByFile {
    pub file: String,
    pub excerpts: Vec<String>,
}

#[derive(Serialize)]
pub struct HtmlExcerpts {
    pub by_file: Vec<HtmlExcerptsByFile>,
}

#[derive(Serialize)]
pub struct TextFile {
    pub mime_type: String,
    pub contents: String,
}

/// A command that takes a single input and produces a single output.  At the
/// start of the pipeline, the input may be ignored / expected to be void.
#[async_trait]
pub trait PipelineCommand: Debug {
    async fn execute(
        &self,
        server: &(dyn AbstractServer + Send + Sync),
        input: PipelineValues,
    ) -> Result<PipelineValues>;
}

/// A command that takes multiple inputs and produces a single output.
/// XXX speculative while implementing parallel processing.
#[async_trait]
pub trait PipelineJunctionCommand: Debug {
    async fn execute(
        &self,
        server: &(dyn AbstractServer + Send + Sync),
        input: Vec<(String, PipelineValues)>,
    ) -> Result<PipelineValues>;
}

/// Multiple-use linear pipeline sequence.
pub struct ServerPipeline {
    pub server_kind: String,
    pub server: Box<dyn AbstractServer + Send + Sync>,
    pub commands: Vec<Box<dyn PipelineCommand + Send + Sync>>,
}

/// A linear pipeline sequence that potentially runs in parallel with other
/// named pipelines in a `ParallelPipelines` node which can be one in a sequence
/// of `ParallelPipelines` in a `ServerpipelineGraph`.  Inputs and outputs are
/// consumed from and added to a global dictionary.
pub struct NamedPipeline {
    /// Previous pipeline's output to consume.
    pub input_name: Option<String>,
    pub output_name: String,
    pub commands: Vec<Box<dyn PipelineCommand + Send + Sync>>,
}

impl NamedPipeline {
    pub async fn run(
        self,
        server: Box<dyn AbstractServer + Send + Sync>,
        mut cur_values: PipelineValues,
        traced: bool,
    ) -> Result<PipelineValues> {
        for cmd in &self.commands {
            let span = trace_span!("run_named_pipeline_step", cmd = ?cmd);

            match cmd
                .execute(server.as_ref(), cur_values)
                .instrument(span.clone())
                .await
            {
                Ok(next_values) => {
                    cur_values = next_values;
                }
                Err(err) => {
                    trace!(err = ?err);
                    return Err(err);
                }
            }

            let _span_guard = span.entered();
            if traced {
                let value_str = to_string_pretty(&cur_values).unwrap();
                trace!(output_json = %value_str);
            }
        }

        Ok(cur_values)
    }
}

/// Consumes one or more inputs from the `NamedPipeline`s that ran prior to it
/// in the same `ParallelPipelines` node or possibly an earlier
/// `ParallelPipelines` node, producting a new output.  Inputs and outputs are
/// consumed from and added to a global dictionary.
pub struct JunctionInvocation {
    pub input_names: Vec<String>,
    pub output_name: String,
    pub command: Box<dyn PipelineJunctionCommand + Send + Sync>,
}

impl JunctionInvocation {
    pub async fn run(
        self,
        server: Box<dyn AbstractServer + Send + Sync>,
        input_values: Vec<(String, PipelineValues)>,
        traced: bool,
    ) -> Result<PipelineValues> {
        let span = trace_span!("run junction step", junction = ?self.command);

        let result = match self
            .command
            .execute(server.as_ref(), input_values)
            .instrument(span.clone())
            .await
        {
            Ok(res) => res,
            Err(err) => {
                trace!(err = ?err);
                return Err(err);
            }
        };

        let _span_guard = span.entered();
        if traced {
            let value_str = to_string_pretty(&result).unwrap();
            trace!(output_json = %value_str);
        }

        Ok(result)
    }
}

pub struct ParallelPipelines {
    pub pipelines: Vec<NamedPipeline>,
    pub junctions: Vec<JunctionInvocation>,
}

/// Single-use pipeline graph.  Calling `run` consumes the graph for lifetime
/// simplicity because multiple parallel tasks are run and the borrows end up
/// awkward.  Also, we always expect the graphs to be built dynamically for each
/// use so we don't actually want to be able to reuse graphs at this time.
pub struct ServerPipelineGraph {
    pub server: Box<dyn AbstractServer + Send + Sync>,
    pub pipelines: Vec<ParallelPipelines>,
}

impl ServerPipeline {
    pub async fn run(&self, traced: bool) -> Result<PipelineValues> {
        let mut cur_values = PipelineValues::Void;

        for cmd in &self.commands {
            let span = trace_span!("run_pipeline_step", cmd = ?cmd);

            match cmd
                .execute(self.server.as_ref(), cur_values)
                .instrument(span.clone())
                .await
            {
                Ok(next_values) => {
                    cur_values = next_values;
                }
                Err(err) => {
                    trace!(err = ?err);
                    return Err(err);
                }
            }

            let _span_guard = span.entered();
            if traced {
                let value_str = to_string_pretty(&cur_values).unwrap();
                trace!(output_json = %value_str);
            }
        }

        Ok(cur_values)
    }
}

impl ServerPipelineGraph {
    pub async fn run(self, traced: bool) -> Result<PipelineValues> {
        let mut named_values: BTreeMap<String, PipelineValues> = BTreeMap::new();

        for pipeline in self.pipelines {
            // ## kick off all the named pipelines in parallel
            let mut pipeline_tasks = vec![];
            for named_pipeline in pipeline.pipelines {
                let output = named_pipeline.output_name.clone();
                let input = match &named_pipeline.input_name {
                    Some(name) => {
                        // TODO: There could be cases like for compile-results
                        // where we would want a second pipeline to be able to
                        // consume the same input.
                        match named_values.remove(name) {
                            Some(val) => val,
                            None => PipelineValues::Void,
                        }
                    }
                    None => PipelineValues::Void,
                };
                let span = trace_span!("pipeline_task", input_name=?named_pipeline.input_name, output_name=?named_pipeline.output_name).or_current();
                pipeline_tasks.push((
                    output,
                    tokio::spawn(
                        named_pipeline
                            .run(self.server.clonify(), input, traced)
                            .instrument(span),
                    ),
                ));
            }

            // ## join the pipelines in sequence
            for (output, handle) in pipeline_tasks {
                let value = handle.await??;
                named_values.insert(output, value);
            }

            // ## kick off junctions in parallel
            let mut junction_tasks = vec![];
            for junction in pipeline.junctions {
                let output = junction.output_name.clone();
                let mut input_values = vec![];
                for name in &junction.input_names {
                    input_values.push((
                        name.clone(),
                        match named_values.remove(name) {
                            Some(val) => val,
                            None => PipelineValues::Void,
                        },
                    ));
                }

                let span = trace_span!("junction_task", input_names=?junction.input_names, output_name=?junction.output_name).or_current();
                junction_tasks.push((
                    output,
                    tokio::spawn(
                        junction
                            .run(self.server.clonify(), input_values, traced)
                            .instrument(span),
                    ),
                ));
            }

            for (output, handle) in junction_tasks {
                let value = handle.await??;
                named_values.insert(output, value);
            }
        }

        Ok(match named_values.remove("result") {
            Some(val) => val,
            None => PipelineValues::Void,
        })
    }
}

```

## tools/src/cmd_pipeline/hier_graph.rs
```
/**
Prototyping pass on building a hierarchical graph representation akin to the
fancy-branch HierBuild/HierNode representation as defined and used in:
- https://github.com/asutherland/mozsearch/blob/fancy/ui/src/grokysis/frontend/diagramming/core_diagram.js
- https://github.com/asutherland/mozsearch/blob/fancy/ui/src/grokysis/frontend/diagramming/class_diagram.js
- https://github.com/asutherland/mozsearch/blob/fancy/ui/src/grok-ui/blockly/hiernode_generator.js

### Defining the Domain

#### Fancy Branch Prototyping Lessons Learned

Fancy branch prototyping resulted in 2 primary ideas for HierNode modeling:

1. Taking a node/edge graph and mapping the nodes into a hierarchical namespace
   and then applying various clustering approaches depending on heuristics that
   leveraged domain-knowledge (classes' ownership of methods/fields is well
   suited to table displays) and awareness of edge complexity (better to not
   use a table if there are a tons of intra-class edges!).

   - Although this didn't get prototyped, it was hypothesized that being able
     to view containment along file hierarchies could be useful either as its
     own axis or as something that could be interposed as an additional layer.
     It definitely would be useful if re-used for faceting, although that might
     not want to reuse the same graphing models.

2. The clustering presentation is also suitable for display of runtime behavior
   and data modeling, and this requires that we be able to have a concept of
   instancing of classes or conceptual objects (ex: windows for different
   origins).

#### Mapping

It seems like having a strongly typed path segment mechanism that can be
directly consumed at a presentation level without having to do any inference
would likely be useful.  Something like:
- PrettySymbol { pretty_segment, pretty_full, symbol }
- OSConcept { os_kind: [process, thread]}
- WebConcept { app_kind: [window, document, worker] }

Of course, having a strongly defined set of concepts in searchfox doesn't seem
useful, so we can perhaps unify that to:
- Concept { namespace, term, name }

This would give us:
- { namespace: "os", term: "thread", name: "main" }
- { namespace: "web", term: "window", name: "example.com" }
- { namespace: "web", term: "origin", name: "example.com" }

This could then be supplemented by an orthogonal instance tagging approach which
would tuple whatever it is attached to at every level of the hierarchy.  This
could be used for:
- Manually created examples.
- Runtime extraction from pernosco via pernosco-bridge where instances could
  could refer into pernosco-bridge's instance identifier map (which is something
  pernosco might be able to provide directly in the future).
- Runtime data retrieved from logs.  For example, GELAM/Workshop.
- Speculative: static specialization of values.  For example, discriminating
  between IPC enum states.

The ability of the concept to carry, for example, an origin name would probably
want to be symbiotic with the use of instances.  So a "window" with an origin
"example.com" would have an instance applied to it, as well as all of the
symbols emplaced within it.  The origin on the window would then be a
redundantly encoded piece of information for readability.  The window concept
would provide a label, whereas the instances would manifest as a color scheme
with an associated legend, but no direct text presentation.

### Relationship to Symbol Graphs; Internal Representation

The hierarchical representation is primarily a transformation from the raw
graph representation into something intended for human consumption.  We have no
foreseen need to be able to run efficient graph algorithms, so we don't need
to build our representation around petgraph.  In fact, our primary concern about
edges is being able to make sure they are emitted at the appropriate level of
the graphviz cluster hierarchy.
*/

pub struct PrettySymbolSegment {
  pretty_segment: String,
  pretty_full: String,
  symbols: Vec<String>,
}

pub struct ConceptSegment {
  namespace: String,
  term: String,
  name: String,
}

pub enum HierPathSegment {
  PrettySymbol(PrettySymbolSegment),
  Concept(ConceptSegment),
}

pub struct HierNode {
  id: String,
  segment: HierPathSegment,
  children: Vec<HierNode>,
}

```

## tools/src/cmd_pipeline/mod.rs
```
extern crate clap;

pub mod builder;
pub mod interface;
pub mod parser;
pub mod symbol_graph;
pub mod transforms;

mod cmd_augment_results;
mod cmd_batch_render;
mod cmd_cat_html;
mod cmd_compile_results;
mod cmd_crossref_expand;
mod cmd_crossref_lookup;
mod cmd_filter_analysis;
mod cmd_format_symbols;
mod cmd_fuse_crossrefs;
mod cmd_graph;
mod cmd_jumpref_lookup;
mod cmd_merge_analyses;
mod cmd_prod_filter;
mod cmd_query;
mod cmd_render;
mod cmd_search;
mod cmd_search_files;
mod cmd_search_identifiers;
mod cmd_search_text;
mod cmd_show_html;
mod cmd_tokenize_source;
mod cmd_traverse;
mod cmd_webtest;

pub use builder::build_pipeline;
pub use interface::{PipelineCommand, PipelineValues};

```

## tools/src/cmd_pipeline/cmd_search_files.rs
```
use async_trait::async_trait;
use clap::{Args, ValueEnum};
use itertools::Itertools;

use super::{
    interface::{BatchGroupItem, BatchGroups, PipelineCommand, PipelineValues},
    transforms::path_glob_transform,
};

use crate::abstract_server::{AbstractServer, FileMatches, Result};

#[derive(Clone, Debug, PartialEq, ValueEnum)]
pub enum GroupFilesBy {
    Directory,
}

/// Perform a fulltext search against our livegrep/codesearch server over gRPC.
/// This is local-only at this time.
#[derive(Debug, Args)]
pub struct SearchFiles {
    /// Path to search for; this will be searchfox glob-transformed.
    #[clap(value_parser)]
    path: Option<String>,

    /// Constrain matching path patterns with a regexp.
    #[clap(long, value_parser)]
    pathre: Option<String>,

    #[clap(short, long, value_parser, default_value = "2000")]
    limit: usize,

    #[clap(long, value_parser)]
    include_dirs: bool,

    #[clap(long, short, value_parser, value_enum)]
    group_by: Option<GroupFilesBy>,
}

#[derive(Debug)]
pub struct SearchFilesCommand {
    pub args: SearchFiles,
}

const FILE_MATCH_LIMIT: usize = 2_000_000;

#[async_trait]
impl PipelineCommand for SearchFilesCommand {
    async fn execute(
        &self,
        server: &(dyn AbstractServer + Send + Sync),
        _input: PipelineValues,
    ) -> Result<PipelineValues> {
        let pathre_pattern = if let Some(pathre) = &self.args.pathre {
            pathre.clone()
        } else if let Some(path) = &self.args.path {
            path_glob_transform(path)
        } else {
            "".to_string()
        };

        // A zero limit implies no limit, but the server currently needs us to
        // provide a limit because it uses take().  Also, it's probably
        // reasonable to have a bit of a limit, so we also use this as a max.
        let use_limit = if self.args.limit == 0 || self.args.limit > FILE_MATCH_LIMIT {
            FILE_MATCH_LIMIT
        } else {
            self.args.limit
        };

        let matches = server
            .search_files(&pathre_pattern, self.args.include_dirs, use_limit)
            .await?;

        match self.args.group_by {
            Some(GroupFilesBy::Directory) => {
                let groups: Vec<_> = matches
                    .file_matches
                    .into_iter()
                    .into_group_map_by(|f| f.get_containing_dir())
                    .into_iter()
                    .map(|(dir, matches)| BatchGroupItem {
                        name: dir.to_string(),
                        value: PipelineValues::FileMatches(FileMatches {
                            file_matches: matches,
                        }),
                    })
                    .collect();

                Ok(PipelineValues::BatchGroups(BatchGroups { groups }))
            }
            None => Ok(PipelineValues::FileMatches(matches)),
        }
    }
}

```

## tools/src/cmd_pipeline/cmd_merge_analyses.rs
```
use async_trait::async_trait;
use clap::Args;
use serde_json::{from_str, Value};

use super::interface::{JsonRecords, PipelineCommand, PipelineValues};
use crate::{
    abstract_server::{AbstractServer, Result, ServerError},
    cmd_pipeline::interface::JsonRecordsByFile,
    file_format::merger::merge_files,
};

/// Merge analysis files from different build configs into one analysis file.
#[derive(Debug, Args)]
pub struct MergeAnalyses {
    /// Tree-relative analysis file paths
    #[clap(value_parser)]
    files: Vec<String>,

    /// The list of platforms to claim the files came from.
    #[clap(long, short, value_parser)]
    platforms: Vec<String>,
}

/// Command brought into existence to test the analysis-merging logic of
/// `merge-analyses.rs`.
#[derive(Debug)]
pub struct MergeAnalysesCommand {
    pub args: MergeAnalyses,
}

#[async_trait]
impl PipelineCommand for MergeAnalysesCommand {
    async fn execute(
        &self,
        server: &(dyn AbstractServer + Send + Sync),
        _input: PipelineValues,
    ) -> Result<PipelineValues> {
        let abs_paths: Result<Vec<String>> = self
            .args
            .files
            .iter()
            .map(|f| {
                server.translate_path(
                    crate::abstract_server::SearchfoxIndexRoot::CompressedAnalysis,
                    f,
                )
            })
            .collect();

        let mut merged_output = Vec::new();
        merge_files(&abs_paths?, &self.args.platforms, &mut merged_output);

        let values: Result<Vec<Value>> = std::str::from_utf8(merged_output.as_slice())
            .unwrap()
            .lines()
            .map(|s| from_str(s).map_err(ServerError::from))
            .collect();

        Ok(PipelineValues::JsonRecords(JsonRecords {
            by_file: vec![JsonRecordsByFile {
                file: self.args.files.first().unwrap().clone(),
                records: values?,
            }],
        }))
    }
}

```

## tools/src/cmd_pipeline/transforms.rs
```
use regex::{Captures, Regex};

/// Apply the searchfox path glob transformation ported from `router.py`.
pub fn path_glob_transform(s: &str) -> String {
    lazy_static! {
        static ref RE_TO_ESCAPE: Regex = Regex::new("[()|.]").unwrap();
        static ref RE_STARS: Regex = Regex::new(r"\*\*?").unwrap();
        static ref RE_BRACES: Regex = Regex::new(r"\{([^}]*)\}").unwrap();
    }
    let backslashed = RE_TO_ESCAPE.replace_all(s, r"\$0");
    let starred = RE_STARS.replace_all(&backslashed, |caps: &Captures| {
        if caps.get(0).unwrap().as_str().len() == 1 {
            "[^/]*"
        } else {
            ".*"
        }
    });
    let questioned = str::replace(&starred, "?", ".");
    let braced = RE_BRACES.replace_all(&questioned, |caps: &Captures| {
        let inside_braces = caps.get(1).unwrap().as_str();
        format!("({})", str::replace(inside_braces, ",", "|"))
    });
    braced.to_string()
}

#[test]
fn test_path_glob_transform() {
    // Test coverage for the cases we documented on the help page.
    assert_eq!(path_glob_transform("test"), "test");
    assert_eq!(path_glob_transform("^js/src"), "^js/src");

    assert_eq!(path_glob_transform("*.cpp"), "[^/]*\\.cpp");
    assert_eq!(path_glob_transform("*.cpp$"), "[^/]*\\.cpp$");

    assert_eq!(
        path_glob_transform("^js/src/*.cpp$"),
        "^js/src/[^/]*\\.cpp$"
    );
    assert_eq!(path_glob_transform("^js/src/**.cpp$"), "^js/src/.*\\.cpp$");
    assert_eq!(
        path_glob_transform("^js/src/**.{cpp,h}$"),
        "^js/src/.*\\.(cpp|h)$"
    );
}

```

## tools/src/cmd_pipeline/cmd_crossref_lookup.rs
```
use async_trait::async_trait;
use clap::Args;
use ustr::{ustr, Ustr};

use super::interface::{
    PipelineCommand, PipelineValues, SymbolCrossrefInfo, SymbolCrossrefInfoList, SymbolMetaFlags,
    SymbolQuality, SymbolRelation,
};

use crate::abstract_server::{AbstractServer, ErrorDetails, ErrorLayer, Result, ServerError};

/// Return the crossref data for one or more symbols received via pipeline or as
/// explicit arguments.
#[derive(Debug, Args)]
pub struct CrossrefLookup {
    /// Explicit symbols to lookup.
    #[clap(value_parser)]
    symbols: Vec<String>,
    // TODO: It might make sense to provide a way to filter the looked up data
    // by kind, although that could of course be its own command too.
    /// If the looked up symbol turns out to be a class with methods, instead of
    /// adding the class to the set, add its methods.
    #[clap(long, action)]
    methods: bool,

    /// Discards symbols whose pretty identifier is not an exact match for the
    /// symbol's `from_ident`.  This allows us to discard symbols from
    /// search-identifiers which were not an absolute identifier match.
    #[clap(short, long, value_parser)]
    exact_match: bool,
}

#[derive(Debug)]
pub struct CrossrefLookupCommand {
    pub args: CrossrefLookup,
}

#[async_trait]
impl PipelineCommand for CrossrefLookupCommand {
    async fn execute(
        &self,
        server: &(dyn AbstractServer + Send + Sync),
        input: PipelineValues,
    ) -> Result<PipelineValues> {
        // Because this pipeline stage can receive symbols from unfiltered user
        // input and we have no reason to believe the `Ustr` interned symbol
        // table contains all potentially known strings, we must operate in
        // String space until we get values back from the crossref lookup!
        let symbol_list: Vec<(String, SymbolQuality, Option<Ustr>)> = match input {
            PipelineValues::SymbolList(sl) => sl
                .symbols
                .into_iter()
                .map(|info| (info.symbol.to_string(), info.quality, info.from_identifier))
                .collect(),
            // Right now we're assuming that we're the first command in the
            // pipeline so that we would have no inputs if someone wants to use
            // arguments...
            PipelineValues::Void => self
                .args
                .symbols
                .iter()
                .map(|sym| (sym.clone(), SymbolQuality::ExplicitSymbol, None))
                .collect(),
            _ => {
                return Err(ServerError::StickyProblem(ErrorDetails {
                    layer: ErrorLayer::ConfigLayer,
                    message: "crossref-lookup needs a Void or SymbolList".to_string(),
                }));
            }
        };

        let mut symbol_crossref_infos = vec![];
        let mut unknown_symbols = vec![];
        for (symbol, quality, from_ident) in symbol_list {
            let info = server.crossref_lookup(&symbol, false).await?;

            if info.is_null() {
                unknown_symbols.push(symbol);
                continue;
            }

            let crossref_info = SymbolCrossrefInfo {
                // Now that we've validted that the symbol exists via crossref
                // lookup, we know it's safe to mint a Ustr for it if it doesn't
                // exist.  (Otherwise hostile/broken callers could explode our
                // interning table.)
                symbol: ustr(&symbol),
                crossref_info: info,
                relation: SymbolRelation::Queried,
                quality,
                overloads_hit: vec![],
                flags: SymbolMetaFlags::default(),
            };
            if let (true, Some(pretty)) = (self.args.exact_match, from_ident) {
                if pretty.to_lowercase() != crossref_info.get_pretty().to_lowercase() {
                    continue;
                }
            }
            if self.args.methods {
                if let Some(method_syms) = crossref_info.get_method_symbols() {
                    for method_sym in method_syms {
                        let method_info = server.crossref_lookup(&method_sym, false).await?;
                        symbol_crossref_infos.push(SymbolCrossrefInfo {
                            symbol: method_sym,
                            crossref_info: method_info,
                            relation: SymbolRelation::Queried,
                            quality: crossref_info.quality.clone(),
                            overloads_hit: vec![],
                            flags: SymbolMetaFlags::default(),
                        });
                    }
                    continue;
                }
            }

            symbol_crossref_infos.push(crossref_info);
        }

        Ok(PipelineValues::SymbolCrossrefInfoList(
            SymbolCrossrefInfoList {
                symbol_crossref_infos,
                unknown_symbols,
            },
        ))
    }
}

```

## tools/src/cmd_pipeline/cmd_prod_filter.rs
```
use async_trait::async_trait;
use clap::Args;
use lazy_static::lazy_static;
use lol_html::{element, rewrite_str, RewriteStrSettings};
use regex::Regex;
use serde_json::Value;

use super::interface::{
    JsonRecords, JsonRecordsByFile, JsonValue, PipelineCommand, PipelineValues,
};
use crate::{
    abstract_server::{AbstractServer, Result},
    cmd_pipeline::interface::{HtmlExcerpts, HtmlExcerptsByFile},
};

/// Normalize HTML or JSON records for production environment checks so that
/// details like line numbers or `data-i` indexes that are subject to churn
/// due to changes elsewhere in the file are normalized to "NORM".
#[derive(Debug, Args)]
pub struct ProductionFilter {}

#[allow(dead_code)]
#[derive(Debug)]
pub struct ProductionFilterCommand {
    pub args: ProductionFilter,
}

/// Normalize JSON values by:
/// - Converting "loc" properties from "LINE:COL-COL" to "NORML:COL-COL".
fn norm_json_value(mut val: Value) -> Value {
    lazy_static! {
        // The column portion can potentially be singular I think so we just
        // treat the half as its own group.
        static ref RE: Regex = Regex::new(r"^(?P<line>\d+):(?P<cols>.+)$").unwrap();
    }

    if let Some(loc_ref) = val.pointer_mut("/loc") {
        let existing_loc = loc_ref.as_str().unwrap();
        *loc_ref = RE.replace_all(existing_loc, "NORM:$cols").into();
    }

    val
}

/// Normalize HTML values by:
/// - Stripping .cov-strip elements.
/// - Stripping .blame-strip elements.
/// - Replacing line numbers with N
/// - Replacing data-i values with "NORM".
fn norm_html_value(s: String) -> String {
    let element_content_handlers = vec![
        element!(r#"div.cov-strip, div.blame-strip"#, |el| {
            el.remove();
            Ok(())
        }),
        // As a transient thing, remove data-i entirely since this will allow us
        // to update the production checks before landing.  This rule can be
        // removed after we've transitioned as "data-i" should no longer exist.
        element!(r#"span[data-i]"#, |el| {
            el.remove_attribute("data-i");
            Ok(())
        }),
        element!(r#"div.source-line-with-number"#, |el| {
            el.set_attribute("id", "line-NORM").unwrap();
            Ok(())
        }),
        element!(r#"div.line-number"#, |el| {
            el.set_attribute("data-line-number", "NORM").unwrap();
            Ok(())
        }),
    ];

    rewrite_str(
        &s,
        RewriteStrSettings {
            element_content_handlers,
            ..RewriteStrSettings::default()
        },
    )
    .unwrap()
}

#[async_trait]
impl PipelineCommand for ProductionFilterCommand {
    async fn execute(
        &self,
        _server: &(dyn AbstractServer + Send + Sync),
        input: PipelineValues,
    ) -> Result<PipelineValues> {
        Ok(match input {
            PipelineValues::JsonRecords(jr) => PipelineValues::JsonRecords(JsonRecords {
                by_file: jr
                    .by_file
                    .into_iter()
                    .map(|jrbf| JsonRecordsByFile {
                        file: jrbf.file,
                        records: jrbf.records.into_iter().map(norm_json_value).collect(),
                    })
                    .collect(),
            }),
            PipelineValues::HtmlExcerpts(he) => PipelineValues::HtmlExcerpts(HtmlExcerpts {
                by_file: he
                    .by_file
                    .into_iter()
                    .map(|hebf| HtmlExcerptsByFile {
                        file: hebf.file,
                        excerpts: hebf.excerpts.into_iter().map(norm_html_value).collect(),
                    })
                    .collect(),
            }),
            // We don't currently handle a lone JsonValue but I guess it could
            // just be the JsonRecords case that gets wrapped and then
            // unwrapped?
            PipelineValues::JsonValue(jv) => PipelineValues::JsonValue(JsonValue {
                value: norm_json_value(jv.value),
            }),
            other => other,
        })
    }
}

```

## tools/src/format.rs
```
use std::collections::{BTreeMap, HashMap};
use std::env;
use std::io::Write;
use std::ops::Deref;
use std::path::Path;
use std::process::Command;
use std::time::Instant;

use crate::blame;
use crate::file_format::analysis_manglings::make_file_sym_from_path;
use crate::file_format::crossref_converter::{
    determine_desired_extra_syms_from_jumpref, extra_syms_next_step_lookups, JumprefTraversals,
};
use crate::file_format::crossref_lookup::CrossrefLookupMap;
use crate::git_ops;
use crate::languages;
use crate::languages::FormatAs;
use crate::links;
use crate::tokenize;

use crate::file_format::analysis::{AnalysisSource, ExpansionInfo, WithLocation};
use crate::file_format::config::{Config, GitData, TreeConfig};
use crate::output::{self, Options, PanelItem, PanelSection, F};
use crate::url_encode_path::url_encode_path;

use chrono::datetime::DateTime;
use chrono::naive::datetime::NaiveDateTime;
use chrono::offset::fixed::FixedOffset;
use itertools::Itertools;
use serde_json::{json, to_string, to_string_pretty, Map};
use ustr::{ustr, Ustr, UstrMap};

#[derive(Debug)]
pub struct FormattedLine {
    pub line: String,
    // If this line should open a new <div> and its <code> line should be position: sticky, this
    // has a String which is the symbol starting the nest.
    pub sym_starts_nest: Option<Ustr>,
    // This line should close this many <div>'s.
    pub pop_nest_count: u32,
}

/// Renders source code into a Vec of HTML-formatted lines wrapped in `FormattedLine` objects that
/// provide the metadata for the position:sticky post-processing step.  Caller is responsible
/// for generating line numbers and any blame information.
pub fn format_code(
    cfg: Option<&Config>,
    jumpref_lookup: &Option<CrossrefLookupMap>,
    format: FormatAs,
    path: &str,
    input: &str,
    analysis: &[WithLocation<Vec<AnalysisSource>>],
) -> (Vec<FormattedLine>, String) {
    let tokens = match format {
        FormatAs::Binary => panic!("Unexpected binary file"),
        FormatAs::CSS => tokenize::tokenize_css(input),
        FormatAs::Plain => tokenize::tokenize_plain(input),
        FormatAs::StaticPrefs => tokenize::tokenize_static_prefs(input),
        FormatAs::FormatCLike(spec) => tokenize::tokenize_c_like(input, spec),
        FormatAs::FormatTagLike(script_spec) => tokenize::tokenize_tag_like(input, script_spec),
    };

    let mut output_lines = Vec::new();
    let mut output = String::new();
    let mut last = 0;

    // The stack of AnalysisSource records that had a valid, non-redundant nesting_range.
    // (It's possible for a single source line to start multiple nesting ranges, but since our
    // use case is making the entire line position:sticky, it only makes sense to create a single
    // range in that case.)
    let mut nesting_stack: Vec<&AnalysisSource> = Vec::new();
    let mut starts_nest: Option<Ustr> = None;

    fn fixup(s: String) -> String {
        s.replace("\r", "\u{21A9}") // U+21A9 = LEFTWARDS ARROW WITH HOOK.
    }

    let mut line_start = 0;
    let mut cur_line = 1;

    let mut cur_datum = 0;

    // The analysis records for the file itself are generated at the beginning.
    // They shouldn't be associated with the actual tokens.
    while cur_datum < analysis.len() && analysis[cur_datum].loc.is_file_target() {
        cur_datum += 1;
    }

    fn entity_replace(s: String) -> String {
        s.replace("&", "&amp;").replace("<", "&lt;")
    }

    // The SYM_INFO dictionary we output into the HTML which provides the symbol
    // information required to populate the context menu as well as providing
    // additional metadata for the super navigation panel.  This replaces the
    // previous ANALYSIS_DATA array which combined information from the crossref
    // generated "jumps" file as well as "source records" at the point of each
    // token.
    let mut generated_sym_info = BTreeMap::new();
    let mut jumpref_traversed: UstrMap<JumprefTraversals> = UstrMap::default();

    // Stuff the file's own info in the symbol info map.
    if let Some(lookup) = jumpref_lookup {
        let file_sym = make_file_sym_from_path(path);
        if let Ok(jumpref) = lookup.lookup(&file_sym) {
            generated_sym_info.insert(ustr(&file_sym), jumpref);
            jumpref_traversed.insert(ustr(&file_sym), JumprefTraversals::empty());
        }
    }

    let mut last_pos = 0;

    for token in tokens {
        //let word = &input[token.start .. token.end];
        //println!("TOK {:?} '{}' {}", token, word, last_pos);

        assert!(last_pos <= token.start);
        assert!(token.start <= token.end);
        last_pos = token.end;

        if token.kind == tokenize::TokenKind::Newline {
            output.push_str(&input[last..token.start]);

            // Pop nesting symbols whose end is on the NEXT line.  That is, it doesn't make
            // sense for the position:sticky overlay to cover up the line that contains the
            // token that closes the nesting range.
            //
            // The check below accomplishes this by scanning until we find an (endline - 1)
            // that is beyond the current line.
            let truncate_to = match nesting_stack
                .iter()
                .rposition(|a| a.nesting_range.end_lineno - 1 > cur_line)
            {
                Some(first_keep) => first_keep + 1,
                None => 0,
            };
            let pop_count = nesting_stack.len() - truncate_to;
            nesting_stack.truncate(truncate_to);

            output_lines.push(FormattedLine {
                line: fixup(output),
                sym_starts_nest: starts_nest.take(),
                pop_nest_count: pop_count as u32,
            });
            output = String::new();

            cur_line += 1;
            line_start = token.end;
            last = token.end;

            continue;
        }

        let column = (token.start - line_start) as u32;

        // Advance cur_datum as long as analysis[cur_datum] is pointing
        // to tokens we've already gone past. This effectively advances
        // cur_datum such that `analysis[cur_datum]` is the analysis data
        // for our current token (if there is any).
        while cur_datum < analysis.len() && cur_line > analysis[cur_datum].loc.lineno {
            cur_datum += 1
        }
        while cur_datum < analysis.len()
            && cur_line == analysis[cur_datum].loc.lineno
            && column > analysis[cur_datum].loc.col_start
        {
            cur_datum += 1
        }

        let datum = if cur_datum < analysis.len()
            && cur_line == analysis[cur_datum].loc.lineno
            && column == analysis[cur_datum].loc.col_start
        {
            let r = &analysis[cur_datum].data;
            cur_datum += 1;
            Some(r)
        } else {
            None
        };

        match (&token.kind, datum) {
            (&tokenize::TokenKind::Identifier(_), Some(d))
            | (&tokenize::TokenKind::StringLiteral, Some(d)) => {
                for a in d.iter() {
                    // If this symbol starts a relevant nesting range and we haven't already pushed a
                    // symbol for this line, push it onto our stack.  Note that the nesting_range
                    // identifies the start/end brace which may not be on the same line as the symbol,
                    // but since we want the symbol to be the thing that's sticky, we start the range
                    // on the symbol.
                    //
                    // A range is "relevant" if:
                    // - It has a valid nesting_range.  (Empty ranges have 0 lineno's for start/end.)
                    // - The range start is on this line or after this line.
                    // - Its end line is not on the current line or the next line and therefore will
                    //   actually trigger the "position:sticky" display scenario.
                    let nests = match (a.nesting_range.start_lineno, nesting_stack.last()) {
                        (0, _) => false,
                        (_, None) => true,
                        (a_start, Some(top)) => {
                            a_start >= cur_line
                                && a_start != top.nesting_range.start_lineno
                                && a.nesting_range.end_lineno > cur_line + 1
                        }
                    };
                    if nests {
                        starts_nest = Some(*a.sym.first().unwrap());
                        nesting_stack.push(a);
                    }

                    for sym in &a.sym {
                        if generated_sym_info.contains_key(sym) {
                            continue;
                        }

                        // Pass-through local symbol information that won't be available from the
                        // cross-reference database because it was marked no_crossref.  This is only
                        // intended to cover type information about the locals; other info like srcsym
                        // and targetsym doesn't make sense for locals.
                        if a.no_crossref {
                            if let Some(type_pretty) = a.type_pretty {
                                let mut obj = Map::new();
                                if let Some(syntax_kind) = a.get_syntax_kind() {
                                    obj.insert(
                                        "syntax".to_string(),
                                        json!(syntax_kind.to_string()),
                                    );
                                }
                                obj.insert("type".to_string(), json!(type_pretty.to_string()));
                                if let Some(type_sym) = &a.type_sym {
                                    obj.insert("typesym".to_string(), json!(type_sym.to_string()));
                                }
                                generated_sym_info.insert(*sym, json!(obj));
                            }
                        } else if let Some(lookup) = jumpref_lookup {
                            if let Ok(jumpref) = lookup.lookup(sym) {
                                // See if there are any binding slot symbols that we should also
                                // include.  This allows us to do things like, when presenting a
                                // context menu for a synthetic XPIDL symbol, we can also provide an
                                // option to go directly to the C++ binding definition.
                                let mut extra_syms =
                                    determine_desired_extra_syms_from_jumpref(&jumpref);
                                jumpref_traversed
                                    .entry(*sym)
                                    .and_modify(|t| *t |= JumprefTraversals::NormalExtra)
                                    .or_insert(JumprefTraversals::NormalExtra);
                                while let Some((extra_sym, next_step)) = extra_syms.pop() {
                                    // No need to lookup and add what we already know if there is
                                    // no next step.  But if there is a next step, we potentially
                                    // need to look-up a third symbol which may not already have
                                    // been loaded.)
                                    let extra_sym = ustr(&extra_sym);
                                    if let Some(extra_traversed) =
                                        jumpref_traversed.get_mut(&extra_sym)
                                    {
                                        // The jumpref should already be in generated_sym_info, it's
                                        // just a question if we need to run an extra traversal for it.
                                        if extra_traversed.contains(next_step) {
                                            continue;
                                        }
                                        *extra_traversed |= next_step;
                                        if let Some(extra_jumpref) =
                                            generated_sym_info.get(&extra_sym)
                                        {
                                            for (next_sym, next_traversals) in
                                                extra_syms_next_step_lookups(
                                                    extra_jumpref,
                                                    next_step,
                                                )
                                            {
                                                extra_syms.push((next_sym, next_traversals));
                                            }
                                        }
                                    } else if let Ok(extra_jumpref) = lookup.lookup(&extra_sym) {
                                        // If there is a next step, process the info for what to contribute
                                        // to extra_syms before we consume the value by storing it.
                                        if !next_step.is_empty() {
                                            for (next_sym, next_traversals) in
                                                extra_syms_next_step_lookups(
                                                    &extra_jumpref,
                                                    next_step,
                                                )
                                            {
                                                extra_syms.push((next_sym, next_traversals));
                                            }
                                        }
                                        jumpref_traversed.insert(extra_sym, next_step);
                                        generated_sym_info.insert(extra_sym, extra_jumpref);
                                    }
                                }
                                generated_sym_info.insert(*sym, jumpref);
                            }
                        }
                    }
                }
            }
            _ => {}
        }

        let get_symbols =
            |token: &tokenize::Token, datum: &mut dyn Iterator<Item = &AnalysisSource>| {
                match &token.kind {
                    &tokenize::TokenKind::Identifier(_) | &tokenize::TokenKind::StringLiteral => {
                        // Build the list of symbols for the highlighter.  We do this for all source
                        // records, even ones marked "no_crossref" because we still want to highlight
                        // locals.  These will be emitted into a `data-symbols` attribute below.
                        let (syms, confidences) = {
                            let mut syms = String::new();
                            let mut confidences = Vec::new();
                            // Suppress including the symbol multiple times.  This was possible under the
                            // ANALYSIS_DATA regime where "source" records mapped directly to "searches",
                            // but this may now be moot.
                            let mut seen_syms = Vec::new();
                            for (sym, confidence) in
                                datum.flat_map(|item| item.sym.iter().zip(item.confidences()))
                            {
                                if let Some(index) = seen_syms.iter().position(|s| s == sym) {
                                    confidences[index] = confidence.max(confidences[index]);
                                    continue;
                                }
                                if !seen_syms.is_empty() {
                                    syms.push(',');
                                }
                                seen_syms.push(*sym);
                                syms.push_str(sym);
                                confidences.push(confidence);
                            }
                            (syms, confidences)
                        };

                        if !syms.is_empty() {
                            format!(
                                "data-symbols=\"{}\" data-confidences=\"{}\"",
                                syms,
                                serde_json::to_string(&confidences)
                                    .unwrap()
                                    .replace('"', "&quot;")
                            )
                        } else {
                            "".to_owned()
                        }
                    }
                    _ => String::new(),
                }
            };

        let get_style = |token: &tokenize::Token,
                         datum: &mut dyn Iterator<Item = &AnalysisSource>| {
            match token.kind {
                tokenize::TokenKind::Identifier(ref maybe_style) => {
                    let mut has_datum = false;
                    let classes = datum.flat_map(|a| {
                        has_datum = true;
                        a.syntax.iter().flat_map(|s| match s.as_ref() {
                            "type" => vec!["syn_type"],
                            "def" | "decl" | "idl" => vec!["syn_def"],
                            _ => vec![],
                        })
                    });
                    let classes = classes.collect::<Vec<_>>();
                    if !classes.is_empty() {
                        format!("class=\"{}\" ", classes.join(" "))
                    } else if has_datum {
                        // If the token has analysis record, do not apply keyword.
                        "".to_owned()
                    } else if let Some(ref style) = maybe_style {
                        style.clone()
                    } else {
                        "".to_owned()
                    }
                }
                tokenize::TokenKind::StringLiteral => "class=\"syn_string\" ".to_owned(),
                tokenize::TokenKind::Comment => "class=\"syn_comment\" ".to_owned(),
                tokenize::TokenKind::TagName => "class=\"syn_tag\" ".to_owned(),
                tokenize::TokenKind::TagAttrName => "class=\"syn_tag\" ".to_owned(),
                tokenize::TokenKind::EndTagName => "class=\"syn_tag\" ".to_owned(),
                tokenize::TokenKind::RegularExpressionLiteral => "class=\"syn_regex\" ".to_owned(),
                _ => "".to_owned(),
            }
        };

        // Only get the symbols and style of the symbols that appear directly in the source code, not in expansions
        let datum_outside_expansions = datum.iter().flat_map(|d| d.iter());
        let has_expansion = |data: &AnalysisSource| {
            matches!(data.expansion_info, Some(ExpansionInfo::ExpandsTo(_)))
        };
        let (symbols, style) = if datum_outside_expansions.clone().any(has_expansion) {
            let symbols = get_symbols(
                &token,
                &mut datum_outside_expansions
                    .clone()
                    .filter(|&a| has_expansion(a)),
            );
            let style = get_style(
                &token,
                &mut datum_outside_expansions
                    .clone()
                    .filter(|&a| has_expansion(a)),
            );
            (symbols, style)
        } else {
            let symbols = get_symbols(&token, &mut datum_outside_expansions.clone());
            let style = get_style(&token, &mut datum_outside_expansions.clone());
            (symbols, style)
        };

        let expansion_to_html = |key: &str, platform: &str, input: &str| {
            let mut html = String::new();

            let tokens = match format {
                FormatAs::Binary => panic!("Unexpected binary file"),
                FormatAs::CSS => tokenize::tokenize_css(input),
                FormatAs::Plain => tokenize::tokenize_plain(input),
                FormatAs::StaticPrefs => tokenize::tokenize_static_prefs(input),
                FormatAs::FormatCLike(spec) => tokenize::tokenize_c_like(input, spec),
                FormatAs::FormatTagLike(script_spec) => {
                    tokenize::tokenize_tag_like(input, script_spec)
                }
            };

            let datum_in_expansion: HashMap<_, _> = datum
                .iter()
                .flat_map(|d| d.iter())
                .flat_map(|data| match data.expansion_info {
                    Some(ExpansionInfo::InExpansionAt(ref offsets)) => Some(
                        offsets
                            .get(key)
                            .and_then(|o| o.get(platform))
                            .into_iter()
                            .flat_map(|v| v.iter())
                            .map(move |&offset| (offset, data)),
                    ),
                    _ => None,
                })
                .flatten()
                .into_group_map();

            let mut last = 0;

            for token in tokens {
                let token_symbols = datum_in_expansion
                    .get(&token.start)
                    .map(Deref::deref)
                    .unwrap_or(&[]);
                let style = get_style(&token, &mut token_symbols.iter().copied());
                let symbols = get_symbols(&token, &mut token_symbols.iter().copied());

                match token.kind {
                    tokenize::TokenKind::Punctuation | tokenize::TokenKind::PlainText => {
                        let mut sanitized = entity_replace(input[last..token.end].to_string());
                        if token.kind == tokenize::TokenKind::PlainText {
                            sanitized = links::linkify_comment(cfg, sanitized);
                        }
                        html.push_str(&sanitized);
                        last = token.end;
                    }
                    _ => {
                        if !style.is_empty() || !symbols.is_empty() {
                            html.push_str(&entity_replace(input[last..token.start].to_string()));
                            html.push_str(&format!("<span {}{}>", style, symbols));
                            let mut sanitized =
                                entity_replace(input[token.start..token.end].to_string());
                            if token.kind == tokenize::TokenKind::Comment
                                || token.kind == tokenize::TokenKind::StringLiteral
                            {
                                sanitized = links::linkify_comment(cfg, sanitized);
                            }
                            html.push_str(&sanitized);
                            html.push_str("</span>");
                            last = token.end;
                        }
                    }
                }
            }

            html.push_str(&entity_replace(input[last..].to_string()));
            html
        };

        let expansions: BTreeMap<_, _> = {
            let expansions = datum_outside_expansions.filter_map(|a| match a.expansion_info {
                Some(ExpansionInfo::ExpandsTo(ref e)) => Some(e),
                _ => None,
            });

            // Turn BTreeMap<String, BTreeMap<String, String>> into Vec<(key: String, (platform: String, expansion: String))> and sort by (key, expansion)
            let mut expansions: Vec<_> = expansions
                .flat_map(|e| {
                    e.iter().flat_map(|(key, expansions)| {
                        expansions.iter().map(move |(platform, expansion)| {
                            (key.to_owned(), (platform.to_owned(), expansion.to_owned()))
                        })
                    })
                })
                .collect();
            expansions.sort_unstable_by(|a, b| Ord::cmp(&(&a.0, &a.1 .1), &(&b.0, &b.1 .1)));

            // Format expansions into html
            let expansions = expansions.into_iter().map(|(key, (platform, expansion))| {
                let html = expansion_to_html(&key, &platform, &expansion);
                (key, (platform, html))
            });

            // Group by key again
            let expansions = expansions.group_by(|(key, _)| key.clone());

            // For each key: merge platforms that yielded the same expansion together
            expansions
                .into_iter()
                .map(|(key, expansions)| {
                    // First into a Vec<(platform: String, expansion: String)>
                    let expansions = expansions.fold(
                        Vec::<(String, String)>::new(),
                        |mut expansions, (_symbol, (platform, expansion))| {
                            if let Some((last_platform, last_expansion)) = expansions.last_mut() {
                                if *last_expansion == expansion {
                                    last_platform.push(' ');
                                    last_platform.push_str(&platform);
                                    return expansions;
                                }
                            }

                            expansions.push((platform.to_owned(), expansion));
                            expansions
                        },
                    );

                    // Then into a BTreeMap<String, String> again
                    let expansions: BTreeMap<_, _> = expansions.into_iter().collect();
                    (key, expansions)
                })
                .collect()
        };

        let expansions = if !expansions.is_empty() {
            format!(
                "data-expansions=\"{}\" ",
                entity_replace(serde_json::to_string(&expansions).unwrap()).replace("\"", "&quot;")
            )
        } else {
            "".to_owned()
        };

        match token.kind {
            tokenize::TokenKind::Punctuation | tokenize::TokenKind::PlainText => {
                let mut sanitized = entity_replace(input[last..token.end].to_string());
                if token.kind == tokenize::TokenKind::PlainText {
                    sanitized = links::linkify_comment(cfg, sanitized);
                }
                output.push_str(&sanitized);
                last = token.end;
            }
            _ => {
                if !expansions.is_empty() || !style.is_empty() || !symbols.is_empty() {
                    output.push_str(&entity_replace(input[last..token.start].to_string()));
                    output.push_str(&format!("<span {}{}{}>", expansions, style, symbols));
                    let mut sanitized = entity_replace(input[token.start..token.end].to_string());
                    if token.kind == tokenize::TokenKind::Comment
                        || token.kind == tokenize::TokenKind::StringLiteral
                    {
                        sanitized = links::linkify_comment(cfg, sanitized);
                    }
                    output.push_str(&sanitized);
                    output.push_str("</span>");
                    last = token.end;
                }
            }
        }
    }

    output.push_str(&entity_replace(input[last..].to_string()));

    if !output.is_empty() {
        output_lines.push(FormattedLine {
            line: fixup(output),
            sym_starts_nest: starts_nest.take(),
            pop_nest_count: nesting_stack.len() as u32,
        });
    }

    let sym_json = if env::var("MOZSEARCH_DIFFABLE").is_err() {
        to_string(&json!(generated_sym_info)).unwrap()
    } else {
        to_string_pretty(&json!(generated_sym_info)).unwrap()
    };
    (output_lines, sym_json)
}

#[derive(Default)]
pub struct FormatPerfInfo {
    pub format_code_duration_us: u64,
    pub blame_lines_duration_us: u64,
    pub commit_info_duration_us: u64,
    pub format_mixing_duration_us: u64,
}

/// Renders source code with blame annotations and semantic analysis data (if provided).
/// The caller provides the panel sections.  Currently used by `output-file.rs` to statically
/// generate the tip of whatever branch it's on with semantic analysis data, and `format_path` to
/// dynamically generate the contents of a file without semantic analysis data.
#[allow(clippy::too_many_arguments)]
pub fn format_file_data(
    cfg: &Config,
    tree_name: &str,
    panel: &[PanelSection],
    info_boxes: String,
    commit: &Option<git2::Commit>,
    blame_commit: &Option<git2::Commit>,
    path: &str,
    data: String,
    crossref_lookup_map: &Option<CrossrefLookupMap>,
    analysis: &[WithLocation<Vec<AnalysisSource>>],
    coverage: &Option<Vec<i64>>,
    writer: &mut dyn Write,
) -> Result<FormatPerfInfo, &'static str> {
    let tree_config = cfg.trees.get(tree_name).ok_or("Invalid tree")?;

    let mut format_perf = FormatPerfInfo::default();

    let format = languages::select_formatting(path);
    if let FormatAs::Binary = format {
        write!(writer, "Binary file").unwrap();
        return Ok(format_perf);
    };

    let slug = format_to_slug_attribute(&format);
    let pre_format_code = Instant::now();
    let (output_lines, sym_json) = format_code(
        Some(cfg),
        crossref_lookup_map,
        format,
        path,
        &data,
        analysis,
    );
    format_perf.format_code_duration_us = pre_format_code.elapsed().as_micros() as u64;

    let pre_blame_lines = Instant::now();
    let blame_lines = git_ops::get_blame_lines(tree_config.git.as_ref(), blame_commit, path);
    format_perf.blame_lines_duration_us = pre_blame_lines.elapsed().as_micros() as u64;

    let pre_commit = Instant::now();
    let revision_owned = match *commit {
        Some(ref commit) => {
            let rev = commit.id().to_string();
            let (header, _) = blame::commit_header(commit)?;
            Some((rev, header))
        }
        None => None,
    };
    let revision = match revision_owned {
        Some((ref rev, ref header)) => Some((rev.as_str(), header.as_str())),
        None => None,
    };
    format_perf.commit_info_duration_us = pre_commit.elapsed().as_micros() as u64;

    let pre_format_mixing = Instant::now();

    let path_wrapper = Path::new(path);
    let filename = path_wrapper.file_name().unwrap().to_str().unwrap();

    let title = format!("{} - mozsearch", filename);
    let opt = Options {
        title: &title,
        tree_name,
        include_date: env::var("MOZSEARCH_DIFFABLE").is_err(),
        revision,
        extra_content_classes: "source-listing not-diff",
    };

    output::generate_header(&opt, writer)?;

    output::generate_breadcrumbs(&opt, writer, path, !analysis.is_empty())?;

    output::generate_panel(&opt, writer, panel, false)?;

    let info_boxes_container = F::Seq(vec![
        F::S(r#"<section class="info-boxes" id="info-boxes-container">"#),
        F::Indent(vec![F::T(info_boxes)]),
        F::S("</section>"),
    ]);
    output::generate_formatted(writer, &info_boxes_container, 0)?;

    if let Some(ext) = path_wrapper.extension() {
        if ext.to_str().unwrap() == "svg" {
            if let Some(ref hg_root) = tree_config.paths.hg_root {
                let url = format!("{}/raw-file/tip/{}", hg_root, path);
                output::generate_svg_preview(writer, &url)?
            }
        }
    }

    let f = F::Seq(vec![F::T(format!(
        "<div id=\"file\" class=\"file\" role=\"table\"{}>",
        slug
    ))]);

    output::generate_formatted(writer, &f, 0).unwrap();

    // Map blame revisions to consecutive integer identifiers so that our aria
    // labels for screen readers can have a more human friendly identifier than
    // (some portion of) the git hash.
    let mut blame_hash_to_human_id = HashMap::new();
    let mut next_human_id = 1;

    // Blame lines and source lines are now interleaved.  Since we already have fully rendered the
    // source above, we output the blame info, line number, and rendered HTML source as we process
    // each line for blame purposes.
    let mut last_revs = None;
    let mut last_color = false;
    let mut nest_depth = 0;
    for (i, line) in output_lines.iter().enumerate() {
        let lineno = i + 1;

        // Compute the coverage data for this line (if any)
        let coverage_data: String = if let Some(ref coverage) = coverage {
            // There's 2 levels of not having data for a line here:
            // 1. We had no coverage data, coverage is None.  In that case,
            //    we'll take the else case.
            // 2. We have coverage data (coverage is Some(x)), but the array
            //    has no data for this line.  This should only happen if the
            //    coverage data is for a different revision control revision
            //    than the source code.  We map this to -4.
            //
            // We also have -3 and -2 from interpolate_coverage, and -1
            // which is directly part of the coverage data we receive (that
            // interpolation converts to -2 and -3.)
            match coverage.get(i).unwrap_or(&-4) {
                -4 => r#" class="cov-strip cov-uncovered cov-unknown" role="button" aria-label="missing data""#.to_owned(),
                -3 => r#" class="cov-strip cov-miss cov-interpolated" role="button" aria-label="uncovered""#.to_owned(),
                -2 => r#" class="cov-strip cov-hit cov-interpolated" role="button" aria-label="uncovered""#.to_owned(),
                -1 => r#" class="cov-strip cov-uncovered cov-known" role="button" aria-label="uncovered""#.to_owned(),
                 0 => r#" class="cov-strip cov-miss cov-known" role="button" aria-label="miss" data-coverage="0""#.to_owned(),
                // Should this directly be a CSS variable?
                 x => format!(
                    r#" class="cov-strip cov-hit cov-known cov-log10-{}" role="button" aria-label="hit {}{}" data-coverage="{}""#,
                    (*x as f64).log10().floor() as u32,
                    if *x < 1000 { *x } else { *x / 1000 },
                    if *x < 1000 { "" } else { "k" },
                    *x)
            }
        } else {
            " class=\"cov-strip cov-no-data\"".to_owned()
        };

        // Compute the blame data for this line (if any)
        let blame_data = if let Some(ref lines) = blame_lines {
            let blame_line = blame::LineData::deserialize(&lines[i]);

            // These store the final data we ship to the front-end.
            // Each of these is a comma-separated list with one element
            // for each blame entry. Currently they only contain one
            // element ever, since the blame-skipping implementation wasn't
            // very good and was removed.
            let revs = blame_line.rev.to_string();
            let filespecs = blame_line.path.to_string();
            let blame_linenos = blame_line.lineno.to_string();

            let human_id = blame_hash_to_human_id
                .entry(revs.clone())
                .or_insert_with(|| {
                    let id = next_human_id;
                    next_human_id += 1;
                    id
                });

            let same_rev_as_last = last_revs.map_or(false, |last| last == revs);
            let color = if same_rev_as_last {
                last_color
            } else {
                !last_color
            };
            last_revs = Some(revs.clone());
            last_color = color;
            let class = if color { 1 } else { 2 };
            let data = format!(
                r#" class="blame-strip c{}" data-blame="{}#{}#{}" role="button" aria-label="{} hash {}" aria-expanded="false""#,
                class,
                revs,
                filespecs,
                blame_linenos,
                if same_rev_as_last { "same" } else { "new" },
                human_id,
            );
            data
        } else {
            " class=\"blame-strip\"".to_owned()
        };

        // If this line starts nesting, we need to create a div that exists strictly to contain the
        // position:sticky element.
        if let Some(nest_sym) = &line.sym_starts_nest {
            write!(
                writer,
                r#"<div class="nesting-container nesting-depth-{}" data-nesting-sym="{}">"#,
                nest_depth, nest_sym
            )
            .unwrap();
            nest_depth += 1;
        }

        // Emit the actual source line here.
        let f = F::Seq(vec![
            F::T(format!(
                "<div role=\"row\" id=\"line-{}\" class=\"source-line-with-number{}\">",
                lineno,
                if line.sym_starts_nest.is_some() {
                    " nesting-sticky-line"
                } else {
                    ""
                }
            )),
            F::Indent(vec![
                // Coverage Info. Its contents go in a div nested inside the
                // "cell" role div because in order to make the hover UI
                // accessible we expose it as a role=button which needs its own
                // element.
                F::T(format!(
                    "<div role=\"cell\"><div{}></div></div>",
                    coverage_data
                )),
                // Blame info.  Contents are nested for the exact same reason as
                // the coverage info (role=button needs its own div).
                F::T(format!(
                    "<div role=\"cell\"><div{}></div></div>",
                    blame_data
                )),
                // The line number.
                F::T(format!(
                    "<div role=\"cell\" class=\"line-number\" data-line-number=\"{}\"></div>",
                    lineno
                )),
                // The source line.
                F::T(format!(
                    "<code role=\"cell\" class=\"source-line\">{}\n</code>",
                    line.line
                )),
            ]),
            F::S("</div>"),
        ]);
        output::generate_formatted(writer, &f, 0).unwrap();

        // And at the end of this line we need to pop off the appropriate number of position:sticky
        // containing elements.
        for _ in 0..line.pop_nest_count {
            nest_depth -= 1;
            write!(writer, "</div>").unwrap();
        }
    }

    let f = F::Seq(vec![F::S("</div>")]);
    output::generate_formatted(writer, &f, 0).unwrap();

    writeln!(writer, "<script>var SYM_INFO = {};</script>", sym_json,).unwrap();

    output::generate_footer(&opt, tree_name, path, writer).unwrap();

    format_perf.format_mixing_duration_us = pre_format_mixing.elapsed().as_micros() as u64;

    Ok(format_perf)
}

fn format_to_slug_attribute(format: &FormatAs) -> String {
    let slug = match format {
        FormatAs::FormatTagLike(spec) => spec.markdown_slug,
        FormatAs::FormatCLike(spec) => spec.markdown_slug,
        _ => "",
    };

    if slug.is_empty() {
        return String::new();
    }

    format!(r#" data-markdown-slug="{}""#, slug)
}

fn entry_to_blob(repo: &git2::Repository, entry: &git2::TreeEntry) -> Result<String, &'static str> {
    match entry.kind() {
        Some(git2::ObjectType::Blob) => {}
        _ => return Err("Invalid path; expected file"),
    }

    if entry.filemode() == 120000 {
        return Err("Path is to a symlink");
    }

    Ok(git_ops::read_blob_entry(repo, entry))
}

/// Dynamically renders the contents of a specific file with blame annotations but without any
/// semantic analysis data available.  Used by the "rev" display and the "diff" mechanism when
/// there aren't actually any changes in the diff.
pub fn format_path(
    cfg: &Config,
    tree_name: &str,
    rev: &str,
    path: &str,
    writer: &mut dyn Write,
) -> Result<(), &'static str> {
    // Get the file data.
    let tree_config = cfg.trees.get(tree_name).ok_or("Invalid tree")?;
    let git = tree_config.get_git()?;
    let commit_obj = git.repo.revparse_single(rev).map_err(|_| "Bad revision")?;
    let commit = commit_obj.into_commit().map_err(|_| "Bad revision")?;
    let commit_tree = commit.tree().map_err(|_| "Bad revision")?;
    let path_obj = Path::new(path);
    let data = match commit_tree.get_path(path_obj) {
        Ok(entry) => entry_to_blob(&git.repo, &entry)?,
        Err(_) => {
            // Check to see if this path is inside a submodule
            let mut test_path = path_obj.parent();
            loop {
                let subrepo_path = match test_path {
                    Some(path) => path,
                    None => return Err("File not found"),
                };
                let entry = match commit_tree.get_path(subrepo_path) {
                    Ok(e) => e,
                    Err(_) => {
                        test_path = subrepo_path.parent();
                        continue;
                    }
                };
                if entry.kind() != Some(git2::ObjectType::Commit) {
                    return Err("File not found");
                }

                // If we get here, the path is inside a submodule
                let subrepo_path = subrepo_path.to_str().ok_or("UTF-8 error")?;
                let subrepo = git
                    .repo
                    .find_submodule(subrepo_path)
                    .map_err(|_| "Can't find submodule")?;
                let subrepo = subrepo.open().map_err(|_| "Can't open submodule")?;
                let path_in_subrepo = path_obj
                    .strip_prefix(subrepo_path)
                    .map_err(|_| "Submodule path error")?;
                let subentry = subrepo
                    .find_commit(entry.id())
                    .and_then(|commit| commit.tree())
                    .and_then(|tree| tree.get_path(path_in_subrepo))
                    .map_err(|_| "File not found in submodule")?;
                break entry_to_blob(&subrepo, &subentry)?;
            }
        }
    };

    // Get blame.
    let blame_commit = if let Some(ref blame_repo) = git.blame_repo {
        let blame_oid = git
            .blame_map
            .get(&commit.id())
            .ok_or("Unable to find blame for revision")?;
        Some(
            blame_repo
                .find_commit(*blame_oid)
                .map_err(|_| "Blame is not a blob")?,
        )
    } else {
        None
    };

    let analysis = Vec::new();

    let hg_rev: &str = tree_config
        .git
        .as_ref()
        .and_then(|git| git.hg_map.get(&commit.id()))
        .map(|rev| rev.as_ref()) // &String to &str conversion
        .unwrap_or("tip");

    let encoded_path = url_encode_path(path);

    let mut vcs_panel_items = vec![];
    vcs_panel_items.push(PanelItem {
        title: "Go to latest version".to_owned(),
        link: format!("/{}/source/{}", tree_name, encoded_path),
        update_link_lineno: "#{}",
        accel_key: None,
        copyable: true,
    });

    let gh_log_link = tree_config
        .paths
        .github_repo
        .as_ref()
        .map(|gh_root| format!("{}/commits/{}/{}", gh_root, commit.id(), encoded_path));
    let hg_log_link = tree_config
        .paths
        .hg_root
        .as_ref()
        .map(|hg_root| format!("{}/log/{}/{}", hg_root, hg_rev, encoded_path));
    if let Some(link) = gh_log_link {
        vcs_panel_items.push(PanelItem {
            title: "Git log".to_owned(),
            link,
            update_link_lineno: "",
            accel_key: hg_log_link.is_none().then_some('L'),
            copyable: true,
        });
    }
    if let Some(link) = hg_log_link {
        vcs_panel_items.push(PanelItem {
            title: "Mercurial log".to_owned(),
            link,
            update_link_lineno: "",
            accel_key: Some('L'),
            copyable: true,
        });
    }

    let gh_raw_link = tree_config
        .paths
        .github_repo
        .as_ref()
        .map(|gh_root| format!("{}/raw/{}/{}", gh_root, commit.id(), encoded_path));
    let hg_raw_link = tree_config
        .paths
        .hg_root
        .as_ref()
        .map(|hg_root| format!("{}/raw-file/{}/{}", hg_root, hg_rev, encoded_path));
    if let Some(link) = gh_raw_link.or(hg_raw_link) {
        vcs_panel_items.push(PanelItem {
            title: "Raw".to_owned(),
            link,
            update_link_lineno: "",
            accel_key: Some('R'),
            copyable: true,
        });
    }

    if tree_config.paths.git_blame_path.is_some() {
        vcs_panel_items.push(PanelItem {
            title: "Blame".to_owned(),
            link:
                "javascript:alert('Hover over the gray bar on the left to see blame information.')"
                    .to_owned(),
            update_link_lineno: "",
            accel_key: None,
            copyable: false,
        });
    }
    let panel = vec![
        PanelSection {
            name: "Revision control".to_owned(),
            items: vcs_panel_items,
            raw_items: vec![],
        },
        create_markdown_panel_section(false),
    ];

    format_file_data(
        cfg,
        tree_name,
        &panel,
        "".to_string(),
        &Some(commit),
        &blame_commit,
        path,
        data,
        &None,
        &analysis,
        &None,
        writer,
    )
    .map(|_| ())
}

pub fn create_markdown_panel_section(add_symbol_link: bool) -> PanelSection {
    let mut markdown_panel_items = vec![];
    markdown_panel_items.push(PanelItem {
        title: "Filename Link".to_owned(),
        link: String::new(),
        update_link_lineno: "",
        accel_key: Some('F'),
        copyable: true,
    });
    if add_symbol_link {
        markdown_panel_items.push(PanelItem {
            title: "Symbol Link".to_owned(),
            link: String::new(),
            update_link_lineno: "",
            accel_key: Some('S'),
            copyable: true,
        });
    }
    markdown_panel_items.push(PanelItem {
        title: "Code Block".to_owned(),
        link: String::new(),
        update_link_lineno: "",
        accel_key: Some('C'),
        copyable: true,
    });
    PanelSection {
        name: "Copy as Markdown".to_owned(),
        items: markdown_panel_items,
        raw_items: vec![],
    }
}

fn split_lines(s: &str) -> Vec<&str> {
    let mut split = s.split('\n').collect::<Vec<_>>();
    if split[split.len() - 1].is_empty() {
        split.pop();
    }
    split
}

/// Dynamically renders a specific diff with blame annotations but without any semantic analysis
/// data available.
pub fn format_diff(
    cfg: &Config,
    tree_name: &str,
    rev: &str,
    path: &str,
    writer: &mut dyn Write,
) -> Result<(), &'static str> {
    let tree_config = cfg.trees.get(tree_name).ok_or("Invalid tree")?;

    let git_path = tree_config.get_git_path()?;
    let output = Command::new("/usr/bin/git")
        .arg("diff-tree")
        .arg("-p")
        .arg("--cc")
        .arg("--patience")
        .arg("--full-index")
        .arg("--no-prefix")
        .arg("-U100000")
        .arg(rev)
        .arg("--")
        .arg(path)
        .current_dir(git_path)
        .output()
        .map_err(|_| "Diff failed 1")?;
    if !output.status.success() {
        println!("ERR\n{}", git_ops::decode_bytes(output.stderr));
        return Err("Diff failed 2");
    }
    let difftxt = git_ops::decode_bytes(output.stdout);

    if difftxt.is_empty() {
        return format_path(cfg, tree_name, rev, path, writer);
    }

    let git = tree_config.get_git()?;
    let commit_obj = git.repo.revparse_single(rev).map_err(|_| "Bad revision")?;
    let commit = commit_obj.as_commit().ok_or("Bad revision")?;

    let mut blames = Vec::new();

    for parent_oid in commit.parent_ids() {
        let blame_repo = match git.blame_repo {
            Some(ref r) => r,
            None => {
                blames.push(None);
                continue;
            }
        };

        let blame_oid = git
            .blame_map
            .get(&parent_oid)
            .ok_or("Unable to find blame")?;
        let blame_commit = blame_repo
            .find_commit(*blame_oid)
            .map_err(|_| "Blame is not a blob")?;
        let blame_tree = blame_commit.tree().map_err(|_| "Bad revision")?;
        match blame_tree.get_path(Path::new(path)) {
            Ok(blame_entry) => {
                let blame = git_ops::read_blob_entry(blame_repo, &blame_entry);
                let blame_lines = blame.lines().map(|s| s.to_owned()).collect::<Vec<_>>();
                blames.push(Some(blame_lines));
            }
            Err(_) => {
                blames.push(None);
            }
        }
    }

    let mut new_lineno = 1;
    let mut old_lineno = commit.parent_ids().map(|_| 1).collect::<Vec<_>>();

    let mut lines = split_lines(&difftxt);
    for i in 0..lines.len() {
        if lines[i].starts_with('@') && i + 1 < lines.len() {
            lines = lines.split_off(i + 1);
            break;
        }
    }

    let mut new_lines = String::new();

    let mut output = Vec::new();
    for line in lines {
        if line.is_empty() || line.starts_with('\\') {
            continue;
        }

        let num_parents = commit.parents().count();
        let (origin, content) = line.split_at(num_parents);
        let origin = origin.chars().collect::<Vec<_>>();
        let mut cur_blame = None;
        for i in 0..num_parents {
            let has_minus = origin.contains(&'-');
            if (has_minus && origin[i] == '-') || (!has_minus && origin[i] != '+') {
                cur_blame = match blames[i] {
                    Some(ref lines) => Some(&lines[old_lineno[i] - 1]),
                    None => return Err("expected blame for '-' line, none found"),
                };
                old_lineno[i] += 1;
            }
        }

        let mut lno = -1;
        if !origin.contains(&'-') {
            new_lines.push_str(content);
            new_lines.push('\n');

            lno = new_lineno;
            new_lineno += 1;
        }

        output.push((lno, cur_blame, origin, content));
    }

    let format = languages::select_formatting(path);
    if let FormatAs::Binary = format {
        return Err("Cannot diff binary file");
    };
    let analysis = Vec::new();
    let slug = format_to_slug_attribute(&format);
    let (formatted_lines, _) = format_code(Some(cfg), &None, format, path, &new_lines, &analysis);

    let (header, _) = blame::commit_header(commit)?;

    let filename = Path::new(path).file_name().unwrap().to_str().unwrap();
    let title = format!("{} - mozsearch", filename);
    let opt = Options {
        title: &title,
        tree_name,
        include_date: true,
        revision: Some((rev, &header)),
        extra_content_classes: "source-listing diff",
    };

    output::generate_header(&opt, writer)?;

    output::generate_breadcrumbs(&opt, writer, path, false)?;

    let encoded_path = url_encode_path(path);

    let mut vcs_panel_items = vec![
        PanelItem {
            title: "Show changeset".to_owned(),
            link: format!("/{}/commit/{}", tree_name, rev),
            update_link_lineno: "",
            accel_key: None,
            copyable: true,
        },
        PanelItem {
            title: "Show file without diff".to_owned(),
            link: format!("/{}/rev/{}/{}", tree_name, rev, encoded_path),
            update_link_lineno: "#{}",
            accel_key: None,
            copyable: true,
        },
        PanelItem {
            title: "Go to latest version".to_owned(),
            link: format!("/{}/source/{}", tree_name, encoded_path),
            update_link_lineno: "#{}",
            accel_key: None,
            copyable: false,
        },
    ];

    let gh_log_link = tree_config
        .paths
        .github_repo
        .as_ref()
        .map(|gh_root| format!("{}/commits/HEAD/{}", gh_root, encoded_path));
    let hg_log_link = tree_config
        .paths
        .hg_root
        .as_ref()
        .map(|hg_root| format!("{}/log/tip/{}", hg_root, encoded_path));
    if let Some(link) = gh_log_link {
        vcs_panel_items.push(PanelItem {
            title: "Git log".to_owned(),
            link,
            update_link_lineno: "",
            accel_key: hg_log_link.is_none().then_some('L'),
            copyable: true,
        });
    }
    if let Some(link) = hg_log_link {
        vcs_panel_items.push(PanelItem {
            title: "Mercurial log".to_owned(),
            link,
            update_link_lineno: "",
            accel_key: Some('L'),
            copyable: true,
        });
    }

    let sections = vec![PanelSection {
        name: "Revision control".to_owned(),
        items: vcs_panel_items,
        raw_items: vec![],
    }];
    output::generate_panel(&opt, writer, &sections, false)?;

    let f = F::Seq(vec![F::T(format!(
        "<div id=\"file\" class=\"file\" role=\"table\"{}>",
        slug
    ))]);

    output::generate_formatted(writer, &f, 0).unwrap();

    fn entity_replace(s: String) -> String {
        s.replace("&", "&amp;").replace("<", "&lt;")
    }

    let mut last_rev = String::new();
    let mut last_color = false;
    for &(lineno, blame, ref origin, content) in &output {
        let blame_data = match blame {
            Some(blame) => {
                let line_data = blame::LineData::deserialize(blame);

                let color = if last_rev == line_data.rev {
                    last_color
                } else {
                    !last_color
                };
                last_rev = line_data.rev.to_string();
                last_color = color;
                let class = if color { 1 } else { 2 };
                format!(
                    r#" class="blame-strip c{}" data-blame="{}#{}#{}" role="button" aria-label="blame" aria-expanded="false""#,
                    class, line_data.rev, line_data.path, line_data.lineno
                )
            }
            None => " class=\"blame-strip\"".to_owned(),
        };

        let content = entity_replace(content.to_owned());
        let content = if lineno > 0 && (lineno as usize) < formatted_lines.len() + 1 {
            &formatted_lines[(lineno as usize) - 1].line
        } else {
            &content
        };

        let origin = origin.iter().cloned().collect::<String>();

        let class = if origin.contains('-') {
            " minus-line"
        } else if origin.contains('+') {
            " plus-line"
        } else {
            ""
        };

        let f = F::Seq(vec![
            F::T(format!(
                "<div role=\"row\" id=\"line-{}\" class=\"source-line-with-number\">",
                // note: this can be -1 but that's the way it's always been.
                lineno
            )),
            F::Indent(vec![
                // Coverage info.
                F::T(format!(
                    "<div role=\"cell\" class=\"blame-container\"><div{}></div></div>",
                    blame_data
                )),
                // Blame info.
                F::T(format!(
                    "<div role=\"cell\" class=\"blame-container\"><div{}></div></div>",
                    blame_data
                )),
                // The line number.
                F::T(format!(
                    "<div role=\"cell\" class=\"line-number\" data-line-number=\"{}\"></div>",
                    if lineno > 0 {
                        format!("{}", lineno)
                    } else {
                        "".to_owned()
                    },
                )),
                // The source line.
                F::T(format!(
                    "<code role=\"cell\" class=\"source-line{}\">{} {}\n</code>",
                    class, origin, content
                )),
            ]),
            F::S("</div>"),
        ]);

        output::generate_formatted(writer, &f, 0).unwrap();
    }

    let f = F::Seq(vec![F::S("</div>")]);
    output::generate_formatted(writer, &f, 0).unwrap();

    output::generate_footer(&opt, tree_name, path, writer).unwrap();

    Ok(())
}

fn generate_commit_info(
    tree_name: &str,
    tree_config: &TreeConfig,
    writer: &mut dyn Write,
    commit: &git2::Commit,
) -> Result<(), &'static str> {
    let (header, remainder) = blame::commit_header(commit)?;

    fn format_rev(tree_name: &str, oid: git2::Oid) -> String {
        format!("<a href=\"/{}/commit/{}\">{}</a>", tree_name, oid, oid)
    }

    fn format_sig(sig: git2::Signature, git: &GitData) -> String {
        let (name, email) = git
            .mailmap
            .lookup(sig.name().unwrap(), sig.email().unwrap());
        format!("{} &lt;{}>", name, email)
    }

    let parents = commit
        .parent_ids()
        .map(|p| {
            F::T(format!(
                "<tr><td>parent</td><td>{}</td></tr>",
                format_rev(tree_name, p)
            ))
        })
        .collect::<Vec<_>>();

    let git = tree_config.get_git()?;
    let hg = match git.hg_map.get(&commit.id()) {
        Some(hg_id) => {
            let hg_link = format!(
                "<a href=\"{}/rev/{}\">{}</a>",
                tree_config.paths.hg_root.as_ref().unwrap(),
                hg_id,
                hg_id
            );
            vec![F::T(format!("<tr><td>hg</td><td>{}</td></tr>", hg_link))]
        }

        None => vec![],
    };

    let id_string = format!("{}", commit.id());
    let gitstr = tree_config.paths.github_repo.as_ref().map(|ref ghurl| {
        format!(
            "<a href=\"{}/commit/{}\">{}</a>",
            ghurl, id_string, id_string
        )
    });

    let naive_t = NaiveDateTime::from_timestamp(commit.time().seconds(), 0);
    let tz = FixedOffset::east(commit.time().offset_minutes() * 60);
    let t: DateTime<FixedOffset> = DateTime::from_utc(naive_t, tz);
    let t = t.to_rfc2822();

    let f = F::Seq(vec![
        F::T(format!("<h3>{}</h3>", header)),
        F::T(format!("<pre><code>{}</code></pre>", remainder)),
        F::S("<table>"),
        F::Indent(vec![
            F::T(format!(
                "<tr><td>commit</td><td>{}</td></tr>",
                format_rev(tree_name, commit.id())
            )),
            F::Seq(parents),
            F::Seq(hg),
            F::T(gitstr.map_or(String::new(), |g| {
                format!("<tr><td>git</td><td>{}</td></tr>", g)
            })),
            F::T(format!(
                "<tr><td>author</td><td>{}</td></tr>",
                format_sig(commit.author(), git)
            )),
            F::T(format!(
                "<tr><td>committer</td><td>{}</td></tr>",
                format_sig(commit.committer(), git)
            )),
            F::T(format!("<tr><td>commit time</td><td>{}</td></tr>", t)),
        ]),
        F::S("</table>"),
    ]);

    output::generate_formatted(writer, &f, 0)?;

    let git_path = tree_config.get_git_path()?;
    let output = Command::new("/usr/bin/git")
        .arg("show")
        .arg("--cc")
        .arg("--pretty=format:")
        .arg("--raw")
        .arg(id_string)
        .current_dir(git_path)
        .output()
        .map_err(|_| "Diff failed 1")?;
    if !output.status.success() {
        println!("ERR\n{}", git_ops::decode_bytes(output.stderr));
        return Err("Diff failed 2");
    }
    let difftxt = git_ops::decode_bytes(output.stdout);

    let lines = split_lines(&difftxt);
    let mut changes = Vec::new();
    for line in lines {
        if line.is_empty() {
            continue;
        }

        let suffix = &line[commit.parents().count()..];
        let prefix_size = 2 * (commit.parents().count() + 1);
        let mut data = suffix.splitn(prefix_size + 1, ' ');
        let data = data.nth(prefix_size).ok_or("Invalid diff output 3")?;
        let file_info = data.split('\t').take(2).collect::<Vec<_>>();

        let f = F::T(format!(
            "<li>{} <a href=\"/{}/diff/{}/{}\">{}</a>",
            file_info[0],
            tree_name,
            commit.id(),
            url_encode_path(file_info[1]),
            file_info[1]
        ));
        changes.push(f);
    }

    let f = F::Seq(vec![F::S("<ul>"), F::Indent(changes), F::S("</ul>")]);
    output::generate_formatted(writer, &f, 0)?;

    Ok(())
}

pub fn format_commit(
    cfg: &Config,
    tree_name: &str,
    rev: &str,
    writer: &mut dyn Write,
) -> Result<(), &'static str> {
    let tree_config = cfg.trees.get(tree_name).ok_or("Invalid tree")?;

    let git = tree_config.get_git()?;
    let commit_obj = git.repo.revparse_single(rev).map_err(|_| "Bad revision")?;
    let commit = commit_obj.as_commit().ok_or("Bad revision")?;

    let title = format!("{} - mozsearch", rev);
    let opt = Options {
        title: &title,
        tree_name,
        include_date: true,
        revision: None,
        extra_content_classes: "commit",
    };

    output::generate_header(&opt, writer)?;

    output::generate_breadcrumbs(&opt, writer, "", false)?;

    output::generate_panel(&opt, writer, &[], true)?;

    generate_commit_info(tree_name, tree_config, writer, commit)?;

    output::generate_footer(&opt, tree_name, "", writer).unwrap();

    Ok(())
}

```

## tools/src/query/chew_query.rs
```
use std::{
    collections::{BTreeMap, BTreeSet, HashMap, HashSet, VecDeque},
    iter::FromIterator,
};

use query_parser::{parse, TermValue};
use serde::{Deserialize, Serialize};
use toml::value::Table;

use crate::{
    abstract_server::{ErrorDetails, ErrorLayer, Result, ServerError},
    cmd_pipeline::transforms::path_glob_transform,
};

/*
  Queries are translated into pipelines in the following steps:

  1. We parse the query string with the `query-parser` crate which provides us
     with a list of terms and values from `term:value` with a special case for
     bare values where there is no term.
  2. We look up each term (including "default" for bare values) which contain
     some combination of:
     - Term aliases: We just re-process the term as if the aliased term had been
       used.  This is intended for short-hands like "C" for "context" where we
       want our UI to act like "context" had been used when "C" is observed so
       that we can explain to the user what is going on without being cryptic.
     - Term expansions: We re-process the term as one or more other terms,
       potentially transforming the value associated with the term.  Allowing
       expansion to multiple terms lets us have a single query run against
       multiple data sources.  For example, our default term expands to
       "file" for filename/path search, "idprefix" for identifier lookup by
       prefix, and "text" for full-text search.  These will result in parallel
       execution pipelines which are stitched back together later via the
       "group" and "junction" config dictionaries.
     - Pipeline command invocations placed in a specific group.  Command
       invocations will create a command with the given name if it does not
       already exist, or reuse an existing command if one already exists
       (searching from the most recently added command).  Arguments are then
       contributed to the command.  This allows terms to add additional
       constraints or settings to a single pipeline command.
     - Maybe in the future: The ability to set some kind of global variable so
       that a single term can influence multiple pipeline commands that may or
       may not exist (and without bringing them into existence)?
  3. After the terms have produced the starting groups, we consult the "group"
     and "junction" nodes that have not yet been processed.  For each group /
     junction, we look up its config settings and:
     - For each group, we set its "output" and if there is a "next" group or a
       "junction" that should process its output, we create the group /
       junction if it does not exist and add/set the group's "output" as the/a
       input to the group/junction.  We populate the new group with a "command"
       and any "args" if they were provided.  We add the group/junction to the
       to-do list.
    - For each junction we similarly look at its "output" and any "next" group.
*/

#[derive(Deserialize)]
pub struct QueryConfig {
    pub term: BTreeMap<String, TermConfig>,
    pub group: BTreeMap<String, GroupConfig>,
    pub junction: BTreeMap<String, JunctionConfig>,
}

#[derive(Deserialize)]
pub struct TermConfig {
    pub alias: Option<String>,
    #[serde(default)]
    pub conflicts: Vec<String>,
    #[serde(default)]
    pub expand: Vec<TermExpansion>,
    #[serde(default)]
    pub group: BTreeMap<String, Vec<PipelineUse>>,
}

#[derive(Deserialize)]
pub struct TermExpansion {
    pub term: String,
    #[serde(default)]
    pub transforms: Vec<String>,
}

#[derive(Deserialize)]
pub struct PipelineUse {
    pub command: String,
    #[serde(default)]
    pub priority: u32,
    #[serde(default)]
    pub args: Table,
}

#[derive(Deserialize)]
pub struct GroupConfig {
    pub output: String,
    #[serde(default)]
    pub commands: Vec<PipelineUse>,
    pub junction: Option<String>,
    pub next: Option<String>,
}

#[derive(Deserialize)]
pub struct JunctionConfig {
    pub command: String,
    #[serde(default)]
    pub args: Table,
    pub output: String,
    pub next: Option<String>,
}

lazy_static! {
    static ref QUERY_CORE: QueryConfig = toml::from_str(include_str!("query_core.toml")).unwrap();
}

#[derive(Default, Serialize)]
pub struct PipelinePhase {
    pub groups: Vec<Vec<String>>,
    pub junctions: Vec<String>,
}

#[derive(Default, Serialize)]
pub struct QueryPipelineGroupBuilder {
    pub groups: BTreeMap<String, PipelineGroup>,
    pub junctions: BTreeMap<String, JunctionNode>,
    pub phases: Vec<PipelinePhase>,
}

fn apply_transforms(user_val: String, transforms: &[String]) -> String {
    let mut val = user_val;
    for transform in transforms.iter() {
        val = match transform.as_str() {
            "regexp_escape" => regex::escape(&val),
            "path_glob" => path_glob_transform(&val),
            _ => val,
        }
    }
    val
}

fn flatten_args(user_val: &str, priority: u32, args: &Table) -> PipelineArgs {
    let mut flattened = PipelineArgs::default();
    for (key, arg_val) in args.iter() {
        if key.as_str() == "positional" {
            if let Some(arg_str) = arg_val.as_str() {
                flattened
                    .positional_args
                    .push(arg_str.replace("$0", user_val));
            }
        } else if let Some(arg_bool) = arg_val.as_bool() {
            // boolean command-line args should be omitted if false
            if arg_bool {
                flattened.bool_args.insert(key.clone());
            }
        } else if let Some(arg_str) = arg_val.as_str() {
            let replaced_arg = arg_str.replace("$0", user_val);
            flattened
                .named_args
                .insert(key.clone(), (replaced_arg, priority));
        }
    }
    flattened
}

impl QueryPipelineGroupBuilder {
    fn ensure_pipeline_step(&mut self, group_name: String, command: String, args: PipelineArgs) {
        let group = self.groups.entry(group_name).or_default();

        group.ensure_pipeline_step(command, args);
    }

    pub fn ingest_term(&mut self, root_term: &str, value: &str) -> Result<()> {
        let mut terms_to_process: VecDeque<(String, String)> = VecDeque::new();
        terms_to_process.push_back((root_term.to_string(), value.to_string()));

        let mut terms_processed = vec![];
        while let Some((term_str, term_value)) = terms_to_process.pop_front() {
            if let Some(term) = QUERY_CORE.term.get(&term_str) {
                if let Some(alias) = &term.alias {
                    terms_to_process.push_back((alias.clone(), term_value.clone()));
                }

                for conflict in term.conflicts.iter() {
                    if terms_processed.iter().any(|x| x == conflict) {
                        return Err(ServerError::StickyProblem(ErrorDetails {
                            layer: ErrorLayer::BadInput,
                            message: format!("{} conflicts with {}", term_str, conflict),
                        }));
                    }
                }

                for expand in term.expand.iter() {
                    terms_to_process.push_back((
                        expand.term.clone(),
                        apply_transforms(term_value.clone(), &expand.transforms),
                    ));
                }

                for (group_name, pipeline_uses) in term.group.iter() {
                    for pipe_use in pipeline_uses.iter() {
                        let flattened_args = flatten_args(&term_value, 0, &pipe_use.args);
                        self.ensure_pipeline_step(
                            group_name.clone(),
                            pipe_use.command.clone(),
                            flattened_args,
                        );
                    }
                }
            }

            terms_processed.push(term_str);
        }

        Ok(())
    }
}

#[derive(Default, Serialize)]
pub struct PipelineGroup {
    pub input: Option<String>,
    pub segments: Vec<PipelineSegment>,
    pub output: Option<String>,
    pub depth: u32,
}

impl PipelineGroup {
    fn ensure_pipeline_step(&mut self, command: String, args: PipelineArgs) {
        match self.segments.iter_mut().rfind(|seg| seg.command == command) {
            Some(seg) => {
                seg.args.merge(args);
            }
            None => {
                self.segments.push(PipelineSegment { command, args });
            }
        }
    }
}

#[derive(Default, Serialize)]
pub struct JunctionNode {
    pub inputs: Vec<String>,
    pub command: PipelineSegment,
    pub output: Option<String>,
    pub depth: u32,
}

#[derive(Default, Serialize)]
pub struct PipelineArgs {
    pub bool_args: BTreeSet<String>,
    // Only the named args need a priority for deciding when to clobber.
    pub named_args: BTreeMap<String, (String, u32)>,
    pub positional_args: Vec<String>,
}

impl PipelineArgs {
    pub fn merge(&mut self, mut other: Self) {
        self.bool_args.append(&mut other.bool_args);
        for (key, (oth_val, oth_pri)) in other.named_args {
            if let Some(ptr) = self.named_args.get_mut(&key) {
                // Only clobber our current value if the new value has a higher priority.
                if oth_pri > ptr.1 {
                    *ptr = (oth_val, oth_pri);
                }
            } else {
                self.named_args.insert(key, (oth_val, oth_pri));
            }
        }
        self.positional_args.append(&mut other.positional_args);
    }

    // ## Escaping
    //
    // We don't need to deal with shell escaping, but we do need to deal with
    // our string-based interaction with clap meaning that clap can't magically
    // distinguish between us indicating an argument by passing a string
    // prefixed with double-dashed and a value that we want to start with
    // double-dashes.  However, we are able to deal with this by making sure
    // that:
    //
    // * Named args use the `--arg=value` syntax since `--arg=--value` is
    //   unambiguous.
    // * Positional args only come after the magic `--` delimiter.
    pub fn to_vec(&self) -> Vec<String> {
        let mut args = vec![];
        for arg in &self.bool_args {
            args.push(format!("--{}", arg));
        }
        for (key, (val, _pri)) in &self.named_args {
            args.push(format!("--{}={}", key, val));
        }
        if !self.positional_args.is_empty() {
            args.push("--".to_string());
            for arg in &self.positional_args {
                args.push(arg.clone());
            }
        }
        args
    }
}

#[derive(Default, Serialize)]
pub struct PipelineSegment {
    pub command: String,
    pub args: PipelineArgs,
}

pub fn chew_query(full_arg_str: &str) -> Result<QueryPipelineGroupBuilder> {
    let mut builder = QueryPipelineGroupBuilder::default();
    // ## 1: Parse the Query
    let q = parse(full_arg_str);

    // ## 2: Ingest / process the terms
    for term in q.terms {
        match term.value {
            TermValue::Simple(value) => {
                if let Some(key) = term.key {
                    builder.ingest_term(&key, &value)?;
                } else {
                    builder.ingest_term("default", &value)?;
                }
            }
        }
    }

    // ## 3: Process group rules to build the graph suggested by the terms above
    let mut unprocessed_groups: VecDeque<String> = builder.groups.keys().cloned().collect();
    let mut unprocessed_junctions: VecDeque<String> = VecDeque::new();
    let mut queued_groups: HashSet<String> = builder.groups.keys().cloned().collect();
    // The set of groups without an input which means they should go in the
    // first `ParallelPipelines` instance.  We remove groups from this set as we
    // determine that they are actually depdendent on some earlier pipeline.
    let mut root_groups = BTreeSet::from_iter(unprocessed_groups.iter().cloned());
    // For phase 4 it's useful for us to be able to map an input to the set of
    // groups that consume it.  This is important because the group/junction
    // names effectively exist in a separate namespace from the inputs/outputs
    // where the name of a group will usually be the name of its output.  This
    // probably isn't strictly necessary if we did more in this pass, but this
    // is all fairly complicated and I do hope having the in-between
    // representations is helpful for the explanations we generate, etc.
    let mut inputs_to_names = HashMap::new();

    while !unprocessed_groups.is_empty() || !unprocessed_junctions.is_empty() {
        let mut next_group: Option<String> = None;
        let mut next_junction: Option<String> = None;
        let mut use_input: Option<String> = None;

        // We process groups first because junctions must have inputs and they
        // should probably already exist as groups, so our life is probably
        // easier if we process them first.
        if let Some(group_name) = unprocessed_groups.pop_front() {
            if let (Some(group_config), Some(group)) = (
                QUERY_CORE.group.get(&group_name),
                builder.groups.get_mut(&group_name),
            ) {
                group.output = Some(group_config.output.clone());
                use_input = group.output.clone();
                if let Some(next_group_name) = &group_config.next {
                    next_group = Some(next_group_name.clone());
                } else if let Some(next_junction_name) = &group_config.junction {
                    next_junction = Some(next_junction_name.clone());
                }
            }
        } else if let Some(junction_name) = unprocessed_junctions.pop_front() {
            if let (Some(junction_config), Some(junction)) = (
                QUERY_CORE.junction.get(&junction_name),
                builder.junctions.get_mut(&junction_name),
            ) {
                junction.output = Some(junction_config.output.clone());
                use_input = junction.output.clone();
                if let Some(next_group_name) = &junction_config.next {
                    next_group = Some(next_group_name.clone());
                }
            }
        }

        // Make the requested thing.
        if let Some(group_name) = next_group {
            if let (Some(group_config), group) = (
                QUERY_CORE.group.get(&group_name),
                builder
                    .groups
                    .entry(group_name.clone())
                    .or_insert_with(PipelineGroup::default),
            ) {
                group.input = use_input;
                if let Some(input_name) = &group.input {
                    root_groups.remove(&group_name);
                    inputs_to_names
                        .entry(input_name.clone())
                        .or_insert_with(std::vec::Vec::new)
                        .push(group_name.clone());
                }
                // group.output will be set and next/junction will be processed
                // in the 1st phase of the loop above; we're just ensuring the
                // group exists, establishing the "input" link, and adding any
                // commands/args listed.
                if queued_groups.insert(group_name.clone()) {
                    // (only process the group if we haven't already)
                    unprocessed_groups.push_back(group_name);
                }

                for cmd in &group_config.commands {
                    let flattened_args = flatten_args("", cmd.priority, &cmd.args);
                    group.ensure_pipeline_step(cmd.command.clone(), flattened_args);
                }
            }
        } else if let Some(junction_name) = next_junction {
            if let (Some(junction_config), junction, Some(input)) = (
                QUERY_CORE.junction.get(&junction_name),
                builder
                    .junctions
                    .entry(junction_name.clone())
                    .or_insert_with(JunctionNode::default),
                use_input,
            ) {
                inputs_to_names
                    .entry(input.clone())
                    .or_insert_with(std::vec::Vec::new)
                    .push(junction_name.clone());
                junction.inputs.push(input);

                // junction.output will be set and next will be processed in the
                // 1st phase of the loop above; we're just ensuring the junction
                // exists and adding the "input" to the list.
                //
                // Logic to run only the first time we're processing the
                // junction:
                if junction.command.command.is_empty() {
                    junction.command.command = junction_config.command.clone();
                    junction.command.args = flatten_args("", 0, &junction_config.args);
                    unprocessed_junctions.push_back(junction_name);
                }
            }
        }
    }

    // ## 4: Walk the graph to build parallel pipelines
    //
    // We want to accumulate linear chains of groups until they hit a junction
    // or terminate with a "result" output.  Once we've hit all junctions, then
    // we want to flush those chains and the junctions they hit as a phase.
    // Then we want to restart the cycle with the outputs of the junctions until
    // we are left with a single "result" output.
    //
    // The primary interesting case for is a scenario like the following:
    //   g-a1 - g-a4 -\
    //   g-a2 - g-a5 --- j1 ----- j2
    //   g-a3 - g-a6 -/        /
    //                        /
    //   g-b1 - g-b2 - g-b3 -/
    //
    // The specific characteristic of note here is that we have a chain of
    // groups that reaches a junction that itself depends on the output of
    // another junction; whether there are groups in between or not doesn't
    // entirely matter but both cases should work.
    //
    // Our execution semantics dictate that all junctions in a phase run in
    // parallel, which means the data-dependency in j2 means that j2 must be in
    // a second phase and j1 in the first.  We can schedule the g-bN nodes in
    // either phase and have things be valid, but we do prefer to do all work as
    // early as possible.  (Note that this does force j1 to wait for g-bN,
    // currently, but this is also just a hypothetical scenario and our goal is
    // to just have a high probability of things working at this point.)
    //
    // Our algorithm is then to maintain 2 key state structures beyond our
    // `cur_phase` that we build incrementally for the current phase:
    // 1. `next_groups`: the set of groups we know we need to investigate next
    //    for this phase, initialized with the content of `root_groups`.  This
    //    is expressed as VecDeque of tuples of (group name, index in the
    //    PipelinePhase::groups array to place this group in).
    // 2. `pending_junctions`: a map from the junction names that groups have
    //    arrived at so far to the number of other groups we are waiting to
    //    arrive at this node.  The value is initialized to the length of the
    //    `JunctionInvocation::input_names` and decremented for each group that
    //    arrives at the junction.
    //
    // Starting from the initial `next_groups` population of `root_groups`, we
    // iteratively consume that deque, looking up the names to find out if they
    // are groups or junctions (which live in the same namespace).  If it's a
    // group, we add the current group to the appropriate
    // `PipelinePhase::groups` vec slot and push the output's name onto
    // `next_groups` including that vec slot.  If it was a junction, we
    // ensure there's an entry in `pending_junctions` for the junction and
    // decrement its waiting count for the current group.  If the the junction's
    // waiting count reaches 0, we push it onto the `PipelinePhase::junctions`
    // vec.  We continue this process until we run out of `next_groups`.
    //
    // Once we have no more `next_groups`, we traverse the list of junctions in
    // the current phase, use those to populate `next_groups`.  Note that
    // "result" is a magic group name for the terminal node (which will also
    // have impacted the logic above) and which will not go into `next_groups`.
    // We then flush the current phase.  If there are `next_groups`, we repeat
    // the loop with a new phase, otherwise we're done.

    let mut next_groups: VecDeque<(String, Option<usize>)> =
        root_groups.into_iter().map(|x| (x, None)).collect();
    let mut pending_junctions = BTreeMap::new();
    let mut seen = HashSet::new();

    // Control flow structure:
    //
    // Each pass through the outer loop creates a new PipelinePhase and pushes
    // it into the list of phases.  Each inner loop fully processes the set of
    // `next_groups` which accumulate into the current phase, and when the inner
    // loop completes it looks for any groups that the junctions in that phase
    // produces.
    while !next_groups.is_empty() {
        let mut cur_phase = PipelinePhase::default();
        while let Some((thing_name, group_slot)) = next_groups.pop_front() {
            if let Some(group) = builder.groups.get(&thing_name) {
                // Check if we've processed this group before.  We can't do this
                // suppression check on adding things to `next_groups` because
                // we could be adding a junction, which absolutely can be
                // arrived at multiple times.
                if !seen.insert(thing_name.clone()) {
                    return Err(ServerError::StickyProblem(ErrorDetails {
                        layer: ErrorLayer::ConfigLayer,
                        message: format!("pipeline loop: group {} used multiple times", thing_name),
                    }));
                }

                // This is a group, put it in the current phase.
                let next_group_slot = match group_slot {
                    Some(slot) => {
                        cur_phase.groups[slot].push(thing_name.clone());
                        Some(slot)
                    }
                    None => {
                        let slot = cur_phase.groups.len();
                        cur_phase.groups.push(vec![thing_name.clone()]);
                        Some(slot)
                    }
                };

                // Figure out what's next for this group
                if let Some(next_input) = &group.output {
                    if next_input.as_str() == "result" {
                        // result is a terminal output and so there's nothing to do.
                    } else {
                        for next_group in inputs_to_names.get(next_input).ok_or_else(|| {
                            ServerError::StickyProblem(ErrorDetails {
                                layer: ErrorLayer::ConfigLayer,
                                message: format!(
                                    "group {} output {} is never consumed",
                                    thing_name, next_input,
                                ),
                            })
                        })? {
                            next_groups.push_back((next_group.clone(), next_group_slot));
                        }
                    }
                }
            } else if let Some(junction) = builder.junctions.get(&thing_name) {
                let waiting_count = pending_junctions
                    .entry(thing_name.clone())
                    .or_insert_with(|| junction.inputs.len());
                *waiting_count -= 1;

                if *waiting_count == 0 {
                    cur_phase.junctions.push(thing_name.clone());
                    pending_junctions.remove(&thing_name);
                }
            }
        }

        for junction_name in cur_phase.junctions.iter() {
            // Both of these Some()s should probably use ok_or_else to error,
            // as these should absolutely exist.
            if let Some(junction) = builder.junctions.get(junction_name) {
                // Complain if this isn't the first time we've processed this
                // junction as it does indicate some kind of loop.
                if !seen.insert(junction_name.clone()) {
                    return Err(ServerError::StickyProblem(ErrorDetails {
                        layer: ErrorLayer::ConfigLayer,
                        message: format!(
                            "pipeline loop: junction {} used multiple times",
                            junction_name
                        ),
                    }));
                }

                if let Some(output) = &junction.output {
                    if output.as_str() == "result" {
                        // result is a terminal output and there's nothing to do
                    } else {
                        for next_group in inputs_to_names.get(output).ok_or_else(|| {
                            ServerError::StickyProblem(ErrorDetails {
                                layer: ErrorLayer::ConfigLayer,
                                message: format!(
                                    "junction {} output {} is never consumed",
                                    junction_name, output,
                                ),
                            })
                        })? {
                            next_groups.push_back((next_group.clone(), None));
                        }
                    }
                }
            }
        }
        builder.phases.push(cur_phase);
    }

    Ok(builder)
}

```

## tools/src/query/query_core.toml
```
[term.C]
alias = "context"

# For graphing we need to build the pipeline through the traverse call but we
# have the "graph-symbols" group automatically add the graphing steps.
[term.calls-to]
[[term.calls-to.group.graph-symbols-default]]
command = "search-identifiers"
args.positional = "$0"
args.exact-match = true
[[term.calls-to.group.graph-symbols-default]]
command = "crossref-lookup"
args.exact-match = true
[[term.calls-to.group.graph-traverse]]
command = "traverse"
args.edge = "uses"

[term.calls-to-sym]
[[term.calls-to-sym.group.graph-symbols-default]]
command = "crossref-lookup"
args.positional = "$0"
[[term.calls-to-sym.group.graph-traverse]]
command = "traverse"
args.edge = "uses"


[term.calls-between]
[[term.calls-between.group.graph-symbols-default]]
command = "search-identifiers"
args.positional = "$0"
args.exact-match = true
[[term.calls-between.group.graph-symbols-default]]
command = "crossref-lookup"
args.exact-match = true
args.methods = true
[[term.calls-between.group.graph-traverse]]
command = "traverse"
args.edge = "uses"
args.paths-between = true


[term.calls-between-source]
[[term.calls-between-source.group.graph-symbols-source]]
command = "search-identifiers"
args.positional = "$0"
args.exact-match = true
[[term.calls-between-source.group.graph-symbols-source]]
command = "crossref-lookup"
args.exact-match = true
args.methods = true
[[term.calls-between-source.group.graph-traverse]]
command = "traverse"
args.edge = "uses"
args.paths-between = true

[term.calls-between-target]
[[term.calls-between-target.group.graph-symbols-target]]
command = "search-identifiers"
args.positional = "$0"
args.exact-match = true
[[term.calls-between-target.group.graph-symbols-target]]
command = "crossref-lookup"
args.exact-match = true
args.methods = true
[[term.calls-between-target.group.graph-traverse]]
command = "traverse"
args.edge = "uses"
args.paths-between = true

[term.calls-from]
[[term.calls-from.group.graph-symbols-default]]
command = "search-identifiers"
args.positional = "$0"
args.exact-match = true
[[term.calls-from.group.graph-symbols-default]]
command = "crossref-lookup"
args.exact-match = true
[[term.calls-from.group.graph-traverse]]
command = "traverse"
args.edge = "callees"


# "inheritance-diagram" is a quick hack to show the full transitive overrides/
# overriddenBy relationship.
[term.inheritance-diagram]
[[term.inheritance-diagram.group.graph-symbols-default]]
command = "search-identifiers"
args.positional = "$0"
args.exact-match = true
[[term.inheritance-diagram.group.graph-symbols-default]]
command = "crossref-lookup"
args.exact-match = true
[[term.inheritance-diagram.group.graph-traverse]]
command = "traverse"
args.edge = "inheritance"

# "class-diagram" currently tries to show the relationship between a given
# class and other classes exclusively via fields, but it would be good to also:
# - Traverse IPC communication relationships
# - Be more intentional/explicit about inheritance; we may do something.
[term.class-diagram]
[[term.class-diagram.group.graph-symbols-default]]
command = "search-identifiers"
args.positional = "$0"
args.exact-match = true
args.types-only = true
[[term.class-diagram.group.graph-symbols-default]]
command = "crossref-lookup"
args.exact-match = true
[[term.class-diagram.group.graph-traverse]]
command = "traverse"
args.edge = "class"

[term.fmus-through-depth]
[[term.fmus-through-depth.group.graph-traverse]]
command = "traverse"
args.traverse-field-member-uses = "$0"

[term.hier]
[[term.hier.group.graph-render]]
command = "graph"
args.hier = "$0"

[term.colorize-callees]
[[term.colorize-callees.group.graph-render]]
command = "graph"
args.colorize-callees = "$0"

[term.graph-debug]
[[term.graph-debug.group.graph-render]]
command = "graph"
args.debug = true

[term.graph-format]
[[term.graph-format.group.graph-render]]
command = "graph"
priority = 1
args.format = "$0"

[term.graph-layout]
[[term.graph-layout.group.graph-render]]
command = "graph"
priority = 1
args.layout = "$0"

# This previously was doing a broken thing to try and avoid creating a weird
# situation if this was specified before "calls-to", but it didn't actually do
# what it thought it was doing because the earlier stages were not actually
# associated with "term.depth" but instead "term.calls-to".  Noting this here
# in case that somehow was doing something important and this breaks things...
[term.depth]
[[term.depth.group.graph-traverse]]
command = "traverse"
args.max-depth = "$0"

# We enforce a range in our clap definition, so this is fine to expose.
[term.node-limit]
[[term.node-limit.group.graph-traverse]]
command = "traverse"
args.node-limit = "$0"

# We enforce a range in our clap definition, so this is fine to expose.
[term.paths-between-node-limit]
[[term.paths-between-node-limit.group.graph-traverse]]
command = "traverse"
args.paths-between-node-limit = "$0"

# This is an opt-out for the heuristic that ignores uses with a ton of paths.
# We do not enforce a range limit on this because the node-limit ends up
# dominating.
[term.path-limit]
[[term.path-limit.group.graph-traverse]]
command = "traverse"
args.skip-uses-at-path-count = "$0"

[term.context]
[[term.context.group.display]]
command = "augment-results"
args.before = "$0"
args.after = "$0"

[term.field-layout]
[[term.field-layout.group.semantic-lookup]]
command = "search-identifiers"
args.positional = "$0"
args.exact-match = true
[[term.field-layout.group.semantic-lookup]]
command = "crossref-lookup"
args.exact-match = true
[[term.field-layout.group.semantic-format]]
command = "format-symbols"
args.mode = "field-layout"

# The default term is what gets applied to things without a term.  It can also
# be explicitly referenced by other terms.
[term.default]
[[term.default.expand]]
term = "file"
[[term.default.expand]]
term = "idprefix"
[[term.default.expand]]
term = "text"

[term.file]
[[term.file.group.file-search]]
command = "search-files"
args.positional = "$0"

[term.id]
# We can't do this and `idprefix` in the same command and it's not clear it's
# worth the effort to support both simultaneously.
conflicts = ["idprefix"]
[[term.id.group.semantic-search]]
command = "search-identifiers"
args.positional = "$0"
args.exact-match = true
[[term.id.group.semantic-search]]
command = "crossref-lookup"
args.exact-match = true
[[term.id.group.semantic-search]]
command = "crossref-expand"

[term.idprefix]
[[term.idprefix.group.semantic-search]]
command = "search-identifiers"
args.positional = "$0"
args.exact-match = false
[[term.idprefix.group.semantic-search]]
command = "crossref-lookup"
[[term.idprefix.group.semantic-search]]
command = "crossref-expand"

[term.pathre]
[[term.pathre.group.file-search]]
command = "search-files"
args.pathre = "$0"
## TODO: Implement filtering crossref output by path
## does this really want to be its own stage, or just part of crossref-lookup?
## I guess this may need to come after crossref-expand, so then it would want
## to be its own command.
#[[term.pathre.group.semantic-search]]
#command = "filter-crossref"
#args.pathre = "$0"
[[term.pathre.group.text-search]]
command = "search-text"
args.pathre = "$0"

[term.p]
alias = "path"

[term.path]
[[term.path.expand]]
term = "pathre"
transforms = ["path_glob"]

[term.re]
[[term.re.group.text-search]]
command = "search-text"
args.re = "$0"

[term.show-cols]
[[term.show-cols.group.semantic-format]]
command = "format-symbols"
args.show-cols = "$0"

[term.hide-cols]
[[term.hide-cols.group.semantic-format]]
command = "format-symbols"
args.hide-cols = "$0"

[term.sym]
alias = "symbol"

[term.symbol]
[[term.symbol.group.semantic-search]]
command = "search-identifiers"
# TODO: implement this
args.add-sym = "$0"
[[term.symbol.group.semantic-search]]
command = "crossref-lookup"
[[term.symbol.group.semantic-search]]
command = "crossref-expand"

[term.text]
[[term.text.expand]]
term = "re"
transforms = ["regexp_escape"]

[group.file-search]
output = "file-search"
junction = "compile"

[group.semantic-search]
output = "semantic-search"
junction = "compile"

[group.text-search]
output = "text-search"
junction = "compile"

[junction.compile]
command = "compile-results"
output = "compiled"
next = "display"

[group.display]
output = "result"
[[group.display.commands]]
command = "augment-results"

[group.graph-symbols-source]
output = "source"
junction = "graph-fuse-symbols"

[group.graph-symbols-target]
output = "target"
junction = "graph-fuse-symbols"

[group.graph-symbols-default]
output = "default"
junction = "graph-fuse-symbols"

[junction.graph-fuse-symbols]
command = "fuse-crossrefs"
output = "fused-symbols"
next = "graph-traverse"

[group.graph-traverse]
output = "graph"
next = "graph-render"

[group.graph-render]
output = "result"
[[group.graph-render.commands]]
command = "graph"
args.format = "mozsearch"

[group.semantic-lookup]
output = "semantic-lookup"
next = "semantic-filter"

[group.semantic-filter]
output = "semantic-filter"
next = "semantic-format"

[group.semantic-format]
output = "result"

```

## tools/src/query/mod.rs
```
pub mod chew_query;

```

## tools/src/doc_trees_handler.rs
```
use crate::file_format::config::Config;
use crate::file_format::doc_trees::{read_doc_trees, DocTrees};
use std::sync::OnceLock;

pub fn find_doc_url(cfg: &Config, src_path: &str) -> Option<String> {
    static DOC_TREES: OnceLock<DocTrees> = OnceLock::new();

    if DOC_TREES.get().is_none() {
        DOC_TREES
            .set(match &cfg.doc_trees_path {
                Some(doc_trees_path) => read_doc_trees(doc_trees_path),
                None => DocTrees::new_empty(),
            })
            .unwrap();
    }

    match DOC_TREES.get().unwrap().find(src_path) {
        // TODO: Make the URL configurable.
        Some(target_path) => {
            Some("https://firefox-source-docs.mozilla.org/".to_string() + target_path.as_str())
        }
        None => None,
    }
}

```

## tools/src/url_encode_path.rs
```
use std::borrow::Cow;
use urlencoding;

pub fn url_encode_path(path: &str) -> String {
    path.split('/')
        .map(|p| urlencoding::encode(p))
        .collect::<Vec<Cow<'_, str>>>()
        .join("/")
}

pub fn url_decode_path(path: &str) -> String {
    match urlencoding::decode(path) {
        Ok(s) => s.to_string(),
        Err(_) => path.to_string(),
    }
}

```

## tools/src/templating/liquid_exts.rs
```
use liquid_core::Expression;
use liquid_core::FilterParameters;
use liquid_core::FromFilterParameters;
use liquid_core::Result;
use liquid_core::Runtime;
use liquid_core::{Display_filter, Filter, FilterReflection, ParseFilter};
use liquid_core::{Value, ValueView};
use regex::Regex;
use serde_json::to_string_pretty;

#[derive(Clone, ParseFilter, FilterReflection)]
#[filter(
    name = "json",
    description = "Render the provided object into pretty-printed JSON.",
    parsed(JsonFilter)
)]
pub struct JsonFilterParser;

#[derive(Debug, Default, Display_filter)]
#[name = "downcase"]
struct JsonFilter;

impl Filter for JsonFilter {
    fn evaluate(&self, input: &dyn ValueView, _runtime: &dyn Runtime) -> Result<Value> {
        let s = to_string_pretty(&input.to_value()).unwrap_or_else(|_e| "".to_string());
        Ok(Value::scalar(s))
    }
}

#[derive(Clone, ParseFilter, FilterReflection)]
#[filter(
    name = "fileext",
    description = "Extract the file extension from a path string, defaulting to the empty string.",
    parsed(FileExtFilter)
)]
pub struct FileExtFilterParser;

#[derive(Debug, Default, Display_filter)]
#[name = "fileext"]
struct FileExtFilter;

impl Filter for FileExtFilter {
    fn evaluate(&self, input: &dyn ValueView, _runtime: &dyn Runtime) -> Result<Value> {
        let s = input.to_kstr();
        let ext = match s.rfind('.') {
            Some(offset) => s[offset + 1..].to_string(),
            None => "".to_string(),
        };
        Ok(Value::scalar(ext))
    }
}

#[derive(Clone, ParseFilter, FilterReflection)]
#[filter(
    name = "compact_pathlike",
    description = "Remove excess whitespace in a path-like string",
    parsed(CompactPathlikeFilter)
)]
pub struct CompactPathlikeFilterParser;

#[derive(Debug, Default, Display_filter)]
#[name = "compact_pathlike"]
struct CompactPathlikeFilter;

impl Filter for CompactPathlikeFilter {
    fn evaluate(&self, input: &dyn ValueView, _runtime: &dyn Runtime) -> Result<Value> {
        lazy_static! {
            // We want to each whitespace adjactent to slashes.
            static ref RE_SLASH_WHITESPACE: Regex = Regex::new(r" */ *").unwrap();
            // We want to compress consecutive whitespace.
            static ref RE_CONSECUTIVE_WHITESPACE: Regex = Regex::new(r" {2,}").unwrap();
        }

        let s = input.to_kstr();
        let slash_normed = RE_SLASH_WHITESPACE.replace_all(&s, "/");
        let consecutived = RE_CONSECUTIVE_WHITESPACE.replace_all(&slash_normed, " ");
        Ok(Value::scalar(consecutived.trim().to_string()))
    }
}

#[derive(Clone, ParseFilter, FilterReflection)]
#[filter(
    name = "ensure_bug_url",
    description = "Given something that may be a bug URL or a bug ID, provide a bug URL",
    parsed(EnsureBugUrlFilter)
)]
pub struct EnsureBugUrlFilterParser;

#[derive(Debug, Default, Display_filter)]
#[name = "ensure_bug_url"]
struct EnsureBugUrlFilter;

impl Filter for EnsureBugUrlFilter {
    fn evaluate(&self, input: &dyn ValueView, _runtime: &dyn Runtime) -> Result<Value> {
        let s = input.to_kstr();
        if s.starts_with("http") {
            Ok(Value::scalar(s.to_string()))
        } else {
            Ok(Value::scalar(format!(
                "https://bugzilla.mozilla.org/show_bug.cgi?id={}",
                s
            )))
        }
    }
}

#[derive(Debug, FilterParameters)]
struct StripPrefixArgs {
    #[parameter(description = "The prefix to remove if it exists.", arg_type = "str")]
    prefix: Expression,
}

#[derive(Clone, ParseFilter, FilterReflection)]
#[filter(
    name = "strip_prefix_or_empty",
    description = "Strip the prefix of the input string if it matches otherwise return an empty string",
    parameters(StripPrefixArgs),
    parsed(StripPrefixOrEmptyFilter)
)]
pub struct StripPrefixOrEmptyFilterParser;

#[derive(Debug, FromFilterParameters, Display_filter)]
#[name = "strip_prefix_or_empty"]
struct StripPrefixOrEmptyFilter {
    #[parameters]
    args: StripPrefixArgs,
}

impl Filter for StripPrefixOrEmptyFilter {
    fn evaluate(&self, input: &dyn ValueView, runtime: &dyn Runtime) -> Result<Value> {
        let args = self.args.evaluate(runtime)?;

        let s = input.to_kstr();
        if let Some(stripped) = s.strip_prefix(args.prefix.as_str()) {
            Ok(Value::scalar(stripped.to_string()))
        } else {
            Ok(Value::scalar("".to_string()))
        }
    }
}

```

## tools/src/templating/mod.rs
```
pub mod builder;
pub mod liquid_exts;

```

## tools/templates/settings.liquid
```
{% include 'header_search.liquid' title: "Searchfox Settings", autofocus: false %}
<div id="scrolling">
  <div id="content" class="content settings-page" data-no-results="No results for current query.">
    {% include 'breadcrumbs.liquid' path: "", hidden: true %}
    {% include 'navigation_panel.liquid' expanded: false %}
    <h1>Searchfox Settings</h1>
    <section>
      <h2>About Searchfox Settings</h2>
      <p>
        This page describes and allows you to change your searchfox settings.
        Settings are stored in LocalStorage and so will be specific to your
        profile and any user container.  Settings only control client-side
        JavaScript decision-making and are never directly sent to the server,
        although settings can affect what requests are sent to the server.  Per
        the <a href="https://www.visophyte.org/blog/2022/10/05/andrews-searchfox-vision-2022/">
        current searchfox vision doc</a>, because URLs are intended to be
        shareable and consistent, a shared URL may assume the existence of a
        feature and effectively enable it while viewing that page so that the UI
        experienced is consistent and not bizarrely broken.  The user's settings
        in LocalStorage will not be affected, however, and when transitioning
        back to a page that doesn't assume a feature, the user will return to
        their normal flow.
      </p>
      <p>
        There is currently no support for synchronizing settings, although if
        searchfox ends up with enough settings, a mechanism to export settings
        by generating a URL or a copy-and-pasteable JSON payload could be added.
      </p>
      <p>
        Settings are organized into three categories:
      </p>
      <ul>
        <li>
          Widget Enable/Disable.  Searchfox widgets are pieces of functionality
          that operate independently from each other.  They are conceptually
          similar to web-extensions; when enabled they can add menu items to the
          context menu and new interactable HTML to the fancy bar, but they will
          not directly interact with other widgets or core features in a way
          that would introduce functionality combinatorial explosions.
        </li>
        <li>
          <p>
            Core Feature Quality Gates.  As new functionality is added to
            searchfox, it moves through several quality levels: alpha, beta, and
            release.  Alpha features are under active development and
            experimentation which can result in continual changes to the user
            experience and where the experience may be brittle.  Beta features
            have reached a stable experience, but one that it's not clear is
            appropriate to yet push to users who primarily are interested in the
            searchfox experience they are used to.  In particular, during 2023
            it's likely new development will only promote features to beta with an
            eye towards polishing their interaction near the end of 2023 and
            then promoting many of those features to release simultaneously.
          </p>
          <p>
            Unless abandoned, core features are always moving in the direction
            of being enabled.  This means we can have features like "remove X"
            as we don't have a concept of "disabling" a feature.  As a user, if
            you don't want a specific feature and it hasn't yet reached
            "release", then you can set your quality gate for that feature to
            "release" even if you your global quality gate set to "alpha" or
            "beta", but eventually that feature will become enabled when it hits
            release.  That said, "behavior" settings are always a possibility as
            long as they don't create combinatiorial explosions that complicate
            maintenance and development.
          </p>
          <p>
            Continuing the theme of avoiding creating combinatorial explosions,
            features do not interact with each other, they only depend on each
            other.  This means as new alpha/beta features evolve that initially
            operate in isolation, new dependencies on other features may be
            added that could disable the feature until you enable the other
            features.
          </p>
        </li>
        <li>
          Behavior: A setting that changes how a core feature or a widget
          behaves.  Behavioral settings should ideally operate independently
          from each other to avoid combinatorial explosion that is hard to test
          or reason about.
        </li>
      </ul>
      <p>
        As we implement new functionality in 2023 and pick new defaults, we are
        currently trying to strike a balance between maintaining muscle memory
        and exposing new functionality that's additive but without disrupting
        the experience you had from searchfox at the end of 2022.  This is not a
        commitment to never change anything or to add preferences for
        everything, but it is a recognition that there needs to be a high bar
        for changes that are not opt-in or which cannot be opted-out of.  It is
        also a recognition that most new functionality will be developed
        iteratively in consultation with the (actively involved) userbase, and
        so there is likely to be a non-trivial amount of churn for new
        functionality, so new functionality needs to be opt-in until the
        experience has stabilized.
      </p>
    </section>
    <section>
      <h2>Alpha/Beta/Release Default Core Feature Gate</h2>
      <p>
        As discussed above, core features are either alpha quality, beta
        quality, or release quality.  This default setting controls what quality
        is chosen if you do not choose a specific per-feature quality.  For
        example, you could choose the default quality gate of "release", but for
        specific features you are interested in, choose "alpha".  If you end up
        not liking the churn of the feature, you could switch those features to
        "beta" so you can experience them again when they're improved /
        stabilized.
      </p>
      <form>
        <label for="global--default-feature-gate">Default feature gate:</label>
        <select id="global--default-feature-gate">
          <option value="release">Release</option>
          <option value="beta">Beta</option>
          <option value="alpha">Alpha</option>
        </select>
      </form>
      <!-- The existence of this template is assumed / hardcoded. Its contents
           are always inserted inside any feature gate setting selects. -->
      <template id="feature-gate-options">
        <option value="">Use your default</option>
        <option value="release">Release</option>
        <option value="beta">Beta</option>
        <option value="alpha">Alpha</option>
      </template>
    </section>
    <section>
      <h2>Source Listings</h2>

      <section>
        <h3>Page Titles Behavior</h3>
        <p>
          What do you want the document.title of source listing pages to be?
        </p>
        <p>
          Currently, searchfox has the following sources of information available
          for use when titling the page from most specific to most generic:
        </p>
        <ul>
          <li>
            The line selection.  Searchfox source listing pages interpret their
            anchors as a comma-delimited list of line-number ranges.  All lines
            covered by the ranges will be higlighted, and the page will be
            scrolled so that the first anchored line is fully visible.  When you
            click on a searchfox link result or use a "go to" context menu
            option, searchfox will currently generate a direct line-number link.
            Heuristics are used to attempt to extract the likely symbol that the
            line selection is attempting to identify.
          </li>
          <li>
            The most specific <code>position: sticky</code> nesting area
            currently displayed at the top of the window.  When possible, source
            listings attempt to provide you with the context of what namespace,
            class, and method you're looking at by having them stick to the top
            of the viewport.
          </li>
          <li>
            The filename of the source file, not including the path.
          </li>
        </ul>
        <p>
          Originally, only the filename (sans path) was displayed, but starting
          in mid 2021 <a href="https://bugzilla.mozilla.org/show_bug.cgi?id=1702319">
          an attempt was made</a> to provide more context by adding line
          selection and "sticky" title in that order, followed by the filename.
        </p>
        <form>
          <fieldset>
            <legend>Page Title Data Sources Other Than Filename</legend>
            <ul>
              <li>
                <input type="checkbox" id="page-title--line-selection">
                <label for="page-title--line-selection">Line Selection</label>
              </li>
              <li>
                <input type="checkbox" id="page-title--sticky-symbol">
                <label for="page-title--sticky-symbol">The most specific position: sticky symbol.</label>
              </li>
            </ul>
          </fieldset>
        </form>
      </section>
    </section>
    <section>
      <h2>Fancy Bar</h2>
      <p>
        The Fancy Bar currently replaces the navigation bar on the right side of
        the screen with a collapsible sidebar.  It is the home of most widgets
        that exist to provide context.  It is currently believed to be
        <b><span id="quality--fancy-bar--enabled"></span></b> quality.
      </p>
      <section>
        <h3>Fancy Bar Feature Gate</h3>
        <form>
          <label for="fancy-bar--enabled">Fancy bar feature gate:</label>
          <select id="fancy-bar--enabled">
          </select>
        </form>
      </section>
    </section>
    <section>
      <h2>Semantic Info Queries</h2>
      <p>
        We're experimenting with exposing information like the field layout of
        classes.  According to our settings code it is
        <b><span id="quality--semantic-info--enabled"></span></b> quality.
      </p>
      <section>
        <h3>Semantic Info Queries Feature Gate</h3>
        <form>
          <label for="semantic-info--enabled">Semantic Info feature gate:</label>
          <select id="semantic-info--enabled">
          </select>
        </form>
      </section>
    </section>
    <section>
      <h2>Macro expansions</h2>
      <p>
        We're experimenting with exposing C++ macro expansions.  According to our
        settings code it is
        <b><span id="quality--semantic-info--enabled"></span></b> quality.
      </p>
      <section>
        <h3>Expansions Feature Gate</h3>
        <form>
          <label for="expansions--enabled">Expansions feature gate:</label>
          <select id="expansions--enabled">
          </select>
        </form>
      </section>
    </section>
    <section>
      <h2>Diagramming</h2>
      <p>
        Searchfox has very experimental diagramming functionality.  According to
        our settings code it is
        <b><span id="quality--diagramming--enabled"></span></b> quality, but it
        does not yet actually meet that bar.
      </p>
      <section>
        <h3>Diagramming Feature Gate</h3>
        <form>
          <label for="diagramming--enabled">Diagramming feature gate:</label>
          <select id="diagramming--enabled">
          </select>
        </form>
      </section>
    </section>
    <section>
      <h2>Debug</h2>

      <form>
        <input type="checkbox" id="debug--ui">
        <label for="debug--ui">Show debugging UI</label>
      </form>
    </section>
  </div>
  {% include 'scroll_footer.liquid' %}
</div>
{% include 'footer.liquid' %}

```

## tools/templates/ontology_ingestion_explainer.liquid
```
{% for log in logs %}
```json
{{ log | json }}
```

{% endfor %}

```

## tools/templates/help_index.liquid
```
{% include 'header_search.liquid' title: "Searchfox", autofocus: true %}
<div id="scrolling">
  <div id="content" class="content" data-no-results="No results for current query.">
    {% include 'breadcrumbs.liquid' path: "", hidden: true %}
    {% include 'navigation_panel.liquid' expanded: false %}
    {{ content }}
  </div>
  {% include 'scroll_footer.liquid' %}
</div>
{% include 'footer.liquid' %}

```

## tools/templates/scroll_footer.liquid
```

<script src="/tree-list.js"></script>
<script src="/{{tree}}/static/js/settings.js"></script>
<script src="/{{tree}}/static/js/search.js"></script>
<script src="/{{tree}}/static/js/context-menu.js"></script>
<script src="/{{tree}}/static/js/panel.js"></script>
<script src="/{{tree}}/static/js/code-highlighter.js"></script>

<footer class="footer">
    This page was generated by Searchfox:
    <a href="https://github.com/mozsearch/mozsearch">source code repository</a> |
    <a href="https://github.com/mozsearch/mozsearch-mozilla">configuration repository</a> |
    <a href="https://bugzilla.mozilla.org/buglist.cgi?product=Webtools&component=Searchfox&resolution=---">see open bugs</a> |
    <a href="https://bugzilla.mozilla.org/enter_bug.cgi?product=Webtools&component=Searchfox">file a new bug</a>
</footer>

```

## tools/templates/navigation_panel.liquid
```
<div class="panel" id="panel">
  <button id="panel-toggle">
    <span class="navpanel-icon icon-down-dir{% if expanded %} expanded{% endif %}" aria-hidden="false"></span>
    Navigation
    <a id="show-settings" title="Go to settings page" href="/{{ tree }}/pages/settings.html"><span class="navpanel-icon icon-cog expanded" aria-hidden="false"></span></a>
  </button>
  <section id="panel-content" {%
           if expanded
           %}aria-expanded="true" aria-hidden="false"{%
           else
           %}aria-expanded="false" aria-hidden="true" style="display: none;"{%
           endif %}>
    <label class="panel-accel"><input type="checkbox" id="panel-accel-enable" checked="checked">Enable keyboard shortcuts</label>
  </section>
</div>

```

## tools/templates/pipeline_explainer.liquid
```
{% for log in logs %}
```json
{{ log | json }}
```

{% endfor %}

```

## tools/templates/dir_listing.liquid
```
{%- if path == empty -%}
  {%- assign title = "/ - mozsearch" -%}
{%- else -%}
  {%- capture title %}{{ path | escape }} - mozsearch{% endcapture -%}
{%- endif -%}
{% include 'header_search.liquid' title: title, autofocus: false %}
<div id="scrolling">
  <div id="content" class="content" data-no-results="No results for current query.">
    {% include 'breadcrumbs.liquid' path: path, hidden: false %}
    {% include 'navigation_panel.liquid' expanded: false %}

    {%- include 'query_results/file_table.liquid' files: files %}
  </div>
  {% include 'scroll_footer.liquid' %}
</div>
{% include 'footer.liquid' %}

```

## tools/templates/repo_ingestion_explainer.liquid
```
{% for log in logs %}
```json
{{ log | json }}
```

{% endfor %}

```

## tools/templates/query_results.liquid
```
{%- capture title %}Q: {{ query | escape }}{% endcapture -%}
{% include 'header_query.liquid' title: title, autofocus: true %}
<div id="scrolling">
  <div id="content" class="content" data-no-results="No results for current query.">
    <div>
      {% include 'breadcrumbs.liquid' path: "", hidden: false %}
      {% include 'navigation_panel.liquid' expanded: false %}
    </div>
    {% for result_pair in results %}
      {% case result_pair[0] %}
        {% when "FlattenedResultsBundle" %}
          {% include 'query_results/rb_root.liquid' %}
        {% when "GraphResultsBundle" %}
          {% include 'query_results/graph_root.liquid' %}
        {% when "SymbolGraphCollection" %}
          {% include 'query_results/graph_collection_root.liquid' %}
        {% when "SymbolTreeTableList" %}
            {% include 'query_results/symbol_tree_table_list_root.liquid' %}
        {% when "TextFile" %}
          {% include 'query_results/text_file_root.liquid' %}
        {% else %}
          Unhandled results type {{result_pair[0]}}!
        {% endcase %}
      {% endfor %}
      {% if logs != empty -%}
      <fieldset id="query-debug-logs">
        <legend>Logs</legend>
        {%- for log in logs -%}
        <pre>
          {{- log | json | escape -}}
        </pre>
        {%- endfor -%}
      </fieldset>
      {%- endif %}
      <fieldset id="query-debug-results-json" aria-hidden="true">
        <legend>results JSON</legend>
        <pre id="query-debug-results-json-pre">
        </pre>
      </fieldset>
  </div>
  <script>
var SYM_INFO = {{ SYM_INFO_STR }};
var QUERY_RESULTS_JSON = {{ results | json }};
// Debug logs are always available for queries, so always set the variable to true;
// other pages will not have the variable defined and so the page will see undefined as false.
var IS_DEBUG_LOGS_AVAILABLE = true;
</script>
  {% include 'scroll_footer.liquid' search: "query" %}
</div>
{% include 'footer.liquid' %}

```

## tools/templates/search_template.liquid
```
{% include 'header_search.liquid' title: "{{TITLE}} - mozsearch", autofocus: false %}
<div id="scrolling">
  <div id="content" class="content" data-no-results="No results for current query.">
    {% include 'breadcrumbs.liquid' path: "", hidden: false %}
    {% include 'navigation_panel.liquid' expanded: false %}
    <script>
      var results = {{ "{{BODY}}" }};
      window.addEventListener("load", function() { showSearchResults(results); });
    </script>
  </div>
  {% include 'scroll_footer.liquid' %}
</div>
{% include 'footer.liquid' %}

```

## tools/templates/footer.liquid
```
</body>
</html>

```

## tools/templates/path.liquid
```
{%- assign path_segments = path | split: "/" %}
<h3 class="path"><div class="mimetype-bullet mimetype-icon-{{ path | fileext }}"></div>
{% for segment in path_segments -%}
    {%-  unless forloop.first -%}
    <span class="path-separator">/</span>
    {%- endunless -%}
    {%- assign path_so_far = path_segments | slice: 0, forloop.index | join: "/" %}
    <a href="/{{tree}}/source/{{path_so_far}}">{{ segment }}</a>
{%- endfor -%}</h3>

```

## tools/templates/header_search.liquid
```
{%- if autofocus -%}
  {%- assign maybe_autofocus = "autofocus " -%}
{%- else -%}
  {%- assign maybe_autofocus = "" -%}
{%- endif -%}
<!DOCTYPE html>
<html lang="en-US">
<head>
  <meta charset="utf-8">
  <meta name="color-scheme" content="light dark">
  <link href="/{{tree}}/static/icons/search.png" rel="shortcut icon">
  <title>{{ title }}</title>

  <link href="/{{tree}}/static/css/mozsearch.css" rel="stylesheet" media="screen"/>
  <link href="/{{tree}}/static/css/icons.css" rel="stylesheet" media="screen" />
  <link href="/{{tree}}/static/css/font-icons.css" rel="stylesheet" media="screen" />
</head>

<body>
<div id="fixed-header">
  <form method="get" action="/{{tree}}/search" class="search-box" id="search-box">
    <fieldset>
      <div id="query-section">
        <label for="query" class="query_label visually-hidden">Find</label>
        <input type="text" name="q"  value="{{query | escape}}" maxlength="2048" id="query" accesskey="s" title="Search" placeholder="Search {{tree}}" autocomplete="off" {{ maybe_autofocus }}/>
        <div class="zero-size-container">
          <div class="bubble" id="query-bubble">
          </div>
        </div>
        <section id="spinner"></section>
      </div>
      <div id="option-section" class="v-flex-container">
        <label for="case">
          <input type="checkbox" name="case" id="case" class="option-checkbox" value="true" accesskey="c" /><span class="access-key">C</span>ase-sensitive
        </label>
        <label for="regexp">
          <input type="checkbox" name="regexp" id="regexp" class="option-checkbox" value="true" accesskey="r"/><span class="access-key">R</span>egexp search
        </label>
      </div>
      <div id="path-section">
        <label for="path" class="query_label visually-hidden">Path</label>
        <input type="text" name="path"  value="" maxlength="2048" id="path" accesskey="s" title="Search" placeholder="Path filter (supports globbing and ^, $)" autocomplete="off" />
        <div class="zero-size-container">
          <div class="bubble" id="path-bubble">
          </div>
        </div>
      </div>
    </fieldset>
    <!-- We're marking this disabled in order to avoid the user pressing enter
         triggering the default submit behavior and conflicting with our dynamic
         logic. -->
    <input type="submit" value="Search" disabled class="visually-hidden" />
    <span id="data" data-root="/" data-search="/{{tree}}/search" data-tree="{{tree}}"></span>
  </form>
</div>

```

## tools/templates/breadcrumbs.liquid
```
{%- assign path_segments = path | split: "/" %}
  <div class="breadcrumbs"{%- if hidden %} style="display: none"{% endif -%}>
    <a href="/{{tree}}/source/">{{ tree }}</a><button id="tree-switcher" title="Open tree switcher menu" aria-expanded="false" aria-haspopup="true" aria-controls="tree-switcher-menu"></button><div id="tree-switcher-menu" title="Tree switcher" role="menu" class="context-menu" style="display: none"></div>
{%- for segment in path_segments -%}
  <span class="path-separator">/</span>
  {%- assign path_so_far = path_segments | slice: 0, forloop.index | join: "/" -%}
  <a href="/{{tree}}/source/{{path_so_far}}">{{ segment }}</a>
{%- endfor %}
  </div>

```

## tools/templates/header_query.liquid
```
{%- if autofocus -%}
  {%- assign maybe_autofocus = "autofocus " -%}
{%- else -%}
  {%- assign maybe_autofocus = "" -%}
{%- endif -%}
  <!DOCTYPE html>
<html lang="en-US">
<head>
  <meta charset="utf-8">
  <meta name="color-scheme" content="light dark">
  <link href="/{{tree}}/static/icons/search.png" rel="shortcut icon">
  <title>{{ title }}</title>

  <link href="/{{tree}}/static/css/mozsearch.css" rel="stylesheet" media="screen"/>
  <link href="/{{tree}}/static/css/icons.css" rel="stylesheet" media="screen" />
  <link href="/{{tree}}/static/css/font-icons.css" rel="stylesheet" media="screen" />
</head>

<body>
<div id="fixed-header">
  <form method="get" action="/{{tree}}/query/{{preset}}" class="search-box" id="search-box">
  <fieldset>
    <div id="query-section">
      <label for="query" class="query_label visually-hidden">Find</label>
      <input type="text" name="q"  value="{{query | escape}}" maxlength="2048" id="query" accesskey="s" title="Search" placeholder="Search {{tree}}" autocomplete="off" {{ maybe_autofocus }}/>
      <div class="zero-size-container">
        <div class="bubble" id="query-bubble">
        </div>
      </div>
      <section id="spinner"></section>
    </div>
  </fieldset>
  <!-- We currently depend on the default submit behavior for Query so we do
        not mark this disabled like we do for "search" -->
  <input type="submit" value="Search" class="visually-hidden" />
  </form>
  <span id="data" data-root="/" data-search="/{{tree}}/query/default" data-tree="{{tree}}"></span>
</div>

```

## tools/templates/query_results/line_span.liquid
```
<div class="file" role="table">
{{ line_span.contents }}
{%- if line_span.context -%}
{%- if line_span.contextsym -%}
<span class="result-context">// found in <code><a href="/{{tree}}/search?q=symbol:{{ line_span.contextsym | escape }}">{{ line_span.context }}</a></code></span>
{%- else -%}
<span class="result-context">// found in <code>{{ line_span.context }}</code></span>
{%- endif -%}
{%- endif -%}
</div>

```

## tools/templates/query_results/kind_group.liquid
```
<details open>
    <summary><h2>{{kind_group.kind}}{% if kind_group.pretty != empty %} ({{ kind_group.pretty }}){% endif %}</h2></summary>
    {%- for facet in kind_group.facets -%}
    {% include 'query_results/facet_root.liquid' facet: facet, forloop: forloop %}
    {%- endfor -%}
    {%- for file_results in kind_group.by_file -%}
    {% include 'query_results/file_results.liquid' file_results: file_results, forloop: forloop %}
    {%- endfor -%}
</details>

```

## tools/templates/query_results/file_table.liquid
```
<table class="folder-content">
    <thead>
      <tr>
        <th scope="col">Name</th>
        <th scope="col">Description</th>
        <th scope="col">Size</th>
      </tr>
    </thead>
    <tbody>
    {%- for file in files %}
        {% capture file_url %}/{{ tree }}/source/{{ file.path }}{% endcapture -%}
        {%- if file.concise.is_dir -%}
            {%- assign icon_kind = "folder" -%}
            {%- assign file_size = "" %}
        {%- else -%}
            {%- capture icon_kind %}{{ file.path | fileext }}{% endcapture -%}
            {%- capture file_size %}{{ file.concise.file_size }}{% endcapture -%}
        {%- endif -%}
        <tr>
          {% comment %}
          We used to have images be their own icon by hitting the hg server, but
          this seems wasteful for both the user and the hg server.  Searchfox is
          also absolutely capable of downsampling the image, so if people ask
          for this feature to come back, we should probably pre-compute low(er)
          resolution versions of the icons.
          {% endcomment -%}
          <td><a href="{{ file_url }}" class="mimetype-fixed-container mimetype-icon-{{ icon_kind }}">{{ file.path | split: "/" | last }}</a></td>
          <td class="description"><a href="{{ file_url }}" title="{{ file.concise.description | default: "" | escape }}">{{ file.concise.description | default: "" | escape }}</td>
          <td><a href="{{ file_url }}">{{ file_size }}</a></td>
        </tr>
{% endfor %}
    </tbody>
</table>

```

## tools/templates/query_results/graph_collection_root.liquid
```
{% for graph in results.SymbolGraphCollection.hierarchicalGraphs %}
<pre>
{{ graph | json | escape }}
</pre>
{% endfor %}

```

## tools/templates/query_results/symbol_tree_table.liquid
```
<table class="symbol-tree-table">
  <thead>
    <tr>
      <th class="name-cell">Name</th>
      <th class="type-cell">Type</th>
      <th class="line-cell">Line</th>
      {%- for platform in table.platforms -%}
        <th colspan="2">{{- platform | escape -}}</th>
      {%- endfor -%}
    </tr>
    <tr>
      <th class="name-cell"></th>
      <th class="type-cell"></th>
      <th class="line-cell"></th>
      {%- for platform in table.platforms -%}
        <th>Offset</th>
        <th>Size</th>
      {%- endfor -%}
    </tr>
  </thead>
  <tbody>
    {%- for kid in table.rows -%}
      {%- include 'query_results/symbol_tree_table_node.liquid' node: kid, platforms: table.platforms -%}
    {%- endfor -%}
  </tbody>
</table>

```

## tools/templates/query_results/file_results.liquid
```
<details open>
    <summary>{% include 'path.liquid' path: file_results.file %}</summary>
    {% for line_span in file_results.line_spans -%}
    {%- include 'query_results/line_span.liquid' line_span: line_span, forloop: forloop -%}
    {%- endfor %}
</details>

```

## tools/templates/query_results/graph_root.liquid
```
{%- if results.GraphResultsBundle.overloads_hit != empty -%}
<h3>Limits Hit:</h3>
<ul>
  {%- for overload in results.GraphResultsBundle.overloads_hit -%}
  <li>{{overload.sym}}: {{ overload.kind }}: {{ overload.exist }} exist, {{ overload.included }} included.
  {%- if overload.local_limit > 0 %} Local limit: {{ overload.local_limit }}{% endif -%}
  {%- if overload.global_limit > 0 %} Global limit: {{ overload.global_limit }}{% endif -%}
  </li>
  {%- endfor -%}
</ul>
{%- endif -%}
<script>var GRAPH_EXTRA = [];</script>
{% for graph in results.GraphResultsBundle.graphs %}
{{ graph.graph }}
<script>GRAPH_EXTRA.push({{ graph.extra | json }});</script>
{% endfor %}

```

## tools/templates/query_results/text_file_root.liquid
```
{% if results.TextFile.mime_type == "image/svg+xml" %}
    {{ results.TextFile.contents }}
{% elsif results.TextFile.mime_type == "text/x-dot" %}
<pre>{{ results.TextFile.contents | escape }}</pre>
{% else %}
Unsupported MIME type: {{ results.TextFile.mime_type }}
{% endif %}

```

## tools/templates/query_results/symbol_crossref_info_list_root.liquid
```
{% for sym_info in results.SymbolCrossrefInfoList.symbol_crossref_infos %}
{% include 'query_results/symbol.liquid' sym_info: sym_info, forloop: forloop %}
{% endfor %}

```

## tools/templates/query_results/symbol_tree_table_node.liquid
```
<tr>
  <td class="base-class-{{ node.isBaseClass }}" colspan="{{ platforms.size | times: 2 | plus: 3 }}">
    <h3>
      <code>
        <span data-symbols="{{ node.symbols }}">
          {{- node.name | escape -}}
          {%- if node.isBaseClass == true %} (base class){% endif -%}
        </span>
      </code>
    </h3>
  </td>
</tr>
{%- if node.isBaseClass == false -%}
  <tr class="class-alignment-and-size">
    <td class="name-cell"></td>
    <td class="type-cell"></td>
    <td class="line-cell"></td>
    {%- for alignmentAndSize in node.alignmentAndSize -%}
      <td class="class-alignment-cell">
        <span class="class-alignment">
          {{- alignmentAndSize.alignment | escape -}}
        </span>
      </td>
      <td class="class-size-cell">
        <span class="class-size">
          {{- alignmentAndSize.size | escape -}}
        </span>
      </td>
    {%- endfor -%}
  </tr>
{%- endif -%}
{%- for item in node.items -%}
  {%- if item contains "Field" -%}
    <tr>
      <td class="name-cell">
        <code>
          <span data-symbols="{{ item.Field.symbols }}">
            {{- item.Field.name | escape -}}
          </span>
        </code>
      </td>
      <td class="type-cell">
        <code class="field-type">
          {%- assign first = true -%}
          {%- for type in item.Field.types -%}
            {%- if first == false -%}
              |<br>
            {%- endif -%}
            <span data-symbols="{{ type.symbols }}">
              {{ type.name | escape }}
            </span>
            {%- assign first = false -%}
          {%- endfor -%}
        </code>
      </td>
      <td class="line-cell">
        {%- assign first = true -%}
        {%- for line in item.Field.lines -%}
          {%- if first == false -%}
            <br>
          {%- endif -%}
          <code>{{- line -}}</code>
          {%- assign first = false -%}
        {%- endfor -%}
      </td>
      {%- for offsetAndSize in item.Field.offsetAndSize -%}
        {%- if offsetAndSize -%}
          <td>
            <span class="field-offset">
              {{- offsetAndSize.offset | escape -}}
            </span>
          </td>
          <td>
            <span class="field-size">
              {{- offsetAndSize.size | escape -}}
            </span>
          </td>
        {%- else -%}
          <td colspan="2">
          </td>
        {%- endif -%}
      {%- endfor -%}
    </tr>
  {%- elsif item contains "Hole" -%}
    <tr>
      <td class="name-cell"></td>
      <td class="type-cell"></td>
      <td class="line-cell"></td>
      {%- for hole in item.Hole -%}
        {%- if hole -%}
          <td colspan="2">
            <span class="field-hole">
              {{- hole | escape -}}
            </span>
          </td>
        {%- else -%}
          <td colspan="2">
          </td>
        {%- endif -%}
      {%- endfor -%}
    </tr>
  {%- elsif item contains "EndPadding" -%}
    <tr>
      <td class="name-cell"></td>
      <td class="type-cell"></td>
      <td class="line-cell"></td>
      {%- for padding in item.EndPadding -%}
        {%- if padding -%}
          <td colspan="2">
            <span class="field-padding">
              {{- padding | escape -}}
            </span>
          </td>
        {%- else -%}
          <td colspan="2">
          </td>
        {%- endif -%}
      {%- endfor -%}
    </tr>
  {%- elsif item contains "Warning" -%}
    <tr>
      <th colspan="{{ platforms.size | times: 2 | plus: 3 }}">
        <em class="warning">
          {{- item.Warning | escape -}}
        </em>
      <th>
    </tr>
  {%- endif -%}
{%- endfor -%}

```

## tools/templates/query_results/facet_root.liquid
```
<div>{{ facet.label }}:
{%- for group in facet.groups -%}
{%- include 'query_results/facet_group.liquid' group: group, forloop: forloop -%}
{%- endfor -%}
</div>

```

## tools/templates/query_results/facet_group.liquid
```
{% unless forloop.first %}, {% endunless -%}
<span>{{ group.label -}}
{%- if group.nested_groups != empty -%}
({% for nested_group in group.nested_groups -%}
{%- include 'query_results/facet_group.liquid' group: nested_group, forloop: forloop -%}
{%- endfor %})
{%- endif -%}
</span>

```

## tools/templates/query_results/symbol_tree_table_list_root.liquid
```
<div id="symbol-tree-table-col-selector">
  Columns:
  <label for="col-show-name"><input type="checkbox" id="col-show-name">Name</label>
  <label for="col-show-type"><input type="checkbox" id="col-show-type">Type</label>
  <label for="col-show-line"><input type="checkbox" id="col-show-line">Line</label>
</div>
<div id="symbol-tree-table-list"
{%- if results.SymbolTreeTableList.className %} class="{{ results.SymbolTreeTableList.className }}"{% endif -%}>
{% for table in results.SymbolTreeTableList.tables %}
    {% include 'query_results/symbol_tree_table.liquid' table: table, forloop: forloop %}
{% endfor %}
</div>

```

## tools/templates/query_results/symbol.liquid
```

```

## tools/templates/query_results/pathkind_group.liquid
```
<details open>
    <summary><h1>
        {%- case pk_group.path_kind -%}
            {%- when "Normal" -%}
                Normal Results
            {%- when "ThirdParty" -%}
                Third-party code
            {%- when "Test" -%}
                Test files
            {%- when "Generated" -%}
                Generated code
        {%- endcase -%}
    </h1></summary>
    {% if pk_group.file_names.length -%}
        <details open>
            <summary>Files</summary>
            <li>
            {% for path in pk_group.file_names %}
                {% include 'path.liquid' path: path %}
            {% endfor %}
            </li>
        </details>
    {%- endif %}
    {% for kind_group in pk_group.kind_groups %}
    {% include 'query_results/kind_group.liquid' kind_group: kind_group, forloop: forloop %}
    {% endfor %}
</details>

```

## tools/templates/query_results/rb_root.liquid
```
{% for pk_group in results.FlattenedResultsBundle.path_kind_results %}
{% include 'query_results/pathkind_group.liquid' pk_group: pk_group, forloop: forloop %}
{% endfor %}

```

## tools/templates/panel.liquid
```
{%- if link != empty -%}
    {%- assign tag = "a" -%}
    {%- capture tag_extra %} href="{{ link }}"{% endcapture -%}
{%- else -%}
    {%- assign tag = "button" -%}
    {%- assign tag_extra = "" -%}
{%- endif -%}
    {%- assign accel = "" -%}
<li>
    <{{tag}} title="{{ title }}" class="icon item"{{ tag_extra }}>
        {{ title }}
        {%- if accel_key != empty -%}
            <span class="accel">{{ accel_key }}</span>"
        {%- endif -%}
        {%- if copyable -%}
            {%- if link != empty -%}
                <button class="icon copy" title="Copy to clipboard">
                    <span class="icon-docs copy-icon"></span><span class="icon-ok tick-icon"></span>
                </button>
            {%- else -%}
                <span class="icon copy indicator">
                    <span class="icon-docs copy-icon"></span><span class="icon-ok tick-icon"></span>
                </span>
            {%- endif -%}
        {%- endif -%}
    </{{tag}}>
</li>

```

## tools/.cargo/config.toml
```
[build]
# This was suggested by
# https://nnethercote.github.io/perf-book/compile-times.html and made a very
# large difference, but creates complications because it also ends up as a
# dependency for rust-analyzer operating outside our VM/container, so we are
# disabling this for now.  We are leaving the lld provisioning mechanism in case
# it becomes easier to conditionally use this in the future without creating
# frustrating experiences.
#
# For example, per the docs on this file at
# https://doc.rust-lang.org/cargo/reference/config.html we could place this file
# inside the VM/container at `~/.cargo/config.toml`.
#
# TODO: evaluate doing the above and whether it creates any complications where
# rust-analyzer is running outside the VM/container without the setting while
# inside the VM/container we're running with it.
#rustflags = ["-C", "link-arg=-fuse-ld=lld"]

```

## tools/languages/tokenizer_queries/python.scm
```
;; derived from the following plus our scip-indexer.rs decisions:
;; - https://github.com/tree-sitter/tree-sitter-python/blob/master/queries/tags.scm

(((class_definition
  name: (identifier) @name) @container)
  (#set! structure.kind "class"))

(((function_definition
  name: (identifier) @name) @container)
  (#set! structure.kind "method"))

```

## tools/languages/tokenizer_queries/rust.scm
```
;; derived from the following plus our scip-indexer.rs decisions:
;; - https://github.com/tree-sitter/tree-sitter-rust/blob/master/queries/tags.scm
;;
;; We retain the tag annotations like "definition" that we don't care about since
;; it might aid in diagnostics.
;;
;; double-comments like this are distinct from the original source.

; ADT definitions

(((struct_item
    name: (type_identifier) @name) @container)
    (#set! structure.kind "struct"))

(((enum_item
    name: (type_identifier) @name) @container)
    (#set! structure.kind "enum"))

(((union_item
    name: (type_identifier) @name) @container)
    (#set! structure.kind "union"))

;; we skip type aliases

; function definitions
;; uh, for "structure.kind" I'm dubiously mapping to clases/methods for
;; expediency.

(((function_item
    name: (identifier) @name) @container)
    (#set! structure.kind "method"))

; trait definitions
(((trait_item
    name: (type_identifier) @name) @container)
    (#set! structure.kind "class"))


; module definitions
(((mod_item
    name: (identifier) @name) @container)
    (#set! structure.kind "namespace"))

;; implementations; we're following our decision in scip-indexer.rs to only care
;; about the type and not the trait, we diverge here.

(((impl_item
    type: (type_identifier) @name) @container)
    (#set! structure.kind "class"))

```

## tools/languages/tokenizer_queries/cpp.scm
```
;; derived from https://github.com/tree-sitter/tree-sitter-cpp/pull/189
;; (tree-sitter-cpp currently otherwise lacks a tags.scm)
;;
;; expanded to also include the parent node, like we want the whole
;; `function_definition`, not just its `declarator: function_declarator` so that
;; we can also get the `body: compound_statement`.
;;
;; We also currently don't want to split out the scope from the identifier.  The
;; trade-off is that we get simplicity since we only need to deal with a single
;; captured node in our code, but with the C++ "::" delimiter baked in to the
;; string.

(((struct_specifier
  name: (type_identifier) @name
  body:(_)) @container)
  (#set! structure.kind "struct"))

(((declaration
  type: (union_specifier
    name: (type_identifier) @name)) @container)
  (#set! structure.kind "union"))

;; We explicitly don't provide a type for the "@name"; this lets us cover all of
;; - `identifier`: top-level function (not part of a class/struct)
;; - `field_identifier`: method decl/inline def (part of a class/struct)
;; - `qualified_identifier`: method def outside of the class def.  Common case
;;   has a `scope: namespace_identifier` and `name: identifier`.  As noted
;;   above, we like just using the full qualified_identifier here.
;;
;; Note that for template functions, the `function_definition` will be the
;; child of a `template_declaration` which we currently don't handle, which
;; means the template won't get marked with the function as context.  An
;; option might be to just have a separate match on `(template_declaration
;; parameters: (template_parameter_list) @name) @container`.  For
;; `template<T, X> void foo(...)` the name is then `<T, X>` which is weird but
;; workable.
;;
;; Also, `function_definition` is for inline definitions, whereas
;; `field_declaration` is for when it's just a decl and the def is elsewhere.
(((function_definition
  declarator: (function_declarator
    declarator: (_) @name)) @container)
  (#set! structure.kind "method"))

(((field_declaration
  declarator: (function_declarator
    declarator: (_) @name)) @container)
  (#set! structure.kind "field"))

;; Field definitions for members will just have a field_identifier (versus the
;; `function_declarator` above.  This also provides containment for any
;; `default_value`.
(((field_declaration
  declarator: (field_identifier) @name) @container)
  (#set! structure.kind "field"))

;; Note that we can end up with multiple declarators as in the example
;; `typedef struct {int a; int b;} S, *pS;` from
;; https://en.cppreference.com/w/cpp/language/typedef but if we just favor the
;; first thing matching the given root container node, that should be fine.
;; (Also, this should be a rare idiom hopefully!)
(((type_definition
  declarator: (type_identifier) @name) @container)
  (#set! structure.kind "typedef"))

(((enum_specifier
  name: (type_identifier) @name) @container)
  (#set! structure.kind "enum"))

(((class_specifier
  name: (type_identifier) @name) @container)
  (#set! structure.kind "class"))

;; For `namespace foo {}` the name is an `identifier`, but for
;; `namespace foo::bar` the name is an `namespace_definition_name` which has
;; multiple `identifier` children.
(((namespace_definition
  name: (_) @name) @container)
  (#set! structure.kind "namespace"))

```

## tools/languages/tokenizer_queries/typescript.scm
```
;; derived from both of the following plus our scip-indexer.rs decisions:
;; - https://github.com/tree-sitter/tree-sitter-javascript/blob/master/queries/tags.scm
;; - https://github.com/tree-sitter/tree-sitter-typescript/blob/master/queries/tags.scm
;;
;; We retain the tag annotations like "definition" that we don't care about since
;; it might aid in diagnostics.
;;
;; double-comments like this are distinct from the original source.

;; JS

(((method_definition
    name: (property_identifier) @name) @container)
  (#set! structure.kind "method"))

(([
  (class
    name: (_) @name)
  (class_declaration
    name: (_) @name)
] @container)
  (#set! structure.kind "class"))

(([
  (function
    name: (identifier) @name)
  (function_declaration
    name: (identifier) @name)
  (generator_function
    name: (identifier) @name)
  (generator_function_declaration
    name: (identifier) @name)
] @container)
  (#set! structure.kind "method"))

;; I'm assuming the lexical_declaration can have multiple children because of
;; JS allowing multiple decls via use of commas, so we're leaving the container
;; on the specific declarator.
((lexical_declaration
  (variable_declarator
    name: (identifier) @name
    value: [(arrow_function) (function)]) @container)
  (#set! structure.kind "lexdecl"))

((variable_declaration
  (variable_declarator
    name: (identifier) @name
    value: [(arrow_function) (function)]) @container)
  (#set! structure.kind "lexdecl"))

(((assignment_expression
  left: [
    (identifier) @name
    (member_expression
      property: (property_identifier) @name)
  ]
  right: [(arrow_function) (function)]
) @container)
  (#set! structure.kind "lexdecl"))


(((pair
  key: (property_identifier) @name
  value: [(arrow_function) (function)]) @container)
  (#set! structure.kind "lexdecl"))

(((export_statement value: (assignment_expression left: (identifier) @name right: ([
 (number)
 (string)
 (identifier)
 (undefined)
 (null)
 (new_expression)
 (binary_expression)
 (call_expression)
]))) @container)
  (#set! structure.kind "lexdecl"))

;; TS

(((function_signature
  name: (identifier) @name) @container)
  (#set! structure.kind "function"))

(((method_signature
  name: (property_identifier) @name) @container)
  (#set! structure.kind "method"))

(((abstract_method_signature
  name: (property_identifier) @name) @container)
  (#set! structure.kind "method"))

(((abstract_class_declaration
  name: (type_identifier) @name) @container)
  (#set! structure.kind "class"))

(((module
  name: (identifier) @name) @container)
  (#set! structure.kind "namespace"))

(((interface_declaration
  name: (type_identifier) @name) @container)
  (#set! structure.kind "class"))

```

## tools/Cargo.toml
```
[package]
name = "tools"
version = "0.1.0"
authors = ["Bill McCloskey <billm@mozilla.com>"]
edition = "2018"

[build-dependencies]
tonic-build = "0.7.1"

[dependencies]
cssparser = "0.29"
itertools = "0.10"
# Note that the "rc" feature as documented at https://serde.rs/feature-flags.html
# does not make any effort to do interning
serde = { version = "1.0.196", features = ["derive", "rc", "std"] }
serde_json = { version = "1.0.113", features = ["preserve_order"] }
serde_repr = "0.1.18"

[target.'cfg(not(target_arch = "wasm32"))'.dependencies]
async-stream = "0.3.2"
async-trait = "0.1.50"
axum = "0.6.17"
axum-macros = "0.3.7"
bitflags = { version = "2.4.2", features = ["serde"] }
chrono = "0.2"
clap = { version = "4.0", features = ["cargo", "derive", "env"] }
dot-generator = "0.2.0"
dot-structures = "0.1.0"
env_logger = "0.7.1"
fantoccini = "0.19.3"
flate2 = { version = "1", features = ["tokio"] }
futures-core = "0.3.17"
getopts = "0.2.19"
graphviz-rust = "0.2.0"
git2 = "0.16.1"
globset = "0.4.8"
hyper = "0.10"
include_dir = "0.7.2"
insta = { version = "1.39.0", features = ["json"] }
ipdl_parser = { path = "./ipdl_parser" }
json-structural-diff = "0.1.0"
lazy_static = "1.1"
lexical-sort = "0.3"
linkify = "0.2.0"
liquid = "0.26.0"
liquid-core = "0.26.0"
log = "0.4.0"
lol_html = "0.3.1"
memmap = { package = "memmap2", version = "0.5.3" }
num_cpus = "1"
petgraph = "0.6.0"
prost = "0.10.1"
protobuf = "3.2"
query-parser = "0.2.0"
regex = "1"
reqwest = "0.11.3"
rls-analysis = "0.18.1"
rls-data = "0.19.1"
scip = "0.3.3"
# NOTE: serde_json dependency is also defined above, without "std" feature.
#       The "std" feature should be enabled only for non-wasm case.
serde_json = { version = "1.0.113", features = ["preserve_order", "std"] }
shell-words = "1.0.0"
termcolor = "1.4.1"
tokio = { version = "1.6.0", features = ["rt-multi-thread", "net", "macros", "fs", "io-util", "signal"] }
tokio-stream = "0.1.8"
tree-sitter = "0.23.0"
# We previously used tree-sitter-mozcpp because it understands our XPCOM
# macrology and doesn't freak out, but since it is only used for our WIP
# hyperblame implementation and lags behind on updates, we are switching to
# tree-sitter-cpp for now.
tree-sitter-cpp = "0.23.0"
tree-sitter-python = "0.23.2"
tree-sitter-rust = "0.23.0"
tree-sitter-typescript = "0.23.0"
tree-sitter-java = "0.23.2"
tree-sitter-kotlin-ng = "1.0.1"
toml = "0.7.3"
tonic = "0.7.1"
tracing = "0.1.37"
# We explicitly do not enable the "uuid" feature because by default it will be
# serialized as a u128 in serde which will error out when attempting to convert
# to a Value or via #flatten, which causes a problem.
tracing-forest = { version = "0.1.5", features = ["smallvec", "tokio", "uuid"] }
tracing-subscriber = { version = "0.3.16", features = ["std", "env-filter", "fmt", "local-time", "registry", "json"] }
url = "2.2.2"
urlencoding = "2.1.2"
ustr = { version = "1.0", features = ["serde"] }
uuid = { version = "1.2.1", features = ["std", "v4"] }
walkdir = "2.3.2"

[patch.crates-io]
# Our very old version of hyper depends on traitobject but rustc does not like
# a formulation it uses.  We are able to use the patch mechanism documented at
# https://doc.rust-lang.org/cargo/reference/overriding-dependencies.html to
# replace the version hyper sees.  Honestly we would be fine with just using the
# crates.io version of https://github.com/philip-peterson/destructure_traitobject
# but my immediate attempts to alias that did not work out.
#
# We use revision
# https://github.com/philip-peterson/destructure_traitobject/commit/d49b0af9087b3b7848d19d5baae43948ebc7fb9d
# because that's the last revision before Cargo.toml updated the package's name
# which causes problems.
traitobject = { git = "https://github.com/philip-peterson/destructure_traitobject", rev = "d49b0af9087b3b7848d19d5baae43948ebc7fb9d" }
# If we want to debug what tracing-forest is seeing, I instrumented this with
# `println!` at some important spots.
#tracing-forest = { git = "https://github.com/asutherland/tracing-forest.git", rev = "0fef62de683f52f7888ad83891203ac4a645bf8d" }

# Build release mode with line number info for easier debugging when
# we hit panics in production
[profile.release]
debug = 1
incremental = true

```

## tools/tests/test_check_insta.rs
```
use std::str;

use serde_json::{json, to_value};
use tokio::fs::{create_dir_all, read_to_string, write};
use tools::{
    abstract_server::ServerError,
    cmd_pipeline::{build_pipeline, PipelineValues},
    glob_helper::block_in_place_glob_tree,
    logging::{init_logging, LoggedSpan},
    templating::builder::build_and_parse_pipeline_explainer,
};
use tracing::Instrument;

/// Glob-style insta test where we process all of the searchfox-tool command
/// lines under TREE/checks/inputs and output the results of those pipelines to
/// TREE/checks/snapshots using `insta` which provides diff functionality.
///
/// This very dubiously currently relies on having an environment variable
/// CHECK_ROOT defined to tell us where the TREE is.  One might wonder whether
/// this should actually be a test at all or whether it should be its own binary
/// or maybe searchfox-tool should know how to do this or what.
///
/// The reality is that we expect this to be invoked indirectly via
/// `check-index.sh` and never directly triggered via `cargo test`, so... yeah,
/// maybe we could do this in a more clean fashion.  Better opinions accepted!
///
/// `insta` does provide support for binding settings using an `async` function,
/// but its "glob" mechanism does not support `async` so we attempt to reproduce
/// the subset we need from tokio::fs.  (We don't need to use tokio for this,
/// but since we've already started down that road, we stay on the road.  The
/// use of tokio for this is separate from the async limitations on insta's glob
/// which necessitate us doing our own file-finding.)
#[tokio::test(flavor = "multi_thread")]
async fn test_check_glob() -> Result<(), std::io::Error> {
    if let Ok(check_root) = std::env::var("CHECK_ROOT") {
        init_logging();

        let explain_template = build_and_parse_pipeline_explainer();

        let input_path = format!("{}/inputs", check_root);
        let snapshot_root_path = format!("{}/snapshots", check_root);

        let mut settings = insta::Settings::clone_current();
        settings.set_prepend_module_to_snapshot(false);

        // ## Figure out the list of input files
        let input_names = block_in_place_glob_tree(&input_path, "**/*");

        for (rel_path, filename) in input_names {
            if filename.ends_with("~") {
                continue;
            }

            let input_path = format!("{}/inputs/{}{}", check_root, rel_path, filename);
            settings.set_input_file(&input_path);
            let snapshot_path = format!("{}/{}", snapshot_root_path, rel_path);
            create_dir_all(snapshot_path.clone()).await?;
            settings.set_snapshot_path(snapshot_path);
            settings.set_snapshot_suffix(filename.clone());

            let logged_span = LoggedSpan::new_logged_span(&input_path);

            let mut server_kind = "unknown".to_string();

            settings
                .bind_async(async {
                    let command = read_to_string(input_path).await.unwrap();

                    let pipeline = match build_pipeline("searchfox-tool", &command) {
                        Ok((pipeline, _)) => pipeline,
                        Err(err) => {
                            insta::assert_snapshot!(format!("Pipeline Build Error: {:?}", err));
                            return;
                        }
                    };
                    server_kind = pipeline.server_kind.clone();
                    let results = pipeline.run(true).await;

                    // TODO: In theory we should perhaps block_in_place here, but also it doesn't
                    // matter.
                    match results {
                        Ok(PipelineValues::Void) => {
                            insta::assert_snapshot!("void");
                        }
                        Ok(PipelineValues::IdentifierList(il)) => {
                            insta::assert_json_snapshot!(json!(il.identifiers));
                        }
                        Ok(PipelineValues::SymbolList(sl)) => {
                            insta::assert_json_snapshot!(&to_value(sl).unwrap());
                        }
                        Ok(PipelineValues::SymbolCrossrefInfoList(scil)) => {
                            // We used to previously just turn this into a list of
                            // just the crossref values, but we now have important
                            // metadata.
                            insta::assert_json_snapshot!(&to_value(scil).unwrap());
                        }
                        Ok(PipelineValues::SymbolGraphCollection(sgc)) => {
                            insta::assert_json_snapshot!(sgc.to_json());
                        }
                        Ok(PipelineValues::FlattenedResultsBundle(frb)) => {
                            insta::assert_json_snapshot!(&to_value(frb).unwrap());
                        }
                        Ok(PipelineValues::GraphResultsBundle(grb)) => {
                            insta::assert_json_snapshot!(&to_value(grb).unwrap());
                        }
                        Ok(PipelineValues::HtmlExcerpts(he)) => {
                            let mut aggr_str = String::new();
                            for file_excerpts in he.by_file {
                                for str in file_excerpts.excerpts {
                                    aggr_str += str.as_str();
                                }
                            }
                            insta::assert_snapshot!(&aggr_str);
                        }
                        Ok(PipelineValues::TextFile(fb)) => {
                            insta::assert_snapshot!(&fb.contents);
                        }
                        Ok(PipelineValues::JsonRecords(jr)) => {
                            let mut json_results = vec![];
                            for file_records in jr.by_file {
                                json_results.extend(file_records.records);
                            }

                            insta::assert_json_snapshot!(&json_results);
                        }
                        Ok(PipelineValues::JsonValue(jv)) => {
                            insta::assert_json_snapshot!(&jv.value);
                        }
                        Ok(PipelineValues::JsonValueList(jvl)) => {
                            insta::assert_json_snapshot!(&to_value(jvl).unwrap());
                        }
                        Ok(PipelineValues::FileMatches(fm)) => {
                            insta::assert_json_snapshot!(&to_value(fm).unwrap());
                        }
                        Ok(PipelineValues::TextMatches(tm)) => {
                            insta::assert_json_snapshot!(&to_value(tm).unwrap());
                        }
                        Ok(PipelineValues::BatchGroups(bg)) => {
                            insta::assert_json_snapshot!(&to_value(bg).unwrap());
                        }
                        Ok(PipelineValues::SymbolTreeTableList(sttl)) => {
                            insta::assert_json_snapshot!(&to_value(sttl).unwrap());
                        }
                        Err(ServerError::Unsupported) => {
                            // We're intentionally skipping doing anything here.
                            // Our assumption is that this error will only be
                            // returned in cases like the local index server
                            // being unable to handle "query" commands.
                        }
                        Err(err) => {
                            insta::assert_snapshot!(format!("Pipeline Error: {:?}", err));
                        }
                    }
                })
                .instrument(logged_span.span.clone())
                .await;

            let log_values = logged_span.retrieve_serde_json().await;

            let explain_dir = format!("{}/explanations/{}", check_root, rel_path);
            create_dir_all(explain_dir.clone()).await?;
            let explain_path = format!("{}{}-{}.md", explain_dir, filename, server_kind);

            let globals = liquid::object!({
                "logs": vec![log_values],
            });

            let output = explain_template.render(&globals).unwrap();
            write(explain_path, output).await?;
        }
    }

    Ok(())
}

```

## Vagrantfile
```
Vagrant.configure("2") do |config|
  config.vm.box = "generic/ubuntu2204"
  config.vm.box_version = "4.0.2"

  config.vm.provision :shell, privileged: false, path: "infrastructure/vagrant/indexer-provision.sh"
  config.vm.provision :shell, privileged: false, path: "infrastructure/common-provision-pre.sh"
  config.vm.provision :shell, privileged: false, path: "infrastructure/indexer-provision.sh"
  config.vm.provision :shell, privileged: false, path: "infrastructure/web-server-provision.sh"
  config.vm.provision :shell, privileged: false, path: "infrastructure/common-provision-post.sh"

  config.vm.network :forwarded_port, guest: 80, host: 16995

  config.vm.provider "virtualbox" do |v, override|
    override.vm.synced_folder './', '/vagrant'

    v.memory = 10000
    v.cpus = 4
  end

  config.vm.provider "libvirt" do |v, override|
    # Need to do this manually for libvirt...
    # local_lock makes flock() be local to the VM and avoids NFS trying to
    # acquire locks via the NLM sideband protocol.  This is sane unless you
    # are trying to run indexing inside the VM and outside the VM at the same
    # time, which you should not do.
    override.vm.synced_folder './', '/vagrant', type: 'nfs', nfs_udp: false, accessmode: "squash", mount_options: ['local_lock=all']

    # If you want to do a mozilla indexer run, and you run out of disk space,
    # the way to go is, from the host:
    #
    #   $ vagrant halt
    #   Find the image (in my case in /var/lib/libvirt/images)
    #   $ sudo qemu-img resize mozsearch_default.img +100G
    #
    # Then from the vm:
    #
    #   $ sudo lvresize -v -l +100%FREE /dev/mapper/ubuntu--vg-ubuntu--lv
    #   $ sudo resize2fs -p /dev/mapper/ubuntu--vg-ubuntu--lv
    #
    # Consider increasing v.memory a bit too, if your hardware supports it, to
    # speed up things preventing swap.
    v.memory = 10000
    v.cpus = 8
  end
end

```

## infrastructure/aws/detach-volume.py
```
#!/usr/bin/env python3

# Detaches an EBS volume from an instance to which it's attached.
#
# Usage: detach-volume.py <instance-id> <volume-id>

from __future__ import absolute_import
import sys
import boto3
import awslib

ec2 = boto3.resource('ec2')
client = boto3.client('ec2')

instanceId = sys.argv[1]
instances = ec2.instances.filter(InstanceIds=[instanceId])
instance = list(instances)[0]

# Detach the index EBS volume from the instance.
volumeId = sys.argv[2]
instance.detach_volume(VolumeId=volumeId)

awslib.await_volume(client, volumeId, 'in-use', 'available')

```

## infrastructure/aws/web-serve.sh
```
#!/usr/bin/env bash

exec &> /home/ubuntu/web-serve-log

set -x # Show commands
set -eu # Errors/undefined vars are fatal
set -o pipefail # Check all commands in a pipeline

if [ $# != 4 ]
then
    echo "usage: $0 <config-repo-path> <config-file-name> <volume-id> <channel>"
    exit 1
fi

SCRIPT_PATH=$(readlink -f "$0")
MOZSEARCH_PATH=$(dirname "$SCRIPT_PATH")/..

CONFIG_REPO=$(readlink -f $1)
CONFIG_INPUT="$2"

VOLUME_ID=$3
CHANNEL=$4

# The EBS volume will no longer be mounted at /dev/xvdf but instead at an
# arbitrarily assigned nvme id.
#
# If we run `nvme list -o json` we get output like the following (note that the
# below is from an indexer with an attached disk, not a web server, but you get
# the idea of the structure):
#
# {
#   "Devices" : [
#     {
#       "DevicePath" : "/dev/nvme0n1",
#       "Firmware" : "0",
#       "Index" : 0,
#       "ModelNumber" : "Amazon EC2 NVMe Instance Storage",
#       "ProductName" : "Unknown Device",
#       "SerialNumber" : "AWS143416FC5A55CA413",
#       "UsedBytes" : 300000000000,
#       "MaximumLBA" : 585937500,
#       "PhysicalSize" : 300000000000,
#       "SectorSize" : 512
#     },
#     {
#       "DevicePath" : "/dev/nvme1n1",
#       "Firmware" : "1.0",
#       "Index" : 1,
#       "ModelNumber" : "Amazon Elastic Block Store",
#       "ProductName" : "Unknown Device",
#       "SerialNumber" : "vol0222cf21e3b3dfbc4",
#       "UsedBytes" : 0,
#       "MaximumLBA" : 16777216,
#       "PhysicalSize" : 8589934592,
#       "SectorSize" : 512
#     }
#   ]
# }
#
# Note that the volume id is exposed as the serial number, so we can use jq to
# locate the given device.  (We do need to remove any dashes, however.)
JQ_QUERY=".Devices[] | select(.SerialNumber == \"${VOLUME_ID/-/}\") | .DevicePath"

set +o pipefail   # The grep command below can return nonzero, so temporarily allow pipefail
for (( i = 0; i < 3600; i++ ))
do
    EBS_NVME_DEV=$(sudo nvme list -o json | jq --raw-output "$JQ_QUERY")
    if [[ $EBS_NVME_DEV ]]
    then break
    fi
    sleep 1
done
set -o pipefail

echo "Index volume detected"

mkdir ~ubuntu/index
sudo mount $EBS_NVME_DEV ~ubuntu/index

# Create a writable directory for nginx caching purposes on the indexer's EBS
# store.  We choose this spot because:
# - It has more free space than our instance's root FS (~3.2G of 7.7G avail.)
# - It's bigger and hence also gets more EBS IO ops.
NGINX_CACHE_DIR=/home/ubuntu/index/nginx-cache
mkdir $NGINX_CACHE_DIR
sudo chown www-data:www-data $NGINX_CACHE_DIR

$MOZSEARCH_PATH/web-server-setup.sh $CONFIG_REPO $CONFIG_INPUT index ~ hsts $NGINX_CACHE_DIR
$MOZSEARCH_PATH/web-server-run.sh $CONFIG_REPO index ~

```

## infrastructure/aws/trigger-provision.py
```
#!/usr/bin/env python3

# trigger-provision.py <kind> <scripts to inline as part of the provisioning process...>
#
# where kind is one of:
# - indexer
# - web-server

from __future__ import absolute_import
import boto3
from datetime import datetime, timedelta
import re
import sys
import os.path

# behold the world's most sophisticated and non-hacky argument parsing!
dry_run = '--dry-run' in sys.argv
# argv0 is this script itself
use_args = [x for x in sys.argv[1:] if not x.startswith('--')]

# (we no longer have argv0 here, so we are zero-indexed)
kind = use_args[0]
provisioners = use_args[1:]

ec2 = boto3.resource('ec2')
client = boto3.client('ec2')

script = ''
for provisioner in provisioners:
    script += open(provisioner).read() + '\n'

# The stdout/stderr from running the following script that we pass can be found
# on the server in `/var/log/cloud-init-output.log`.  (Note that there's also
# a file `/var/log/cloud-init.log` that is fairly verbose that will describe
# when the script is getting launched, etc.)

user_data = f'''#!/usr/bin/env bash

cat > ~ubuntu/provision.sh <<"FINAL"
{script}
FINAL

AWS_ROOT=~ubuntu/mozsearch/infrastructure/aws
DEST_EMAIL="searchfox-aws@mozilla.com"
chmod +x ~ubuntu/provision.sh
# NOTE! The exit code from the call to provision.sh below is load-bearing,
# please do not add any statements between it and the `if` below it!
sudo -i -u ubuntu ~ubuntu/provision.sh > ~ubuntu/provision.log 2>&1
if [[ $? -ne 0 ]]; then
  # In the event of failure it's possible we don't have AWS commands, so
  # schedule our shutdown, which should STOP our EC2 instance, leaving the log
  # intact.  We schedule this to happen after 10 mins to give an opportunity for
  # investigation but also shutdown in a timely fashion if no one was actively
  # watching things.
  sudo shutdown -h +10
  echo "Provisioning failed, shutdown scheduled and sending email." >> ~ubuntu/provision.log
  $AWS_ROOT/send-provision-email.py "[{kind}]" "$DEST_EMAIL" "failed"
  exit 1
fi

echo "Provisioning complete.  Attempting Registration." >> ~ubuntu/provision.log

# AWS commands, etc. should work now if provisioning completed.
INSTANCE_ID=$(ec2metadata --instance-id)
# include the hour and minute for sufficient uniqueness
DATE_STAMP=$(date +"%Y-%m-%d-%H-%M")
AWS_REGION=us-west-2
OLD_JSON_AMI="old-ami-details.json"
NEW_JSON_AMI="new-ami-details.json"

# We get the AMI id almost immediately but the creation takes time.
aws ec2 create-image \
    --region $AWS_REGION \
    --instance-id $INSTANCE_ID \
    --name "{kind}-$DATE_STAMP" \
    --no-reboot >$NEW_JSON_AMI
NEW_AMI_ID=$(jq -r '.ImageId' $NEW_JSON_AMI)

echo "Image $NEW_AMI_ID created, waiting for it to become available." >> ~ubuntu/provision.log

# Wait for the State to become "available"
until [[ "available" == $(aws ec2 describe-images --region $AWS_REGION --image-ids $NEW_AMI_ID | jq -r '.Images[0].State') ]]
do
  sleep 10
done

echo "Image now available, updating tags." >> ~ubuntu/provision.log

# Locate the old / existing AMI (it may not exist)
aws ec2 describe-images \
    --region $AWS_REGION \
    --filters "Name=tag-key,Values={kind}" \
    >$OLD_JSON_AMI
OLD_AMI_ID=$(jq -r '.Images[0].ImageId' $OLD_JSON_AMI)

# Tag our new AMI as usable
aws ec2 create-tags \
    --region $AWS_REGION \
    --resources $NEW_AMI_ID \
    --tags "Key={kind},Value=$DATE_STAMP"

echo "New image tagged, possibly removing tag from $OLD_AMI_ID." >> ~ubuntu/provision.log

# Remove the tag from the old AMI
if [[ $OLD_AMI_ID != "null" ]]; then
    aws ec2 delete-tags \
        --region $AWS_REGION \
        --resources $OLD_AMI_ID \
        --tags "Key={kind}"
    echo "Removed tag from $OLD_AMI_ID." >> ~ubuntu/provision.log
fi

echo "Tagging complete, sending email." >> ~ubuntu/provision.log
# failsafe shutdown, although the termination should take effect first
sudo shutdown -h +10
$AWS_ROOT/send-provision-email.py "[{kind}]" "$DEST_EMAIL" "succeeded"
echo "Email sent.  Sleeping for a minute, then terminating." >> ~ubuntu/provision.log
sleep 60
aws ec2 terminate-instances --region $AWS_REGION --instance-ids $INSTANCE_ID
'''

## Shrink user_data to remain under the AWS 16384 byte user data limit
#
# We uncovered a 16384 byte limit on the user data that we weren't actively
# aware of.  Our current hacky fix is to remove comments from the payload we're
# sending.

# this converts comment lines into empty lines, but uses a negative lookahead
# assertion to leave lines that start with `#!` because those are potentially
# load-bearing.
RE_EXCLUDE_COMMENTS = re.compile('^#(?!!)[^\n]*$', re.MULTILINE)
# this merges multiple empty lines into a single line
RE_MERGE_EMPTY_LINES = re.compile('\n\n\n*')
user_data = RE_EXCLUDE_COMMENTS.sub('', user_data)
user_data = RE_MERGE_EMPTY_LINES.sub('\n\n', user_data)

if dry_run:
    print('User Data is below the line:')
    print(user_data)
    sys.exit(0)

# Performing lookup from https://cloud-images.ubuntu.com/locator/ec2/ by
# searching on "22.04 us-west-2 amd64" we get:
#
# us-west-2	Noble Numbat	24.04 LTS	amd64	hvm:ebs-ssd-gp3	20240927	ami-04dd23e62ed049936	hvm
#
# and then we copy the ami ID into here:
image_id = 'ami-04dd23e62ed049936'

launch_spec = {
    'ImageId': image_id,
    'KeyName': 'Main Key Pair',
    'SecurityGroups': ['indexer'],
    'UserData': user_data,
    'InstanceType': 'c5d.2xlarge',
    'BlockDeviceMappings': [],
    # In order to be able to automatically have the `aws` command work so that
    # we can resize our root partition and now to create the AMI, we need to
    # assign an IAM role.
    #
    # This also could potentially let the provisioning process checkpoint itself
    # into a new AMI.
    'IamInstanceProfile': {
        'Name': 'indexer-role',
    },
    'TagSpecifications': [{
        'ResourceType': 'instance',
        'Tags': [{
            'Key': 'provisioner',
            'Value': kind,
         }],
    }],
}

client.run_instances(MinCount=1, MaxCount=1, **launch_spec)

```

## infrastructure/aws/awslib.py
```
from __future__ import absolute_import
from __future__ import print_function
import boto3
import time

def await_volume(client, volumeId, waitingState, finishedState):
    while True:
        volumes = client.describe_volumes(VolumeIds=[volumeId])
        state = volumes['Volumes'][0]['State']
        if state != waitingState:
            break
        time.sleep(1)

    if state != finishedState:
        print('Unexpected volume state (expected {}): {}'.format(finishedState, volumes))
        sys.exit(1)

def await_instance(client, instanceId, waitingState, finishedState):
    exceptionCount = 0
    while True:
        try:
            instances = client.describe_instances(InstanceIds=[instanceId])
            state = instances['Reservations'][0]['Instances'][0]['State']['Name']
            if waitingState and state != waitingState:
                break
            if state == finishedState:
                break
        except:
            if exceptionCount > 2:
                raise
            exceptionCount += 1
            print('Unable to describe instance ID {}, will retry...'.format(instanceId))
            time.sleep(10)
        time.sleep(1)

    if state != finishedState:
        print('Unexpected instance state (expected {}): {}'.format(finishedState, instances))
        sys.exit(1)

```

## infrastructure/aws/upload.py
```
#!/usr/bin/env python3

# Uploads data to an S3 bucket
#
# Usage: upload.py <filename> <bucket> <key>

from __future__ import absolute_import
import sys
import boto3
import awslib

filename = sys.argv[1]
bucket = sys.argv[2]
key = sys.argv[3]

s3 = boto3.resource('s3')

data = open(filename, 'rb')
s3.Bucket(bucket).upload_fileobj(data, key)
s3.ObjectAcl(bucket, key).put(ACL='public-read')

```

## infrastructure/aws/upload-lambda-zips-from-outside-vm.sh
```
#!/usr/bin/env bash

# This script is intended to be run outside the VM with AWS creds active after
# running `build-lambda-zips-from-inside-vm.sh`.  There should be 5
# `lambda-releaseN.zip` files in the root of the MOZSEARCH dir and you should
# be running the script from that root.  See `aws.md` for more details.

set -x # Show commands
set -eu # Errors/undefined vars are fatal
set -o pipefail # Check all commands in a pipeline


aws lambda update-function-code \
  --function-name start-release1-indexer \
  --zip-file fileb://lambda-release1.zip
rm lambda-release1.zip

aws lambda update-function-code \
  --function-name start-release2-indexer \
  --zip-file fileb://lambda-release2.zip
rm lambda-release2.zip

aws lambda update-function-code \
  --function-name start-release3-indexer \
  --zip-file fileb://lambda-release3.zip
rm lambda-release3.zip

aws lambda update-function-code \
  --function-name start-release4-indexer \
  --zip-file fileb://lambda-release4.zip
rm lambda-release4.zip

aws lambda update-function-code \
  --function-name start-release5-indexer \
  --zip-file fileb://lambda-release5.zip
rm lambda-release5.zip

```

## infrastructure/aws/mkscratch.sh
```
#!/usr/bin/env bash

set -x # Show commands
set -eu # Errors/undefined vars are fatal
set -o pipefail # Check all commands in a pipeline

# Create the "index-scratch" directory where each specific tree's indexing
# byproducts live while the indexing is ongoing.  To do this, we need to figure
# out what the device's partition is.
#
# If we run `nvme list -o json` on an m5d.2xlarge we get output like the
# following (as of Aug 7th, 2024):
#
# {
#   "Devices" : [
#     {
#       "NameSpace" : 1,
#       "DevicePath" : "/dev/nvme0n1",
#       "Firmware" : "1.0",
#       "Index" : 0,
#       "ModelNumber" : "Amazon Elastic Block Store",
#       "ProductName" : "Non-Volatile memory controller: Amazon.com, Inc. NVMe EBS Controller",
#       "SerialNumber" : "vol0bbec15c8360404c3",
#       "UsedBytes" : 21474836480,
#       "MaximumLBA" : 41943040,
#       "PhysicalSize" : 21474836480,
#       "SectorSize" : 512
#     },
#     {
#       "NameSpace" : 1,
#       "DevicePath" : "/dev/nvme1n1",
#       "Firmware" : "0",
#       "Index" : 1,
#       "ModelNumber" : "Amazon EC2 NVMe Instance Storage",
#       "ProductName" : "Non-Volatile memory controller: Amazon.com, Inc. Me SSD Controller",
#       "SerialNumber" : "AWS271A679A97BB1AC18",
#       "UsedBytes" : 300000000000,
#       "MaximumLBA" : 585937500,
#       "PhysicalSize" : 300000000000,
#       "SectorSize" : 512
#     }
#   ]
# }
#
# If we run it on an m5d.4xlarge which has 2 instance SSD's attached (as of
# Aug 7th, 2024), we get:
#
# {
#   "Devices" : [
#     {
#       "NameSpace" : 1,
#       "DevicePath" : "/dev/nvme0n1",
#       "Firmware" : "1.0",
#       "Index" : 0,
#       "ModelNumber" : "Amazon Elastic Block Store",
#       "ProductName" : "Non-Volatile memory controller: Amazon.com, Inc. NVMe EBS Controller",
#       "SerialNumber" : "vol0469084c4103e93f6",
#       "UsedBytes" : 21474836480,
#       "MaximumLBA" : 41943040,
#       "PhysicalSize" : 21474836480,
#       "SectorSize" : 512
#     },
#     {
#       "NameSpace" : 1,
#       "DevicePath" : "/dev/nvme1n1",
#       "Firmware" : "0",
#       "Index" : 1,
#       "ModelNumber" : "Amazon EC2 NVMe Instance Storage",
#       "ProductName" : "Non-Volatile memory controller: Amazon.com, Inc. Me SSD Controller",
#       "SerialNumber" : "AWS3874D9799F2AE3EBE",
#       "UsedBytes" : 300000000000,
#       "MaximumLBA" : 585937500,
#       "PhysicalSize" : 300000000000,
#       "SectorSize" : 512
#     },
#     {
#       "NameSpace" : 1,
#       "DevicePath" : "/dev/nvme2n1",
#       "Firmware" : "0",
#       "Index" : 2,
#       "ModelNumber" : "Amazon EC2 NVMe Instance Storage",
#       "ProductName" : "Non-Volatile memory controller: Amazon.com, Inc. Me SSD Controller",
#       "SerialNumber" : "AWS2DECD522BEB58C35D",
#       "UsedBytes" : 300000000000,
#       "MaximumLBA" : 585937500,
#       "PhysicalSize" : 300000000000,
#       "SectorSize" : 512
#     }
#   ]
# }
#
# We are interested in the "Instance Storage" device, and so we can use jq to
# filter this.  Note that on larger instances like m5d.4xlarge, there will be
# multiple instance SSDs and currently we only want the 1st one we find.
#
# TODO: In the future we might consider doing some kind of performance RAID when
# there are multiple SSDs.
INSTANCE_STORAGE_DEV=$(sudo nvme list -o json | jq --raw-output 'first(.Devices[] | select(.ModelNumber | contains("Instance Storage")) | .DevicePath)')
sudo mkfs -t ext4 $INSTANCE_STORAGE_DEV
sudo mount $INSTANCE_STORAGE_DEV /mnt
sudo mkdir /mnt/index-scratch
sudo chown ubuntu.ubuntu /mnt/index-scratch

# For swap purposes, let's see if there was a 2nd instance storage; we use nth(1; ...)
# for this.  If there is no 2nd entry, we will get an empty string.
#
# FIXME: On jq 1.6, nth() prints the last item instead of an empty string
#        even if the index is out of range, which can be the same device as
#        the first one, used by INSTANCE_STORAGE_DEV.
#        We should bump jq to 1.7, but it's not available on ubuntu jammy.
SWAP_STORAGE_DEV=$(sudo nvme list -o json | jq --raw-output 'nth(1; .Devices[] | select(.ModelNumber | contains("Instance Storage")) | .DevicePath)')

# FIXME: Once jq is bumped to 1.7+, the comparison against INSTANCE_STORAGE_DEV
#        should be removed.
if [[ $SWAP_STORAGE_DEV && $SWAP_STORAGE_DEV != $INSTANCE_STORAGE_DEV ]]; then
  sudo mkswap $SWAP_STORAGE_DEV
  sudo swapon $SWAP_STORAGE_DEV
else
  SWAP_FILE=/mnt/swapfile
  # 8 GiB swap
  sudo dd if=/dev/zero of=$SWAP_FILE bs=128M count=64
  sudo chmod 600 $SWAP_FILE
  sudo mkswap $SWAP_FILE
  sudo swapon $SWAP_FILE
fi

```

## infrastructure/aws/attach-index-volume.py
```
#!/usr/bin/env python3

# Creates an EBS volume for the index and attaches it to a given instance as /dev/xvdf.
# Prints the volume ID on stdout.
# Usage: attach-index-volume.py <channel> <instance-id>

from __future__ import absolute_import
from __future__ import print_function
import sys
import boto3
import awslib
from datetime import datetime

channel = sys.argv[1]
instanceId = sys.argv[2]

ec2 = boto3.resource('ec2')
client = boto3.client('ec2')

# Find availability zone
instances = ec2.instances.filter(InstanceIds=[instanceId])
instance = list(instances)[0]

r = client.create_volume(
    Size=300,
    VolumeType='gp2',
    AvailabilityZone=instance.placement['AvailabilityZone'],
)

volumeId = r['VolumeId']
awslib.await_volume(client, volumeId, 'creating', 'available')

client.create_tags(Resources=[volumeId], Tags=[{
    'Key': 'index',
    'Value': str(datetime.now()),
}, {
    'Key': 'channel',
    'Value': channel,
}])


instance.attach_volume(VolumeId=volumeId, Device='xvdf')

awslib.await_volume(client, volumeId, 'available', 'in-use')

instance.modify_attribute(BlockDeviceMappings=[{
    'DeviceName': 'xvdf',
    'Ebs': {
        'DeleteOnTermination': True,
    },
}])

print(volumeId)

```

## infrastructure/aws/warning-suppression.patterns
```
^Warning: The unit file, source configuration file or drop-ins of
^warning: redirecting to https://hg-edge.mozilla.org/

```

## infrastructure/aws/index.sh
```
#!/usr/bin/env bash

set -x # Show commands
set -eu # Errors/undefined vars are fatal
set -o pipefail # Check all commands in a pipeline

if [ $# != 6 ]
then
    echo "usage: $0 <branch> <channel> <mozsearch-repo-url> <config-repo-url> <config-repo-path> <config-file-name>"
    exit 1
fi

SCRIPT_PATH=$(readlink -f "$0")
MOZSEARCH_PATH=$(dirname "$SCRIPT_PATH")/../..

BRANCH=$1
CHANNEL=$2
MOZSEARCH_REPO_URL=$3
CONFIG_REPO_URL=$4
CONFIG_REPO_PATH=$(readlink -f $5)
CONFIG_FILE_NAME="$6"

EC2_INSTANCE_ID=$(ec2metadata --instance-id)

# Find the revisions we actually checked out so that we can pass these to the
# web server because indexer and web server are a matched pair and we don't
# randomly want it running a different revision!
pushd mozsearch
MOZSEARCH_REV=$(git rev-parse HEAD)
popd

pushd config
CONFIG_REV=$(git rev-parse HEAD)
popd

echo "Branch is $BRANCH"
echo "  mozsearch repo $MOZSEARCH_REPO_URL rev is $MOZSEARCH_REV"
echo "  config repo $CONFIG_REPO_URL rev is $CONFIG_REV"
echo "Channel is $CHANNEL"

VOLUME_ID=$($AWS_ROOT/attach-index-volume.py $CHANNEL $EC2_INSTANCE_ID)

# Since we know the volume id and it's exposed as the `SerialNumber` in the JSON
# structure (see above), we can look that up here too.  Note that we need to
# remove the/any dash from the volume id.
JQ_QUERY=".Devices[] | select(.SerialNumber == \"${VOLUME_ID/-/}\") | .DevicePath"

set +o pipefail   # The grep command below can return nonzero, so temporarily allow pipefail
for (( i = 0; i < 3600; i++ ))
do
    EBS_NVME_DEV=$(sudo nvme list -o json | jq --raw-output "$JQ_QUERY")
    if [[ $EBS_NVME_DEV ]]
    then break
    fi
    sleep 1
done
set -o pipefail

echo "Index volume detected"

# Create the "index" directory where the byproducts of indexing will permanently
# live.
sudo mkfs -t ext4 $EBS_NVME_DEV
sudo mkdir /index
sudo mount $EBS_NVME_DEV /index
sudo chown ubuntu.ubuntu /index

# Do indexer setup locally on disk.
$MOZSEARCH_PATH/infrastructure/indexer-setup.sh $CONFIG_REPO_PATH $CONFIG_FILE_NAME /mnt/index-scratch
case "$CHANNEL" in
release* )
    # Only upload files on release channels.
    $MOZSEARCH_PATH/infrastructure/indexer-upload.sh $CONFIG_REPO_PATH /mnt/index-scratch
    ;;
* )
    ;;
esac
# Now actually run the indexing, telling the scripts to move the data to the
# permanent index directory.
$MOZSEARCH_PATH/infrastructure/indexer-run.sh $CONFIG_REPO_PATH /mnt/index-scratch /index

date
echo "Indexing complete"

# Copy indexing log to index mount so it's easy to get to from the
# web server instance
cp ~ubuntu/index-log /index/index-log

# Because it's possible for shells to be in the "/index" dir and for this to
# cause problems unmounting /index (:asuth has done this a lot...), terminate
# all extant ssh sessions for our normal user as a one-off.  This does not
# preclude logging back in.
#
# pkill returns an exit status of 1 if no processes matched, so we need to make
# sure that we don't care about the exit code as this is just a hygiene measure,
# we don't actually expect for there to usually be sshd instances.
pkill -u ubuntu sshd || true
# And then sleep a little just in case there's some cleanup time required.
sleep 1
sudo umount /index

$AWS_ROOT/detach-volume.py $EC2_INSTANCE_ID $VOLUME_ID
$AWS_ROOT/trigger-web-server.py \
    $CHANNEL \
    $MOZSEARCH_REPO_URL \
    $MOZSEARCH_REV \
    $CONFIG_REPO_URL \
    $CONFIG_REV \
    $CONFIG_FILE_NAME \
    $VOLUME_ID \
    "$MOZSEARCH_PATH/infrastructure/web-server-check.sh" \
    $CONFIG_REPO_PATH \
    "/mnt/index-scratch"

case "$CHANNEL" in
release* )
    DEST_EMAIL="searchfox-aws@mozilla.com"
    ;;
* )
    # For dev-channel runs, send emails to the author of the HEAD commit in the
    # repo.
    DEST_EMAIL=$(git --git-dir="$MOZSEARCH_PATH/.git" show --format="%aE" --no-patch HEAD)
    ;;
esac

$AWS_ROOT/send-warning-email.py "$AWS_ROOT/warning-suppression.patterns" "$CHANNEL/$BRANCH" "$DEST_EMAIL" /home/ubuntu/index-log

gzip -k ~ubuntu/index-log
$AWS_ROOT/upload.py ~ubuntu/index-log.gz indexer-logs "index-$(date -Iminutes)_${CHANNEL}_${CONFIG_FILE_NAME%.*}.gz"

case "$CHANNEL" in
release* )
    # Don't send completion email notification for release channel.
    ;;
* )
    $AWS_ROOT/send-done-email.py "$CHANNEL/$BRANCH" "$DEST_EMAIL"
    ;;
esac

# Give logger time to catch up
sleep 30
$AWS_ROOT/terminate-indexer.py $EC2_INSTANCE_ID

```

## infrastructure/aws/send-warning-email.py
```
#!/usr/bin/env python3

from __future__ import absolute_import
import sys
import boto3
import os
import subprocess

client = boto3.client('ses')
suppression_file = sys.argv[1]
subj_prefix = sys.argv[2]
dest_email = sys.argv[3]
log_location = sys.argv[4]
warning_limit = "50"

# We use the idiom described at
# https://docs.python.org/3/library/subprocess.html#replacing-shell-pipeline
# to run a grep that first excludes any warnings we don't care about and
# then pipe that to our grep that finds warnings and provides "before"
# context.
suppress_proc = subprocess.Popen(["grep", "--invert-match", "-f", suppression_file, log_location], stdout=subprocess.PIPE)

# The regex here intentionally matches any `warn!` logger output from rust code
matches_proc = subprocess.Popen(["grep", "-B16", "-i", "-m", warning_limit, "-P", "^([ ]*|\\[\\d{4}-\\d{2}-\\d{2}T\\d{2}:\\d{2}:\\d{2}Z )warn", '-'],
                                stdin=suppress_proc.stdout,
                                stdout=subprocess.PIPE)
suppress_proc.stdout.close() # allow SIGPIPE from matches_proc to suppress_proc
warnings, _ =  matches_proc.communicate()

if matches_proc.returncode:
    # grep found no matches, so no need to send this email
    sys.exit(0)

if dest_email == "test":
    print("warnings:\n", warnings.decode())
    sys.exit(0)

warnings = warnings.decode('utf-8', 'replace')

response = client.send_email(
    Source='daemon@searchfox.org',
    Destination={
        'ToAddresses': [
            dest_email,
        ]
    },
    Message={
        'Subject': {
            'Data': subj_prefix + ' Searchfox warnings',
        },
        'Body': {
            'Text': {
                'Data': 'Searchfox produced warnings during indexing! The first ' + warning_limit + ' warnings:\n\n' + warnings,
            },
        }
    }
)

```

## infrastructure/aws/send-done-email.py
```
#!/usr/bin/env python3

from __future__ import absolute_import
import sys
import boto3
import os
import subprocess

client = boto3.client('ses')
subj_prefix = sys.argv[1]
dest_email = sys.argv[2]

response = client.send_email(
    Source='daemon@searchfox.org',
    Destination={
        'ToAddresses': [
            dest_email,
        ]
    },
    Message={
        'Subject': {
            'Data': subj_prefix + ' Searchfox indexing complete',
        },
        'Body': {
            'Text': {
                'Data': 'Searchfox completed indexing successfully!',
            },
        }
    }
)

```

## infrastructure/aws/trigger_common.py
```
# Note that any 3rd party lib dependencies will also need to be added to the
# install steps in `build-lambda-indexer-start.sh`.
import boto3
import argparse
from datetime import datetime, timedelta
import sys
import os.path

class TriggerCommandBase:
    '''
    Helper class for launching indexers to extract out the commonality.  This
    doesn't need to get particularly complex.

    The general idea for now is:
    - If you have a new high-level action that wants to trigger an indexer,
      create a new script like `trigger_indexer.py` that's a thin wrapper
      around this class/module.
    - Place logic that can be used across all VM variants in here, or specialize
      in the subclass as appropriate.
    '''
    def __init__(self, indexer_type, core_script, max_runtime_hours):
        self.indexer_type = indexer_type
        self.core_script = core_script
        self.max_runtime_hours = max_runtime_hours

        self.args = None

    def make_parser(self):
        parser = argparse.ArgumentParser()
        parser.add_argument('mozsearch_repo')
        parser.add_argument('config_repo')
        parser.add_argument('config_input')
        parser.add_argument('branch')
        parser.add_argument('channel')

        parser.add_argument('--verbose', '-v', action='count', default=0)

        parser.add_argument('--setenv', dest='env_vars', action='append', default=[])

        return parser

    def parse_args(self, args=None):
        parser = self.make_parser()
        self.args = parser.parse_args(args)

    def script_args_after_branch_and_channel(self, args):
        '''
        This method allows subclasses to contribute arguments to their script.
        Note that all scripts will be provided with the branch and channel as
        their first two arguments because `main.sh` needs this information and
        assumes those arguments exist.

        This method's return value is interpolated directly into the bash shell
        script built by `trigger` without any escaping.  This means arguments
        should probably be quoted and escaped as appropriate.
        '''
        return ""

    def build_extra_commands(self, args):
        '''
        Builds a single command-string, including newlines, that will be
        inserted into the bash shell script built by `trigger`.
        '''
        cmds = []

        for setenv in args.env_vars:
            cmds.append('export ' + setenv)

        return "\n".join(cmds)

    def trigger(self):
        if self.args is None:
            raise Exception('Arguments were not parsed first!')
        args=self.args
        extra_args = self.script_args_after_branch_and_channel(args)

        extra_commands = self.build_extra_commands(args)

        ec2 = boto3.resource('ec2')
        client = boto3.client('ec2')

        # Indexers that want more powerful instance:
        # - release4 (bug 1922407); runtimes have hit and timed out at 12 hours
        #   using an m5d.2xlarge
        # - release5 (bug 1912078 ish): runtime hit 8.5 hours and much of this
        #   is simply build duration for webkit, so should parallelize easily.
        #   This also gives us a ton of SSD-backed swap as a release valve.
        #
        # This decision is baked into the script here rather than present in
        # config files because we run this script as part of a lambda job run
        # out of a zipball we upload to AWS without doing any git checkouts,
        # etc.  The git stuff happens on the indexer after it is spawned.  (This
        # could of course be changed, but potentially would make the lambda jobs
        # more complex / brittle.)
        if args.channel == "release4" or args.channel == "release5":
            instance_type = 'm5d.4xlarge'
        else:
            instance_type = 'm5d.2xlarge'

        # Terminate any "running" or "stopped" instances.  We used to only
        # terminate "running" instances with the theory that someone might get
        # around to investigating the "stopped" instance, but the reality is
        # that:
        # - Frequently failures are one-offs that have an obvious cause in the
        #   emailed log.  And we can provide more log context!
        # - If someone is going to look at the problem, they can usually decide
        #   to do that before the next indexer run.  The investigation doesn't
        #   need to complete, the indexer just needs to be re-tagged to not look
        #   like an indexer.  Currently this would require using the EC2 console
        #   but this can easily be added to `channel-tool.py`.
        instances = ec2.instances.filter(Filters=[{'Name': 'tag-key', 'Values': [self.indexer_type]},
                                            {'Name': 'tag:channel', 'Values': [args.channel]},
                                            {'Name': 'instance-state-name', 'Values': ['running', 'stopping', 'stopped']}])
        for instance in instances:
            print(f"Terminating existing {instance.state['Name']} {self.indexer_type} {instance.instance_id} for channel {args.channel}")
            instance.terminate()

        user_data = '''#!/usr/bin/env bash

    cd ~ubuntu
    {extra_commands}
    sudo -i -u ubuntu {cmd_env_vars} ./update.sh "{mozsearch_repo}" "{branch}" "{config_repo}" "{branch}"
    sudo -i -u ubuntu {cmd_env_vars} mozsearch/infrastructure/aws/main.sh {core_script} {max_runtime_hours} "{branch}" "{channel}" {extra_args}
    '''.format(
        core_script=self.core_script,
        max_runtime_hours=self.max_runtime_hours,
        branch=args.branch,
        channel=args.channel,
        mozsearch_repo=args.mozsearch_repo,
        config_repo=args.config_repo,
        cmd_env_vars=" ".join(args.env_vars),
        extra_commands=extra_commands,
        extra_args=extra_args
        )

        block_devices = []

        # We only have "indexer" and "web-server" AMI types, and currently all
        # subclasses do want to be using an indexer AMI which is consistent with
        # our hardcoded choice of InstanceType and role, etc.
        images = client.describe_images(Filters=[{'Name': 'tag-key', 'Values': ['indexer']}])
        # TODO: sort/pick the highest datestamp-y "indexer" tag Value.
        image_id = images['Images'][0]['ImageId']

        launch_spec = {
            'ImageId': image_id,
            'KeyName': 'Main Key Pair',
            'SecurityGroups': ['indexer-secure'],
            'UserData': user_data,
            'InstanceType': instance_type,
            'BlockDeviceMappings': block_devices,
            'IamInstanceProfile': {
                'Name': 'indexer-role',
            },
            'TagSpecifications': [{
                'ResourceType': 'instance',
                'Tags': [{
                    'Key': self.indexer_type,
                    'Value': str(datetime.now())
                }, {
                    'Key': 'channel',
                    'Value': args.channel,
                }, {
                    'Key': 'branch',
                    'Value': args.branch,
                }, {
                    'Key': 'mrepo',
                    'Value': args.mozsearch_repo,
                }, {
                    'Key': 'crepo',
                    'Value': args.config_repo,
                }, {
                    'Key': 'cfile',
                    'Value': args.config_input,
                }],
            }],
        }

        if args.verbose > 0:
            print('Launch Spec:')
            print(repr(launch_spec))

        return client.run_instances(MinCount=1, MaxCount=1, **launch_spec)

```

## infrastructure/aws/make-crontab.py
```
#!/usr/bin/env python3

from __future__ import absolute_import
from __future__ import print_function
import datetime
import os
import subprocess
import sys

subj_prefix = sys.argv[1]
dest_email = sys.argv[2]
allowed_runtime_hours = int(sys.argv[3])

dir_path = os.path.dirname(os.path.realpath(__file__))

delta = datetime.timedelta(hours=allowed_runtime_hours)
when = datetime.datetime.now() + delta
s = when.strftime('%M %H %d %m *')

s += ' ' + os.path.join(dir_path, 'send-failure-email.py') + ' ' + subj_prefix + ' ' + dest_email + '\n'

print(s)

p = subprocess.Popen(['crontab', '-'], stdin=subprocess.PIPE)
p.communicate(s.encode())

```

## infrastructure/aws/main.sh
```
#!/usr/bin/env bash

exec &> /home/ubuntu/index-log

set -x # Show commands
set -u # Undefined vars are fatal
set -o pipefail # Check all commands in a pipeline

# Don't set -e here, because that will just exit the script on error. As
# this is the "root" script of the indexer process, exiting this script
# will effectively result in a silent failure. Instead, we set a trap to
# catch errors emanating from commands in this script and trigger a failure
# email.

trap 'handle_error' ERR

handle_error() {
    # In the event of failure, we will have byproducts leftover on the local
    # drive that will be lost if we don't first move them to the persistent EBS
    # store.  We create an "interrupted" parent directory for these contents in
    # order to avoid any ambiguities about what the state of the scratch drive
    # was. We only do this if we got far enough to actually start indexing.
    if [ -d "/index" ]; then
        mkdir -p /index/interrupted
        if [ -d "/mnt/index-scratch" ]; then
            mv -f /mnt/index-scratch/* /index/interrupted
        fi
        # try and get a list of open files that might have caused problems
        # unmounting /index or moving things from /mnt/index-scratch to /index.
        lsof | grep /index
    fi

    # Send failure email and shut down. Release channel failures get sent to the
    # default email address, other channel failures get sent to the author of
    # the head commit.
    $AWS_ROOT/send-failure-email.py "${EMAIL_PREFIX}" "${DEST_EMAIL}"

    # Need to terminate the script on error explicitly, otherwise bash
    # will continue the script after running the trap handler.
    exit 1
}

# Pull out the first two arguments, which are for consumption by this
# main.sh script. The rest of the arguments get passed as arguments to
# TARGETSCRIPT, so we leave them in $*
TARGETSCRIPT=$1; shift
MAXHOURS=$1; shift

# See index.sh and rebuild-blame.sh for the arguments to this script.
# This code assumes that the first two arguments for both of those scripts
# are the branch and channel values.

# Note that we set up the EMAIL_PREFIX and DEST_EMAIL variables as early
# as possible, so that they can be used by the handle_error function in
# case anything goes wrong.

SELF=$(readlink -f "$0")
BRANCH=$1
CHANNEL=$2
export AWS_ROOT=$(dirname "$SELF")

EMAIL_PREFIX="${CHANNEL}/${BRANCH}"

case "${CHANNEL}" in
    release* )
        DEST_EMAIL="searchfox-aws@mozilla.com"
        ;;
    * )
        DEST_EMAIL=$(git --git-dir="${AWS_ROOT}/../../.git" show --format="%aE" --no-patch HEAD)
        ;;
esac

mkdir -p ~/.aws
cat > ~/.aws/config <<"STOP"
[default]
region = us-west-2
STOP

# Create a crontab entry to send failure email if TARGETSCRIPT takes too long. This
# is basically a failsafe for if this instance doesn't shut down within
# 10 hours.
${AWS_ROOT}/make-crontab.py "${EMAIL_PREFIX}/timeout" "${DEST_EMAIL}" ${MAXHOURS}

# Daily cron jobs can include things like the `locate` `updatedb` script which
# can end up tying up the indexer's mount point.  These are run via `run-parts`
# which only runs executable files, so we remove that bit from all of the daily
# jobs.  We do this here as part of running the indexer rather than as part of
# provisioning because we don't want to disable the cron jobs in our local
# testing VMs, etc.
#
# We also disable weekly cron jobs because we don't need them either.  We don't
# bother with any of the longer time intervals because the directories are
# currently empty and so the globbing gets more complicated for no point.
echo "Disabling daily and weekly cron jobs for this indexing run"
sudo chmod -x /etc/cron.daily/* /etc/cron.weekly/*

echo "Creating index-scratch on local instance SSD"
${AWS_ROOT}/mkscratch.sh

# Put our tmp directory on index scratch instead of /tmp which is on our EBS
# root image and which would be both slower and has had problems with filling
# up (bug 1712578).
mkdir -p /mnt/index-scratch/tmp
export TMPDIR=/mnt/index-scratch/tmp

# Run target script with arguments supplied to this script.
${AWS_ROOT}/${TARGETSCRIPT} $*

```

## infrastructure/aws/scp-while-sshed.py
```
#!/usr/bin/env python3

# This is a helper to let you copy files from a machine that you've already
# ssh'ed into via `ssh.py`.  `ssh.py` handles changing the security bits for
# the instance so that ssh/scp can connect to the server.  If you try and run
# this script without ssh.py currently being connected, this script will not
# work.
#
# The general syntax is
#   ./scp-while-sshed.py <instance-id> <remote-file-path> <local-target>
#
# If you wanted to copy /foo/bar/baz to the current directory
#   ./scp-while-sshed.py INSTANCEID /foo/bar/baz .
#
# If you wanted to copy all log files from /fancy/path to ~/local/dir
#   ./scp-while-sshed.py INSTANCEID "/fancy/path/*.log" ~/local/dir
#
# The key thing with wildcards is the standard shell expansion thing where if
# you fail to double-quote the argument, path expansion may be performed in your
# own local context.  (Sometimes it turns out okay because if the shell doesn't
# find any matches, it just passes the string through unchanged.)
#
# You CANNOT pass multiple arguments for the source, because the script is
# currently very simple and only prefixes the first argument you give it, and
# only consumes 2 path arguments.
#
# Other usage notes:
# - If you don't pass enough arguments the script yells at you and prints the
#   correct usage.
# - Unlike ssh.py, we don't show a list of all instances because you already
#   need to be connected.  (And you might be connected to multiple instance, so
#   we can't guess.)
# - If you want to copy a file TO the machine, you need to update this script or
#   fork it to make a TO version.

from __future__ import absolute_import
from __future__ import print_function
import boto3
from datetime import datetime
import os
import sys
import subprocess
import time

ec2 = boto3.resource('ec2')

def scp_from(instance, file_on_host, local_target):
    # If there is a private key at ~/.aws/private_key.pem, use it
    identity_args = []
    privkey_file = os.path.expanduser('~/.aws/private_key.pem')
    if os.path.isfile(privkey_file):
        print('Using %s as identity keyfile' % privkey_file)
        identity_args = ['-i', privkey_file]

    # see rationale in ssh.py
    hostkey_args = ["-o", "UserKnownHostsFile=/dev/null", "-o", "StrictHostKeyChecking=no"]

    print('Connecting to', instance.public_ip_address)
    p = subprocess.Popen(['scp'] + hostkey_args + identity_args + ['-r', 'ubuntu@' + instance.public_ip_address + ':' + file_on_host, local_target])
    p.wait()

    sys.exit(p.returncode)

if len(sys.argv) < 4:
    print('usage: %s <instance-id> <remote-file-path> <local-target>' % sys.argv[0])
    sys.exit(0)

id = sys.argv[1]
instance = ec2.Instance(id)
file_on_host = sys.argv[2]
local_target = sys.argv[3]
scp_from(instance, file_on_host, local_target)

```

## infrastructure/aws/delete-volume.py
```
#!/usr/bin/env python3

# Deletes the specified EBS volume. If it is in use by an
# instance, we wait until it is detached and in the 'available'
# state before deleting it.
#
# Usage: delete-volume.py <volume-id>

from __future__ import absolute_import
from __future__ import print_function
import sys
import boto3
import awslib

ec2 = boto3.client('ec2')

volumeId = sys.argv[1]
volume = ec2.describe_volumes(VolumeIds=[volumeId])['Volumes'][0]
if volume['State'] != 'available':
    print("Volume is in state %s, waiting for it to go into state available..." % volume['State'])
    awslib.await_volume(ec2, volumeId, volume['State'], 'available')

ec2.delete_volume(VolumeId=volumeId)
print("Volume %s deleted" % volumeId)

```

## infrastructure/aws/indexer-provision.sh
```
#!/usr/bin/env bash

set -x # Show commands
set -eu # Errors/undefined vars are fatal
set -o pipefail # Check all commands in a pipeline

date

# as of Ubuntu 22.04 /home/ubuntu is no longer o+rx so we need to manually do it.
chmod a+rx ~

# ## Script Ordering
#
# This script now gets run before the non-AWS provisioner so that we can
# increase the size of the partition now and have the rest of the process
# benefit from the increased partition size.  This does mean that we do some
# redundant things necessary to make this script work independently of that
# script.

# We need to know about our packages below...
sudo apt-get update

# We want the NVME tools, that's how EBS gets mounted now on "nitro" instances.
# We need unzip to install the AWS CLI
sudo apt-get install -y nvme-cli unzip

# In order to do the re-partitioning again, we need jq now, even though we'll
# also try and install it in the non-AWS scripts.
sudo apt-get install -y jq

# Install AWS scripts and command-line tool.
#
# In order to get the AWS CLI v2 the current options[1] are to use snap or do
# the curl + shell dance.  We don't have snap support installed by default and are
# currently intentionally avoiding adding snaps, so we choose curl + shell.
#
# 1: https://docs.aws.amazon.com/cli/latest/userguide/getting-started-install.html
#
# awscli can get credentials via `Ec2InstanceMetadata` which will give it the
# authorities of the role assigned to the image it's running in.  Look for the
# `IamInstanceProfile` definition in `trigger_indexer.py` and similar.

mkdir -p awscliv2-install
pushd awscliv2-install
curl "https://awscli.amazonaws.com/awscli-exe-linux-x86_64.zip" -o "awscliv2.zip"
unzip awscliv2.zip
sudo ./aws/install
popd

date

# Size up our root partition to 20G
#
# We will be growing from a size of 8G.  Previously we increased the indexer
# size to 12G, but now with "wubkat" and "blinkyum" we have at least seen that
# the extra install steps can result in us hitting the 12G limit.  The choice of
# 20G is intended to be more than is required and we can potentially reel this
# back in.
#
# To this end we need to know the volume id in order to issue an EBS resizing
# command.  Note that the select constraint here is intended more as a check
# that our assumption about partition sizes hasn't changed, as when provisioning
# there should only be this single EBS mount.
ROOT_DEV_INFO=$(sudo nvme list -o json | jq --raw-output '.Devices[] | select(.PhysicalSize < 9000000000)')
ROOT_VOL_ID=$(jq -M -r '.SerialNumber | sub("^vol"; "vol-")' <<< "$ROOT_DEV_INFO")
ROOT_DEV=$(jq -M -r '.DevicePath' <<< "$ROOT_DEV_INFO")

AWS_REGION=us-west-2
# The size is in gigs.
aws ec2 modify-volume --region ${AWS_REGION} --volume-id ${ROOT_VOL_ID} --size 20

# We use an until loop because it can take some time for the change to
# propagate to this VM.  The error will look like:
#   "NOCHANGE: partition 1 is size 16775135. it cannot be grown"
# And success will look like:
#   "CHANGED: partition=1 start=2048 old: size=16775135 end=16777183 new: size=25163743 end=25165791"
#
# The 5 is arbitrary in both cases.
sleep 5
# note the partition is the 2nd arg here
until sudo growpart ${ROOT_DEV} 1
do
  sleep 5
done
# and here we identify the partition as part of the block device
sudo resize2fs ${ROOT_DEV}p1

date

```

## infrastructure/aws/web-server-provision.sh
```
#!/usr/bin/env bash

set -x # Show commands
set -eu # Errors/undefined vars are fatal
set -o pipefail # Check all commands in a pipeline

date

# as of Ubuntu 22.04 /home/ubuntu is no longer o+rx so we need to manually do it.
chmod a+rx ~

# ## Script Ordering
#
# This script now gets run before the non-AWS provisioner so that we can
# increase the size of the partition now and have the rest of the process
# benefit from the increased partition size.  This does mean that we do some
# redundant things necessary to make this script work independently of that
# script.

# We need to know about our packages below...
sudo apt-get update

# We want the NVME tools, that's how EBS gets mounted now on "nitro" instances.
# We need unzip to install the AWS CLI
sudo apt-get install -y nvme-cli unzip

# In order to do the re-partitioning again, we need jq now, even though we'll
# also try and install it in the non-AWS scripts.
sudo apt-get install -y jq

# Install AWS scripts and command-line tool.
#
# In order to get the AWS CLI v2 the current options[1] are to use snap or do
# the curl + shell dance.  We don't have snap support installed by default and are
# currently intentionally avoiding adding snaps, so we choose curl + shell.
#
# 1: https://docs.aws.amazon.com/cli/latest/userguide/getting-started-install.html
#
# awscli can get credentials via `Ec2InstanceMetadata` which will give it the
# authorities of the role assigned to the image it's running in.  Look for the
# `IamInstanceProfile` definition in `trigger_indexer.py` and similar.

mkdir -p awscliv2-install
pushd awscliv2-install
curl "https://awscli.amazonaws.com/awscli-exe-linux-x86_64.zip" -o "awscliv2.zip"
unzip awscliv2.zip
sudo ./aws/install
popd

date

# Size up our root partition to 16G.  This is up from 12G when we were hitting
# the limit for the self-update process during the rust build.
#
# To this end we need to know the volume id in order to issue an EBS resizing
# command.  Note that the select constraint here is intended more as a check
# that our assumption about partition sizes hasn't changed, as when provisioning
# there should only be this single EBS mount.
ROOT_DEV_INFO=$(sudo nvme list -o json | jq --raw-output '.Devices[] | select(.PhysicalSize < 9000000000)')
ROOT_VOL_ID=$(jq -M -r '.SerialNumber | sub("^vol"; "vol-")' <<< "$ROOT_DEV_INFO")
ROOT_DEV=$(jq -M -r '.DevicePath' <<< "$ROOT_DEV_INFO")

AWS_REGION=us-west-2
# The size is in gigs.
aws ec2 modify-volume --region ${AWS_REGION} --volume-id ${ROOT_VOL_ID} --size 16

# We use an until loop because it can take some time for the change to
# propagate to this VM.  The error will look like:
#   "NOCHANGE: partition 1 is size 16775135. it cannot be grown"
# And success will look like:
#   "CHANGED: partition=1 start=2048 old: size=16775135 end=16777183 new: size=25163743 end=25165791"
#
# The 5 is arbitrary in both cases.
sleep 5
# note the partition is the 2nd arg here
until sudo growpart ${ROOT_DEV} 1
do
  sleep 5
done
# and here we identify the partition as part of the block device
sudo resize2fs ${ROOT_DEV}p1

```

## infrastructure/aws/send-failure-email.py
```
#!/usr/bin/env python3

from __future__ import absolute_import
import sys
import boto3
import os
import subprocess

client = boto3.client('ses')
subj_prefix = sys.argv[1]
dest_email = sys.argv[2]

log_tail = subprocess.check_output(["tail", "-n", "120", "/home/ubuntu/index-log"])
log_tail = log_tail.decode('utf-8', 'replace')

response = client.send_email(
    Source='daemon@searchfox.org',
    Destination={
        'ToAddresses': [
            dest_email,
        ]
    },
    Message={
        'Subject': {
            'Data': subj_prefix + ' Searchfox indexing error',
        },
        'Body': {
            'Text': {
                'Data': 'Searchfox failed to index successfully! Last 120 lines of log:\n\n' + log_tail,
            },
        }
    }
)

os.system("sudo /sbin/shutdown -h +5")

```

## infrastructure/aws/shell-setup.sh
```
#!/usr/bin/env bash

set -x # Show commands
set -eu # Errors/undefined vars are fatal
set -o pipefail # Check all commands in a pipeline

if [ $# != 6 ]
then
    echo "usage: $0 <branch> <channel> <mozsearch-repo-url> <config-repo-url> <config-repo-path> <config-file-name>"
    exit 1
fi

SCRIPT_PATH=$(readlink -f "$0")
MOZSEARCH_PATH=$(dirname "$SCRIPT_PATH")/../..

BRANCH=$1
CHANNEL=$2
MOZSEARCH_REPO_URL=$3
CONFIG_REPO_URL=$4
CONFIG_REPO_PATH=$(readlink -f $5)
CONFIG_INPUT="$6"

EC2_INSTANCE_ID=$(ec2metadata --instance-id)

echo "Branch is $BRANCH"
echo "Channel is $CHANNEL"
echo ""
echo "For this shell we are NOT creating an EC2 volume, instead you get to use"
echo "/mnt/index-scratch which is the local (fast) SSD.  This got setup in main.sh."
echo
echo "Running indexer setup, but stopping before we upload anything."

$MOZSEARCH_PATH/infrastructure/indexer-setup.sh $CONFIG_REPO_PATH $CONFIG_INPUT /mnt/index-scratch

echo "Setup complete, go crazy."

```

## infrastructure/aws/trigger-web-server.py
```
#!/usr/bin/env python3

# Responsibilities:
# - Start the web server instance, tag it as a web server
# - Attach INDEX_VOL to the web server
# - Wait until the web server is ready to serve requests
#   I could just make requests until they succeed
# - Attach the elastic IP to the new web server
# - Shut down any old web servers (not equal to the one I've started)
# - Delete any old EBS index volumes
#
# Usage: ./trigger-web-server.py <channel> <mozsearch-repo-url> <mozsearch-rev>
#     <config-repo-url> <config-rev> <config-file-name> <index-volume-id>
#     <check-script> <config-repo-path> <working-dir>

from __future__ import absolute_import
from __future__ import print_function
import sys
from datetime import datetime, timedelta
import dateutil.parser
import boto3
import awslib
import json
import os
import os.path
import subprocess
import time

channel = sys.argv[1]
mozsearch_repo = sys.argv[2]
mozsearch_rev = sys.argv[3]
config_repo = sys.argv[4]
config_rev = sys.argv[5]
config_file_name = sys.argv[6]
volumeId = sys.argv[7]
check_script = sys.argv[8]
config_repo_path = sys.argv[9]
working_dir = sys.argv[10]

targetGroup = "%s-target" % channel

ec2_resource = boto3.resource('ec2')
ec2 = boto3.client('ec2')
elb = boto3.client('elbv2')

userData = f'''#!/usr/bin/env bash

cd ~ubuntu
touch web_server_started
sudo -i -u ubuntu ./update.sh "{mozsearch_repo}" "{mozsearch_rev}" "{config_repo}" "{config_rev}"
sudo -i -u ubuntu mozsearch/infrastructure/aws/web-serve.sh config "{config_file_name}" "{volumeId}" "{channel}"
'''

volumes = ec2.describe_volumes(VolumeIds=[volumeId])
availability_zone = volumes['Volumes'][0]['AvailabilityZone']

# - Start the web server instance, tag it as a web server

print('Starting web server instance...')

images = ec2.describe_images(Filters=[{'Name': 'tag-key', 'Values': ['web-server']}])
# TODO: sort/pick the highest datestamp-y "web-server" tag Value.
image_id = images['Images'][0]['ImageId']

# Config files shouldn't be able to do whatever they want.  Instance types must
# first be explicitly allow-listed here.
LEGAL_INSTANCE_TYPES = ['t3.xlarge', 't3.2xlarge']
# Our new default is the 4-core 16GiB t3.xlarge
instance_type = 't3.xlarge'

try:
    config = json.load(open(os.path.join(config_repo_path, config_file_name)))
    maybe_instance_type = config['instance_type']
    if not channel.startswith('release'):
        if maybe_instance_type != instance_type:
            print(f'Non-release channel so using default instance type of {instance_type} instead of {maybe_instance_type}')
        else:
            print(f'Non-release channel using requested (default) instance type of {instance_type}')
    elif maybe_instance_type in LEGAL_INSTANCE_TYPES:
        instance_type = maybe_instance_type
        print(f'Using config file instance type of: "{instance_type}"')
    else:
        print(f'Unknown instance type {maybe_instance_type} requested, falling back to {instance_type}')
except Exception as e:
    print(f'Problem figuring out instance_type from config file: {e}')

r = ec2.run_instances(
    ImageId=image_id,
    MinCount=1,
    MaxCount=1,
    KeyName='Main Key Pair',
    SecurityGroups=['web-server-secure'],
    UserData=userData,
    InstanceType=instance_type,
    Placement={'AvailabilityZone': availability_zone},
)

webServerInstanceId = r['Instances'][0]['InstanceId']

awslib.await_instance(ec2, webServerInstanceId, 'pending', 'running')

print('  State is running.')

print('Tagging web server instance...')

instances = ec2_resource.instances.filter(InstanceIds=[webServerInstanceId])
webServerInstance = list(instances)[0]

ec2.create_tags(Resources=[webServerInstanceId], Tags=[{
    'Key': 'web-server',
    'Value': str(datetime.now()),
}, {
    'Key': 'channel',
    'Value': channel,
}, {
    'Key': 'cfile',
    'Value': config_file_name,
}])

print('Attaching index volume to web server instance...')

# - Attach INDEX_VOL to the web server
ec2.attach_volume(VolumeId=volumeId, InstanceId=webServerInstanceId, Device='xvdf')

# - Wait for it to be attached, and then mark it as DeleteOnTermination
awslib.await_volume(ec2, volumeId, 'available', 'in-use')
webServerInstance.modify_attribute(BlockDeviceMappings=[{
    'DeviceName': 'xvdf',
    'Ebs': {
        'DeleteOnTermination': True,
    },
}])

# - Wait until the web server is ready to serve requests

ip = webServerInstance.private_ip_address

print('Pinging web-server at %s to check readiness...' % ip)

while True:
    try:
        status = subprocess.check_output(
            ["curl", "-f", "-s", "-m", "10.0", "http://%s/status.txt" % ip])
        print('Got status.txt: [%s]' % status)
        if len(status.splitlines()) < 2:
            time.sleep(10)
            continue
    except:
        time.sleep(10)
        continue
    break

# - Run the sanity checks on the web server to ensure it is serving things fine

print('Checking web-server at %s to ensure served data seems reasonable...' % ip)

subprocess.run([check_script, config_repo_path, working_dir, "http://%s/" % ip],
               check=True)

# - Attach the elastic IP to the new web server

print('Switching requests to new server...')

r = elb.describe_target_groups(Names=[targetGroup])
targetGroupArn = r['TargetGroups'][0]['TargetGroupArn']

r = elb.describe_target_health(TargetGroupArn=targetGroupArn)
oldTargets = []
for targetInfo in r['TargetHealthDescriptions']:
    oldTargets.append(targetInfo['Target'])

elb.register_targets(TargetGroupArn=targetGroupArn,
                     Targets=[{'Id': webServerInstanceId, 'Port': 80}])

if oldTargets:
    elb.deregister_targets(TargetGroupArn=targetGroupArn,
                           Targets=oldTargets)

# - Shut down any old web server (a web server not equal to the one I've started)

print('Shutting down old servers...')

r = ec2.describe_instances(Filters=[{'Name': 'tag-key', 'Values': ['web-server']},
                                    {'Name': 'tag:channel', 'Values': [channel]}])
terminate = []
backups_retained = 0
for reservation in r['Reservations']:
    for instance in reservation['Instances']:
        instanceId = instance['InstanceId']
        if instanceId == webServerInstanceId:
            # Don't kill the one we just started
            continue
        tags = instance['Tags']
        kill = False
        for tag in tags:
            if tag['Key'] == 'web-server':
                t = dateutil.parser.parse(tag['Value'])
                # Leave old release1-channel servers around so we can switch
                # to them in an emergency or for quick testing.
                if channel != "release1" or datetime.now() - t >= timedelta(1.5):
                    kill = True
                # The time heuristic would catch up to 3 backups, so once we've
                # found a recent-ish back-up to use, stop the others.  This
                # is inherently biased by the sort order so it might be better
                # to create a list of candidates to retain and then pick more
                # deliberately, but this is probably fine given that in steady
                # state we'll always be replacing an instance that was started
                # 12 hours ago so a pathological situation is not possible
                # unless indexers are failing a lot, but this heuristic was
                # never meant to address that.
                elif backups_retained >= 1:
                    kill = True
                else:
                    backups_retained += 1

        if kill:
            terminate.append(instanceId)

print('Terminating {}'.format(terminate))

if len(terminate):
    ec2.terminate_instances(InstanceIds=terminate)

for instanceId in terminate:
    awslib.await_instance(ec2, instanceId, None, 'terminated')

# - Report any old EBS unattached index volumes.
# Since they are marked as DeleteOnTermination we shouldn't need to delete
# volumes explicitly, but let's report if we find any that are older than half
# a day. This is because within the first half-day of a volume's life it may
# be temporarily unattached (on creation, or during transfer from indexer to
# web-server).

print('Checking for old EBS index volumes...')

volumes = ec2_resource.volumes.filter(
    Filters=[{'Name': 'tag-key', 'Values': ['index']},
             {'Name': 'tag:channel', 'Values': [channel]},
             {'Name': 'status', 'Values': ['available']}])
for volume in volumes:
    for tag in volume.tags:
        if tag['Key'] == 'index':
            t = dateutil.parser.parse(tag['Value'])
            if datetime.now() - t >= timedelta(0.5):
                print('WARNING: Found stray index volume %s created on %s' % (volume.volume_id, tag['Value']))

```

## infrastructure/aws/trigger_indexer.py
```
#!/usr/bin/env python3

from trigger_common import TriggerCommandBase

# Usage: trigger_indexer.py <mozsearch-repo> <config-repo> <config-input> <branch> <channel>

# Note that this class is also used by `build-lambda-indexer-start.sh`'s
# dynamically generated hard-coded python script thing.
class TriggerIndexerCommand(TriggerCommandBase):
    def __init__(self):
        super().__init__('indexer', 'index.sh', 12)

    def script_args_after_branch_and_channel(self, args):
        return '''"{mozsearch_repo}" "{config_repo}" config "{config_input}"'''.format(
            mozsearch_repo=args.mozsearch_repo,
            config_repo=args.config_repo,
            config_input=args.config_input
        )

if __name__ == '__main__':
    cmd = TriggerIndexerCommand()
    cmd.parse_args()
    cmd.trigger()

```

## infrastructure/aws/send-provision-email.py
```
#!/usr/bin/env python3

# This is a variation of send-done-email.py created for provisioning.  If this
# doesn't end up getting specialized to perform a grep, this could potentially
# be unified.

from __future__ import absolute_import
import sys
import boto3
import os
import subprocess

# we need to specify the region for provisioning because we don't have
# ~/.aws/config setup.  We probably do want to address this, but there's no
# current harm in hard-coding this for resilience (especially if provisioning
# fails).
client = boto3.client('ses', region_name='us-west-2')
subj_prefix = sys.argv[1]
dest_email = sys.argv[2]
what_happened = sys.argv[3]

response = client.send_email(
    Source='daemon@searchfox.org',
    Destination={
        'ToAddresses': [
            dest_email,
        ]
    },
    Message={
        'Subject': {
            'Data': f'{subj_prefix} Searchfox provisioning {what_happened}',
        },
        'Body': {
            'Text': {
                'Data': f'Searchfox provisioning {what_happened}!',
            },
        }
    }
)

```

## infrastructure/aws/ssh.py
```
#!/usr/bin/env python3

# SSH into a server. This command opens the SSH port before connecting
# and closes it after SSH is finished.
#
# Usage:
#   Without arguments, prints a list of instances to connect to.
#   With an instance ID as argument, connects to that instance.

from __future__ import absolute_import
from __future__ import print_function
import boto3
from datetime import datetime
import os
import sys
import subprocess
import time

import awslib
from six.moves import input

ec2 = boto3.resource('ec2')
client = boto3.client('ec2')

def print_instances(select):
    now = None

    ids = {}
    current_index = 1

    for instance in ec2.instances.all():
        if len(instance.security_groups) != 1:
            continue

        state = instance.state['Name']

        group = instance.security_groups[0]['GroupName']

        tags = {}
        if instance.tags:
            for tag in instance.tags:
                tags[tag['Key']] = tag['Value']

        # datetime.now() is timezone-naive which means if we try and subtract
        # to get a timedelta without a tz, we'll get an error.  Since under
        # Python2 it's a little annoying to get the UTC timezone, we steal it.
        if now is None:
            now = datetime.now(instance.launch_time.tzinfo)
        age = now - instance.launch_time
        age_str = str(age)
        # strip off sub-seconds
        age_str = age_str[:age_str.find('.')]

        if select:
            print(' {}) '.format(current_index), end='')
            ids[str(current_index)] = instance.id
            current_index += 1

        print(instance.id, state, group, age_str, ["%s: %s" % (k, tags[k]) for k in sorted(tags.keys())])

    if select:
        print()
        while True:
            index = input('index: ')
            if index in ids:
                return ids[index]

def prompt(text):
    while True:
        reply = str(input(text + " (y/n) ")).lower()
        if reply[0] == 'y':
            return True
        elif reply[0] == 'n':
            return False

def ensure_started(instance):
    state = instance.state['Name']
    if state == 'running':
        return False

    if not prompt("Instance is currently %s, attempt to start it?" % state):
        print("Cannot connect to stopped instance!")
        sys.exit(1)

    client.start_instances(InstanceIds=[instance.id])
    print("Awaiting instance start...")
    awslib.await_instance(client, instance.id, None, 'running')
    print("Instance switched to running state, waiting 20s for SSH server to start...")
    time.sleep(20)
    return state

def restore_state(instance, old_state):
    if old_state == 'stopped':
        client.stop_instances(InstanceIds=[instance.id])
        print("Awaiting instance stop...")
        awslib.await_instance(client, instance.id, None, old_state)
    else:
        print("Unrecognized initial state %s, cannot restore state!" % old_state)

def change_security(instance, make_secure):
    secure_suffix = '-secure'

    group = instance.security_groups[0]['GroupName']
    if group.endswith(secure_suffix):
        new_group_name = group[:-len(secure_suffix)]
    else:
        new_group_name = group

    if make_secure:
        new_group_name += secure_suffix

    if new_group_name == group:
        return False

    vpc = list(ec2.vpcs.all())[0]
    new_group = vpc.security_groups.filter(GroupNames=[new_group_name])
    new_group = list(new_group)[0]

    print('Changing instance security group to', new_group.group_name, '--', new_group)

    instance.modify_attribute(Groups=[new_group.id])
    return True

def log_into(instance):
    old_state = ensure_started(instance)
    sec_changed = change_security(instance, False)

    # If there is a private key at ~/.aws/private_key.pem, use it
    identity_args = []
    privkey_file = os.path.expanduser('~/.aws/private_key.pem')
    if os.path.isfile(privkey_file):
        print('Using %s as identity keyfile' % privkey_file)
        identity_args = ['-i', privkey_file]

    # Disable host key checking and the pollution of the user's own known host keys.
    # The rationale is:
    # - These server keys are basically ephemeral.
    # - Before this change, we already didn't bother verifying that the ssh keys
    #   were as expected.
    # Good next steps would be:
    # - Use the AWS API to find out the server's ssh key and create a transient
    #   known hosts file that's pre-populated and that we can use.
    hostkey_args = ["-o", "UserKnownHostsFile=/dev/null", "-o", "StrictHostKeyChecking=no"]

    print('Connecting to', instance.public_ip_address)
    p = subprocess.Popen(['ssh'] + hostkey_args + identity_args + ['ubuntu@' + instance.public_ip_address])
    p.wait()

    if sec_changed:
        change_security(instance, True)
    if old_state is not False:
        if prompt("Instance was started before connection, attempt to restore original state '%s'?" % old_state):
            restore_state(instance, old_state)

    sys.exit(p.returncode)

if len(sys.argv) == 1:
    print('usage: %s (<instance-id>|-)' % sys.argv[0])
    print()
    print('  -: Show the instances and prompt for selecting it')
    print()
    print('Current instances:')
    print_instances(select=False)
    sys.exit(0)

id = sys.argv[1]

if id == '-':
    print('Current instances:')
    id = print_instances(select=True)

instance = ec2.Instance(id)
log_into(instance)

```

## infrastructure/aws/channel-tool.py
```
#!/usr/bin/env python3

# Elastic Load Balancer (ELB) support code to support dynamic use of ELB
# instances for dev channels so that we don't need to leave them existing all
# the time, as there is a carrying cost to them.
#
# The general desired workflow is:
# - trigger-web-server.py creates a dev channel on demand if it doesn't already
#   exist.  This includes doing everything documented at
#   https://github.com/mozsearch/mozsearch/blob/master/docs/aws.md#creating-additional-development-channels
#   automatically.
# - terminate_indexer.py destroys a dev channel by default when terminating a
#   web-server.
#
# Probably fine logistical details:
# - The ELB gets a random internal DNS name which means that we have to
#   dynamically update the subdomain even if it already existed.  This means
#   that it's preferable for us to delete DNS entries when not using a channel.
#
# Disclaimers:
# - I'm really struggling naming-wise in python, using a very inconsistent
#   mixture of snake_case and camelCase.  Sorry!


import argparse
from datetime import datetime
import re
import sys

import boto3
from rich import box, print
from rich.console import Console
from rich.markup import escape
from rich.table import Table
from rich.tree import Tree
from rich.traceback import install

console = Console()

# These are documented in the `HelperResult` doc-block below.
HANDS_OFF_RELEASE_CHANNELS = 1
NO_SUCH_CHANNEL = 2
NO_SUCH_INSTANCE = 3
WEIRD_CHANNEL = 4

class HelperResult:
    '''
    Hacky attempt for the helper to provide suggested error codes and maybe some
    additional details, even though the helper will print out a tremendous
    amount of information itself.

    We probably should be using exceptions instead.

    Error codes:
    - 1: You're trying to mess with a release channel and we don't like that.
    - 2: You named a channel that doesn't exist and a channel does need to
         exist.
    - 3: You named an instance that doesn't exist.
    - 4: The channel's state is inconsistent and so can't be used for the
         requested task.  It's probably appropriate to run the "add" command
         again.
    '''
    def __init__(self, errcode=0):
        self.errcode = errcode

class ChannelHelper:
    '''
    Logic for dealing with our application load balancers via the elastic load
    balancer client, plus any associated DNS setup.
    '''
    def __init__(self):
        self.ec2_resource = boto3.resource('ec2')
        self.ec2 = boto3.client('ec2')
        self.elb = boto3.client('elbv2')
        self.dns = boto3.client('route53')

        ## State populated as a side-effect of calls to `determine_channels`.

        # Dict of channels by name.
        #
        # self.channels will be directly returned from the call above; this is
        # mainly intended as a convenience for hacky REPL-based debugging.
        self.channels = None


        # Dict of instances by id.
        #
        # This is not directly returned, and so this is a totally legitimate way
        # of getting at this information.  Not sketchy!
        self.instances = None

        self.zoneId = None

    def determine_channels(self):
        channels = self.channels = {}
        instances = self.instances = {}
        now = None

        def get_channel(name):
            channel = channels.get(name, None)
            if channel is None:
                if name.startswith('release'):
                    kind = 'release'
                else:
                    kind = 'development'
                channel = {
                    'name': name,
                    'kind': kind,
                    'instances': [],
                    'activeInstanceId': None,
                    # what components of a channel did we find?
                    # - lb: load balancer
                    # - tg: target group
                    # - dns: domain name info
                    'pieces': [],
                }
                channels[name] = channel
            return channel

        # Most of the instance logic is from ssh.py and should potentially be
        # extracted out into another method on this instance.  ssh.py should
        # likely just use this new class though since it can determine
        # interesting stuff about the channels
        for instance in self.ec2_resource.instances.all():
            # ssh.py did this for some reason
            if len(instance.security_groups) != 1:
                continue

            state = instance.state['Name']

            group = instance.security_groups[0]['GroupName']
            is_indexer = group.startswith('indexer')

            tags = {}
            if instance.tags:
                for tag in instance.tags:
                    tags[tag['Key']] = tag['Value']

            # datetime.now() is timezone-naive which means if we try and subtract
            # to get a timedelta without a tz, we'll get an error.  Since under
            # Python2 it's a little annoying to get the UTC timezone, we steal it.
            if now is None:
                now = datetime.now(instance.launch_time.tzinfo)
            age = now - instance.launch_time
            age_str = str(age)
            # strip off sub-seconds
            age_str = age_str[:age_str.find('.')]


            irep = instances[instance.id] = {
                'id': instance.id,
                'state': state,
                'group': group,
                'tags': tags,
                'age': age,
                'age_str': age_str,
                # Assume the instance is a spare unless we see it active in a
                # target group.
                'role': 'indexer' if is_indexer else 'spare',
                'target_groups': [],
            }

            channel = get_channel(tags.get('channel', 'None'))
            channel['instances'].append(irep)

        # The sub-domains live as "resource record sets" under the
        # "searchfox.org." host zone.
        hr = self.dns.list_hosted_zones()
        zoneId = None
        for zone in hr['HostedZones']:
            if zone['Name'] == 'searchfox.org.':
                zoneId = zone['Id']
                # normalize off any preceding `/hostedzone/`; not sure what's up
                # with that.
                RE_ZONE_ID = re.compile('^/hostedzone/(.+)$')
                m = RE_ZONE_ID.match(zoneId)
                if m:
                    zoneId = m.group(1)
                self.zoneId = zoneId

        if zoneId is not None:
            RE_SUB = re.compile(r'^([^.]+)\.searchfox\.org\.$')
            dr = self.dns.list_resource_record_sets(HostedZoneId=zoneId)
            for rrset in dr['ResourceRecordSets']:
                # We only care about searchfox.org subdomains
                m = RE_SUB.match(rrset['Name'])
                if m is None or rrset['Type'] != 'A':
                    continue
                name = m.group(1)
                channel = get_channel(name)
                channel['zoneId'] = zoneId
                channel['rrset'] = rrset
                channel['pieces'].append('dns')

        lbr = self.elb.describe_load_balancers()
        RE_LB_NAME = re.compile('^([^-]+)-lb$')
        for balancer in lbr['LoadBalancers']:
            lbName = balancer['LoadBalancerName']
            m = RE_LB_NAME.match(lbName)
            if m is None:
                continue
            name = m.group(1)
            channel = get_channel(name)
            lbArn = balancer['LoadBalancerArn']
            channel['loadBalancerArn'] = lbArn
            channel['pieces'].append('lb')

        RE_TARGET = re.compile('^([^-]+)-target$')
        tgr = self.elb.describe_target_groups()
        for tgroup in tgr['TargetGroups']:
            m = RE_TARGET.match(tgroup['TargetGroupName'])
            if m is None:
                continue
            name = m.group(1)
            channel = get_channel(name)
            tgArn = channel['targetGroupArn'] = tgroup['TargetGroupArn']
            if tgArn:
                channel['pieces'].append('tg')

            tg_health = self.elb.describe_target_health(TargetGroupArn=tgArn)
            for targetInfo in tg_health['TargetHealthDescriptions']:
                targetId = targetInfo['Target']['Id']
                inst = instances[targetId]
                inst['role'] = 'active'
                inst['target_groups'].append(name)
                channel['activeInstanceId'] = targetId

        ### Compute Channel Info
        for name, channel in channels.items():
            # Sort the instances by age.
            channel['instances'].sort(key=lambda x: x['age'])

        return channels

    def format_channels(self, channels):
        '''
        Given channel data from `determine_channels`, build a rich Tree suitable
        for (rich) print()ing.
        '''
        release_channels = []
        dev_channels = []

        for name, channel in channels.items():
            if channel['kind'] == 'release':
                release_channels.append(channel)
            else:
                dev_channels.append(channel)

        tree = Tree('Channels')

        def colorize_state(state):
            if state == 'stopped':
                return '[red]stopped[/red]'
            return state

        def populate_node_with_instances(node, instances):
            if len(instances) == 0:
                return

            # Build up a table to describe the list of instances so we get a
            # grid/table visual layout.
            table = Table(box=box.SIMPLE)

            # build up all known tags first
            tag_keys = ['(start time)']
            for instance in instances:
                for key in instance['tags'].keys():
                    # suppress tags we've already handled
                    # - channel: already a grouping heuristic
                    # - web-server/indexer: these encode the start time and
                    #   for table purposes it's preferable to fold them, but it
                    #   is misleading.
                    if key == 'channel' or key == 'web-server' or key == 'indexer':
                        continue
                    if key not in tag_keys:
                        tag_keys.append(key)
            tag_keys.sort()

            # populate the table headers
            table.add_column('role')
            table.add_column('target_groups')
            table.add_column('id')
            table.add_column('state')
            table.add_column('group')
            for key in tag_keys:
                table.add_column(key)


            # Instances as table rows
            for instance in instances:
                cells = [
                    instance['role'],
                    ','.join(instance['target_groups']),
                    instance['id'],
                    colorize_state(instance['state']),
                    instance['group'],
                ]
                # NB: It's possible the instance has keys the source of the keys
                # did not.
                for key in tag_keys:
                    if key == '(start time)':
                        if instance['role'] == 'indexer':
                            key = 'indexer'
                        else:
                            key = 'web-server'
                    value = instance['tags'].get(key, None)
                    if value and key.endswith('repo'):
                        value = value.replace('https://github.com/', '')
                    cells.append(value)
                table.add_row(*cells)

            node.add(table)


        def populate_node_with_channels(node, chans):
            chans.sort(key=lambda x: x['name'])
            for channel in chans:
                node_name = f"{channel['name']}: ({', '.join(channel['pieces'])})"
                chan_node = node.add(node_name)
                populate_node_with_instances(chan_node, channel['instances'])

        populate_node_with_channels(tree.add("Release"), release_channels)
        populate_node_with_channels(tree.add("Development"), dev_channels)

        return tree

    def ensure_channel(self, name):
        '''
        Ensure that all the pieces of a channel exist; intended as an idempotent
        version of adding a channel.
        '''
        if name.startswith('release'):
            print('[red]Nope! Release channels need to be manually configured![/red]')
            print('(Release channels share a common load balancer.)')
            return HelperResult(HANDS_OFF_RELEASE_CHANNELS)

        all_channels = self.determine_channels()

        channel = all_channels.get(name, None)
        if channel:
            pieces = channel['pieces']
        else:
            # define an empty channel so we can call get with a None default.
            channel = {}
            pieces = []

        print(f'Channel {name} currently has the following pieces existing:', pieces)
        print()

        ## Load Balancer ##
        console.rule(f'Ensuring "{name}" Load Balancer')
        lbArn = channel.get('loadBalancerArn', None)
        lbInfo = None
        if lbArn:
            print('Reusing load balancer with ARN:', escape(lbArn))
            lbr = self.elb.describe_load_balancers(LoadBalancerArns=[lbArn])
            lbInfo = lbr['LoadBalancers'][0]
        else:
            # Get the list of subnets for the availability zones in our region,
            # as we want the LB to cover all of them.
            snr = self.ec2.describe_subnets()
            use_subnets = []
            for subnet in snr['Subnets']:
                use_subnets.append(subnet['SubnetId'])

            # Get the 'load-balancer' security group id
            sgr = self.ec2.describe_security_groups(GroupNames=['load-balancer'])
            security_group_id = sgr['SecurityGroups'][0]['GroupId']

            lb_name =f'{name}-lb'
            cr = self.elb.create_load_balancer(
                Name=lb_name,
                Subnets=use_subnets,
                SecurityGroups=[security_group_id],
                Scheme='internet-facing',
                # no tags for now
                Type='application',
                # Confusingly, we use a "dualstack."-prefixed DNS, but we don't
                # want to pass "dualstack" here (which means IPv4 and IPv6)
                # because then it gets upset about our subnets not having an
                # IPv6 CIDR block.
                IpAddressType='ipv4',
            )

            print('Load Balancer Creation result:', cr)
            lbInfo = cr['LoadBalancers'][0]
            lbArn = lbInfo['LoadBalancerArn']
        lbDNS = lbInfo['DNSName']
        lbVpcId = lbInfo['VpcId']
        lbZoneId = lbInfo['CanonicalHostedZoneId']

        ## DNS ##
        console.rule(f'Ensuring "{name}" DNS')

        # The web UI puts this on for us, but it's not clear the API does.
        if not lbDNS.startswith('dualstack.'):
            lbDNS = 'dualstack.' + lbDNS
        print('Want to use Load Balancer DNS:', lbDNS)

        # Mention the state of the existing record for context
        existing_rrset = channel.get('rrset', None)
        if existing_rrset:
            print('Updating existing DNS RRSet', existing_rrset)
        else:
            print('No existing record, creating a new one.')

        # But we actually just do an UPSERT
        new_rrset = {
            'Name': f'{name}.searchfox.org.',
            'Type': 'A',
            'AliasTarget': {
                # the target needs to use the load-balancer's zone id, which is
                # very explicitly different from our searchfox.org zone id.
                'HostedZoneId': lbZoneId,
                'DNSName': lbDNS,
                'EvaluateTargetHealth': False
            },
        }

        dr = self.dns.change_resource_record_sets(
                    HostedZoneId=self.zoneId,
                    ChangeBatch={
                        'Comment': f"upsert channel {name}",
                        'Changes': [{
                            'Action': 'UPSERT',
                            'ResourceRecordSet': new_rrset,
                        }],
                    })
        print('DNS Upsert result:', dr)

        ## Target Group ##
        tgName = f'{name}-target'
        console.rule(f'Ensuring Target Group tgName')
        tgArn = channel.get('targetGroupArn', None)
        if tgArn:
            # The VpcId is specific to the load balancer, but we really should
            # not be in a situation where we had a target group but not the
            # corresponding load balancer.  So we'll just throw an error if
            # there is a mismatch.
            tgr = self.elb.describe_target_groups(LoadBalancerArn=lbArn)
            if len(tgr['TargetGroups']) == 0:
                print('[red]The target group is not associated with the balancer![/red]')
                print('This can happen if the add process failed partway through.')
                print('Please use the "remove" command to totally remove the channel,')
                print('then you can call this command again!')
                return HelperResult(WEIRD_CHANNEL)
            tgInfo = tgr['TargetGroups'][0]
            if tgInfo['VpcId'] != lbVpcId:
                print('[red]Target Group VpcId does not match Balancer VpcId![/red]')
                print('Please use the "remove" command to totally remove the channel,')
                print('then you can call this command again!')
                return HelperResult(WEIRD_CHANNEL)

            print('Reusing target group with ARN:', escape(tgArn))
        else:
            print('Creating new target group.')
            tgr = self.elb.create_target_group(
                Name=tgName,
                # we talk to the instances unencrypted over HTTP
                Protocol='HTTP',
                # This is the default and what we've been using; it might make
                # sense to change this in the future.
                ProtocolVersion='HTTP1',
                Port=80,
                VpcId=lbVpcId,
                # We just use whatever the defaults are for all the health
                # check settings right now as we don't even do sane things with
                # the health check and disable it on the instances.
                #
                # instance is the default and the right choice for this
                TargetType='instance',
                # We didn't previously create tags, but I figure why not?
                # Maybe this will help in the AWS Web UI.
                Tags=[{
                    'Key': 'channel',
                    'Value': name,
                }],
            )
            print('Target Group creation result:', tgr)
            tgArn = tgr['TargetGroups'][0]['TargetGroupArn']

        ## Listeners (HTTP and HTTPS) ##
        console.rule('Ensuring Listeners and their Rules')
        needed_listeners = {
            'HTTP': {
                'Protocol': 'HTTP',
                'Port': 80,
            },
            'HTTPS': {
                'Protocol': 'HTTPS',
                'Port': 443,
                # We just hardcode this because it probably won't change?
                'Certificates': [{
                    'CertificateArn': 'arn:aws:acm:us-west-2:653057761566:certificate/f40d4a04-a58b-4b19-a1e2-daaaa70abc43'
                }],
                # This is the default and therefore what we've been using, but
                # not exactly modern best practices.
                'SslPolicy': 'ELBSecurityPolicy-2016-08',
            }
        }
        lir = self.elb.describe_listeners(LoadBalancerArn=lbArn)
        for listener in lir['Listeners']:
            print('Reusing listener for protocol:', listener['Protocol'])
            needed_listeners.pop(listener['Protocol'])
        for protocol, mix_params in needed_listeners.items():
            print(f'Creating {protocol} listener')
            clr = self.elb.create_listener(
                LoadBalancerArn=lbArn,
                DefaultActions=[{
                    'TargetGroupArn': tgArn,
                    'Type': 'forward',
                }],
                **mix_params,
            )
            print(f'Created {protocol} listener:', clr)

        ## Done!
        console.rule('Done!')
        return HelperResult()


    def remove_channel(self, name):
        '''
        Attempt to remove a channel, printing progress output as we go and
        returning a boolean that indicates whether we think we removed the
        channel.
        '''
        if name.startswith('release'):
            print('[red]This tool does not mess with release channels![/red]')
            return HelperResult(HANDS_OFF_RELEASE_CHANNELS)

        all_channels = self.determine_channels()

        channel = all_channels.get(name, None)
        if channel is None:
            print('[red]No such channel[/red]:', name)
            return HelperResult(NO_SUCH_CHANNEL)

        lbArn = channel.get('loadBalancerArn', None)
        if lbArn:
            print('Located channel load balancer ARN:', escape(lbArn))

            # Per docs this also deletes the associated listeners (and their
            # rules).
            r = self.elb.delete_load_balancer(LoadBalancerArn=lbArn)
            print('Deleted load balancer (and listeners)!', r)
        else:
            print('[yellow]There was no load balancer to delete[/yellow]')

        tgArn = channel.get('targetGroupArn', None)
        if tgArn:
            print('Located channel target group ARN:', escape(tgArn))

            r = self.elb.delete_target_group(TargetGroupArn=tgArn)
            print('Deleted target group!', r)
        else:
            print('[yellow]There was no target group to delete[/yellow]')

        rrset = channel.get('rrset')
        if rrset:
            print('[green]Located channel subdomain:[/green]', rrset)

            r = self.dns.change_resource_record_sets(
                    HostedZoneId=channel['zoneId'],
                    ChangeBatch={
                        'Comment': f"remove channel {name}",
                        'Changes': [{
                            'Action': 'DELETE',
                            'ResourceRecordSet': rrset,
                        }],
                    })
            print('[green]Deleted subdomain:[/green]', r)
        else:
            print('[yellow]There was no DNS sub-domain to delete[/yellow]')

        return HelperResult()

    def inspect_channel(self, name):
        '''
        Dump detailed information about a channel's configuration.  This is
        primarily intended as a development tool.
        '''
        all_channels = self.determine_channels()
        channel = all_channels.get(name, None)
        if channel is None:
            print('[red]No such channel[/red]:', name)
            return HelperResult(NO_SUCH_CHANNEL)

        ## Dump non-channel-specific context
        ### Subnets (we need this for load balance creation)
        snr = self.ec2.describe_subnets()
        print('Context: Subnets:', snr)

        sgr = self.ec2.describe_security_groups()
        print('Context: Security Groups:', sgr)

        ## Dump channel-specific info
        lbArn = channel.get('loadBalancerArn', None)
        if lbArn:
            lbr = self.elb.describe_load_balancers(LoadBalancerArns=[lbArn])
            print('Load Balancer Description:', lbr)

            lir = self.elb.describe_listeners(LoadBalancerArn=lbArn)

            for listener in lir['Listeners']:
                listenerArn = listener['ListenerArn']
                print('Listener:', listener)

                rr = self.elb.describe_rules(ListenerArn=listenerArn)
                rules = rr['Rules']
                print('Listener Rules:', rules)

            tgr = self.elb.describe_target_groups(LoadBalancerArn=lbArn)
            print('Target Group Description:', tgr)

        rrset = channel.get('rrset')
        if rrset:
            print('DNS RRSet:', rrset)

        return HelperResult()

    def move_server(self, server_id, name):
        '''
        This method allows subclasses to contribute arguments to their script.ting (web) server to be the active server on a channel.
        This could actually be the same channel the server is already part of,
        but where the server is currently a spare and not the active server.

        This will (potentially) change the channel tag of the instance.

        This can result in up to 2 targets being de-registered:
        1. Any existing target on the target channel.
        2. The instance that's being moved _if it was active_.

        And then this will result in this instance being registered.
        '''
        channels = self.determine_channels()
        channel = channels.get(name, None)

        if channel is None:
            print('[red]No such channel:[/red]', name)
            # typo's are very likely
            print('Run the "add" command first if this was not a typo.')
            return HelperResult(NO_SUCH_CHANNEL)
        if 'targetGroupArn' not in channel:
            print(f'[red]Channel [white]{name}[/white] is not associated with a load balancer![/red]')
            return HelperResult(WEIRD_CHANNEL)

        instance = self.instances.get(server_id, None)
        if instance is None:
            print('[red]No such instance:[/red]', server_id)
            return HelperResult(NO_SUCH_INSTANCE)

        old_channel_name = instance['tags'].get('channel', None)
        old_channel = channels.get(old_channel_name, None)
        if old_channel is None:
            print('[yellow]The old channel does not exist?  Weird.[/yellow]')
        else:
            if old_channel_name in instance['target_groups']:
                tgArn = old_channel.get('targetGroupArn', None)
                if tgArn is None:
                    print('[yellow]No old target group ARN, cannot deregister.[/yellow]')
                else:
                    print('Deregistering server from old target group.')
                    dr = self.elb.deregister_targets(TargetGroupArn=tgArn,
                                                     Targets=[{'Id': server_id, 'Port': 80 }])
                    print('Deregistered:', dr)

        activeInstanceId = channel.get('activeInstanceId')
        tgArn = channel['targetGroupArn']
        if activeInstanceId:
            print('Deregistering currently active server from new channel:', activeInstanceId)
            dr = self.elb.deregister_targets(TargetGroupArn=tgArn,
                                             Targets=[{'Id': activeInstanceId, 'Port': 80 }])
            print('Deregistered:', dr)

        print('Updating channel tag from', old_channel_name, 'to', name)
        # create_tags will over-write existing tags
        ctr = self.ec2.create_tags(
            Resources=[server_id],
            Tags=[{
                'Key': 'channel',
                'Value': name,
            }])
        print('Updated tag:', ctr)

        print('Registering server as target for requested channel')
        rtr = self.elb.register_targets(
            TargetGroupArn=tgArn,
            Targets=[{
                'Id': server_id,
                'Port': 80,
            }])
        print('Registered:', rtr)

        print('Done!  But note that it can take some time for the target groups to update!')
        return HelperResult()


class ChannelCommand:
    '''
    Shallow exposure of the ChannelHelper logic on the command line.  The intent
    is that the ChannelHelper can be used directly by other scripts, so there
    should be no meaningful application logic here, just parsing and glue.
    '''
    def __init__(self):
        self.args = None

    def make_parser(self):
        parser = argparse.ArgumentParser()
        parser.add_argument('--verbose', '-v', action='count', default=0)

        subparsers = parser.add_subparsers()

        list_parser = subparsers.add_parser('list', help='List active and possible channels.')
        inspect_parser = subparsers.add_parser('inspect', help='Show detail channel debug info.')
        #cleanup_parser = subparsers.add_parser('cleanup', help='Cleanup active but unused load balancers.')
        add_parser = subparsers.add_parser('add', help='Add a possible channel.')
        remove_parser = subparsers.add_parser('remove', help='Remove a channel.')
        move_server_parser = subparsers.add_parser('move-server', help='Move a server between channels and make it active.')

        list_parser.set_defaults(func=self.do_list)

        inspect_parser.add_argument('name')
        inspect_parser.set_defaults(func=self.do_inspect)

        #cleanup_parser.set_defaults(func=self.do_cleanup)

        add_parser.add_argument('name')
        add_parser.set_defaults(func=self.do_add)

        remove_parser.add_argument('name')
        remove_parser.set_defaults(func=self.do_remove)

        move_server_parser.add_argument('server_id')
        move_server_parser.add_argument('channel_name')
        move_server_parser.set_defaults(func=self.do_move_server)

        return parser

    def parse_args(self):
        self.parser = self.make_parser()
        self.args = self.parser.parse_args()

    def run(self):
        if 'func' not in self.args:
            self.parser.print_help()
            return

        self.helper = ChannelHelper()
        result = self.args.func(self.helper, self.args)
        sys.exit(result.errcode)

    def do_list(self, helper, args):
        channels = helper.determine_channels()
        print(helper.format_channels(channels))
        return HelperResult()

    def do_inspect(self, helper, args):
        return helper.inspect_channel(args.name)

    def do_cleanup(self, helper, args):
        pass

    def do_add(self, helper, args):
        return helper.ensure_channel(args.name)

    def do_remove(self, helper, args):
        return helper.remove_channel(args.name)

    def do_move_server(self, helper, args):
        return helper.move_server(args.server_id, args.channel_name)


if __name__ == '__main__':
    install(show_locals=True)
    cmd = ChannelCommand()
    cmd.parse_args()
    cmd.run() # calls sys.exit and never returns

```

## infrastructure/aws/terminate-indexer.py
```
#!/usr/bin/env python3

# Shuts down the indexer instance.
# Usage: terminate-indexer.py <indexer-instance-id>

from __future__ import absolute_import
import sys
import boto3

client = boto3.client('ec2')

indexerInstanceId = sys.argv[1]
terminate = [indexerInstanceId]
client.terminate_instances(InstanceIds=terminate)

```

## infrastructure/aws/trigger_shell.py
```
#!/usr/bin/env python3

from trigger_common import TriggerCommandBase

# Usage: trigger_indexer.py <mozsearch-repo> <config-repo> <config-input> <branch> <channel>


class TriggerShellCommand(TriggerCommandBase):
    def __init__(self):
        max_hours = 6
        super().__init__('shell', 'shell-setup.sh', max_hours)

    def script_args_after_branch_and_channel(self, args):
        return '''"{mozsearch_repo}" "{config_repo}" config "{config_input}"'''.format(
            mozsearch_repo=args.mozsearch_repo,
            config_repo=args.config_repo,
            config_input=args.config_input
        )

if __name__ == '__main__':
    cmd = TriggerShellCommand()
    cmd.parse_args()
    cmd.trigger()

```

## infrastructure/web-server-run.sh
```
#!/usr/bin/env bash

set -x # Show commands
set -eu # Errors/undefined vars are fatal
set -o pipefail # Check all commands in a pipeline

if [[ $# -lt 3 ]]
then
    echo "usage: $0 <config-repo-path> <index-path> <server-root> [WAIT]"
    echo ""
    echo "WAIT can optionally be passed to wait until the web server is ready."
    exit 1
fi

MOZSEARCH_PATH=$(readlink -f $(dirname "$0")/..)

WORKING=$(readlink -f $2)
CONFIG_FILE=$WORKING/config.json
SERVER_ROOT=$(readlink -f $3)
STATUS_FILE="${SERVER_ROOT}/docroot/status.txt"

pkill codesearch || true
pkill -f router/router.py || true
pkill -f tools/target/release/web-server || true
pkill -f tools/target/release/pipeline-server || true

sleep 0.1s

# activate the venv we created for livegrep so we have access to the grpc
# dependencies.
LIVEGREP_VENV=$HOME/livegrep-venv
source $LIVEGREP_VENV/bin/activate

nohup $MOZSEARCH_PATH/router/router.py $CONFIG_FILE $STATUS_FILE > $SERVER_ROOT/router.log 2> $SERVER_ROOT/router.err < /dev/null &

export RUST_BACKTRACE=1
nohup $MOZSEARCH_PATH/tools/target/release/web-server $CONFIG_FILE $STATUS_FILE > $SERVER_ROOT/rust-server.log 2> $SERVER_ROOT/rust-server.err < /dev/null &

# Let's try and stop the pipeline-server from causing problems by setting a ulimit
# on virtual memory usage.  We use du to figure out the total sizes of all of
# the files we will mmap, specifically: identifiers, crossref/crossref-extra,
# and jumpref/jumpref-extra.  We then add an allowance for other libraries and
# fundamental mapping, plus an allowance for runtime memory usage.
#
# Resulting units are KiB in all cases, which is also what ulimit takes.
MAPPED_FILES_USAGE_K=$(du -c $WORKING/*/identifiers $WORKING/*/crossref* $WORKING/*/jumpref* | cut -f1 | tail -1)
# When first adding the ulimit, our VM size was 13.7G with resident usage of
# 390M.  When writing this on the spare config1 I'm seeing 13.5G VM with 668M
# resident with the MAPPED_FILES_USAGE_K above reporting ~12.7G which gives 800M
STEADY_STATE_ASSUMED_K=$((800 * 1024))
# Allowed growth.  When first adding the ulimit, we allowed 10.3G of VM usage
# which paired with a 10.7G of resident usage.  I'm going to round this down to
# 10G since we already grew the steady state above.
ALLOWED_GROWTH_K=$((10 * 1024 * 1024))

# I've also just manually confirmed that this works as expected for config4
# where our sum below ends up at ~48G and the pipeline-server VM ends up at
# ~38G, although that's after doing some brief diagram testing to RES is also
# 1410M, but it works out okay.
PIPELINE_SERVER_VM_LIMIT_K=$(($MAPPED_FILES_USAGE_K + $STEADY_STATE_ASSUMED_K + $ALLOWED_GROWTH_K))

# ulimit -v units are kilobytes
ulimit -v $PIPELINE_SERVER_VM_LIMIT_K

# Note that we do not currently wait for the pipeline-server and it does not
# write to the STATUS_FILE.
nohup $MOZSEARCH_PATH/tools/target/release/pipeline-server $CONFIG_FILE > $SERVER_ROOT/pipeline-server.log 2> $SERVER_ROOT/pipeline-server.err < /dev/null &

# If WAIT was passed, wait until the servers report they loaded.
if [[ ${4:-} = "WAIT" ]]; then
  until [[ $(grep -c loaded ${STATUS_FILE}) -eq 2 ]]; do
    sleep 0.1s
  done
fi

```

## infrastructure/common-provision-post.sh
```
#!/usr/bin/env bash

set -x # Show commands
set -eu # Errors/undefined vars are fatal
set -o pipefail # Check all commands in a pipeline

```

## infrastructure/vagrant/indexer-provision.sh
```
#!/usr/bin/env bash

set -x # Show commands
set -eu # Errors/undefined vars are fatal
set -o pipefail # Check all commands in a pipeline

# Fail on reprovisioning attempts, as we don't support them
if [ -f $HOME/.provisioned ]; then
    echo "Sorry! Re-provisioning is not supported. Please destroy your vagrant box and re-create/provision it from scratch, or manually apply changes."
    echo "If you want to manually apply changes, here is the commit at which this box was last provisioned:"
    cat $HOME/.provisioned
    exit 1
fi
# Bug 1766697:
# Compensate for UID/GID mis-matches that freak out git after
# https://github.blog/2022-04-12-git-security-vulnerability-announced/.
# We could alternately fix the problem by involving vagrant-bindfs.
git config --global --add safe.directory /vagrant

git -C /vagrant log -1 > $HOME/.provisioned

# /home/vagrant lost the o+rx at some point, so we put it back so that nginx's
# www-data user can read the file contents.
chmod a+rx /home/vagrant

# Install SpiderMonkey.
rm -rf jsshell-linux-x86_64.zip js
wget -nv https://firefox-ci-tc.services.mozilla.com/api/index/v1/task/gecko.v2.mozilla-central.latest.firefox.linux64-opt/artifacts/public/build/target.jsshell.zip
mkdir js
pushd js
unzip ../target.jsshell.zip
sudo install js /usr/local/bin
sudo install *.so /usr/local/lib
sudo ldconfig
popd

```

## infrastructure/web-server-check.sh
```
#!/usr/bin/env bash

set -x # Show commands
set -eu # Errors/undefined vars are fatal
set -o pipefail # Check all commands in a pipeline

if [ $# != 3 ]
then
    echo "usage: $0 <config-repo-path> <index-path> <base-url>"
    exit 1
fi

export MOZSEARCH_PATH=$(readlink -f $(dirname "$0")/..)
export CONFIG_REPO=$(readlink -f $1)
WORKING=$(readlink -f $2)
CONFIG_FILE=$WORKING/config.json
BASE_URL=$3

for TREE_NAME in $(jq -r ".trees|keys_unsorted|.[]" ${CONFIG_FILE})
do
  . $MOZSEARCH_PATH/scripts/load-vars.sh $CONFIG_FILE $TREE_NAME
  $MOZSEARCH_PATH/scripts/check-index.sh $CONFIG_FILE $TREE_NAME "" "$BASE_URL"
done

```

## infrastructure/indexer-update.sh
```
#!/usr/bin/env bash
#
# This script is run on the indexer by the `update.sh` script created by the
# provisioning process.  Its purpose is to:
# 1. Download/update dependencies that change frequently and need to be
#    up-to-date for indexing/analysis reasons (ex: spidermonkey for JS, rust).
# 2. Perform the build steps for mozsearch.
#
# When developing, this is also a good place to:
# - Install any additional dependencies you might need.
# - Perform any new build steps your changes need.
#
# However, when it comes time to land, it's preferable to make sure that
# dependencies that don't change should just be installed once at provisioning
# time.
#

set -x # Show commands
set -eu # Errors/undefined vars are fatal
set -o pipefail # Check all commands in a pipeline

# Update Rust (make sure we have the latest version).
# We need rust nightly to use the save-analysis, and firefox requires recent
# versions of Rust.
#
# Before we do the update, we remove some components that we don't need and
# that are sometimes missing. If they are missing, `rustup update` will try
# to use a previous nightly instead that does have the components, which means
# we end up with a slightly older rustc. Using rustc from a few days ago is
# usually fine, but in cases where we hit ICEs that have been fixed upstream,
# we want the very latest rustc to get the fix. Removing these components also
# reduces download time during `rustup update`.
#
# Note that these commands are not idempotent, so we need to `|| true` for cases
# where they've already been removed by a prior invocation of this script.
# (Originally this script would only ever be run on the indexers and web-servers
# at most once because the script would not be run during provisioning and each
# VM's root partition would be discarded after running.  Now we run this script
# as part of provisioning for side-effects.)
rustup component remove clippy || true
rustup component remove rustfmt || true
rustup component remove rust-docs || true
rustup component add rust-analyzer || true
rustup update

# Install SpiderMonkey.
rm -rf target.jsshell.zip js
wget -nv https://firefox-ci-tc.services.mozilla.com/api/index/v1/task/gecko.v2.mozilla-central.latest.firefox.linux64-opt/artifacts/public/build/target.jsshell.zip
mkdir js
pushd js
unzip ../target.jsshell.zip
sudo install js /usr/local/bin
sudo install *.so /usr/local/lib
sudo ldconfig
popd

pushd mozsearch/clang-plugin
make
popd

pushd mozsearch/tools
CARGO_INCREMENTAL=false cargo build --release --verbose
rm -rf target/build target/deps
popd

pushd mozsearch/scripts/web-analyze/wasm-css-analyzer
./build.sh
popd

```

## infrastructure/web-server-update.sh
```
#!/usr/bin/env bash
#
# This script is run on the web server by the `update.sh` script created by the
# provisioning process.  Its purpose is to:
# 1. Download/update dependencies that change frequently and need to be
#    up-to-date.  Currently this is rust and we stay up-to-date for consistency
#    with the indexer.
# 2. Perform any necessary build steps for mozsearch for web serving.
#
# When developing, this is also a good place to:
# - Install any additional dependencies you might need.
# - Perform any new build steps your changes need.
#
# However, when it comes time to land, it's preferable to make sure that
# dependencies that don't change should just be installed once at provisioning
# time.
#

set -x # Show commands
set -eu # Errors/undefined vars are fatal
set -o pipefail # Check all commands in a pipeline

# See comments in indexer-update.sh
rustup component remove clippy || true
rustup component remove rustfmt || true
rustup component remove rust-docs || true
rustup update

pushd mozsearch/tools
CARGO_INCREMENTAL=false cargo build --release --verbose
rm -rf target/build target/deps
popd

```

## infrastructure/Dockerfile
```
# syntax=docker/dockerfile:1
FROM ubuntu:24.04
ARG LOCAL_UID
ARG LOCAL_GID

COPY *.sh /infrastructure/
# this will create our user "vagrant"
RUN /infrastructure/docker-provision.sh $LOCAL_UID $LOCAL_GID
USER vagrant
WORKDIR /home/vagrant

RUN /infrastructure/common-provision-pre.sh

# common-provision-pre.sh installed cargo, make sure the path is available for
# the next commands
ENV PATH=/home/vagrant/.cargo/bin:$PATH

RUN /infrastructure/indexer-provision.sh

# indexer-provision.sh installed Coursier, make sure the path is available for
# the next commands
ENV PATH=$PATH:/home/vagrant/.local/share/coursier/bin

RUN /infrastructure/web-server-provision.sh
RUN /infrastructure/common-provision-post.sh

EXPOSE 80/tcp

CMD ["/usr/bin/bash"]

```

## infrastructure/indexer-upload.sh
```
#!/usr/bin/env bash

set -x # Show commands
set -eu # Errors/undefined vars are fatal
set -o pipefail # Check all commands in a pipeline

if [ $# != 2 ]
then
    echo "usage: $0 <config-repo-path> <working-path>"
    exit 1
fi

MOZSEARCH_PATH=$(cd $(dirname "$0") && git rev-parse --show-toplevel)

export CONFIG_REPO=$(readlink -f $1)
WORKING=$(readlink -f $2)

CONFIG_FILE=$WORKING/config.json

export AWS_ROOT=$MOZSEARCH_PATH/infrastructure/aws

for TREE_NAME in $(jq -r ".trees|keys_unsorted|.[]" ${CONFIG_FILE})
do
    . $MOZSEARCH_PATH/scripts/load-vars.sh $CONFIG_FILE $TREE_NAME

    if [[ -f $CONFIG_REPO/$TREE_NAME/upload ]]
    then
        $CONFIG_REPO/$TREE_NAME/upload || handle_tree_error "tree upload script"
    fi
done

```

## infrastructure/docker-provision.sh
```
#!/usr/bin/env bash

set -x # Show commands
set -eu # Errors/undefined vars are fatal
set -o pipefail # Check all commands in a pipeline

USE_UID=$1
USE_GID=$2

# This all happens as root right now!

# Install core tools our Ubuntu install usually has:
apt-get update
apt-get install -y apt-utils lsb-release sudo curl wget

# Create the vagrant user with the same UID/GID as the current host user.
#
# Remove the the default "ubuntu" user, because it may conflict with UID.
userdel ubuntu
USERNAME=vagrant
useradd -u $USE_UID -o -ms /bin/bash $USERNAME
groupmod -o -g $USE_GID $USERNAME
usermod -aG sudo $USERNAME && echo "$USERNAME ALL=(ALL) NOPASSWD: ALL" > /etc/sudoers.d/$USERNAME
chmod 0440 /etc/sudoers.d/$USERNAME

chmod a+rx /home/$USERNAME

# This bind point `/vagrant` is technically separate from the username.
mkdir /vagrant
chown $USERNAME:$USERNAME /vagrant

```

## infrastructure/web-server-setup.sh
```
#!/usr/bin/env bash

set -x # Show commands
set -eu # Errors/undefined vars are fatal
set -o pipefail # Check all commands in a pipeline

if [ $# != 4 -a $# != 5 -a $# != 6 ]
then
    echo "usage: $0 <config-repo-path> <config-file-name> <index-path> <server-root> [<use_hsts>] [nginx-cache-dir]"
    exit 1
fi

MOZSEARCH_PATH=$(readlink -f $(dirname "$0")/..)

CONFIG_REPO=$(readlink -f $1)
CONFIG_INPUT="$2"
WORKING=$(readlink -f $3)
CONFIG_FILE=$WORKING/config.json
SERVER_ROOT=$(readlink -f $4)
USE_HSTS=${5:-}
NGINX_CACHE_DIR=${6:-}

$MOZSEARCH_PATH/scripts/generate-config.sh $CONFIG_REPO $CONFIG_INPUT $WORKING

sudo mkdir -p /etc/nginx/sites-enabled
sudo rm -f /etc/nginx/sites-enabled/default

# ### Create the docroot
#
# There is some awkwardness here where we create hierarchies centered on the
# docroot because nginx-setup.py is using `root` directives in a bunch of places
# where we probably should be using `alias`.  The main difference is whether the
# `location` path is used when looking on disk; if you have a location of
# "/foo/" and a request of "/foo/bar", then a `root`` of "/blah" will be
# "/blah/foo/bar" retaining the "/foo/" whereas an `alias` would be "/blah/bar".
#
# We likely want to change to using `alias` in cases where we conceptually are
# mapping a directory on a 1:1 basis.  For cases where we are mapping individual
# files, symlinks are probably still appropriate.
rm -rf $SERVER_ROOT/docroot
mkdir -p $SERVER_ROOT/docroot
DOCROOT=$(realpath $SERVER_ROOT/docroot)

DEFAULT_TREE_NAME=$(jq -r ".default_tree // empty" ${CONFIG_FILE})

for TREE_NAME in $(jq -r ".trees|keys_unsorted|.[]" ${CONFIG_FILE})
do
    mkdir -p $DOCROOT/file/$TREE_NAME
    mkdir -p $DOCROOT/dir/$TREE_NAME
    mkdir -p $DOCROOT/raw-analysis/$TREE_NAME
    mkdir -p $DOCROOT/file-lists/$TREE_NAME/file-lists
    ln -s $WORKING/$TREE_NAME/file $DOCROOT/file/$TREE_NAME/source
    ln -s $WORKING/$TREE_NAME/dir $DOCROOT/dir/$TREE_NAME/source
    ln -s $WORKING/$TREE_NAME/analysis $DOCROOT/raw-analysis/$TREE_NAME/raw-analysis
    for FILE_LIST in repo-files objdir-files; do
        ln -s $WORKING/$TREE_NAME/$FILE_LIST $DOCROOT/file-lists/$TREE_NAME/file-lists/$FILE_LIST
    done

    # Only update the help file if no default tree was specified OR
    # The tree was specified and this is that tree.
    if [ -z "$DEFAULT_TREE_NAME" -o "$DEFAULT_TREE_NAME" == "$TREE_NAME" ]
    then
        rm -f $DOCROOT/help.html
        ln -s $WORKING/$TREE_NAME/templates/help.html $DOCROOT
    fi
done

rm -f $DOCROOT/tree-list.js
ln -s $CONFIG_REPO/tree-list.js $DOCROOT

# ### Create and emplace the nginx configuration file
$MOZSEARCH_PATH/scripts/nginx-setup.py $CONFIG_FILE $DOCROOT "$USE_HSTS" "$NGINX_CACHE_DIR" > /tmp/nginx
sudo mv /tmp/nginx /etc/nginx/sites-enabled/mozsearch.conf
sudo chmod 0644 /etc/nginx/sites-enabled/mozsearch.conf

# ### Caching
#
# Iterate over the tree names in order of increasing priority so that we can
# make sure that the most important (by higher priority value) trees get their
# data cached last so if we run out of spare memory capacity, it's the less
# important trees that get their data evicted.
#
# (If we wanted decreasing priority, we would `reverse` the array after sorting.)
for TREE_NAME in $(jq -r ".trees|to_entries|sort_by(.value.priority)|.[].key" ${CONFIG_FILE})
do
    # source load-vars.sh to get our `cache_when_*` helpers.
    . $MOZSEARCH_PATH/scripts/load-vars.sh $CONFIG_FILE $TREE_NAME

    # The livegrep.idx is the most important file, so it's always the last thing
    # we cache.  These helpers also take into considerationg the "cache" setting
    # in the tree config.
    cache_when_everything crossref-extra
    cache_when_everything crossref
    cache_when_codesearch livegrep.idx
done

# ### Ensure nginx is running
#
# Under docker nginx might not be running, in which case we need to start it,
# but only if we don't see existing processes.
pgrep nginx || sudo /etc/init.d/nginx start
sudo /etc/init.d/nginx reload

```

## infrastructure/reblame-run.sh
```
#!/usr/bin/env bash

set -x # Show commands
set -eu # Errors/undefined vars are fatal
set -o pipefail # Check all commands in a pipeline

if [ $# -lt 3 ]
then
    echo "usage: $0 <config-repo-path> <config-file-name> <working-dir> [extra-args-for-reblame]"
    exit 1
fi

export MOZSEARCH_PATH=$(readlink -f $(dirname "$0")/..)
export CONFIG_REPO=$(readlink -f $1)
CONFIG_INPUT="$2"
export WORKING=$(readlink -f $3)

# Remove first three command-line args from the $* variable, so we're just left with the
# "extra arguments" to pass on to the per-repo reblame script.
shift 3

if [ -z "${CLEAN_WORKING:-}" ]; then
    echo "Keeping old contents of $WORKING/. Set CLEAN_WORKING=1 to remove the contents of $WORKING/."
else
    echo "Removing old contents of $WORKING/."
    rm -rf $WORKING/*
fi

$MOZSEARCH_PATH/scripts/generate-config.sh $CONFIG_REPO $CONFIG_INPUT $WORKING
CONFIG_FILE=$WORKING/config.json

for TREE_NAME in $(jq -r ".trees|keys_unsorted|.[]" ${CONFIG_FILE})
do
    . $MOZSEARCH_PATH/scripts/load-vars.sh $CONFIG_FILE $TREE_NAME
    mkdir -p $INDEX_ROOT
    cd $INDEX_ROOT

    if [ -f "$CONFIG_REPO/$TREE_NAME/reblame" ]; then
        $CONFIG_REPO/$TREE_NAME/reblame $*
    fi
done

```

## infrastructure/indexer-setup.sh
```
#!/usr/bin/env bash

set -x # Show commands
set -eu # Errors/undefined vars are fatal
set -o pipefail # Check all commands in a pipeline

if [ $# != 3 ]
then
    echo "usage: $0 <config-repo-path> <config-file-name> <index-path>"
    exit 1
fi

export MOZSEARCH_PATH=$(readlink -f $(dirname "$0")/..)
export CONFIG_REPO=$(readlink -f $1)
CONFIG_INPUT="$2"
export WORKING=$(readlink -f $3)

if [ -z "${CLEAN_WORKING:-}" ]; then
    echo "Keeping old contents of $WORKING/. Set CLEAN_WORKING=1 to remove the contents of $WORKING/."
else
    echo "Removing old contents of $WORKING/."
    rm -rf $WORKING/*
fi

$MOZSEARCH_PATH/scripts/generate-config.sh $CONFIG_REPO $CONFIG_INPUT $WORKING
CONFIG_FILE=$WORKING/config.json

for TREE_NAME in $(jq -r ".trees|keys_unsorted|.[]" ${CONFIG_FILE})
do
    . $MOZSEARCH_PATH/scripts/load-vars.sh $CONFIG_FILE $TREE_NAME
    mkdir -p $INDEX_ROOT

    # If this fails, we let it propagate to abort the entire indexer. Setup
    # failure is generally pretty bad and can leave the git repo in a weird
    # state that doesn't lend itself to graceful fallback. See bug 1842632.
    $CONFIG_REPO/$TREE_NAME/setup
done

```

## infrastructure/indexer-provision.sh
```
#!/usr/bin/env bash

set -x # Show commands
set -eu # Errors/undefined vars are fatal
set -o pipefail # Check all commands in a pipeline

MOZSEARCH_REPO="${MOZSEARCH_REPO:-https://github.com/mozsearch/mozsearch}"
MOZSEARCH_BRANCH="${MOZSEARCH_BRANCH:-master}"
MOZSEARCH_CONFIG_REPO="${MOZSEARCH_CONFIG_REPO:-https://github.com/mozsearch/mozsearch-mozilla}"
MOZSEARCH_CONFIG_BRANCH="${MOZSEARCH_CONFIG_BRANCH:-master}"

# Install zlib.h (needed for NSS build)
sudo apt-get install -y zlib1g-dev

# Building LLVM likes to have ninja; pernosco also can use it if we ever index that.
sudo apt-get install -y ninja-build

# cargo-insta makes it possible to use the UI documented at
# https://insta.rs/docs/cli/ to review changes to "check" scripts.  For the test
# repo, this is used by `make review-test-repo`.  It's not expected that this
# will actually be necessary on the production indexer and so this isn't part of
# the update process.
cargo install cargo-insta

# Install node.js for scip-typescript; github lists v18 and v20 as supported;
# we are sticking with v18 for now because currently all the invocations
# hardcode v18 as well; that will need to be addressed.
sudo apt install -y npm

# Install scip-typescript under node.js v18
sudo npm install -g @sourcegraph/scip-typescript

# Install scip-python under node.js v18 as well
#npm install -g @sourcegraph/scip-python
# To get my fix https://github.com/sourcegraph/scip-python/pull/150
sudo npm install -g @asutherland/scip-python

# Install a JDK and Coursier.
# v21 is currently the most recent available version of Ubuntu 24.04 (and v19 was
# removed).
sudo apt install -y openjdk-21-jdk
curl -fL "https://github.com/coursier/launchers/raw/master/cs-x86_64-pc-linux.gz" | gzip -d > cs
chmod +x cs
./cs setup --yes
# Coursier adds itself to the path from ~/.profile, but add it now too
PATH="$PATH:$HOME/.local/share/coursier/bin"

# Install scip-java
cs install --contrib scip-java
rm -rf ~/.cache/coursier

# Create update script.
cat > update.sh <<"THEEND"
#!/usr/bin/env bash

set -x # Show commands
set -eu # Errors/undefined vars are fatal
set -o pipefail # Check all commands in a pipeline

exec > >(tee -a update-log) 2>&1

date

if [ $# != 4 ]
then
    echo "usage: $0 <mozsearch-repo> <mozsearch-rev> <config-repo> <config-rev>"
    exit 1
fi

MOZSEARCH_REPO=$1
MOZSEARCH_REV=$2
CONFIG_REPO=$3
CONFIG_REV=$4

echo Mozsearch repository is $MOZSEARCH_REPO rev $MOZSEARCH_REV
echo Config repository is $CONFIG_REPO rev $CONFIG_REV

# Install mozsearch.
rm -rf mozsearch
mkdir mozsearch
pushd mozsearch
git init
git remote add origin "$MOZSEARCH_REPO"
git fetch origin --depth=1 "$MOZSEARCH_REV"
git reset --hard FETCH_HEAD
git submodule update --init --depth 1
popd

# Install files from the config repo.
rm -rf config
mkdir config
pushd config
git init
git remote add origin "$CONFIG_REPO"
git fetch origin --depth=1 "$CONFIG_REV"
git reset --hard FETCH_HEAD
popd

date

# Let mozsearch tell us what commonly changing dependencies to install plus
# perform any build steps.
mozsearch/infrastructure/indexer-update.sh

date
THEEND

chmod +x update.sh

# Run the update script for a side effect of downloading the crates.io
# dependencies ahead of time since we're seeing intermittent network problems
# downloading crates in https://bugzilla.mozilla.org/show_bug.cgi?id=1720037.
#
# Note that because the update script fully deletes the mozsearch directory,
# this really is just:
# - Validating the image can compile and use rust and clang correctly.
# - Caching some crates in `~/.cargo`.
./update.sh "$MOZSEARCH_REPO" "$MOZSEARCH_BRANCH" "$MOZSEARCH_CONFIG_REPO" "$MOZSEARCH_CONFIG_BRANCH"
mv update-log provision-update-log-1

# Run this a second time to make sure the script is actually idempotent, so we
# don't have any surprises when the update script gets run when the VM spins up.
./update.sh "$MOZSEARCH_REPO" "$MOZSEARCH_BRANCH" "$MOZSEARCH_CONFIG_REPO" "$MOZSEARCH_CONFIG_BRANCH"
mv update-log provision-update-log-2

```

## infrastructure/web-server-provision.sh
```
#!/usr/bin/env bash

set -x # Show commands
set -eu # Errors/undefined vars are fatal
set -o pipefail # Check all commands in a pipeline

MOZSEARCH_REPO="${MOZSEARCH_REPO:-https://github.com/mozsearch/mozsearch}"
MOZSEARCH_BRANCH="${MOZSEARCH_BRANCH:-master}"
MOZSEARCH_CONFIG_REPO="${MOZSEARCH_CONFIG_REPO:-https://github.com/mozsearch/mozsearch-mozilla}"
MOZSEARCH_CONFIG_BRANCH="${MOZSEARCH_CONFIG_BRANCH:-master}"

# Nginx
sudo apt-get install -y nginx

# Create update script.
cat > update.sh <<"THEEND"
#!/usr/bin/env bash

set -x # Show commands
set -eu # Errors/undefined vars are fatal
set -o pipefail # Check all commands in a pipeline

exec > >(tee -a update-log) 2>&1

date

if [ $# != 4 ]
then
    echo "usage: $0 <mozsearch-repo> <mozsearch-rev> <config-repo> <config-rev>"
    exit 1
fi

MOZSEARCH_REPO=$1
MOZSEARCH_REV=$2
CONFIG_REPO=$3
CONFIG_REV=$4

echo Mozsearch repository is $MOZSEARCH_REPO rev $MOZSEARCH_REV
echo Config repository is $CONFIG_REPO rev $CONFIG_REV

# Install mozsearch.
# Note: This seems needlessly wasteful but I'm not going to change this while
# changing other things.
rm -rf mozsearch
mkdir mozsearch
pushd mozsearch
git init
git remote add origin "$MOZSEARCH_REPO"
git fetch origin --depth=1 "$MOZSEARCH_REV"
git reset --hard FETCH_HEAD
git submodule update --init --depth 1
popd

# Install files from the config repo.
rm -rf config
mkdir config
pushd config
git init
git remote add origin "$CONFIG_REPO"
git fetch origin --depth=1 "$CONFIG_REV"
git reset --hard FETCH_HEAD
popd

date

# Let mozsearch tell us what commonly changing dependencies to install plus
# perform any build steps.
mozsearch/infrastructure/web-server-update.sh

date
THEEND

chmod +x update.sh

# Run the update script for a side effect of downloading the crates.io
# dependencies ahead of time since we're seeing intermittent network problems
# downloading crates in https://bugzilla.mozilla.org/show_bug.cgi?id=1720037.
#
# Note that because the update script fully deletes the mozsearch directory,
# this really is just:
# - Validating the image can compile and use rust and clang correctly.
# - Caching some crates in `~/.cargo`.
./update.sh "$MOZSEARCH_REPO" "$MOZSEARCH_BRANCH" "$MOZSEARCH_CONFIG_REPO" "$MOZSEARCH_CONFIG_BRANCH"
mv update-log provision-update-log-1

# Run this a second time to make sure the script is actually idempotent, so we
# don't have any surprises when the update script gets run when the VM spins up.
./update.sh "$MOZSEARCH_REPO" "$MOZSEARCH_BRANCH" "$MOZSEARCH_CONFIG_REPO" "$MOZSEARCH_CONFIG_BRANCH"
mv update-log provision-update-log-2

```

## infrastructure/common-provision-pre.sh
```
#!/usr/bin/env bash

set -x # Show commands
set -eu # Errors/undefined vars are fatal
set -o pipefail # Check all commands in a pipeline

MOZSEARCH_REPO="${MOZSEARCH_REPO:-https://github.com/mozsearch/mozsearch}"
MOZSEARCH_BRANCH="${MOZSEARCH_BRANCH:-master}"
MOZSEARCH_CONFIG_REPO="${MOZSEARCH_CONFIG_REPO:-https://github.com/mozsearch/mozsearch-mozilla}"
MOZSEARCH_CONFIG_BRANCH="${MOZSEARCH_CONFIG_BRANCH:-master}"

# We currently try to keep the version of clang we use matching the one that
# will be used by the Firefox build process.  If you have a "mach bootstrap"ped
# system then you can see the current version locally via
# "~/.mozbuild/clang/bin/clang --version"
#
# Note that for the most recent LLVM/clang release (ex: right now v13), you
# would actually want to leave this empty.  Check out https://apt.llvm.org/ for
# the latest info in all cases.
CLANG_SUFFIX=-18
# Bumping the priority with each version upgrade lets running the provisioning
# script on an already provisioned machine do the right thing alternative-wise.
# Actually, we no longer support re-provisioning, but it's fun to increment
# numbers.
CLANG_PRIORITY=414
# The clang packages build the Ubuntu release name in; let's dynamically extract
# it since I, asuth, once forgot to update this.
UBUNTU_RELEASE=$(lsb_release -cs)

sudo apt-get update
# software-properties-common: necessary for apt-add-repository to exist
# gettext-base: necessary for `envsubst` to exist
# zip: used to create lambda zips
sudo apt-get install -y software-properties-common gettext-base rsync zip

sudo apt-add-repository -y ppa:git-core/ppa    # For latest git
sudo apt-get update
sudo apt-get install -y git
git config --global pull.ff only

# we have git, so let's check out mozsearch now so we can have our email sending
# script in case of an error.
if [ ! -d mozsearch ]; then
  mkdir mozsearch
  pushd mozsearch
  git init
  git remote add origin "$MOZSEARCH_REPO"
  git fetch origin "$MOZSEARCH_BRANCH"
  git switch --detach FETCH_HEAD
  popd
fi

# the base image we're building against is inherently not up-to-date (new base
# images are released only monthly), so let's be consistently up-to-date.
sudo DEBIAN_FRONTEND=noninteractive \
  apt-get \
  -o Dpkg::Options::=--force-confold \
  -o Dpkg::Options::=--force-confdef \
  -y --allow-downgrades --allow-remove-essential \
  dist-upgrade

# unattended upgrades pose a problem for debugging running processes because we
# end up running version N but have debug symbols for N+1 and that doesn't work.
sudo apt-get remove -y unattended-upgrades
# and we want to be able to debug python
sudo apt-get install -y gdb python3-dbg

# Other
sudo apt-get install -y parallel unzip python3-pip python3-venv lz4 file

# We want to be able to extract stuff from json (jq) and yaml (yq) and more
# easily emit JSON from the shell (jo).
sudo apt-get install -y jq jo yq

# dos2unix is used to normalize generated files from windows
sudo apt-get install -y dos2unix

# emoji font so graphviz/pango understands emoji font metrics
sudo apt-get install -y fonts-noto-color-emoji

# graphviz for diagramming
#
# We initially started using the official graphviz project debs because 22.04
# was so far behind, but now we're sticking with the official upstream because
# they update so frequently and we are a cutting edge user of graphviz so it's
# nice to have all the fixes and enhancements ASAP.
GRAPHVIZ_DEB_BUNDLE=ubuntu_24.04_graphviz-12.0.0-debs.tar.xz
if [ ! -d $HOME/graphviz-install ]; then
  mkdir -p $HOME/graphviz-install
  pushd $HOME/graphviz-install
  curl -O https://gitlab.com/api/v4/projects/4207231/packages/generic/graphviz-releases/12.0.0/$GRAPHVIZ_DEB_BUNDLE
  tar xvf $GRAPHVIZ_DEB_BUNDLE
  # using constrained wildcards here to not care too much about these versions
  sudo apt-get install -y ./graphviz_*_amd64.deb ./libgraphviz4_*_amd64.deb ./libgraphviz-dev_*_amd64.deb
  popd
fi

# Prior livegrep deps, now rust wants libssl-dev still
sudo apt-get install -y unzip libssl-dev

# Install Bazelisk to install bazel as needed.  bazezlisk is like nvm.
if [ ! -d bazelisk ]; then
  mkdir bazelisk
  pushd bazelisk
    curl -sSfL -O https://github.com/bazelbuild/bazelisk/releases/download/v1.11.0/bazelisk-linux-amd64
    chmod +x bazelisk-linux-amd64
  popd
fi
BAZEL=~/bazelisk/bazelisk-linux-amd64

# Install gcc-12 because bazel 5.x can't build with gcc-13 because of problems
# with abseil and simply telling the livesearch bazel to use the latest bazel or
# clang just gives us different problems.
sudo apt-get install -y gcc-12 g++-12
sudo update-alternatives --install /usr/bin/gcc gcc /usr/bin/gcc-12 ${CLANG_PRIORITY}
sudo update-alternatives --install /usr/bin/g++ g++ /usr/bin/g++-12 ${CLANG_PRIORITY}

# Clang
wget -O - https://apt.llvm.org/llvm-snapshot.gpg.key | sudo apt-key add -
sudo apt-add-repository -y "deb https://apt.llvm.org/${UBUNTU_RELEASE}/ llvm-toolchain-${UBUNTU_RELEASE}${CLANG_SUFFIX} main"
sudo apt-get update
sudo apt-get install -y clang${CLANG_SUFFIX} libclang${CLANG_SUFFIX}-dev lld${CLANG_SUFFIX}

# Setup direct links to clang, including having clang be cc/c++
sudo update-alternatives --install /usr/bin/llvm-config llvm-config /usr/bin/llvm-config${CLANG_SUFFIX} ${CLANG_PRIORITY}
sudo update-alternatives --install /usr/bin/clang clang /usr/bin/clang${CLANG_SUFFIX} ${CLANG_PRIORITY}
sudo update-alternatives --install /usr/bin/cc cc /usr/bin/clang${CLANG_SUFFIX} ${CLANG_PRIORITY}
sudo update-alternatives --install /usr/bin/clang++ clang++ /usr/bin/clang++${CLANG_SUFFIX} ${CLANG_PRIORITY}
sudo update-alternatives --install /usr/bin/c++ c++ /usr/bin/clang${CLANG_SUFFIX} ${CLANG_PRIORITY}
sudo update-alternatives --install /usr/bin/llvm-symbolizer llvm-symbolizer /usr/bin/llvm-symbolizer${CLANG_SUFFIX} ${CLANG_PRIORITY}
sudo update-alternatives --install /usr/bin/lld lld /usr/bin/lld${CLANG_SUFFIX} ${CLANG_PRIORITY}
sudo update-alternatives --install /usr/bin/ld.lld ld.lld /usr/bin/ld.lld${CLANG_SUFFIX} ${CLANG_PRIORITY}

# Install pkg-config (needed for Rust's OpenSSL wrappers)
# Install vmtouch for caching files into memory
sudo apt-get install -y pkg-config vmtouch

# rust gRPC via tonic/tonic-build and prost-build needs protoc (and cmake?)
sudo apt-get install -y cmake protobuf-compiler

# Install Rust. We need rust nightly to use the save-analysis
if [ ! -d $HOME/.cargo ]; then
  curl https://sh.rustup.rs -sSf | sh -s -- -y
  source $HOME/.cargo/env
fi
rustup install nightly
rustup default nightly
rustup uninstall stable
rustup component add rust-analyzer

# install ripgrep so we can stop experiencing grep pain / footguns
sudo apt-get install ripgrep

# Install tools for web-analyze WASM bindings.
cargo install wasm-pack
cargo install wasm-snip

# Install codesearch.
if [ ! -d livegrep ]; then
  git clone -b mozsearch-version7 https://github.com/mozsearch/livegrep --depth=1
  pushd livegrep
  $BAZEL build //src/tools:codesearch
  sudo install bazel-bin/src/tools/codesearch /usr/local/bin
  popd
  # Remove ~2G of build artifacts that we don't need anymore
  rm -rf .cache/bazel

  # Install gRPC python libs and generate the python modules to communicate with the codesearch server
  # We need to create a venv for this because Ubuntu 24.04 gets very angry if we
  # use pip to install things outside of a venv.
  LIVEGREP_VENV=$HOME/livegrep-venv
  python3 -m venv $LIVEGREP_VENV
  $LIVEGREP_VENV/bin/pip install grpcio grpcio-tools
  # also install "six" in this venv for xpidl.py for now
  $LIVEGREP_VENV/bin/pip install six

  mkdir livegrep-grpc3
  $LIVEGREP_VENV/bin/python3 -m grpc_tools.protoc --python_out=livegrep-grpc3 --grpc_python_out=livegrep-grpc3 -I livegrep/ livegrep/src/proto/config.proto
  $LIVEGREP_VENV/bin/python3 -m grpc_tools.protoc --python_out=livegrep-grpc3 --grpc_python_out=livegrep-grpc3 -I livegrep/ livegrep/src/proto/livegrep.proto
  touch livegrep-grpc3/src/__init__.py
  touch livegrep-grpc3/src/proto/__init__.py
  # Add the generated modules to the python path
  SITEDIR=$($LIVEGREP_VENV/bin/python3 -c "import site; print(site.getsitepackages()[0])")
  mkdir -p "$SITEDIR"
  echo "$PWD/livegrep-grpc3" > "$SITEDIR/livegrep.pth"
  rm -rf livegrep
fi

sudo apt-get install -y python3-boto3 python3-rich

# Install git-cinnabar.

# Need mercurial to prevent cinnabar from spewing warnings.
sudo apt-get install -y mercurial

# We started pinning in https://bugzilla.mozilla.org/show_bug.cgi?id=1779939
# and it seems reasonable to stick to this for more deterministic provisioning.
CINNABAR_REVISION=0.7.2
if [ ! -d git-cinnabar ]; then
  git clone https://github.com/glandium/git-cinnabar -b $CINNABAR_REVISION --depth=1
else
  pushd git-cinnabar
    git fetch --tags --depth=1 origin $CINNABAR_REVISION
    git checkout $CINNABAR_REVISION
  popd
fi
pushd git-cinnabar
  ./download.py --branch release
  # These need to be symlinks rather than `install`d binaries because cinnabar
  # uses other python code from the repo.
  for file in git-cinnabar git-cinnabar-helper git-remote-hg; do
    sudo ln -fs $(pwd)/$file /usr/local/bin/$file
  done
popd

# Install scip
SCIP_VERSION=v0.5.0
curl -L https://github.com/sourcegraph/scip/releases/download/$SCIP_VERSION/scip-linux-amd64.tar.gz | tar xzf - scip
sudo ln -fs $(pwd)/scip /usr/local/bin/scip

```

## infrastructure/indexer-run.sh
```
#!/usr/bin/env bash

set -x # Show commands
set -eu # Errors/undefined vars are fatal
set -o pipefail # Check all commands in a pipeline

if [ $# -lt 2 ]
then
    echo "usage: $0 <config-repo-path> <index-path> [permanent-path]"
    exit 1
fi

export MOZSEARCH_PATH=$(readlink -f $(dirname "$0")/..)
export CONFIG_REPO=$(readlink -f $1)
export WORKING=$(readlink -f $2)
export PERMANENT=${3:+$(readlink -f $3)}

CONFIG_FILE=$WORKING/config.json

for TREE_NAME in $(jq -r ".trees|keys_unsorted|.[]" ${CONFIG_FILE})
do
    . $MOZSEARCH_PATH/scripts/load-vars.sh $CONFIG_FILE $TREE_NAME
    $MOZSEARCH_PATH/scripts/mkindex.sh $CONFIG_REPO $CONFIG_FILE $TREE_NAME || handle_tree_error "mkindex.sh"
    # If we were given a permanent path, move the index results there and
    # symlink from the old location to the new location.
    if [ -n "$PERMANENT" ]
    then
      mv $INDEX_ROOT ${INDEX_ROOT/$WORKING/$PERMANENT}
      ln -s ${INDEX_ROOT/$WORKING/$PERMANENT} $INDEX_ROOT
    fi
done

```

## .eslintrc.js
```
"use strict";

module.exports = {
  parserOptions: {
    ecmaVersion: 8,
  },
  rules: {
    curly: "error",
  },
};

```

## CODE_OF_CONDUCT.md
```
# Community Participation Guidelines

This repository is governed by Mozilla's code of conduct and etiquette guidelines.
For more details, please read the
[Mozilla Community Participation Guidelines](https://www.mozilla.org/about/governance/policies/participation/).

## How to Report
For more information on how to report violations of the Community Participation Guidelines, please read our '[How to Report](https://www.mozilla.org/about/governance/policies/participation/reporting/)' page.

<!--
## Project Specific Etiquette

In some cases, there will be additional project etiquette i.e.: (https://bugzilla.mozilla.org/page.cgi?id=etiquette.html).
Please update for your project.
-->

```

## .prettierrc
```
{
  "endOfLine": "lf",
  "printWidth": 80,
  "tabWidth": 2,
  "trailingComma": "es5",
  "arrowParens": "avoid",
}

```

## trees/README.md
```
Put symlinks to trees you want to index in here.

The configs for the trees go in the parallel `tree-configs` directory.

You can then build the trees via `make build-trees` from `/vagrant`
inside of your docker image if your config file is the default of
`config.json`.  If your config file is named something else, then
you can set the `CONFIG` env variable by doing something like
`CONFIG=my-config.json make build-trees`.  Note that although make
can accept variable assignments as part of its arguments, that's not
how this is intended to work, and so maybe it won't work!

```

## flake.nix
```
{
  inputs = {
    nixpkgs.url = github:NixOS/nixpkgs/nixpkgs-unstable;
    flake-utils.url = github:numtide/flake-utils;
    fenix = {
      url = "github:nix-community/fenix";
      inputs.nixpkgs.follows = "nixpkgs";
    };
  };

  outputs = {
    self,
    nixpkgs,
    flake-utils,
    fenix,
  }: (
    flake-utils.lib.eachDefaultSystem (
      system: let
        pkgs = nixpkgs.legacyPackages.${system}.extend fenix.overlays.default;

        rustToolchain = pkgs.fenix.stable.toolchain;

        # A symlink from `docker` to `podman`, because the scripts call `docker`.
        dockerCompat = pkgs.runCommandNoCC "docker-podman-compat" {} ''
          mkdir -p $out/bin
          ln -s ${pkgs.podman}/bin/podman $out/bin/docker
        '';

        pythonPackages = p:
          with p; [
            boto3
            rich
          ];
      in {
        devShells.default = pkgs.mkShell {
          packages = with pkgs; [
            dockerCompat
            podman

            # Those are probably not all required, copied from
            # https://gist.github.com/adisbladis/187204cb772800489ee3dac4acdd9947
            runc # Container runtime
            conmon # Container runtime monitor
            skopeo # Interact with container registry
            slirp4netns # User-mode networking for unprivileged namespaces
            fuse-overlayfs # CoW for images, much faster than default vfs

            jq

            (python3.withPackages pythonPackages)
            awscli2

            # Dependencies required to build tools
            rustToolchain
            openssl
            cmake
            pkg-config

            # Must be before (unwrapped) clang in path
            clang-tools_19

            # Dependencies required to build clang-plugin
            clang_19
            llvmPackages_19.libllvm
            llvmPackages_19.libclang

            gdb

            scip
            protobuf

            pre-commit
          ];

          PODMAN_USERNS = "keep-id";

          AWS_PROFILE = "searchfox";

          shellHook = ''
            echo "TL;DR:"
            echo "- './build-docker.sh' to build the container"
            echo "- './run-docker.sh' to enter the container"
            echo "- 'cd /vagrant; make build-test-repo' inside to build"
            echo "- open http://localhost:16995 in a browser"
          '';
        };

        formatter = pkgs.alejandra;
      }
    )
  );
}

```

## Makefile
```
help:
	@echo "This Makefile provides some useful targets to run:"
	@echo "  build-test-repo - Builds the index and starts the web server for the test repo"
	@echo "  build-mozilla-repo - Builds the index and starts the web server for the repos in mozsearch-mozilla/config1.json"
	@echo ""
	@echo "To build a local index from a try push of mozilla-central:"
	@echo "  TRYPUSH_REV=7b25952b97afc2a34cc31701ffb185222727be72 make trypush # set TRYPUSH_REV to the full hg rev of your try push"

.DEFAULT_GOAL := help

.PHONY: help check-in-vagrant build-clang-plugin build-rust-tools test-rust-tools build-test-repo build-mozilla-repo baseline comparison

check-in-vagrant:
	@[ -d /vagrant ] || (echo "This command must be run inside the vagrant instance" > /dev/stderr; exit 1)

build-clang-plugin: check-in-vagrant
	$(MAKE) -C clang-plugin build_with_version_check

# This can be built outside the vagrant instance too
# We specify "--all-targets" in order to minimize rebuilding required when we invoke
# `cargo test` to validate the build.
build-rust-tools:
	cd tools && cargo build --release --all-targets
	cd scripts/web-analyze/wasm-css-analyzer && ./build.sh

test-rust-tools:
	cd tools && cargo test --release --verbose

build-test-repo: check-in-vagrant build-clang-plugin build-rust-tools
	mkdir -p ~/index
	/vagrant/infrastructure/indexer-setup.sh /vagrant/tests config.json ~/index
	/vagrant/infrastructure/indexer-run.sh /vagrant/tests ~/index
	/vagrant/infrastructure/web-server-setup.sh /vagrant/tests config.json ~/index ~
	/vagrant/infrastructure/web-server-run.sh /vagrant/tests ~/index ~ WAIT
	/vagrant/infrastructure/web-server-check.sh /vagrant/tests ~/index "http://localhost/"

serve-test-repo: check-in-vagrant build-clang-plugin build-rust-tools
	/vagrant/infrastructure/web-server-setup.sh /vagrant/tests config.json ~/index ~
	/vagrant/infrastructure/web-server-run.sh /vagrant/tests ~/index ~ WAIT

check-test-repo:
	/vagrant/infrastructure/web-server-check.sh /vagrant/tests ~/index "http://localhost/"

# Target that:
# - Runs the check scripts in a special mode that lets the tests run without
#   failing, instead generating the revised expectations for anything that has
#   changed.
#   - We need to re-run `indexer-run.sh` too because it embeds the disk check
#     inside `mkindex.sh`.  Arguably maybe we want to fix web-server-check.sh
#     to perhaps help run the indexer check.
# - Runs the `cargo insta review` command which has a cool interactive UI that
#   shows any differences.
#
# You would likely want to run this if:
# - You ran `make build-test-repo` and got errors and you were like, "oh, yeah,
#   stuff might have changed and I should look at it and maybe approve those
#   changes."
# - You know you already have changed stuff and need to review those changes.
#
# Depends on `cargo install cargo-insta`.
review-test-repo:
	INSTA_FORCE_PASS=1 /vagrant/infrastructure/indexer-run.sh /vagrant/tests ~/index
	/vagrant/infrastructure/web-server-setup.sh /vagrant/tests config.json ~/index ~
	/vagrant/infrastructure/web-server-run.sh /vagrant/tests ~/index ~ WAIT
	INSTA_FORCE_PASS=1 /vagrant/infrastructure/web-server-check.sh /vagrant/tests ~/index "http://localhost/"
	cargo insta review --workspace-root=/vagrant/tests/tests/checks

build-searchfox-repo: check-in-vagrant build-clang-plugin build-rust-tools
	mkdir -p ~/searchfox-index
	/vagrant/infrastructure/indexer-setup.sh /vagrant/tests searchfox-config.json ~/searchfox-index
	/vagrant/infrastructure/indexer-run.sh /vagrant/tests ~/searchfox-index
	/vagrant/infrastructure/web-server-setup.sh /vagrant/tests searchfox-config.json ~/searchfox-index ~
	/vagrant/infrastructure/web-server-run.sh /vagrant/tests ~/searchfox-index ~

# Notes:
# - If you want to use a modified version of mozsearch-mozilla, such as one
#   checked out under "config" in the check-out repo, you can create a symlink
#   in the VM's home directory via `pushd ~; ln -s /vagrant/config mozilla-config`.
# - This also works with `export TRYPUSH_REV=full-40char-hash` for try runs
#   that have the relevant jobs scheduled on them.  In particular:
#   `./mach try fuzzy --full -q "'searchfox" -q "'bugzilla-component"`
build-mozilla-repo: check-in-vagrant build-clang-plugin build-rust-tools
	[ -e ~/mozilla-config ] || git clone https://github.com/mozsearch/mozsearch-mozilla ~/mozilla-config
	mkdir -p ~/mozilla-index
	/vagrant/infrastructure/indexer-setup.sh ~/mozilla-config just-mc.json ~/mozilla-index
	/vagrant/infrastructure/indexer-run.sh ~/mozilla-config ~/mozilla-index
	/vagrant/infrastructure/web-server-setup.sh ~/mozilla-config just-mc.json ~/mozilla-index ~
	/vagrant/infrastructure/web-server-run.sh ~/mozilla-config ~/mozilla-index ~

serve-mozilla-repo: check-in-vagrant build-clang-plugin build-rust-tools
	/vagrant/infrastructure/web-server-setup.sh ~/mozilla-config just-mc.json ~/mozilla-index ~
	/vagrant/infrastructure/web-server-run.sh ~/mozilla-config ~/mozilla-index ~

# This builds both mozsearch and mozsearch-mozilla using the trees as they exist
# on github rather than your local copies.  This differs from the
# "build-searchfox-repo" make target which uses your current tree, which can be
# useful but where anything that isn't checked-in can cause failures.  (That is,
# if you run `git status` and anything is modified, output-files can crash when
# the local checked-out status does not have the same number of lines as the
# blame repo says there should be.)
#
# Notes:
# - If you want to use a modified version of mozsearch-mozilla, such as one
#   checked out under "config" in the check-out repo, you can create a symlink
#   in the VM's home directory via `pushd ~; ln -s /vagrant/config mozsearch-config`.
build-mozsearch-repo: check-in-vagrant build-clang-plugin build-rust-tools
	[ -e ~/mozsearch-config ] || git clone https://github.com/mozsearch/mozsearch-mozilla ~/mozsearch-config
	mkdir -p ~/mozsearch-index
	/vagrant/infrastructure/indexer-setup.sh ~/mozsearch-config just-mozsearch.json ~/mozsearch-index
	/vagrant/infrastructure/indexer-run.sh ~/mozsearch-config ~/mozsearch-index
	/vagrant/infrastructure/web-server-setup.sh ~/mozsearch-config just-mozsearch.json ~/mozsearch-index ~
	/vagrant/infrastructure/web-server-run.sh ~/mozsearch-config ~/mozsearch-index ~

serve-mozsearch-repo: check-in-vagrant build-clang-plugin build-rust-tools
	/vagrant/infrastructure/web-server-setup.sh ~/mozsearch-config just-mozsearch.json ~/mozsearch-index ~
	/vagrant/infrastructure/web-server-run.sh ~/mozsearch-config ~/mozsearch-index ~

# Notes:
# - If you want to use a modified version of mozsearch-mozilla, such as one
#   checked out under "config" in the check-out repo, you can create a symlink
#   in the VM's home directory via `pushd ~; ln -s /vagrant/config llvm-config`.
build-llvm-repo: check-in-vagrant build-clang-plugin build-rust-tools
	[ -e ~/llvm-config ] || git clone https://github.com/mozsearch/mozsearch-mozilla ~/llvm-config
	mkdir -p ~/llvm-index
	/vagrant/infrastructure/indexer-setup.sh ~/llvm-config just-llvm.json ~/llvm-index
	/vagrant/infrastructure/indexer-run.sh ~/llvm-config ~/llvm-index
	/vagrant/infrastructure/web-server-setup.sh ~/llvm-config just-llvm.json ~/llvm-index ~
	/vagrant/infrastructure/web-server-run.sh ~/llvm-config ~/llvm-index ~

serve-llvm-repo: check-in-vagrant build-clang-plugin build-rust-tools
	/vagrant/infrastructure/web-server-setup.sh ~/llvm-config just-llvm.json ~/llvm-index ~
	/vagrant/infrastructure/web-server-run.sh ~/llvm-config ~/llvm-index ~

# Notes:
# - If you want to use a modified version of mozsearch-mozilla, such as one
#   checked out under "config" in the check-out repo, you can create a symlink
#   in the VM's home directory via `pushd ~; ln -s /vagrant/config graphviz-config`.
build-graphviz-repo: check-in-vagrant build-clang-plugin build-rust-tools
	[ -e ~/graphviz-config ] || git clone https://github.com/mozsearch/mozsearch-mozilla ~/graphviz-config
	mkdir -p ~/graphviz-index
	/vagrant/infrastructure/indexer-setup.sh ~/graphviz-config just-graphviz.json ~/graphviz-index
	/vagrant/infrastructure/indexer-run.sh ~/graphviz-config ~/graphviz-index
	/vagrant/infrastructure/web-server-setup.sh ~/graphviz-config just-graphviz.json ~/graphviz-index ~
	/vagrant/infrastructure/web-server-run.sh ~/graphviz-config ~/graphviz-index ~

serve-graphviz-repo: check-in-vagrant build-clang-plugin build-rust-tools
	/vagrant/infrastructure/web-server-setup.sh ~/graphviz-config just-graphviz.json ~/graphviz-index ~
	/vagrant/infrastructure/web-server-run.sh ~/graphviz-config ~/graphviz-index ~

build-trees: check-in-vagrant build-clang-plugin build-rust-tools
	mkdir -p ~/trees-index
	/vagrant/infrastructure/indexer-setup.sh /vagrant/tree-configs config.json ~/trees-index
	/vagrant/infrastructure/indexer-run.sh /vagrant/tree-configs ~/trees-index
	/vagrant/infrastructure/web-server-setup.sh /vagrant/tree-configs config.json ~/trees-index ~
	/vagrant/infrastructure/web-server-run.sh /vagrant/tree-configs ~/trees-index ~ WAIT
	/vagrant/infrastructure/web-server-check.sh /vagrant/tree-configs ~/trees-index "http://localhost/"

serve-trees: check-in-vagrant build-clang-plugin build-rust-tools
	/vagrant/infrastructure/web-server-setup.sh /vagrant/tree-configs config.json ~/trees-index ~
	/vagrant/infrastructure/web-server-run.sh /vagrant/tree-configs ~/trees-index ~ WAIT

# This is similar to build-mozilla-repo, except it strips out the non-mozilla-central trees
# from config.json and puts the stripped version into trypush.json.
trypush: check-in-vagrant build-clang-plugin build-rust-tools
	[ -d ~/mozilla-config ] || git clone https://github.com/mozsearch/mozsearch-mozilla ~/mozilla-config
	jq '{mozsearch_path, config_repo, default_tree, trees: {"mozilla-central": .trees["mozilla-central"]}}' ~/mozilla-config/config1.json > ~/mozilla-config/trypush.json
	mkdir -p ~/trypush-index
	/vagrant/infrastructure/indexer-setup.sh ~/mozilla-config trypush.json ~/trypush-index
	/vagrant/infrastructure/indexer-run.sh ~/mozilla-config ~/trypush-index
	/vagrant/infrastructure/web-server-setup.sh ~/mozilla-config trypush.json ~/trypush-index ~
	/vagrant/infrastructure/web-server-run.sh ~/mozilla-config ~/trypush-index ~

nss-reblame: check-in-vagrant build-rust-tools
	[ -d ~/mozilla-config ] || git clone https://github.com/mozsearch/mozsearch-mozilla ~/mozilla-config
	jq '{mozsearch_path, config_repo, default_tree, trees: {"nss": .trees["nss"]}}' ~/mozilla-config/config1.json > ~/mozilla-config/nss.json
	mkdir -p ~/reblame
	/vagrant/infrastructure/reblame-run.sh ~/mozilla-config nss.json ~/reblame

# To test changes to indexing, run this first to generate the baseline. Then
# make your changes, and run `make comparison`. Note that we generate
# the index into ~/diffable and move it to ~/baseline so that when we
# generate the index with modifications we can also generate it into the same
# ~/diffable folder. This eliminates spurious diff results that might
# come from different absolute paths during the index generation step
baseline: check-in-vagrant build-clang-plugin build-rust-tools
	rm -rf ~/diffable ~/baseline
	mkdir -p ~/diffable
	/vagrant/infrastructure/indexer-setup.sh /vagrant/tests config.json ~/diffable
	MOZSEARCH_DIFFABLE=1 /vagrant/infrastructure/indexer-run.sh /vagrant/tests ~/diffable
	mv ~/diffable ~/baseline

comparison: check-in-vagrant build-clang-plugin build-rust-tools
	rm -rf ~/diffable ~/modified
	mkdir -p ~/diffable
	/vagrant/infrastructure/indexer-setup.sh /vagrant/tests config.json ~/diffable
	MOZSEARCH_DIFFABLE=1 /vagrant/infrastructure/indexer-run.sh /vagrant/tests ~/diffable
	mv ~/diffable ~/modified
	@echo "------------------- Below is the diff between baseline and modified. ---------------------"
	diff -u -r -x objdir ~/baseline/tests ~/modified/tests || true
	@echo "------------------- Above is the diff between baseline and modified. ---------------------"
	@echo "--- Run 'diff -u -r -x objdir ~/{baseline,modified}/tests | less' to see it in a pager ---"

build-webtest-repo: check-in-vagrant build-clang-plugin build-rust-tools
	mkdir -p ~/index
	/vagrant/infrastructure/indexer-setup.sh /vagrant/tests webtest-config.json ~/index
	/vagrant/infrastructure/indexer-run.sh /vagrant/tests ~/index
	/vagrant/infrastructure/web-server-setup.sh /vagrant/tests webtest-config.json ~/index ~
	/vagrant/infrastructure/web-server-run.sh /vagrant/tests ~/index ~ WAIT

webtest: build-webtest-repo
	./scripts/webtest.sh

```

## .travis.yml
```
language: rust
cache: cargo
rust:
    - nightly
script:
    - make test-rust-tools

```

## CONTRIBUTING.md
```
# Contributing to Searchfox

## Filing issues/bugs

Please file bugs in
[Bugzilla](https://bugzilla.mozilla.org/enter_bug.cgi?product=Webtools&component=Searchfox).
We do not accept bug reports on GitHub (and the issues tab is disabled
for this reason).

## Submitting patches

Before writing a patch, please
[file a bug](https://bugzilla.mozilla.org/enter_bug.cgi?product=Webtools&component=Searchfox)
describing the problem you wish to solve. See the documentation in the
[README.md](README.md) file on how to set up a local development environment
with which you can test your changes. We realize the process of setting up a
local dev environment is quite cumbersome and can be a deterrent; so for
small patches that you're fairly confident in, you may just skip to
submitting a PR and include a note that you haven't tested it. In this
scenario one of the project maintainers can test it for you before merging,
or ping you to address any issues discovered.

When creating a PR, please ensure your changes are grouped into separate
commits logically, so that they are easy to review. The commits that are
functionally involved in fixing the bug should be tagged with the bug number
from Bugzilla (i.e. `Bug 12345 - Summary of commit` should be the first line
of the commit message). Commits that are doing cleanup/refactoring that are
not strictly related to the fix may omit the bug number.

After creating the PR, please comment on the bug with a link to the PR.
Project maintainers get notifications of all activity and should review
the patch soon. If you don't get a response within a few days, feel free
to ping @staktrace on the PR or needinfo :kats on the bug.

```

## sax/README
```
Import from https://github.com/isaacs/sax-js
Commit 5da00d213a8cae94be72ce155e7cf8fda600b94c, with sax.patch

```

## sax/LICENSE
```
The ISC License

Copyright (c) 2010-2022 Isaac Z. Schlueter and Contributors

Permission to use, copy, modify, and/or distribute this software for any
purpose with or without fee is hereby granted, provided that the above
copyright notice and this permission notice appear in all copies.

THE SOFTWARE IS PROVIDED "AS IS" AND THE AUTHOR DISCLAIMS ALL WARRANTIES
WITH REGARD TO THIS SOFTWARE INCLUDING ALL IMPLIED WARRANTIES OF
MERCHANTABILITY AND FITNESS. IN NO EVENT SHALL THE AUTHOR BE LIABLE FOR
ANY SPECIAL, DIRECT, INDIRECT, OR CONSEQUENTIAL DAMAGES OR ANY DAMAGES
WHATSOEVER RESULTING FROM LOSS OF USE, DATA OR PROFITS, WHETHER IN AN
ACTION OF CONTRACT, NEGLIGENCE OR OTHER TORTIOUS ACTION, ARISING OUT OF OR
IN CONNECTION WITH THE USE OR PERFORMANCE OF THIS SOFTWARE.

====

`String.fromCodePoint` by Mathias Bynens used according to terms of MIT
License, as follows:

Copyright (c) 2010-2022 Mathias Bynens <https://mathiasbynens.be/>

    Permission is hereby granted, free of charge, to any person obtaining
    a copy of this software and associated documentation files (the
    "Software"), to deal in the Software without restriction, including
    without limitation the rights to use, copy, modify, merge, publish,
    distribute, sublicense, and/or sell copies of the Software, and to
    permit persons to whom the Software is furnished to do so, subject to
    the following conditions:

    The above copyright notice and this permission notice shall be
    included in all copies or substantial portions of the Software.

    THE SOFTWARE IS PROVIDED "AS IS", WITHOUT WARRANTY OF ANY KIND,
    EXPRESS OR IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF
    MERCHANTABILITY, FITNESS FOR A PARTICULAR PURPOSE AND
    NONINFRINGEMENT. IN NO EVENT SHALL THE AUTHORS OR COPYRIGHT HOLDERS BE
    LIABLE FOR ANY CLAIM, DAMAGES OR OTHER LIABILITY, WHETHER IN AN ACTION
    OF CONTRACT, TORT OR OTHERWISE, ARISING FROM, OUT OF OR IN CONNECTION
    WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE SOFTWARE.

```

## sax/sax.js
```
;(function (sax) { // wrapper for non-node envs
  sax.parser = function (strict, opt) { return new SAXParser(strict, opt) }
  sax.SAXParser = SAXParser
  sax.SAXStream = SAXStream
  sax.createStream = createStream

  // When we pass the MAX_BUFFER_LENGTH position, start checking for buffer overruns.
  // When we check, schedule the next check for MAX_BUFFER_LENGTH - (max(buffer lengths)),
  // since that's the earliest that a buffer overrun could occur.  This way, checks are
  // as rare as required, but as often as necessary to ensure never crossing this bound.
  // Furthermore, buffers are only tested at most once per write(), so passing a very
  // large string into write() might have undesirable effects, but this is manageable by
  // the caller, so it is assumed to be safe.  Thus, a call to write() may, in the extreme
  // edge case, result in creating at most one complete copy of the string passed in.
  // Set to Infinity to have unlimited buffers.
  sax.MAX_BUFFER_LENGTH = 64 * 1024

  var buffers = [
    'comment', 'sgmlDecl', 'textNode', 'tagName', 'doctype',
    'procInstName', 'procInstBody', 'entity', 'attribName',
    'attribValue', 'cdata', 'script'
  ]

  sax.EVENTS = [
    'text',
    'processinginstruction',
    'sgmldeclaration',
    'doctype',
    'comment',
    'opentagstart',
    'attribute',
    'opentag',
    'closetag',
    'opencdata',
    'cdata',
    'closecdata',
    'error',
    'end',
    'ready',
    'script',
    'opennamespace',
    'closenamespace'
  ]

  function SAXParser (strict, opt) {
    if (!(this instanceof SAXParser)) {
      return new SAXParser(strict, opt)
    }

    var parser = this
    clearBuffers(parser)
    parser.q = parser.c = ''
    parser.bufferCheckPosition = sax.MAX_BUFFER_LENGTH
    parser.opt = opt || {}
    parser.opt.lowercase = parser.opt.lowercase || parser.opt.lowercasetags
    parser.looseCase = parser.opt.lowercase ? 'toLowerCase' : 'toUpperCase'
    parser.tags = []
    parser.closed = parser.closedRoot = parser.sawRoot = false
    parser.tag = parser.error = null
    parser.strict = !!strict
    parser.noscript = !!(strict || parser.opt.noscript)
    parser.state = S.BEGIN
    parser.strictEntities = parser.opt.strictEntities
    parser.ENTITIES = parser.strictEntities ? Object.create(sax.XML_ENTITIES) : Object.create(sax.ENTITIES)
    parser.attribList = []

    // namespaces form a prototype chain.
    // it always points at the current tag,
    // which protos to its parent tag.
    if (parser.opt.xmlns) {
      parser.ns = Object.create(rootNS)
    }

    // mostly just for error reporting
    parser.trackPosition = parser.opt.position !== false
    if (parser.trackPosition) {
      parser.position = parser.line = parser.column = 0
    }
    emit(parser, 'onready')
  }

  if (!Object.create) {
    Object.create = function (o) {
      function F () {}
      F.prototype = o
      var newf = new F()
      return newf
    }
  }

  if (!Object.keys) {
    Object.keys = function (o) {
      var a = []
      for (var i in o) if (o.hasOwnProperty(i)) a.push(i)
      return a
    }
  }

  function checkBufferLength (parser) {
    var maxAllowed = Math.max(sax.MAX_BUFFER_LENGTH, 10)
    var maxActual = 0
    for (var i = 0, l = buffers.length; i < l; i++) {
      var len = parser[buffers[i]].length
      if (len > maxAllowed) {
        // Text/cdata nodes can get big, and since they're buffered,
        // we can get here under normal conditions.
        // Avoid issues by emitting the text node now,
        // so at least it won't get any bigger.
        switch (buffers[i]) {
          case 'textNode':
            closeText(parser)
            break

          case 'cdata':
            emitNode(parser, 'oncdata', parser.cdata)
            parser.cdata = ''
            break

          case 'script':
            emitNode(parser, 'onscript', parser.script)
            parser.script = ''
            break

          default:
            error(parser, 'Max buffer length exceeded: ' + buffers[i])
        }
      }
      maxActual = Math.max(maxActual, len)
    }
    // schedule the next check for the earliest possible buffer overrun.
    var m = sax.MAX_BUFFER_LENGTH - maxActual
    parser.bufferCheckPosition = m + parser.position
  }

  function clearBuffers (parser) {
    for (var i = 0, l = buffers.length; i < l; i++) {
      parser[buffers[i]] = ''
    }
  }

  function flushBuffers (parser) {
    closeText(parser)
    if (parser.cdata !== '') {
      emitNode(parser, 'oncdata', parser.cdata)
      parser.cdata = ''
    }
    if (parser.script !== '') {
      emitNode(parser, 'onscript', parser.script)
      parser.script = ''
    }
  }

  SAXParser.prototype = {
    end: function () { end(this) },
    write: write,
    resume: function () { this.error = null; return this },
    close: function () { return this.write(null) },
    flush: function () { flushBuffers(this) }
  }

  var Stream
  try {
    Stream = require('stream').Stream
  } catch (ex) {
    Stream = function () {}
  }

  var streamWraps = sax.EVENTS.filter(function (ev) {
    return ev !== 'error' && ev !== 'end'
  })

  function createStream (strict, opt) {
    return new SAXStream(strict, opt)
  }

  function SAXStream (strict, opt) {
    if (!(this instanceof SAXStream)) {
      return new SAXStream(strict, opt)
    }

    Stream.apply(this)

    this._parser = new SAXParser(strict, opt)
    this.writable = true
    this.readable = true

    var me = this

    this._parser.onend = function () {
      me.emit('end')
    }

    this._parser.onerror = function (er) {
      me.emit('error', er)

      // if didn't throw, then means error was handled.
      // go ahead and clear error, so we can write again.
      me._parser.error = null
    }

    this._decoder = null

    streamWraps.forEach(function (ev) {
      Object.defineProperty(me, 'on' + ev, {
        get: function () {
          return me._parser['on' + ev]
        },
        set: function (h) {
          if (!h) {
            me.removeAllListeners(ev)
            me._parser['on' + ev] = h
            return h
          }
          me.on(ev, h)
        },
        enumerable: true,
        configurable: false
      })
    })
  }

  SAXStream.prototype = Object.create(Stream.prototype, {
    constructor: {
      value: SAXStream
    }
  })

  SAXStream.prototype.write = function (data) {
    if (typeof Buffer === 'function' &&
      typeof Buffer.isBuffer === 'function' &&
      Buffer.isBuffer(data)) {
      if (!this._decoder) {
        var SD = require('string_decoder').StringDecoder
        this._decoder = new SD('utf8')
      }
      data = this._decoder.write(data)
    }

    this._parser.write(data.toString())
    this.emit('data', data)
    return true
  }

  SAXStream.prototype.end = function (chunk) {
    if (chunk && chunk.length) {
      this.write(chunk)
    }
    this._parser.end()
    return true
  }

  SAXStream.prototype.on = function (ev, handler) {
    var me = this
    if (!me._parser['on' + ev] && streamWraps.indexOf(ev) !== -1) {
      me._parser['on' + ev] = function () {
        var args = arguments.length === 1 ? [arguments[0]] : Array.apply(null, arguments)
        args.splice(0, 0, ev)
        me.emit.apply(me, args)
      }
    }

    return Stream.prototype.on.call(me, ev, handler)
  }

  // this really needs to be replaced with character classes.
  // XML allows all manner of ridiculous numbers and digits.
  var CDATA = '[CDATA['
  var DOCTYPE = 'DOCTYPE'
  var XML_NAMESPACE = 'http://www.w3.org/XML/1998/namespace'
  var XMLNS_NAMESPACE = 'http://www.w3.org/2000/xmlns/'
  var rootNS = { xml: XML_NAMESPACE, xmlns: XMLNS_NAMESPACE }

  // http://www.w3.org/TR/REC-xml/#NT-NameStartChar
  // This implementation works on strings, a single character at a time
  // as such, it cannot ever support astral-plane characters (10000-EFFFF)
  // without a significant breaking change to either this  parser, or the
  // JavaScript language.  Implementation of an emoji-capable xml parser
  // is left as an exercise for the reader.
  var nameStart = /[:_A-Za-z\u00C0-\u00D6\u00D8-\u00F6\u00F8-\u02FF\u0370-\u037D\u037F-\u1FFF\u200C-\u200D\u2070-\u218F\u2C00-\u2FEF\u3001-\uD7FF\uF900-\uFDCF\uFDF0-\uFFFD]/

  var nameBody = /[:_A-Za-z\u00C0-\u00D6\u00D8-\u00F6\u00F8-\u02FF\u0370-\u037D\u037F-\u1FFF\u200C-\u200D\u2070-\u218F\u2C00-\u2FEF\u3001-\uD7FF\uF900-\uFDCF\uFDF0-\uFFFD\u00B7\u0300-\u036F\u203F-\u2040.\d-]/

  var entityStart = /[#:_A-Za-z\u00C0-\u00D6\u00D8-\u00F6\u00F8-\u02FF\u0370-\u037D\u037F-\u1FFF\u200C-\u200D\u2070-\u218F\u2C00-\u2FEF\u3001-\uD7FF\uF900-\uFDCF\uFDF0-\uFFFD]/
  var entityBody = /[#:_A-Za-z\u00C0-\u00D6\u00D8-\u00F6\u00F8-\u02FF\u0370-\u037D\u037F-\u1FFF\u200C-\u200D\u2070-\u218F\u2C00-\u2FEF\u3001-\uD7FF\uF900-\uFDCF\uFDF0-\uFFFD\u00B7\u0300-\u036F\u203F-\u2040.\d-]/

  function isWhitespace (c) {
    return c === ' ' || c === '\n' || c === '\r' || c === '\t'
  }

  function isQuote (c) {
    return c === '"' || c === '\''
  }

  function isAttribEnd (c) {
    return c === '>' || isWhitespace(c)
  }

  function isMatch (regex, c) {
    return regex.test(c)
  }

  function notMatch (regex, c) {
    return !isMatch(regex, c)
  }

  var S = 0
  sax.STATE = {
    BEGIN: S++, // leading byte order mark or whitespace
    BEGIN_WHITESPACE: S++, // leading whitespace
    TEXT: S++, // general stuff
    TEXT_ENTITY: S++, // &amp and such.
    OPEN_WAKA: S++, // <
    SGML_DECL: S++, // <!BLARG
    SGML_DECL_QUOTED: S++, // <!BLARG foo "bar
    DOCTYPE: S++, // <!DOCTYPE
    DOCTYPE_QUOTED: S++, // <!DOCTYPE "//blah
    DOCTYPE_DTD: S++, // <!DOCTYPE "//blah" [ ...
    DOCTYPE_DTD_QUOTED: S++, // <!DOCTYPE "//blah" [ "foo
    COMMENT_STARTING: S++, // <!-
    COMMENT: S++, // <!--
    COMMENT_ENDING: S++, // <!-- blah -
    COMMENT_ENDED: S++, // <!-- blah --
    CDATA: S++, // <![CDATA[ something
    CDATA_ENDING: S++, // ]
    CDATA_ENDING_2: S++, // ]]
    PROC_INST: S++, // <?hi
    PROC_INST_BODY: S++, // <?hi there
    PROC_INST_ENDING: S++, // <?hi "there" ?
    OPEN_TAG: S++, // <strong
    OPEN_TAG_SLASH: S++, // <strong /
    ATTRIB: S++, // <a
    ATTRIB_NAME: S++, // <a foo
    ATTRIB_NAME_SAW_WHITE: S++, // <a foo _
    ATTRIB_VALUE: S++, // <a foo=
    ATTRIB_VALUE_QUOTED: S++, // <a foo="bar
    ATTRIB_VALUE_CLOSED: S++, // <a foo="bar"
    ATTRIB_VALUE_UNQUOTED: S++, // <a foo=bar
    ATTRIB_VALUE_ENTITY_Q: S++, // <foo bar="&quot;"
    ATTRIB_VALUE_ENTITY_U: S++, // <foo bar=&quot
    CLOSE_TAG: S++, // </a
    CLOSE_TAG_SAW_WHITE: S++, // </a   >
    SCRIPT: S++, // <script> ...
    SCRIPT_ENDING: S++, // <script> ... <
    SCRIPT_COMMENT_START: S++, // <script> ... <! ...
    SCRIPT_COMMENT: S++, // <script> ... <!-- ...
    SCRIPT_COMMENT_END: S++, // <script> ... <!-- ... - ...
  }

  sax.XML_ENTITIES = {
    'amp': '&',
    'gt': '>',
    'lt': '<',
    'quot': '"',
    'apos': "'"
  }

  sax.ENTITIES = {
    'amp': '&',
    'gt': '>',
    'lt': '<',
    'quot': '"',
    'apos': "'",
    'AElig': 198,
    'Aacute': 193,
    'Acirc': 194,
    'Agrave': 192,
    'Aring': 197,
    'Atilde': 195,
    'Auml': 196,
    'Ccedil': 199,
    'ETH': 208,
    'Eacute': 201,
    'Ecirc': 202,
    'Egrave': 200,
    'Euml': 203,
    'Iacute': 205,
    'Icirc': 206,
    'Igrave': 204,
    'Iuml': 207,
    'Ntilde': 209,
    'Oacute': 211,
    'Ocirc': 212,
    'Ograve': 210,
    'Oslash': 216,
    'Otilde': 213,
    'Ouml': 214,
    'THORN': 222,
    'Uacute': 218,
    'Ucirc': 219,
    'Ugrave': 217,
    'Uuml': 220,
    'Yacute': 221,
    'aacute': 225,
    'acirc': 226,
    'aelig': 230,
    'agrave': 224,
    'aring': 229,
    'atilde': 227,
    'auml': 228,
    'ccedil': 231,
    'eacute': 233,
    'ecirc': 234,
    'egrave': 232,
    'eth': 240,
    'euml': 235,
    'iacute': 237,
    'icirc': 238,
    'igrave': 236,
    'iuml': 239,
    'ntilde': 241,
    'oacute': 243,
    'ocirc': 244,
    'ograve': 242,
    'oslash': 248,
    'otilde': 245,
    'ouml': 246,
    'szlig': 223,
    'thorn': 254,
    'uacute': 250,
    'ucirc': 251,
    'ugrave': 249,
    'uuml': 252,
    'yacute': 253,
    'yuml': 255,
    'copy': 169,
    'reg': 174,
    'nbsp': 160,
    'iexcl': 161,
    'cent': 162,
    'pound': 163,
    'curren': 164,
    'yen': 165,
    'brvbar': 166,
    'sect': 167,
    'uml': 168,
    'ordf': 170,
    'laquo': 171,
    'not': 172,
    'shy': 173,
    'macr': 175,
    'deg': 176,
    'plusmn': 177,
    'sup1': 185,
    'sup2': 178,
    'sup3': 179,
    'acute': 180,
    'micro': 181,
    'para': 182,
    'middot': 183,
    'cedil': 184,
    'ordm': 186,
    'raquo': 187,
    'frac14': 188,
    'frac12': 189,
    'frac34': 190,
    'iquest': 191,
    'times': 215,
    'divide': 247,
    'OElig': 338,
    'oelig': 339,
    'Scaron': 352,
    'scaron': 353,
    'Yuml': 376,
    'fnof': 402,
    'circ': 710,
    'tilde': 732,
    'Alpha': 913,
    'Beta': 914,
    'Gamma': 915,
    'Delta': 916,
    'Epsilon': 917,
    'Zeta': 918,
    'Eta': 919,
    'Theta': 920,
    'Iota': 921,
    'Kappa': 922,
    'Lambda': 923,
    'Mu': 924,
    'Nu': 925,
    'Xi': 926,
    'Omicron': 927,
    'Pi': 928,
    'Rho': 929,
    'Sigma': 931,
    'Tau': 932,
    'Upsilon': 933,
    'Phi': 934,
    'Chi': 935,
    'Psi': 936,
    'Omega': 937,
    'alpha': 945,
    'beta': 946,
    'gamma': 947,
    'delta': 948,
    'epsilon': 949,
    'zeta': 950,
    'eta': 951,
    'theta': 952,
    'iota': 953,
    'kappa': 954,
    'lambda': 955,
    'mu': 956,
    'nu': 957,
    'xi': 958,
    'omicron': 959,
    'pi': 960,
    'rho': 961,
    'sigmaf': 962,
    'sigma': 963,
    'tau': 964,
    'upsilon': 965,
    'phi': 966,
    'chi': 967,
    'psi': 968,
    'omega': 969,
    'thetasym': 977,
    'upsih': 978,
    'piv': 982,
    'ensp': 8194,
    'emsp': 8195,
    'thinsp': 8201,
    'zwnj': 8204,
    'zwj': 8205,
    'lrm': 8206,
    'rlm': 8207,
    'ndash': 8211,
    'mdash': 8212,
    'lsquo': 8216,
    'rsquo': 8217,
    'sbquo': 8218,
    'ldquo': 8220,
    'rdquo': 8221,
    'bdquo': 8222,
    'dagger': 8224,
    'Dagger': 8225,
    'bull': 8226,
    'hellip': 8230,
    'permil': 8240,
    'prime': 8242,
    'Prime': 8243,
    'lsaquo': 8249,
    'rsaquo': 8250,
    'oline': 8254,
    'frasl': 8260,
    'euro': 8364,
    'image': 8465,
    'weierp': 8472,
    'real': 8476,
    'trade': 8482,
    'alefsym': 8501,
    'larr': 8592,
    'uarr': 8593,
    'rarr': 8594,
    'darr': 8595,
    'harr': 8596,
    'crarr': 8629,
    'lArr': 8656,
    'uArr': 8657,
    'rArr': 8658,
    'dArr': 8659,
    'hArr': 8660,
    'forall': 8704,
    'part': 8706,
    'exist': 8707,
    'empty': 8709,
    'nabla': 8711,
    'isin': 8712,
    'notin': 8713,
    'ni': 8715,
    'prod': 8719,
    'sum': 8721,
    'minus': 8722,
    'lowast': 8727,
    'radic': 8730,
    'prop': 8733,
    'infin': 8734,
    'ang': 8736,
    'and': 8743,
    'or': 8744,
    'cap': 8745,
    'cup': 8746,
    'int': 8747,
    'there4': 8756,
    'sim': 8764,
    'cong': 8773,
    'asymp': 8776,
    'ne': 8800,
    'equiv': 8801,
    'le': 8804,
    'ge': 8805,
    'sub': 8834,
    'sup': 8835,
    'nsub': 8836,
    'sube': 8838,
    'supe': 8839,
    'oplus': 8853,
    'otimes': 8855,
    'perp': 8869,
    'sdot': 8901,
    'lceil': 8968,
    'rceil': 8969,
    'lfloor': 8970,
    'rfloor': 8971,
    'lang': 9001,
    'rang': 9002,
    'loz': 9674,
    'spades': 9824,
    'clubs': 9827,
    'hearts': 9829,
    'diams': 9830
  }

  Object.keys(sax.ENTITIES).forEach(function (key) {
    var e = sax.ENTITIES[key]
    var s = typeof e === 'number' ? String.fromCharCode(e) : e
    sax.ENTITIES[key] = s
  })

  for (var s in sax.STATE) {
    sax.STATE[sax.STATE[s]] = s
  }

  // shorthand
  S = sax.STATE

  function emit (parser, event, data) {
    parser[event] && parser[event](data)
  }

  function emitNode (parser, nodeType, data) {
    if (parser.textNode) closeText(parser)
    emit(parser, nodeType, data)
  }

  function closeText (parser) {
    parser.textNode = textopts(parser.opt, parser.textNode)
    if (parser.textNode) emit(parser, 'ontext', parser.textNode)
    parser.textNode = ''
  }

  function textopts (opt, text) {
    if (opt.trim) text = text.trim()
    if (opt.normalize) text = text.replace(/\s+/g, ' ')
    return text
  }

  function error (parser, er) {
    closeText(parser)
    if (parser.trackPosition) {
      er += '\nLine: ' + parser.line +
        '\nColumn: ' + parser.column +
        '\nChar: ' + parser.c
    }
    er = new Error(er)
    parser.error = er
    emit(parser, 'onerror', er)
    return parser
  }

  function end (parser) {
    if (parser.sawRoot && !parser.closedRoot) strictFail(parser, 'Unclosed root tag')
    if ((parser.state !== S.BEGIN) &&
      (parser.state !== S.BEGIN_WHITESPACE) &&
      (parser.state !== S.TEXT)) {
      error(parser, 'Unexpected end')
    }
    closeText(parser)
    parser.c = ''
    parser.closed = true
    emit(parser, 'onend')
    SAXParser.call(parser, parser.strict, parser.opt)
    return parser
  }

  function strictFail (parser, message) {
    if (typeof parser !== 'object' || !(parser instanceof SAXParser)) {
      throw new Error('bad call to strictFail')
    }
    if (parser.strict) {
      error(parser, message)
    }
  }

  function newTag (parser) {
    if (!parser.strict) parser.tagName = parser.tagName[parser.looseCase]()
    var parent = parser.tags[parser.tags.length - 1] || parser
    var tag = parser.tag = { name: parser.tagName, attributes: {} }

    // will be overridden if tag contails an xmlns="foo" or xmlns:foo="bar"
    if (parser.opt.xmlns) {
      tag.ns = parent.ns
    }
    parser.attribList.length = 0
    emitNode(parser, 'onopentagstart', tag)
  }

  function qname (name, attribute) {
    var i = name.indexOf(':')
    var qualName = i < 0 ? [ '', name ] : name.split(':')
    var prefix = qualName[0]
    var local = qualName[1]

    // <x "xmlns"="http://foo">
    if (attribute && name === 'xmlns') {
      prefix = 'xmlns'
      local = ''
    }

    return { prefix: prefix, local: local }
  }

  function attrib (parser) {
    if (!parser.strict) {
      parser.attribName = parser.attribName[parser.looseCase]()
    }

    if (parser.attribList.indexOf(parser.attribName) !== -1 ||
      parser.tag.attributes.hasOwnProperty(parser.attribName)) {
      parser.attribName = parser.attribValue = ''
      return
    }

    if (parser.opt.xmlns) {
      var qn = qname(parser.attribName, true)
      var prefix = qn.prefix
      var local = qn.local

      if (prefix === 'xmlns') {
        // namespace binding attribute. push the binding into scope
        if (local === 'xml' && parser.attribValue !== XML_NAMESPACE) {
          strictFail(parser,
            'xml: prefix must be bound to ' + XML_NAMESPACE + '\n' +
            'Actual: ' + parser.attribValue)
        } else if (local === 'xmlns' && parser.attribValue !== XMLNS_NAMESPACE) {
          strictFail(parser,
            'xmlns: prefix must be bound to ' + XMLNS_NAMESPACE + '\n' +
            'Actual: ' + parser.attribValue)
        } else {
          var tag = parser.tag
          var parent = parser.tags[parser.tags.length - 1] || parser
          if (tag.ns === parent.ns) {
            tag.ns = Object.create(parent.ns)
          }
          tag.ns[local] = parser.attribValue
        }
      }

      // defer onattribute events until all attributes have been seen
      // so any new bindings can take effect. preserve attribute order
      // so deferred events can be emitted in document order
      parser.attribList.push([parser.attribName, parser.attribValue,
                              parser.attribNameLine,
                              parser.attribNameColumn,
                              parser.attribValueLine,
                              parser.attribValueColumn])
    } else {
      // in non-xmlns mode, we can emit the event right away
      parser.tag.attributes[parser.attribName] = parser.attribValue
      emitNode(parser, 'onattribute', {
        name: parser.attribName,
        value: parser.attribValue
      })
    }

    parser.attribName = parser.attribValue = ''
  }

  function openTag (parser, selfClosing) {
    if (parser.opt.xmlns) {
      // emit namespace binding events
      var tag = parser.tag

      // add namespace info to tag
      var qn = qname(parser.tagName)
      tag.prefix = qn.prefix
      tag.local = qn.local
      tag.uri = tag.ns[qn.prefix] || ''

      if (tag.prefix && !tag.uri) {
        strictFail(parser, 'Unbound namespace prefix: ' +
          JSON.stringify(parser.tagName))
        tag.uri = qn.prefix
      }

      var parent = parser.tags[parser.tags.length - 1] || parser
      if (tag.ns && parent.ns !== tag.ns) {
        Object.keys(tag.ns).forEach(function (p) {
          emitNode(parser, 'onopennamespace', {
            prefix: p,
            uri: tag.ns[p]
          })
        })
      }

      // handle deferred onattribute events
      // Note: do not apply default ns to attributes:
      //   http://www.w3.org/TR/REC-xml-names/#defaulting
      for (var i = 0, l = parser.attribList.length; i < l; i++) {
        var nv = parser.attribList[i]
        var name = nv[0]
        var value = nv[1]
        var qualName = qname(name, true)
        var prefix = qualName.prefix
        var local = qualName.local
        var uri = prefix === '' ? '' : (tag.ns[prefix] || '')
        var a = {
          name: name,
          value: value,
          prefix: prefix,
          local: local,
          uri: uri,
          nameLine: nv[2],
          nameColumn: nv[3],
          valueLine: nv[4],
          valueColumn: nv[5],
        }

        // if there's any attributes with an undefined namespace,
        // then fail on them now.
        if (prefix && prefix !== 'xmlns' && !uri) {
          strictFail(parser, 'Unbound namespace prefix: ' +
            JSON.stringify(prefix))
          a.uri = prefix
        }
        parser.tag.attributes[name] = a
        emitNode(parser, 'onattribute', a)
      }
      parser.attribList.length = 0
    }

    parser.tag.isSelfClosing = !!selfClosing

    // process the tag
    parser.sawRoot = true
    parser.tags.push(parser.tag)
    emitNode(parser, 'onopentag', parser.tag)
    if (!selfClosing) {
      // special case for <script> in non-strict mode.
      if (!parser.noscript && parser.tagName.toLowerCase() === 'script') {
        parser.state = S.SCRIPT
      } else {
        parser.state = S.TEXT
      }
      parser.tag = null
      parser.tagName = ''
    }
    parser.attribName = parser.attribValue = ''
    parser.attribList.length = 0
  }

  function closeTag (parser) {
    if (!parser.tagName) {
      strictFail(parser, 'Weird empty close tag.')
      parser.textNode += '</>'
      parser.state = S.TEXT
      return
    }

    // first make sure that the closing tag actually exists.
    // <a><b></c></b></a> will close everything, otherwise.
    var t = parser.tags.length
    var tagName = parser.tagName
    if (!parser.strict) {
      tagName = tagName[parser.looseCase]()
    }
    var closeTo = tagName
    while (t--) {
      var close = parser.tags[t]
      if (close.name !== closeTo) {
        // fail the first time in strict mode
        strictFail(parser, 'Unexpected close tag')
      } else {
        break
      }
    }

    // didn't find it.  we already failed for strict, so just abort.
    if (t < 0) {
      strictFail(parser, 'Unmatched closing tag: ' + parser.tagName)
      parser.textNode += '</' + parser.tagName + '>'
      parser.state = S.TEXT
      return
    }
    parser.tagName = tagName
    var s = parser.tags.length
    while (s-- > t) {
      var tag = parser.tag = parser.tags.pop()
      parser.tagName = parser.tag.name
      emitNode(parser, 'onclosetag', parser.tagName)

      var x = {}
      for (var i in tag.ns) {
        x[i] = tag.ns[i]
      }

      var parent = parser.tags[parser.tags.length - 1] || parser
      if (parser.opt.xmlns && tag.ns !== parent.ns) {
        // remove namespace bindings introduced by tag
        Object.keys(tag.ns).forEach(function (p) {
          var n = tag.ns[p]
          emitNode(parser, 'onclosenamespace', { prefix: p, uri: n })
        })
      }
    }
    if (t === 0) parser.closedRoot = true
    parser.tagName = parser.attribValue = parser.attribName = ''
    parser.attribList.length = 0
    parser.state = S.TEXT
  }

  function parseEntity (parser) {
    var entity = parser.entity
    var entityLC = entity.toLowerCase()
    var num
    var numStr = ''

    if (parser.ENTITIES[entity]) {
      return parser.ENTITIES[entity]
    }
    if (parser.ENTITIES[entityLC]) {
      return parser.ENTITIES[entityLC]
    }
    entity = entityLC
    if (entity.charAt(0) === '#') {
      if (entity.charAt(1) === 'x') {
        entity = entity.slice(2)
        num = parseInt(entity, 16)
        numStr = num.toString(16)
      } else {
        entity = entity.slice(1)
        num = parseInt(entity, 10)
        numStr = num.toString(10)
      }
    }
    entity = entity.replace(/^0+/, '')
    if (isNaN(num) || numStr.toLowerCase() !== entity) {
      strictFail(parser, 'Invalid character entity')
      return '&' + parser.entity + ';'
    }

    if (num < 0 || num > 0x10FFFF) {
      strictFail(parser, 'Invalid character entity')
      return '&' + parser.entity + ';'
    }

    return String.fromCodePoint(num)
  }

  function beginWhiteSpace (parser, c) {
    if (c === '<') {
      parser.state = S.OPEN_WAKA
      parser.startTagPosition = parser.position
    } else if (!isWhitespace(c)) {
      // have to process this as a text node.
      // weird, but happens.
      strictFail(parser, 'Non-whitespace before first tag.')
      parser.textNode = c
      parser.state = S.TEXT
    }
  }

  function charAt (chunk, i) {
    var result = ''
    if (i < chunk.length) {
      result = chunk.charAt(i)
    }
    return result
  }

  function write (chunk) {
    var parser = this
    if (this.error) {
      throw this.error
    }
    if (parser.closed) {
      return error(parser,
        'Cannot write after close. Assign an onready handler.')
    }
    if (chunk === null) {
      return end(parser)
    }
    if (typeof chunk === 'object') {
      chunk = chunk.toString()
    }
    var i = 0
    var c = ''
    while (true) {
      c = charAt(chunk, i++)
      parser.c = c

      if (!c) {
        break
      }

      if (parser.trackPosition) {
        parser.position++
        if (c === '\n') {
          parser.line++
          parser.column = 0
        } else {
          parser.column++
        }
      }

      switch (parser.state) {
        case S.BEGIN:
          parser.state = S.BEGIN_WHITESPACE
          if (c === '\uFEFF') {
            continue
          }
          beginWhiteSpace(parser, c)
          continue

        case S.BEGIN_WHITESPACE:
          beginWhiteSpace(parser, c)
          continue

        case S.TEXT:
          if (parser.sawRoot && !parser.closedRoot) {
            var starti = i - 1
            while (c && c !== '<' && c !== '&') {
              c = charAt(chunk, i++)
              if (c && parser.trackPosition) {
                parser.position++
                if (c === '\n') {
                  parser.line++
                  parser.column = 0
                } else {
                  parser.column++
                }
              }
            }
            parser.textNode += chunk.substring(starti, i - 1)
          }
          if (c === '<' && !(parser.sawRoot && parser.closedRoot && !parser.strict)) {
            parser.state = S.OPEN_WAKA
            parser.startTagPosition = parser.position
          } else {
            if (!isWhitespace(c) && (!parser.sawRoot || parser.closedRoot)) {
              strictFail(parser, 'Text data outside of root node.')
            }
            if (c === '&') {
              parser.state = S.TEXT_ENTITY
            } else {
              parser.textNode += c
            }
          }
          continue

        case S.SCRIPT:
          // only non-strict
          if (c === '<') {
            parser.state = S.SCRIPT_ENDING
            parser.scriptEnding = c
          } else {
            parser.script += c
          }
          continue

        case S.SCRIPT_ENDING: {
          parser.scriptEnding += c

          const current = parser.scriptEnding.toLowerCase()
          if (current === "</script>") {
            emitNode(parser, 'onscript', parser.script)
            parser.script = ''

            parser.tagName = 'script'
            closeTag(parser)
          } else if (current === "<!") {
            parser.script += parser.scriptEnding
            parser.scriptCommentStart = parser.scriptEnding
            parser.scriptEnding = ''
            parser.state = S.SCRIPT_COMMENT_START
          } else if (!"</script>".startsWith(current)) {
            parser.script += parser.scriptEnding
            parser.scriptEnding = ''
            parser.state = S.SCRIPT
          }
          continue
        }

        case S.SCRIPT_COMMENT_START: {
          parser.script += c
          parser.scriptCommentStart += c

          const current = parser.scriptCommentStart
          if (current == "<!--") {
            parser.state = S.SCRIPT_COMMENT
          } else if (!"<!--".startsWith(current)) {
            parser.state = S.SCRIPT
          }
          continue
        }

        case S.SCRIPT_COMMENT: {
          parser.script += c

          if (c == '-') {
            parser.state = S.SCRIPT_COMMENT_END
            parser.scriptCommentEnd = c
          }
          continue
        }

        case S.SCRIPT_COMMENT_END: {
          parser.script += c
          parser.scriptCommentEnd += c

          const current = parser.scriptCommentEnd
          if (current == "-->") {
            parser.state = S.SCRIPT
          } else if (!"-->".startsWith(current)) {
            parser.state = S.SCRIPT_COMMENT
          }
          continue
        }

        case S.OPEN_WAKA:
          // either a /, ?, !, or text is coming next.
          if (c === '!') {
            parser.state = S.SGML_DECL
            parser.sgmlDecl = ''
          } else if (isWhitespace(c)) {
            // wait for it...
          } else if (isMatch(nameStart, c)) {
            parser.state = S.OPEN_TAG
            parser.tagName = c
          } else if (c === '/') {
            parser.state = S.CLOSE_TAG
            parser.tagName = ''
          } else if (c === '?') {
            parser.state = S.PROC_INST
            parser.procInstName = parser.procInstBody = ''
          } else {
            strictFail(parser, 'Unencoded <')
            // if there was some whitespace, then add that in.
            if (parser.startTagPosition + 1 < parser.position) {
              var pad = parser.position - parser.startTagPosition
              c = new Array(pad).join(' ') + c
            }
            parser.textNode += '<' + c
            parser.state = S.TEXT
          }
          continue

        case S.SGML_DECL:
          if ((parser.sgmlDecl + c).toUpperCase() === CDATA) {
            emitNode(parser, 'onopencdata')
            parser.state = S.CDATA
            parser.sgmlDecl = ''
            parser.cdata = ''
          } else if (parser.sgmlDecl + c === '--') {
            parser.state = S.COMMENT
            parser.comment = ''
            parser.sgmlDecl = ''
          } else if ((parser.sgmlDecl + c).toUpperCase() === DOCTYPE) {
            parser.state = S.DOCTYPE
            if (parser.doctype || parser.sawRoot) {
              strictFail(parser,
                'Inappropriately located doctype declaration')
            }
            parser.doctype = ''
            parser.sgmlDecl = ''
          } else if (c === '>') {
            emitNode(parser, 'onsgmldeclaration', parser.sgmlDecl)
            parser.sgmlDecl = ''
            parser.state = S.TEXT
          } else if (isQuote(c)) {
            parser.state = S.SGML_DECL_QUOTED
            parser.sgmlDecl += c
          } else {
            parser.sgmlDecl += c
          }
          continue

        case S.SGML_DECL_QUOTED:
          if (c === parser.q) {
            parser.state = S.SGML_DECL
            parser.q = ''
          }
          parser.sgmlDecl += c
          continue

        case S.DOCTYPE:
          if (c === '>') {
            parser.state = S.TEXT
            emitNode(parser, 'ondoctype', parser.doctype)
            parser.doctype = true // just remember that we saw it.
          } else {
            parser.doctype += c
            if (c === '[') {
              parser.state = S.DOCTYPE_DTD
            } else if (isQuote(c)) {
              parser.state = S.DOCTYPE_QUOTED
              parser.q = c
            }
          }
          continue

        case S.DOCTYPE_QUOTED:
          parser.doctype += c
          if (c === parser.q) {
            parser.q = ''
            parser.state = S.DOCTYPE
          }
          continue

        case S.DOCTYPE_DTD:
          parser.doctype += c
          if (c === ']') {
            parser.state = S.DOCTYPE
          } else if (isQuote(c)) {
            parser.state = S.DOCTYPE_DTD_QUOTED
            parser.q = c
          }
          continue

        case S.DOCTYPE_DTD_QUOTED:
          parser.doctype += c
          if (c === parser.q) {
            parser.state = S.DOCTYPE_DTD
            parser.q = ''
          }
          continue

        case S.COMMENT:
          if (c === '-') {
            parser.state = S.COMMENT_ENDING
          } else {
            parser.comment += c
          }
          continue

        case S.COMMENT_ENDING:
          if (c === '-') {
            parser.state = S.COMMENT_ENDED
            parser.comment = textopts(parser.opt, parser.comment)
            if (parser.comment) {
              emitNode(parser, 'oncomment', parser.comment)
            }
            parser.comment = ''
          } else {
            parser.comment += '-' + c
            parser.state = S.COMMENT
          }
          continue

        case S.COMMENT_ENDED:
          if (c !== '>') {
            strictFail(parser, 'Malformed comment')
            // allow <!-- blah -- bloo --> in non-strict mode,
            // which is a comment of " blah -- bloo "
            parser.comment += '--' + c
            parser.state = S.COMMENT
          } else {
            parser.state = S.TEXT
          }
          continue

        case S.CDATA:
          if (c === ']') {
            parser.state = S.CDATA_ENDING
          } else {
            parser.cdata += c
          }
          continue

        case S.CDATA_ENDING:
          if (c === ']') {
            parser.state = S.CDATA_ENDING_2
          } else {
            parser.cdata += ']' + c
            parser.state = S.CDATA
          }
          continue

        case S.CDATA_ENDING_2:
          if (c === '>') {
            if (parser.cdata) {
              emitNode(parser, 'oncdata', parser.cdata)
            }
            emitNode(parser, 'onclosecdata')
            parser.cdata = ''
            parser.state = S.TEXT
          } else if (c === ']') {
            parser.cdata += ']'
          } else {
            parser.cdata += ']]' + c
            parser.state = S.CDATA
          }
          continue

        case S.PROC_INST:
          if (c === '?') {
            parser.state = S.PROC_INST_ENDING
          } else if (isWhitespace(c)) {
            parser.state = S.PROC_INST_BODY
          } else {
            parser.procInstName += c
          }
          continue

        case S.PROC_INST_BODY:
          if (!parser.procInstBody && isWhitespace(c)) {
            continue
          } else if (c === '?') {
            parser.state = S.PROC_INST_ENDING
          } else {
            parser.procInstBody += c
          }
          continue

        case S.PROC_INST_ENDING:
          if (c === '>') {
            emitNode(parser, 'onprocessinginstruction', {
              name: parser.procInstName,
              body: parser.procInstBody
            })
            parser.procInstName = parser.procInstBody = ''
            parser.state = S.TEXT
          } else {
            parser.procInstBody += '?' + c
            parser.state = S.PROC_INST_BODY
          }
          continue

        case S.OPEN_TAG:
          if (isMatch(nameBody, c)) {
            parser.tagName += c
          } else {
            newTag(parser)
            if (c === '>') {
              openTag(parser)
            } else if (c === '/') {
              parser.state = S.OPEN_TAG_SLASH
            } else {
              if (!isWhitespace(c)) {
                strictFail(parser, 'Invalid character in tag name')
              }
              parser.state = S.ATTRIB
            }
          }
          continue

        case S.OPEN_TAG_SLASH:
          if (c === '>') {
            openTag(parser, true)
            closeTag(parser)
          } else {
            strictFail(parser, 'Forward-slash in opening tag not followed by >')
            parser.state = S.ATTRIB
          }
          continue

        case S.ATTRIB:
          // haven't read the attribute name yet.
          if (isWhitespace(c)) {
            continue
          } else if (c === '>') {
            openTag(parser)
          } else if (c === '/') {
            parser.state = S.OPEN_TAG_SLASH
          } else if (isMatch(nameStart, c)) {
            parser.attribName = c
            parser.attribNameLine = parser.line
            parser.attribNameColumn = parser.column - 1
            parser.attribValue = ''
            parser.state = S.ATTRIB_NAME
          } else {
            strictFail(parser, 'Invalid attribute name')
          }
          continue

        case S.ATTRIB_NAME:
          if (c === '=') {
            parser.state = S.ATTRIB_VALUE
          } else if (c === '>') {
            strictFail(parser, 'Attribute without value')
            parser.attribValue = parser.attribName
            attrib(parser)
            openTag(parser)
          } else if (isWhitespace(c)) {
            parser.state = S.ATTRIB_NAME_SAW_WHITE
          } else if (isMatch(nameBody, c)) {
            parser.attribName += c
          } else {
            strictFail(parser, 'Invalid attribute name')
          }
          continue

        case S.ATTRIB_NAME_SAW_WHITE:
          if (c === '=') {
            parser.state = S.ATTRIB_VALUE
          } else if (isWhitespace(c)) {
            continue
          } else {
            strictFail(parser, 'Attribute without value')
            parser.tag.attributes[parser.attribName] = ''
            parser.attribValue = ''
            parser.attribNameLine = parser.line
            parser.attribNameColumn = parser.column - 1
            emitNode(parser, 'onattribute', {
              name: parser.attribName,
              value: ''
            })
            parser.attribName = ''
            if (c === '>') {
              openTag(parser)
            } else if (isMatch(nameStart, c)) {
              parser.attribName = c
              parser.attribNameLine = parser.line
              parser.attribNameColumn = parser.column - 1
              parser.state = S.ATTRIB_NAME
            } else {
              strictFail(parser, 'Invalid attribute name')
              parser.state = S.ATTRIB
            }
          }
          continue

        case S.ATTRIB_VALUE:
          if (isWhitespace(c)) {
            continue
          } else if (isQuote(c)) {
            parser.q = c
            parser.state = S.ATTRIB_VALUE_QUOTED
            parser.attribValueLine = parser.line
            parser.attribValueColumn = parser.column
          } else {
            strictFail(parser, 'Unquoted attribute value')
            parser.state = S.ATTRIB_VALUE_UNQUOTED
            parser.attribValue = c
            parser.attribValueLine = parser.line
            parser.attribValueColumn = parser.column - 1
          }
          continue

        case S.ATTRIB_VALUE_QUOTED:
          if (c !== parser.q) {
            if (c === '&') {
              parser.state = S.ATTRIB_VALUE_ENTITY_Q
            } else {
              parser.attribValue += c
            }
            continue
          }
          attrib(parser)
          parser.q = ''
          parser.state = S.ATTRIB_VALUE_CLOSED
          continue

        case S.ATTRIB_VALUE_CLOSED:
          if (isWhitespace(c)) {
            parser.state = S.ATTRIB
          } else if (c === '>') {
            openTag(parser)
          } else if (c === '/') {
            parser.state = S.OPEN_TAG_SLASH
          } else if (isMatch(nameStart, c)) {
            strictFail(parser, 'No whitespace between attributes')
            parser.attribName = c
            parser.attribNameLine = parser.line
            parser.attribNameColumn = parser.column - 1
            parser.attribValue = ''
            parser.state = S.ATTRIB_NAME
          } else {
            strictFail(parser, 'Invalid attribute name')
          }
          continue

        case S.ATTRIB_VALUE_UNQUOTED:
          if (!isAttribEnd(c)) {
            if (c === '&') {
              parser.state = S.ATTRIB_VALUE_ENTITY_U
            } else {
              parser.attribValue += c
            }
            continue
          }
          attrib(parser)
          if (c === '>') {
            openTag(parser)
          } else {
            parser.state = S.ATTRIB
          }
          continue

        case S.CLOSE_TAG:
          if (!parser.tagName) {
            if (isWhitespace(c)) {
              continue
            } else if (notMatch(nameStart, c)) {
              strictFail(parser, 'Invalid tagname in closing tag.')
            } else {
              parser.tagName = c
            }
          } else if (c === '>') {
            closeTag(parser)
          } else if (isMatch(nameBody, c)) {
            parser.tagName += c
          } else {
            if (!isWhitespace(c)) {
              strictFail(parser, 'Invalid tagname in closing tag')
            }
            parser.state = S.CLOSE_TAG_SAW_WHITE
          }
          continue

        case S.CLOSE_TAG_SAW_WHITE:
          if (isWhitespace(c)) {
            continue
          }
          if (c === '>') {
            closeTag(parser)
          } else {
            strictFail(parser, 'Invalid characters in closing tag')
          }
          continue

        case S.TEXT_ENTITY:
        case S.ATTRIB_VALUE_ENTITY_Q:
        case S.ATTRIB_VALUE_ENTITY_U:
          var returnState
          var buffer
          switch (parser.state) {
            case S.TEXT_ENTITY:
              returnState = S.TEXT
              buffer = 'textNode'
              break

            case S.ATTRIB_VALUE_ENTITY_Q:
              returnState = S.ATTRIB_VALUE_QUOTED
              buffer = 'attribValue'
              break

            case S.ATTRIB_VALUE_ENTITY_U:
              returnState = S.ATTRIB_VALUE_UNQUOTED
              buffer = 'attribValue'
              break
          }

          if (c === ';') {
            parser[buffer] += parseEntity(parser)
            parser.entity = ''
            parser.state = returnState
          } else if (isMatch(parser.entity.length ? entityBody : entityStart, c)) {
            parser.entity += c
          } else {
            strictFail(parser, 'Invalid character in entity name')
            parser[buffer] += '&' + parser.entity + c
            parser.entity = ''
            parser.state = returnState
          }

          continue

        default: /* istanbul ignore next */ {
          throw new Error(parser, 'Unknown state: ' + parser.state)
        }
      }
    } // while

    if (parser.position >= parser.bufferCheckPosition) {
      checkBufferLength(parser)
    }
    return parser
  }

  /*! http://mths.be/fromcodepoint v0.1.0 by @mathias */
  /* istanbul ignore next */
  if (!String.fromCodePoint) {
    (function () {
      var stringFromCharCode = String.fromCharCode
      var floor = Math.floor
      var fromCodePoint = function () {
        var MAX_SIZE = 0x4000
        var codeUnits = []
        var highSurrogate
        var lowSurrogate
        var index = -1
        var length = arguments.length
        if (!length) {
          return ''
        }
        var result = ''
        while (++index < length) {
          var codePoint = Number(arguments[index])
          if (
            !isFinite(codePoint) || // `NaN`, `+Infinity`, or `-Infinity`
            codePoint < 0 || // not a valid Unicode code point
            codePoint > 0x10FFFF || // not a valid Unicode code point
            floor(codePoint) !== codePoint // not an integer
          ) {
            throw RangeError('Invalid code point: ' + codePoint)
          }
          if (codePoint <= 0xFFFF) { // BMP code point
            codeUnits.push(codePoint)
          } else { // Astral code point; split in surrogate halves
            // http://mathiasbynens.be/notes/javascript-encoding#surrogate-formulae
            codePoint -= 0x10000
            highSurrogate = (codePoint >> 10) + 0xD800
            lowSurrogate = (codePoint % 0x400) + 0xDC00
            codeUnits.push(highSurrogate, lowSurrogate)
          }
          if (index + 1 === length || codeUnits.length > MAX_SIZE) {
            result += stringFromCharCode.apply(null, codeUnits)
            codeUnits.length = 0
          }
        }
        return result
      }
      /* istanbul ignore next */
      if (Object.defineProperty) {
        Object.defineProperty(String, 'fromCodePoint', {
          value: fromCodePoint,
          configurable: true,
          writable: true
        })
      } else {
        String.fromCodePoint = fromCodePoint
      }
    }())
  }
})(typeof exports === 'undefined' ? this.sax = {} : exports)

```

## sax/sax.patch
```
--- ../../sax-js/lib/sax.js	2023-08-17 15:40:28.000000000 -0400
+++ sax.js	2024-05-17 02:02:31
@@ -335,17 +335,20 @@
     ATTRIB_VALUE_QUOTED: S++, // <a foo="bar
     ATTRIB_VALUE_CLOSED: S++, // <a foo="bar"
     ATTRIB_VALUE_UNQUOTED: S++, // <a foo=bar
     ATTRIB_VALUE_ENTITY_Q: S++, // <foo bar="&quot;"
     ATTRIB_VALUE_ENTITY_U: S++, // <foo bar=&quot
     CLOSE_TAG: S++, // </a
     CLOSE_TAG_SAW_WHITE: S++, // </a   >
     SCRIPT: S++, // <script> ...
-    SCRIPT_ENDING: S++ // <script> ... <
+    SCRIPT_ENDING: S++, // <script> ... <
+    SCRIPT_COMMENT_START: S++, // <script> ... <! ...
+    SCRIPT_COMMENT: S++, // <script> ... <!-- ...
+    SCRIPT_COMMENT_END: S++, // <script> ... <!-- ... - ...
   }
 
   sax.XML_ENTITIES = {
     'amp': '&',
     'gt': '>',
     'lt': '<',
     'quot': '"',
     'apos': "'"
@@ -740,17 +743,21 @@
           }
           tag.ns[local] = parser.attribValue
         }
       }
 
       // defer onattribute events until all attributes have been seen
       // so any new bindings can take effect. preserve attribute order
       // so deferred events can be emitted in document order
-      parser.attribList.push([parser.attribName, parser.attribValue])
+      parser.attribList.push([parser.attribName, parser.attribValue,
+                              parser.attribNameLine,
+                              parser.attribNameColumn,
+                              parser.attribValueLine,
+                              parser.attribValueColumn])
     } else {
       // in non-xmlns mode, we can emit the event right away
       parser.tag.attributes[parser.attribName] = parser.attribValue
       emitNode(parser, 'onattribute', {
         name: parser.attribName,
         value: parser.attribValue
       })
     }
@@ -796,17 +803,21 @@
         var prefix = qualName.prefix
         var local = qualName.local
         var uri = prefix === '' ? '' : (tag.ns[prefix] || '')
         var a = {
           name: name,
           value: value,
           prefix: prefix,
           local: local,
-          uri: uri
+          uri: uri,
+          nameLine: nv[2],
+          nameColumn: nv[3],
+          valueLine: nv[4],
+          valueColumn: nv[5],
         }
 
         // if there's any attributes with an undefined namespace,
         // then fail on them now.
         if (prefix && prefix !== 'xmlns' && !uri) {
           strictFail(parser, 'Unbound namespace prefix: ' +
             JSON.stringify(prefix))
           a.uri = prefix
@@ -840,27 +851,16 @@
   function closeTag (parser) {
     if (!parser.tagName) {
       strictFail(parser, 'Weird empty close tag.')
       parser.textNode += '</>'
       parser.state = S.TEXT
       return
     }
 
-    if (parser.script) {
-      if (parser.tagName !== 'script') {
-        parser.script += '</' + parser.tagName + '>'
-        parser.tagName = ''
-        parser.state = S.SCRIPT
-        return
-      }
-      emitNode(parser, 'onscript', parser.script)
-      parser.script = ''
-    }
-
     // first make sure that the closing tag actually exists.
     // <a><b></c></b></a> will close everything, otherwise.
     var t = parser.tags.length
     var tagName = parser.tagName
     if (!parser.strict) {
       tagName = tagName[parser.looseCase]()
     }
     var closeTo = tagName
@@ -929,16 +929,21 @@
       } else {
         entity = entity.slice(1)
         num = parseInt(entity, 10)
         numStr = num.toString(10)
       }
     }
     entity = entity.replace(/^0+/, '')
     if (isNaN(num) || numStr.toLowerCase() !== entity) {
+      strictFail(parser, 'Invalid character entity')
+      return '&' + parser.entity + ';'
+    }
+
+    if (num < 0 || num > 0x10FFFF) {
       strictFail(parser, 'Invalid character entity')
       return '&' + parser.entity + ';'
     }
 
     return String.fromCodePoint(num)
   }
 
   function beginWhiteSpace (parser, c) {
@@ -1041,30 +1046,81 @@
             }
           }
           continue
 
         case S.SCRIPT:
           // only non-strict
           if (c === '<') {
             parser.state = S.SCRIPT_ENDING
+            parser.scriptEnding = c
           } else {
             parser.script += c
           }
           continue
 
-        case S.SCRIPT_ENDING:
-          if (c === '/') {
-            parser.state = S.CLOSE_TAG
-          } else {
-            parser.script += '<' + c
+        case S.SCRIPT_ENDING: {
+          parser.scriptEnding += c
+
+          const current = parser.scriptEnding.toLowerCase()
+          if (current === "</script>") {
+            emitNode(parser, 'onscript', parser.script)
+            parser.script = ''
+
+            parser.tagName = 'script'
+            closeTag(parser)
+          } else if (current === "<!") {
+            parser.script += parser.scriptEnding
+            parser.scriptCommentStart = parser.scriptEnding
+            parser.scriptEnding = ''
+            parser.state = S.SCRIPT_COMMENT_START
+          } else if (!"</script>".startsWith(current)) {
+            parser.script += parser.scriptEnding
+            parser.scriptEnding = ''
             parser.state = S.SCRIPT
           }
           continue
+        }
 
+        case S.SCRIPT_COMMENT_START: {
+          parser.script += c
+          parser.scriptCommentStart += c
+
+          const current = parser.scriptCommentStart
+          if (current == "<!--") {
+            parser.state = S.SCRIPT_COMMENT
+          } else if (!"<!--".startsWith(current)) {
+            parser.state = S.SCRIPT
+          }
+          continue
+        }
+
+        case S.SCRIPT_COMMENT: {
+          parser.script += c
+
+          if (c == '-') {
+            parser.state = S.SCRIPT_COMMENT_END
+            parser.scriptCommentEnd = c
+          }
+          continue
+        }
+
+        case S.SCRIPT_COMMENT_END: {
+          parser.script += c
+          parser.scriptCommentEnd += c
+
+          const current = parser.scriptCommentEnd
+          if (current == "-->") {
+            parser.state = S.SCRIPT
+          } else if (!"-->".startsWith(current)) {
+            parser.state = S.SCRIPT_COMMENT
+          }
+          continue
+        }
+
         case S.OPEN_WAKA:
           // either a /, ?, !, or text is coming next.
           if (c === '!') {
             parser.state = S.SGML_DECL
             parser.sgmlDecl = ''
           } else if (isWhitespace(c)) {
             // wait for it...
           } else if (isMatch(nameStart, c)) {
@@ -1302,16 +1358,18 @@
           if (isWhitespace(c)) {
             continue
           } else if (c === '>') {
             openTag(parser)
           } else if (c === '/') {
             parser.state = S.OPEN_TAG_SLASH
           } else if (isMatch(nameStart, c)) {
             parser.attribName = c
+            parser.attribNameLine = parser.line
+            parser.attribNameColumn = parser.column - 1
             parser.attribValue = ''
             parser.state = S.ATTRIB_NAME
           } else {
             strictFail(parser, 'Invalid attribute name')
           }
           continue
 
         case S.ATTRIB_NAME:
@@ -1335,43 +1393,51 @@
           if (c === '=') {
             parser.state = S.ATTRIB_VALUE
           } else if (isWhitespace(c)) {
             continue
           } else {
             strictFail(parser, 'Attribute without value')
             parser.tag.attributes[parser.attribName] = ''
             parser.attribValue = ''
+            parser.attribNameLine = parser.line
+            parser.attribNameColumn = parser.column - 1
             emitNode(parser, 'onattribute', {
               name: parser.attribName,
               value: ''
             })
             parser.attribName = ''
             if (c === '>') {
               openTag(parser)
             } else if (isMatch(nameStart, c)) {
               parser.attribName = c
+              parser.attribNameLine = parser.line
+              parser.attribNameColumn = parser.column - 1
               parser.state = S.ATTRIB_NAME
             } else {
               strictFail(parser, 'Invalid attribute name')
               parser.state = S.ATTRIB
             }
           }
           continue
 
         case S.ATTRIB_VALUE:
           if (isWhitespace(c)) {
             continue
           } else if (isQuote(c)) {
             parser.q = c
             parser.state = S.ATTRIB_VALUE_QUOTED
+            parser.attribValueLine = parser.line
+            parser.attribValueColumn = parser.column
           } else {
             strictFail(parser, 'Unquoted attribute value')
             parser.state = S.ATTRIB_VALUE_UNQUOTED
             parser.attribValue = c
+            parser.attribValueLine = parser.line
+            parser.attribValueColumn = parser.column - 1
           }
           continue
 
         case S.ATTRIB_VALUE_QUOTED:
           if (c !== parser.q) {
             if (c === '&') {
               parser.state = S.ATTRIB_VALUE_ENTITY_Q
             } else {
@@ -1389,16 +1455,18 @@
             parser.state = S.ATTRIB
           } else if (c === '>') {
             openTag(parser)
           } else if (c === '/') {
             parser.state = S.OPEN_TAG_SLASH
           } else if (isMatch(nameStart, c)) {
             strictFail(parser, 'No whitespace between attributes')
             parser.attribName = c
+            parser.attribNameLine = parser.line
+            parser.attribNameColumn = parser.column - 1
             parser.attribValue = ''
             parser.state = S.ATTRIB_NAME
           } else {
             strictFail(parser, 'Invalid attribute name')
           }
           continue
 
         case S.ATTRIB_VALUE_UNQUOTED:
@@ -1418,33 +1486,24 @@
           }
           continue
 
         case S.CLOSE_TAG:
           if (!parser.tagName) {
             if (isWhitespace(c)) {
               continue
             } else if (notMatch(nameStart, c)) {
-              if (parser.script) {
-                parser.script += '</' + c
-                parser.state = S.SCRIPT
-              } else {
-                strictFail(parser, 'Invalid tagname in closing tag.')
-              }
+              strictFail(parser, 'Invalid tagname in closing tag.')
             } else {
               parser.tagName = c
             }
           } else if (c === '>') {
             closeTag(parser)
           } else if (isMatch(nameBody, c)) {
             parser.tagName += c
-          } else if (parser.script) {
-            parser.script += '</' + parser.tagName
-            parser.tagName = ''
-            parser.state = S.SCRIPT
           } else {
             if (!isWhitespace(c)) {
               strictFail(parser, 'Invalid tagname in closing tag')
             }
             parser.state = S.CLOSE_TAG_SAW_WHITE
           }
           continue
 

```

## .pre-commit-config.yaml
```
exclude: (?:^flake.lock$|/__GENERATED__/|^tests/tests/mc-(?:analysis|generated)|\.snap$)
repos:
  # Common
  - repo: https://github.com/pre-commit/pre-commit-hooks
    rev: v5.0.0
    hooks:
      - id: trailing-whitespace
        exclude: (?:\.patch$)
      - id: end-of-file-fixer
      - id: check-added-large-files
      - id: check-case-conflict
      - id: check-xml
      - id: check-yaml
      - id: check-json
        exclude: (?:^tests/tests/files/test_talosconfig_browser_config.json$)
      - id: check-symlinks
      - id: destroyed-symlinks
      - id: check-executables-have-shebangs

  # C++
  - repo: https://github.com/pre-commit/mirrors-clang-format
    rev: 'v19.1.4'
    hooks:
      - id: clang-format
        exclude: (?:\.js$|\.json$|\.java$|\.mjs$)

  # Rust
  - repo: https://github.com/kykosic/pre-commit-rust
    rev: '0.4.0'
    hooks:
      - id: cargo-fmt
      - id: cargo-clippy

```

## clang-plugin/BindingOperations.h
```
/* -*- Mode: C++; tab-width: 2; indent-tabs-mode: nil; c-basic-offset: 2 -*- */
/* This Source Code Form is subject to the terms of the Mozilla Public
 * License, v. 2.0. If a copy of the MPL was not distributed with this
 * file, You can obtain one at http://mozilla.org/MPL/2.0/. */

#include <clang/AST/ASTContext.h>
#include <clang/AST/Decl.h>
#include <clang/AST/DeclCXX.h>
#include <llvm/Support/JSON.h>

void findBindingToJavaClass(clang::ASTContext &C, clang::CXXRecordDecl &klass);
void findBoundAsJavaClasses(clang::ASTContext &C, clang::CXXRecordDecl &klass);
void findBindingToJavaFunction(clang::ASTContext &C,
                               clang::FunctionDecl &function);
void findBindingToJavaMember(clang::ASTContext &C,
                             clang::CXXMethodDecl &method);
void findBindingToJavaConstant(clang::ASTContext &C, clang::VarDecl &field);

void emitBindingAttributes(llvm::json::OStream &json, const clang::Decl &decl);

```

## clang-plugin/StringOperations.cpp
```
/* -*- Mode: C++; tab-width: 2; indent-tabs-mode: nil; c-basic-offset: 2 -*- */
/* This Source Code Form is subject to the terms of the Mozilla Public
 * License, v. 2.0. If a copy of the MPL was not distributed with this
 * file, You can obtain one at http://mozilla.org/MPL/2.0/. */

#include "StringOperations.h"

static unsigned long djbHash(const char *Str) {
  unsigned long Hash = 5381;

  for (const char *P = Str; *P; P++) {
    // Hash * 33 + c
    Hash = ((Hash << 5) + Hash) + *P;
  }

  return Hash;
}

// This doesn't actually return a hex string of |hash|, but it
// does... something. It doesn't really matter what.
static void hashToString(unsigned long Hash, char *Buffer) {
  const char Table[] = {"0123456789abcdef"};
  char *P = Buffer;
  while (Hash) {
    *P = Table[Hash & 0xf];
    Hash >>= 4;
    P++;
  }

  *P = 0;
}

std::string hash(const std::string &Str) {
  static char HashStr[41];
  unsigned long H = djbHash(Str.c_str());
  hashToString(H, HashStr);
  return std::string(HashStr);
}

std::string toString(int N) { return stringFormat("%d", N); }

```

## clang-plugin/MozsearchIndexer.cpp
```
/* -*- Mode: C++; tab-width: 2; indent-tabs-mode: nil; c-basic-offset: 2 -*- */
/* This Source Code Form is subject to the terms of the Mozilla Public
 * License, v. 2.0. If a copy of the MPL was not distributed with this
 * file, You can obtain one at http://mozilla.org/MPL/2.0/. */

#include "clang/AST/AST.h"
#include "clang/AST/ASTConsumer.h"
#include "clang/AST/ASTContext.h"
#include "clang/AST/Expr.h"
#include "clang/AST/ExprCXX.h"
#include "clang/AST/Mangle.h"
#include "clang/AST/RecordLayout.h"
#include "clang/AST/RecursiveASTVisitor.h"
#include "clang/Basic/FileManager.h"
#include "clang/Basic/SourceManager.h"
#include "clang/Basic/Version.h"
#include "clang/Format/Format.h"
#include "clang/Frontend/CompilerInstance.h"
#include "clang/Frontend/FrontendPluginRegistry.h"
#include "clang/Lex/Lexer.h"
#include "clang/Lex/PPCallbacks.h"
#include "clang/Lex/Preprocessor.h"
#include "clang/Lex/TokenConcatenation.h"
#include "llvm/ADT/SmallString.h"
#include "llvm/Support/JSON.h"
#include "llvm/Support/raw_ostream.h"

#include <algorithm>
#include <fstream>
#include <iostream>
#include <map>
#include <memory>
#include <sstream>
#include <stack>
#include <string>
#include <tuple>
#include <unordered_set>

#include <stdio.h>
#include <stdlib.h>

#include "BindingOperations.h"
#include "FileOperations.h"
#include "StringOperations.h"
#include "from-clangd/HeuristicResolver.h"

#if CLANG_VERSION_MAJOR < 8
// Starting with Clang 8.0 some basic functions have been renamed
#define getBeginLoc getLocStart
#define getEndLoc getLocEnd
#endif
// We want std::make_unique, but that's only available in c++14.  In versions
// prior to that, we need to fall back to llvm's make_unique.  It's also the
// case that we expect clang 10 to build with c++14 and clang 9 and earlier to
// build with c++11, at least as suggested by the llvm-config --cxxflags on
// non-windows platforms.  mozilla-central seems to build with -std=c++17 on
// windows so we need to make this decision based on __cplusplus instead of
// the CLANG_VERSION_MAJOR.
#if __cplusplus < 201402L
using llvm::make_unique;
#else
using std::make_unique;
#endif

using namespace clang;

const std::string GENERATED("__GENERATED__" PATHSEP_STRING);

// Absolute path to directory containing source code.
std::string Srcdir;

// Absolute path to objdir (including generated code).
std::string Objdir;

// Absolute path where analysis JSON output will be stored.
std::string Outdir;

enum class FileType {
  // The file was either in the source tree nor objdir. It might be a system
  // include, for example.
  Unknown,
  // A file from the source tree.
  Source,
  // A file from the objdir.
  Generated,
};

// Takes an absolute path to a file, and returns the type of file it is. If
// it's a Source or Generated file, the provided inout path argument is modified
// in-place so that it is relative to the source dir or objdir, respectively.
FileType relativizePath(std::string &path) {
  if (path.compare(0, Objdir.length(), Objdir) == 0) {
    path.replace(0, Objdir.length(), GENERATED);
    return FileType::Generated;
  }
  // Empty filenames can get turned into Srcdir when they are resolved as
  // absolute paths, so we should exclude files that are exactly equal to
  // Srcdir or anything outside Srcdir.
  if (path.length() > Srcdir.length() &&
      path.compare(0, Srcdir.length(), Srcdir) == 0) {
    // Remove the trailing `/' as well.
    path.erase(0, Srcdir.length() + 1);
    return FileType::Source;
  }
  return FileType::Unknown;
}

#if !defined(_WIN32) && !defined(_WIN64)
#include <sys/time.h>

static double time() {
  struct timeval Tv;
  gettimeofday(&Tv, nullptr);
  return double(Tv.tv_sec) + double(Tv.tv_usec) / 1000000.;
}
#endif

// Return true if |input| is a valid C++ identifier. We don't want to generate
// analysis information for operators, string literals, etc. by accident since
// it trips up consumers of the data.
static bool isValidIdentifier(std::string Input) {
  for (char C : Input) {
    if (!(isalpha(C) || isdigit(C) || C == '_')) {
      return false;
    }
  }
  return true;
}

template <size_t N>
static bool stringStartsWith(const std::string &Input,
                             const char (&Prefix)[N]) {
  return Input.length() > N - 1 && memcmp(Input.c_str(), Prefix, N - 1) == 0;
}

static bool isASCII(const std::string &Input) {
  for (char C : Input) {
    if (C & 0x80) {
      return false;
    }
  }
  return true;
}

struct RAIITracer {
  RAIITracer(const char *log) : mLog(log) { printf("<%s>\n", mLog); }

  ~RAIITracer() { printf("</%s>\n", mLog); }

  const char *mLog;
};

#define TRACEFUNC RAIITracer tracer(__FUNCTION__);

// Sets variable to value on creation then resets variable to its original
// value on destruction
template <typename T> class ValueRollback {
public:
  template <typename U = T>
  ValueRollback(T &variable, U &&value)
      : mVariable{&variable},
        mSavedValue{std::exchange(variable, std::forward<U>(value))} {}

  ValueRollback(ValueRollback &&other) noexcept
      : mVariable{std::exchange(other.mVariable, nullptr)},
        mSavedValue{std::move(other.mSavedValue)} {}

  ValueRollback(const ValueRollback &) = delete;
  ValueRollback &operator=(ValueRollback &&) = delete;
  ValueRollback &operator=(const ValueRollback &) = delete;

  ~ValueRollback() {
    if (mVariable)
      *mVariable = std::move(mSavedValue);
  }

private:
  T *mVariable;
  T mSavedValue;
};

class IndexConsumer;

bool isPure(FunctionDecl *D) {
#if CLANG_VERSION_MAJOR >= 18
  return D->isPureVirtual();
#else
  return D->isPure();
#endif
}

// For each C++ file seen by the analysis (.cpp or .h), we track a
// FileInfo. This object tracks whether the file is "interesting" (i.e., whether
// it's in the source dir or the objdir). We also store the analysis output
// here.
struct FileInfo {
  FileInfo(std::string &Rname) : Realname(Rname) {
    switch (relativizePath(Realname)) {
    case FileType::Generated:
      Interesting = true;
      Generated = true;
      break;
    case FileType::Source:
      Interesting = true;
      Generated = false;
      break;
    case FileType::Unknown:
      Interesting = false;
      Generated = false;
      break;
    }
  }
  std::string Realname;
  std::vector<std::string> Output;
  bool Interesting;
  bool Generated;
};

struct MacroExpansionState {
  Token MacroNameToken;
  const MacroInfo *MacroInfo = nullptr;
  // other macro symbols this expansion depends on
  std::vector<std::string> Dependencies;
  std::string Expansion;
  std::map<SourceLocation, unsigned> TokenLocations;
  SourceRange Range;
  Token PrevPrevTok;
  Token PrevTok;
};

struct ExpandedMacro {
  std::string Symbol;
  std::string Key; // "{Symbol}(,{Dependencies})..."
  std::string Expansion;
  std::map<SourceLocation, unsigned> TokenLocations;
};

class IndexConsumer;

class PreprocessorHook : public PPCallbacks {
  IndexConsumer *Indexer;

public:
  PreprocessorHook(IndexConsumer *C) : Indexer(C) {}

  virtual void FileChanged(SourceLocation Loc, FileChangeReason Reason,
                           SrcMgr::CharacteristicKind FileType,
                           FileID PrevFID) override;

  virtual void InclusionDirective(SourceLocation HashLoc,
                                  const Token &IncludeTok, StringRef FileName,
                                  bool IsAngled, CharSourceRange FileNameRange,
#if CLANG_VERSION_MAJOR >= 16
                                  OptionalFileEntryRef File,
#elif CLANG_VERSION_MAJOR >= 15
                                  Optional<FileEntryRef> File,
#else
                                  const FileEntry *File,
#endif
                                  StringRef SearchPath, StringRef RelativePath,
#if CLANG_VERSION_MAJOR >= 19
                                  const Module *SuggestedModule,
                                  bool ModuleImported,
#else
                                  const Module *Imported,
#endif
                                  SrcMgr::CharacteristicKind FileType) override;

  virtual void MacroDefined(const Token &Tok,
                            const MacroDirective *Md) override;

  virtual void MacroExpands(const Token &Tok, const MacroDefinition &Md,
                            SourceRange Range, const MacroArgs *Ma) override;
  virtual void MacroUndefined(const Token &Tok, const MacroDefinition &Md,
                              const MacroDirective *Undef) override;
  virtual void Defined(const Token &Tok, const MacroDefinition &Md,
                       SourceRange Range) override;
  virtual void Ifdef(SourceLocation Loc, const Token &Tok,
                     const MacroDefinition &Md) override;
  virtual void Ifndef(SourceLocation Loc, const Token &Tok,
                      const MacroDefinition &Md) override;
};

class IndexConsumer : public ASTConsumer,
                      public RecursiveASTVisitor<IndexConsumer>,
                      public DiagnosticConsumer {
private:
  CompilerInstance &CI;
  SourceManager &SM;
  LangOptions &LO;
  std::map<FileID, std::unique_ptr<FileInfo>> FileMap;
  MangleContext *CurMangleContext;
  ASTContext *AstContext;
  std::unique_ptr<clangd::HeuristicResolver> Resolver;

  // Used during a macro expansion to build the expanded string
  TokenConcatenation ConcatInfo;
  std::optional<MacroExpansionState> MacroExpansionState;
  // Keeps track of the positions of tokens inside each expanded macro
  std::map<SourceLocation, ExpandedMacro> MacroMaps;

  typedef RecursiveASTVisitor<IndexConsumer> Super;

  // Tracks the set of declarations that the current expression/statement is
  // nested inside of.
  struct AutoSetContext {
    AutoSetContext(IndexConsumer *Self, NamedDecl *Context,
                   bool VisitImplicit = false)
        : Self(Self), Prev(Self->CurDeclContext), Decl(Context) {
      this->VisitImplicit =
          VisitImplicit || (Prev ? Prev->VisitImplicit : false);
      Self->CurDeclContext = this;
    }

    ~AutoSetContext() { Self->CurDeclContext = Prev; }

    IndexConsumer *Self;
    AutoSetContext *Prev;
    NamedDecl *Decl;
    bool VisitImplicit;
  };
  AutoSetContext *CurDeclContext;

  FileInfo *getFileInfo(SourceLocation Loc) {
    FileID Id = SM.getFileID(Loc);

    std::map<FileID, std::unique_ptr<FileInfo>>::iterator It;
    It = FileMap.find(Id);
    if (It == FileMap.end()) {
      // We haven't seen this file before. We need to make the FileInfo
      // structure information ourselves
      std::string Filename = std::string(SM.getFilename(Loc));
      std::string Absolute;
      // If Loc is a macro id rather than a file id, it Filename might be
      // empty. Also for some types of file locations that are clang-internal
      // like "<scratch>" it can return an empty Filename. In these cases we
      // want to leave Absolute as empty.
      if (!Filename.empty()) {
        Absolute = getAbsolutePath(Filename);
        if (Absolute.empty()) {
          Absolute = Filename;
        }
      }
      std::unique_ptr<FileInfo> Info = make_unique<FileInfo>(Absolute);
      It = FileMap.insert(std::make_pair(Id, std::move(Info))).first;
    }
    return It->second.get();
  }

  // Helpers for processing declarations
  // Should we ignore this location?
  bool isInterestingLocation(SourceLocation Loc) {
    if (SM.isMacroBodyExpansion(Loc)) {
      Loc = SM.getFileLoc(Loc);
    }

    normalizeLocation(&Loc);
    if (Loc.isInvalid()) {
      return false;
    }

    return getFileInfo(Loc)->Interesting;
  }

  // Convert location to "line:column" or "line:column-column" given length.
  // In resulting string rep, line is 1-based and zero-padded to 5 digits, while
  // column is 0-based and unpadded.
  std::string locationToString(SourceLocation Loc, size_t Length = 0) {
    std::pair<FileID, unsigned> Pair = SM.getDecomposedLoc(Loc);

    bool IsInvalid;
    unsigned Line = SM.getLineNumber(Pair.first, Pair.second, &IsInvalid);
    if (IsInvalid) {
      return "";
    }
    unsigned Column = SM.getColumnNumber(Pair.first, Pair.second, &IsInvalid);
    if (IsInvalid) {
      return "";
    }

    if (Length) {
      return stringFormat("%05d:%d-%d", Line, Column - 1, Column - 1 + Length);
    } else {
      return stringFormat("%05d:%d", Line, Column - 1);
    }
  }

  // Convert SourceRange to "line-line" or "line".
  // In the resulting string rep, line is 1-based.
  std::string lineRangeToString(SourceRange Range, bool omitEnd = false) {
    std::pair<FileID, unsigned> Begin = SM.getDecomposedLoc(Range.getBegin());
    std::pair<FileID, unsigned> End = SM.getDecomposedLoc(Range.getEnd());

    bool IsInvalid;
    unsigned Line1 = SM.getLineNumber(Begin.first, Begin.second, &IsInvalid);
    if (IsInvalid) {
      return "";
    }
    unsigned Line2 = SM.getLineNumber(End.first, End.second, &IsInvalid);
    if (IsInvalid) {
      return "";
    }

    if (omitEnd && Line1 == Line2) {
      return stringFormat("%d", Line1);
    }

    return stringFormat("%d-%d", Line1, Line2);
  }

  // Convert SourceRange to "PATH#line-line" or "PATH#line".
  // If Range's file is same as fromFileID, PATH is omitted.
  std::string pathAndLineRangeToString(FileID fromFileID, SourceRange Range) {
    FileInfo *toFile = getFileInfo(Range.getBegin());
    FileInfo *fromFile = FileMap.find(fromFileID)->second.get();

    auto lineRange = lineRangeToString(Range, true);

    if (lineRange.empty()) {
      return "";
    }

    if (toFile == fromFile) {
      return "#" + lineRange;
    }

    if (toFile->Realname.empty()) {
      return "#" + lineRange;
    }

    std::string result = toFile->Realname;
    result += "#";
    result += lineRange;
    return result;
  }

  // Convert SourceRange to "line:column-line:column".
  // In the resulting string rep, line is 1-based, column is 0-based.
  std::string fullRangeToString(SourceRange Range) {
    std::pair<FileID, unsigned> Begin = SM.getDecomposedLoc(Range.getBegin());
    std::pair<FileID, unsigned> End = SM.getDecomposedLoc(Range.getEnd());

    bool IsInvalid;
    unsigned Line1 = SM.getLineNumber(Begin.first, Begin.second, &IsInvalid);
    if (IsInvalid) {
      return "";
    }
    unsigned Column1 =
        SM.getColumnNumber(Begin.first, Begin.second, &IsInvalid);
    if (IsInvalid) {
      return "";
    }
    unsigned Line2 = SM.getLineNumber(End.first, End.second, &IsInvalid);
    if (IsInvalid) {
      return "";
    }
    unsigned Column2 = SM.getColumnNumber(End.first, End.second, &IsInvalid);
    if (IsInvalid) {
      return "";
    }

    return stringFormat("%d:%d-%d:%d", Line1, Column1 - 1, Line2, Column2 - 1);
  }

  // Returns the qualified name of `d` without considering template parameters.
  std::string getQualifiedName(const NamedDecl *D) {
    const DeclContext *Ctx = D->getDeclContext();
    if (Ctx->isFunctionOrMethod()) {
      return D->getQualifiedNameAsString();
    }

    std::vector<const DeclContext *> Contexts;

    // Collect contexts.
    while (Ctx && isa<NamedDecl>(Ctx)) {
      Contexts.push_back(Ctx);
      Ctx = Ctx->getParent();
    }

    std::string Result;

    std::reverse(Contexts.begin(), Contexts.end());

    for (const DeclContext *DC : Contexts) {
      if (const auto *Spec = dyn_cast<ClassTemplateSpecializationDecl>(DC)) {
        Result += Spec->getNameAsString();

        if (Spec->getSpecializationKind() == TSK_ExplicitSpecialization) {
          std::string Backing;
          llvm::raw_string_ostream Stream(Backing);
          const TemplateArgumentList &TemplateArgs = Spec->getTemplateArgs();
          printTemplateArgumentList(Stream, TemplateArgs.asArray(),
                                    PrintingPolicy(CI.getLangOpts()));
          Result += Stream.str();
        }
      } else if (const auto *Nd = dyn_cast<NamespaceDecl>(DC)) {
        if (Nd->isAnonymousNamespace() || Nd->isInline()) {
          continue;
        }
        Result += Nd->getNameAsString();
      } else if (const auto *Rd = dyn_cast<RecordDecl>(DC)) {
        if (!Rd->getIdentifier()) {
          Result += "(anonymous)";
        } else {
          Result += Rd->getNameAsString();
        }
      } else if (const auto *Fd = dyn_cast<FunctionDecl>(DC)) {
        Result += Fd->getNameAsString();
      } else if (const auto *Ed = dyn_cast<EnumDecl>(DC)) {
        // C++ [dcl.enum]p10: Each enum-name and each unscoped
        // enumerator is declared in the scope that immediately contains
        // the enum-specifier. Each scoped enumerator is declared in the
        // scope of the enumeration.
        if (Ed->isScoped() || Ed->getIdentifier())
          Result += Ed->getNameAsString();
        else
          continue;
      } else {
        Result += cast<NamedDecl>(DC)->getNameAsString();
      }
      Result += "::";
    }

    if (D->getDeclName())
      Result += D->getNameAsString();
    else
      Result += "(anonymous)";

    return Result;
  }

  std::string mangleLocation(SourceLocation Loc,
                             std::string Backup = std::string()) {
    FileInfo *F = getFileInfo(Loc);
    std::string Filename = F->Realname;
    if (Filename.length() == 0 && Backup.length() != 0) {
      return Backup;
    }
    if (F->Generated) {
      // Since generated files may be different on different platforms,
      // we need to include a platform-specific thing in the hash. Otherwise
      // we can end up with hash collisions where different symbols from
      // different platforms map to the same thing.
      char *Platform = getenv("MOZSEARCH_PLATFORM");
      Filename =
          std::string(Platform ? Platform : "") + std::string("@") + Filename;
    }
    return hash(Filename + std::string("@") + locationToString(Loc));
  }

  bool isAcceptableSymbolChar(char c) {
    return isalpha(c) || isdigit(c) || c == '_' || c == '/';
  }

  std::string mangleFile(std::string Filename, FileType Type) {
    // "Mangle" the file path, such that:
    // 1. The majority of paths will still be mostly human-readable.
    // 2. The sanitization algorithm doesn't produce collisions where two
    //    different unsanitized paths can result in the same sanitized paths.
    // 3. The produced symbol doesn't cause problems with downstream consumers.
    // In order to accomplish this, we keep alphanumeric chars, underscores,
    // and slashes, and replace everything else with an "@xx" hex encoding.
    // The majority of path characters are letters and slashes which don't get
    // encoded, so that satisfies (1). Since "@" characters in the unsanitized
    // path get encoded, there should be no "@" characters in the sanitized path
    // that got preserved from the unsanitized input, so that should satisfy
    // (2). And (3) was done by trial-and-error. Note in particular the dot (.)
    // character needs to be encoded, or the symbol-search feature of mozsearch
    // doesn't work correctly, as all dot characters in the symbol query get
    // replaced by #.
    for (size_t i = 0; i < Filename.length(); i++) {
      char c = Filename[i];
      if (isAcceptableSymbolChar(c)) {
        continue;
      }
      char hex[4];
      sprintf(hex, "@%02X", ((int)c) & 0xFF);
      Filename.replace(i, 1, hex);
      i += 2;
    }

    if (Type == FileType::Generated) {
      // Since generated files may be different on different platforms,
      // we need to include a platform-specific thing in the hash. Otherwise
      // we can end up with hash collisions where different symbols from
      // different platforms map to the same thing.
      char *Platform = getenv("MOZSEARCH_PLATFORM");
      Filename =
          std::string(Platform ? Platform : "") + std::string("@") + Filename;
    }
    return Filename;
  }

  std::string mangleURL(std::string Url) {
    return mangleFile(Url, FileType::Source);
  }

  std::string mangleQualifiedName(std::string Name) {
    std::replace(Name.begin(), Name.end(), ' ', '_');
    return Name;
  }

  std::string getMangledName(clang::MangleContext *Ctx,
                             const clang::NamedDecl *Decl) {
    // Main functions will tend to collide because they inherently have similar
    // signatures, so let's provide a custom location-based signature.
    if (isa<FunctionDecl>(Decl) && cast<FunctionDecl>(Decl)->isMain()) {
      return std::string("MF_") + mangleLocation(Decl->getLocation());
    }

    if (isa<FunctionDecl>(Decl) && cast<FunctionDecl>(Decl)->isExternC()) {
      return cast<FunctionDecl>(Decl)->getNameAsString();
    }

    if (isa<FunctionDecl>(Decl) || isa<VarDecl>(Decl)) {
      const DeclContext *DC = Decl->getDeclContext();
      if (isa<TranslationUnitDecl>(DC) || isa<NamespaceDecl>(DC) ||
          isa<LinkageSpecDecl>(DC) ||
          // isa<ExternCContextDecl>(DC) ||
          isa<TagDecl>(DC)) {
        llvm::SmallVector<char, 512> Output;
        llvm::raw_svector_ostream Out(Output);
#if CLANG_VERSION_MAJOR >= 11
        // This code changed upstream in version 11:
        // https://github.com/llvm/llvm-project/commit/29e1a16be8216066d1ed733a763a749aed13ff47
        GlobalDecl GD;
        if (const CXXConstructorDecl *D = dyn_cast<CXXConstructorDecl>(Decl)) {
          GD = GlobalDecl(D, Ctor_Complete);
        } else if (const CXXDestructorDecl *D =
                       dyn_cast<CXXDestructorDecl>(Decl)) {
          GD = GlobalDecl(D, Dtor_Complete);
        } else {
          GD = GlobalDecl(Decl);
        }
        Ctx->mangleName(GD, Out);
#else
        if (const CXXConstructorDecl *D = dyn_cast<CXXConstructorDecl>(Decl)) {
          Ctx->mangleCXXCtor(D, CXXCtorType::Ctor_Complete, Out);
        } else if (const CXXDestructorDecl *D =
                       dyn_cast<CXXDestructorDecl>(Decl)) {
          Ctx->mangleCXXDtor(D, CXXDtorType::Dtor_Complete, Out);
        } else {
          Ctx->mangleName(Decl, Out);
        }
#endif
        return Out.str().str();
      } else {
        return std::string("V_") + mangleLocation(Decl->getLocation()) +
               std::string("_") + hash(std::string(Decl->getName()));
      }
    } else if (isa<TagDecl>(Decl) || isa<ObjCInterfaceDecl>(Decl)) {
      if (!Decl->getIdentifier()) {
        // Anonymous.
        return std::string("T_") + mangleLocation(Decl->getLocation());
      }

      return std::string("T_") + mangleQualifiedName(getQualifiedName(Decl));
    } else if (isa<TypedefNameDecl>(Decl)) {
      if (!Decl->getIdentifier()) {
        // Anonymous.
        return std::string("TA_") + mangleLocation(Decl->getLocation());
      }

      return std::string("TA_") + mangleQualifiedName(getQualifiedName(Decl));
    } else if (isa<NamespaceDecl>(Decl) || isa<NamespaceAliasDecl>(Decl)) {
      if (!Decl->getIdentifier()) {
        // Anonymous.
        return std::string("NS_") + mangleLocation(Decl->getLocation());
      }

      return std::string("NS_") + mangleQualifiedName(getQualifiedName(Decl));
    } else if (const ObjCIvarDecl *D2 = dyn_cast<ObjCIvarDecl>(Decl)) {
      const ObjCInterfaceDecl *Iface = D2->getContainingInterface();
      return std::string("F_<") + getMangledName(Ctx, Iface) + ">_" +
             D2->getNameAsString();
    } else if (const FieldDecl *D2 = dyn_cast<FieldDecl>(Decl)) {
      const RecordDecl *Record = D2->getParent();
      return std::string("F_<") + getMangledName(Ctx, Record) + ">_" +
             D2->getNameAsString();
    } else if (const EnumConstantDecl *D2 = dyn_cast<EnumConstantDecl>(Decl)) {
      const DeclContext *DC = Decl->getDeclContext();
      if (const NamedDecl *Named = dyn_cast<NamedDecl>(DC)) {
        return std::string("E_<") + getMangledName(Ctx, Named) + ">_" +
               D2->getNameAsString();
      }
    }

    assert(false);
    return std::string("");
  }

  void debugLocation(SourceLocation Loc) {
    std::string S = locationToString(Loc);
    StringRef Filename = SM.getFilename(Loc);
    printf("--> %s %s\n", std::string(Filename).c_str(), S.c_str());
  }

  void debugRange(SourceRange Range) {
    printf("Range\n");
    debugLocation(Range.getBegin());
    debugLocation(Range.getEnd());
  }

public:
  IndexConsumer(CompilerInstance &CI)
      : CI(CI), SM(CI.getSourceManager()), LO(CI.getLangOpts()),
        CurMangleContext(nullptr), AstContext(nullptr),
        ConcatInfo(CI.getPreprocessor()), CurDeclContext(nullptr),
        TemplateStack(nullptr) {
    CI.getPreprocessor().addPPCallbacks(make_unique<PreprocessorHook>(this));
    CI.getPreprocessor().setTokenWatcher(
        [this](const auto &token) { onTokenLexed(token); });
  }

  virtual DiagnosticConsumer *clone(DiagnosticsEngine &Diags) const {
    return new IndexConsumer(CI);
  }

#if !defined(_WIN32) && !defined(_WIN64)
  struct AutoTime {
    AutoTime(double *Counter) : Counter(Counter), Start(time()) {}
    ~AutoTime() {
      if (Start) {
        *Counter += time() - Start;
      }
    }
    void stop() {
      *Counter += time() - Start;
      Start = 0;
    }
    double *Counter;
    double Start;
  };
#endif

  // All we need is to follow the final declaration.
  virtual void HandleTranslationUnit(ASTContext &Ctx) {
    CurMangleContext =
        clang::ItaniumMangleContext::create(Ctx, CI.getDiagnostics());

    AstContext = &Ctx;
    Resolver = std::make_unique<clangd::HeuristicResolver>(Ctx);
    TraverseDecl(Ctx.getTranslationUnitDecl());

    // Emit the JSON data for all files now.
    std::map<FileID, std::unique_ptr<FileInfo>>::iterator It;
    for (It = FileMap.begin(); It != FileMap.end(); It++) {
      if (!It->second->Interesting) {
        continue;
      }

      FileInfo &Info = *It->second;

      std::string Filename = Outdir + Info.Realname;
      std::string SrcFilename =
          Info.Generated ? Objdir + Info.Realname.substr(GENERATED.length())
                         : Srcdir + PATHSEP_STRING + Info.Realname;

      ensurePath(Filename);

      // We lock the output file in case some other clang process is trying to
      // write to it at the same time.
      AutoLockFile Lock(SrcFilename, Filename);

      if (!Lock.success()) {
        fprintf(stderr, "Unable to lock file %s\n", Filename.c_str());
        exit(1);
      }

      // Merge our results with the existing lines from the output file.
      // This ensures that header files that are included multiple times
      // in different ways are analyzed completely.
      std::ifstream Fin(Filename.c_str(), std::ios::in | std::ios::binary);
      FILE *OutFp = Lock.openTmp();
      if (!OutFp) {
        fprintf(stderr, "Unable to open tmp out file for %s\n",
                Filename.c_str());
        exit(1);
      }

      // Sort our new results and get an iterator to them
      std::sort(Info.Output.begin(), Info.Output.end());
      std::vector<std::string>::const_iterator NewLinesIter =
          Info.Output.begin();
      std::string LastNewWritten;

      // Loop over the existing (sorted) lines in the analysis output file.
      // (The good() check also handles the case where Fin did not exist when we
      // went to open it.)
      while (Fin.good()) {
        std::string OldLine;
        std::getline(Fin, OldLine);
        // Skip blank lines.
        if (OldLine.length() == 0) {
          continue;
        }
        // We need to put the newlines back that getline() eats.
        OldLine.push_back('\n');

        // Write any results from Info.Output that are lexicographically
        // smaller than OldLine (read from the existing file), but make sure
        // to skip duplicates. Keep advancing NewLinesIter until we reach an
        // entry that is lexicographically greater than OldLine.
        for (; NewLinesIter != Info.Output.end(); NewLinesIter++) {
          if (*NewLinesIter > OldLine) {
            break;
          }
          if (*NewLinesIter == OldLine) {
            continue;
          }
          if (*NewLinesIter == LastNewWritten) {
            // dedupe the new entries being written
            continue;
          }
          if (fwrite(NewLinesIter->c_str(), NewLinesIter->length(), 1, OutFp) !=
              1) {
            fprintf(stderr,
                    "Unable to write %zu bytes[1] to tmp output file for %s\n",
                    NewLinesIter->length(), Filename.c_str());
            exit(1);
          }
          LastNewWritten = *NewLinesIter;
        }

        // Write the entry read from the existing file.
        if (fwrite(OldLine.c_str(), OldLine.length(), 1, OutFp) != 1) {
          fprintf(stderr,
                  "Unable to write %zu bytes[2] to tmp output file for %s\n",
                  OldLine.length(), Filename.c_str());
          exit(1);
        }
      }

      // We finished reading from Fin
      Fin.close();

      // Finish iterating our new results, discarding duplicates
      for (; NewLinesIter != Info.Output.end(); NewLinesIter++) {
        if (*NewLinesIter == LastNewWritten) {
          continue;
        }
        if (fwrite(NewLinesIter->c_str(), NewLinesIter->length(), 1, OutFp) !=
            1) {
          fprintf(stderr,
                  "Unable to write %zu bytes[3] to tmp output file for %s\n",
                  NewLinesIter->length(), Filename.c_str());
          exit(1);
        }
        LastNewWritten = *NewLinesIter;
      }

      // Done writing all the things, close it and replace the old output file
      // with the new one.
      fclose(OutFp);
      if (!Lock.moveTmp()) {
        fprintf(stderr,
                "Unable to move tmp output file into place for %s (err %d)\n",
                Filename.c_str(), errno);
        exit(1);
      }
    }
  }

  // Unfortunately, we have to override all these methods in order to track the
  // context we're inside.

  bool TraverseEnumDecl(EnumDecl *D) {
    AutoSetContext Asc(this, D);
    return Super::TraverseEnumDecl(D);
  }
  bool TraverseRecordDecl(RecordDecl *D) {
    AutoSetContext Asc(this, D);
    return Super::TraverseRecordDecl(D);
  }
  bool TraverseCXXRecordDecl(CXXRecordDecl *D) {
    AutoSetContext Asc(this, D);
    return Super::TraverseCXXRecordDecl(D);
  }
  bool TraverseFunctionDecl(FunctionDecl *D) {
    AutoSetContext Asc(this, D);
    const FunctionDecl *Def;
    // (See the larger AutoTemplateContext comment for more information.) If a
    // method on a templated class is declared out-of-line, we need to analyze
    // the definition inside the scope of the template or else we won't properly
    // handle member access on the templated type.
    if (TemplateStack && D->isDefined(Def) && Def && D != Def) {
      const auto _ = ValueRollback(CurDeclContext, nullptr);
      TraverseFunctionDecl(const_cast<FunctionDecl *>(Def));
    }
    return Super::TraverseFunctionDecl(D);
  }
  bool TraverseCXXMethodDecl(CXXMethodDecl *D) {
    AutoSetContext Asc(this, D);
    const FunctionDecl *Def;
    // See TraverseFunctionDecl.
    if (TemplateStack && D->isDefined(Def) && Def && D != Def) {
      const auto _ = ValueRollback(CurDeclContext, nullptr);
      TraverseFunctionDecl(const_cast<FunctionDecl *>(Def));
    }
    return Super::TraverseCXXMethodDecl(D);
  }
  bool TraverseCXXConstructorDecl(CXXConstructorDecl *D) {
    AutoSetContext Asc(this, D, /*VisitImplicit=*/true);
    const FunctionDecl *Def;
    // See TraverseFunctionDecl.
    if (TemplateStack && D->isDefined(Def) && Def && D != Def) {
      const auto _ = ValueRollback(CurDeclContext, nullptr);
      TraverseFunctionDecl(const_cast<FunctionDecl *>(Def));
    }
    return Super::TraverseCXXConstructorDecl(D);
  }
  bool TraverseCXXConversionDecl(CXXConversionDecl *D) {
    AutoSetContext Asc(this, D);
    const FunctionDecl *Def;
    // See TraverseFunctionDecl.
    if (TemplateStack && D->isDefined(Def) && Def && D != Def) {
      const auto _ = ValueRollback(CurDeclContext, nullptr);
      TraverseFunctionDecl(const_cast<FunctionDecl *>(Def));
    }
    return Super::TraverseCXXConversionDecl(D);
  }
  bool TraverseCXXDestructorDecl(CXXDestructorDecl *D) {
    AutoSetContext Asc(this, D);
    const FunctionDecl *Def;
    // See TraverseFunctionDecl.
    if (TemplateStack && D->isDefined(Def) && Def && D != Def) {
      const auto _ = ValueRollback(CurDeclContext, nullptr);
      TraverseFunctionDecl(const_cast<FunctionDecl *>(Def));
    }
    return Super::TraverseCXXDestructorDecl(D);
  }

  bool TraverseLambdaExpr(LambdaExpr *E) {
    AutoSetContext Asc(this, nullptr, true);
    return Super::TraverseLambdaExpr(E);
  }

  // Used to keep track of the context in which a token appears.
  struct Context {
    // Ultimately this becomes the "context" JSON property.
    std::string Name;

    // Ultimately this becomes the "contextsym" JSON property.
    std::string Symbol;

    Context() {}
    Context(std::string Name, std::string Symbol)
        : Name(Name), Symbol(Symbol) {}
  };

  Context translateContext(NamedDecl *D) {
    const FunctionDecl *F = dyn_cast<FunctionDecl>(D);
    if (F && F->isTemplateInstantiation()) {
      D = F->getTemplateInstantiationPattern();
    }

    return Context(D->getQualifiedNameAsString(),
                   getMangledName(CurMangleContext, D));
  }

  Context getContext(SourceLocation Loc) {
    if (SM.isMacroBodyExpansion(Loc)) {
      // If we're inside a macro definition, we don't return any context. It
      // will probably not be what the user expects if we do.
      return Context();
    }

    AutoSetContext *Ctxt = CurDeclContext;
    while (Ctxt) {
      if (Ctxt->Decl) {
        return translateContext(Ctxt->Decl);
      }
      Ctxt = Ctxt->Prev;
    }
    return Context();
  }

  // Similar to GetContext(SourceLocation), but it skips the declaration passed
  // in. This is useful if we want the context of a declaration that's already
  // on the stack.
  Context getContext(Decl *D) {
    if (SM.isMacroBodyExpansion(D->getLocation())) {
      // If we're inside a macro definition, we don't return any context. It
      // will probably not be what the user expects if we do.
      return Context();
    }

    AutoSetContext *Ctxt = CurDeclContext;
    while (Ctxt) {
      if (Ctxt->Decl && Ctxt->Decl != D) {
        return translateContext(Ctxt->Decl);
      }
      Ctxt = Ctxt->Prev;
    }
    return Context();
  }

  // Searches for the closest CurDeclContext parent that is a function template
  // instantiation
  const FunctionDecl *getCurrentFunctionTemplateInstantiation() {
    const auto *Ctxt = CurDeclContext;
    while (Ctxt) {
      if (Ctxt->Decl && isa<FunctionDecl>(Ctxt->Decl)) {
        const auto *F = Ctxt->Decl->getAsFunction();
        if (F->isTemplateInstantiation())
          return F;
      }
      Ctxt = Ctxt->Prev;
    }
    return nullptr;
  }

  // Analyzing template code is tricky. Suppose we have this code:
  //
  //   template<class T>
  //   bool Foo(T* ptr) { return T::StaticMethod(ptr); }
  //
  // If we analyze the body of Foo without knowing the type T, then we will not
  // be able to generate any information for StaticMethod. However, analyzing
  // Foo for every possible instantiation is inefficient and it also generates
  // too much data in some cases. For example, the following code would generate
  // one definition of Baz for every instantiation, which is undesirable:
  //
  //   template<class T>
  //   class Bar { struct Baz { ... }; };
  //
  // To solve this problem, we analyze templates only once. We do so in a
  // GatherDependent mode where we look for "dependent scoped member
  // expressions" (i.e., things like StaticMethod). We keep track of the
  // locations of these expressions. If we find one or more of them, we analyze
  // the template for each instantiation, in an AnalyzeDependent mode. This mode
  // ignores all source locations except for the ones where we found dependent
  // scoped member expressions before. For these locations, we generate a
  // separate JSON result for each instantiation.
  //
  // We inherit our parent's mode if it is exists.  This is because if our
  // parent is in analyze mode, it means we've already lived a full life in
  // gather mode and we must not restart in gather mode or we'll cause the
  // indexer to visit EVERY identifier, which is way too much data.
  struct AutoTemplateContext {
    AutoTemplateContext(IndexConsumer *Self)
        : Self(Self), CurMode(Self->TemplateStack ? Self->TemplateStack->CurMode
                                                  : Mode::GatherDependent),
          Parent(Self->TemplateStack) {
      Self->TemplateStack = this;
    }

    ~AutoTemplateContext() { Self->TemplateStack = Parent; }

    // We traverse templates in two modes:
    enum class Mode {
      // Gather mode does not traverse into specializations. It looks for
      // locations where it would help to have more info from template
      // specializations.
      GatherDependent,

      // Analyze mode traverses into template specializations and records
      // information about token locations saved in gather mode.
      AnalyzeDependent,
    };

    // We found a dependent scoped member expression! Keep track of it for
    // later.
    void visitDependent(SourceLocation Loc) {
      if (CurMode == Mode::AnalyzeDependent) {
        return;
      }

      DependentLocations.insert(Loc.getRawEncoding());
      if (Parent) {
        Parent->visitDependent(Loc);
      }
    }

    bool inGatherMode() { return CurMode == Mode::GatherDependent; }

    // Do we need to perform the extra AnalyzeDependent passes (one per
    // instantiation)?
    bool needsAnalysis() const {
      if (!DependentLocations.empty()) {
        return true;
      }
      if (Parent) {
        return Parent->needsAnalysis();
      }
      return false;
    }

    void switchMode() { CurMode = Mode::AnalyzeDependent; }

    // Do we want to analyze each template instantiation separately?
    bool shouldVisitTemplateInstantiations() const {
      if (CurMode == Mode::AnalyzeDependent) {
        return true;
      }
      if (Parent) {
        return Parent->shouldVisitTemplateInstantiations();
      }
      return false;
    }

    // For a given expression/statement, should we emit JSON data for it?
    bool shouldVisit(SourceLocation Loc) {
      if (CurMode == Mode::GatherDependent) {
        return true;
      }
      if (DependentLocations.find(Loc.getRawEncoding()) !=
          DependentLocations.end()) {
        return true;
      }
      if (Parent) {
        return Parent->shouldVisit(Loc);
      }
      return false;
    }

  private:
    IndexConsumer *Self;
    Mode CurMode;
    std::unordered_set<unsigned> DependentLocations;
    AutoTemplateContext *Parent;
  };

  AutoTemplateContext *TemplateStack;

  std::unordered_multimap<const FunctionDecl *, const Stmt *>
      ForwardingTemplates;
  std::unordered_set<unsigned> ForwardedTemplateLocations;

  bool shouldVisitTemplateInstantiations() const {
    if (TemplateStack) {
      return TemplateStack->shouldVisitTemplateInstantiations();
    }
    return false;
  }

  bool shouldVisitImplicitCode() const {
    return CurDeclContext && CurDeclContext->VisitImplicit;
  }

  // We don't want to traverse all specializations everytime we find a forward
  // declaration, so only traverse specializations related to an actual
  // definition.
  //
  // ```
  // // This is the canonical declaration for Maybe but isn't really useful.
  // template <typename T>
  // struct Maybe;
  //
  // // This is another ClassTemplateDecl, but not the canonical one, where we
  // // actually have the definition. This is the one we want to traverse.
  // template <typename T>
  // struct Maybe {
  //   // This is both the canonical declaration and the definition for
  //   // inline_method and we want to traverse it.
  //   template <typename... Args>
  //   T *inline_method(Args&&... args) {
  //     // definition
  //   }
  //
  //   // This is the canonical declaration, TraverseFunctionTemplateDecl
  //   // traverses its out of line definition too.
  //   template <typename... Args>
  //   T *out_of_line_method(Args&&... args);
  // }
  //
  // // This is the definition for Maybe<T>::out_of_line_method<Args...>
  // // It is traversed when calling TraverseFunctionTemplateDecl on the
  // // canonical declaration.
  // template <typename T>
  // template <typename... Args>
  // T *maybe(Args&&... args) {
  //   // definition
  // }
  // ```
  //
  // So:
  // - for class templates we check isThisDeclarationADefinition
  // - for function templates we check isCanonicalDecl
  bool TraverseClassTemplateDecl(ClassTemplateDecl *D) {
    AutoTemplateContext Atc(this);
    Super::TraverseClassTemplateDecl(D);

    // Gather dependent locations from partial specializations too
    SmallVector<ClassTemplatePartialSpecializationDecl *> PS;
    D->getPartialSpecializations(PS);
    for (auto *Spec : PS) {
      for (auto *Rd : Spec->redecls()) {
        TraverseDecl(Rd);
      }
    }

    if (!Atc.needsAnalysis()) {
      return true;
    }

    Atc.switchMode();

    if (!D->isThisDeclarationADefinition())
      return true;

    for (auto *Spec : D->specializations()) {
      for (auto *Rd : Spec->redecls()) {
        // We don't want to visit injected-class-names in this traversal.
        if (cast<CXXRecordDecl>(Rd)->isInjectedClassName())
          continue;

        TraverseDecl(Rd);
      }
    }

    return true;
  }

  // See also comment above TraverseClassTemplateDecl
  bool TraverseFunctionTemplateDecl(FunctionTemplateDecl *D) {
    AutoTemplateContext Atc(this);
    if (Atc.inGatherMode()) {
      Super::TraverseFunctionTemplateDecl(D);
    }

    if (!Atc.needsAnalysis()) {
      return true;
    }

    Atc.switchMode();

    if (!D->isCanonicalDecl())
      return true;

    for (auto *Spec : D->specializations()) {
      for (auto *Rd : Spec->redecls()) {
        TraverseDecl(Rd);
      }
    }

    return true;
  }

  bool shouldVisit(SourceLocation Loc) {
    if (TemplateStack) {
      return TemplateStack->shouldVisit(Loc);
    }
    return true;
  }

  enum {
    // Flag to omit the identifier from being cross-referenced across files.
    // This is usually desired for local variables.
    NoCrossref = 1 << 0,
    // Flag to indicate the token with analysis data is not an identifier.
    // Indicates
    // we want to skip the check that tries to ensure a sane identifier token.
    NotIdentifierToken = 1 << 1,
    // This indicates that the end of the provided SourceRange is valid and
    // should be respected. If this flag is not set, the visitIdentifier
    // function should use only the start of the SourceRange and auto-detect
    // the end based on whatever token is found at the start.
    LocRangeEndValid = 1 << 2,
    // Indicates this record was generated through heuristic template
    // resolution.
    Heuristic = 1 << 3,
  };

  void emitStructuredRecordInfo(llvm::json::OStream &J, SourceLocation Loc,
                                const RecordDecl *decl) {
    J.attribute("kind",
                TypeWithKeyword::getTagTypeKindName(decl->getTagKind()));

    const ASTContext &C = *AstContext;
    const ASTRecordLayout &Layout = C.getASTRecordLayout(decl);

    J.attribute("sizeBytes", Layout.getSize().getQuantity());
    J.attribute("alignmentBytes", Layout.getAlignment().getQuantity());

    emitBindingAttributes(J, *decl);

    auto cxxDecl = dyn_cast<CXXRecordDecl>(decl);

    if (cxxDecl) {
      if (Layout.hasOwnVFPtr()) {
        // Encode the size of virtual function table pointer
        // instead of just true/false, for 2 reasons:
        //  * having the size here is easier for the consumer
        //  * the size string 4/8 is shorter than true/false in the analysis
        //    file
        const QualType ptrType = C.getUIntPtrType();
        J.attribute("ownVFPtrBytes",
                    C.getTypeSizeInChars(ptrType).getQuantity());
      }

      J.attributeBegin("supers");
      J.arrayBegin();
      for (const CXXBaseSpecifier &Base : cxxDecl->bases()) {
        const CXXRecordDecl *BaseDecl = Base.getType()->getAsCXXRecordDecl();

        J.objectBegin();

        J.attribute("sym", getMangledName(CurMangleContext, BaseDecl));

        if (Base.isVirtual()) {
          CharUnits superOffsetBytes = Layout.getVBaseClassOffset(BaseDecl);
          J.attribute("offsetBytes", superOffsetBytes.getQuantity());
        } else {
          CharUnits superOffsetBytes = Layout.getBaseClassOffset(BaseDecl);
          J.attribute("offsetBytes", superOffsetBytes.getQuantity());
        }

        J.attributeBegin("props");
        J.arrayBegin();
        if (Base.isVirtual()) {
          J.value("virtual");
        }
        J.arrayEnd();
        J.attributeEnd();

        J.objectEnd();
      }
      J.arrayEnd();
      J.attributeEnd();

      J.attributeBegin("methods");
      J.arrayBegin();
      for (const CXXMethodDecl *MethodDecl : cxxDecl->methods()) {
        J.objectBegin();

        J.attribute("pretty", getQualifiedName(MethodDecl));
        J.attribute("sym", getMangledName(CurMangleContext, MethodDecl));

        // TODO: Better figure out what to do for non-isUserProvided methods
        // which means there's potentially semantic data that doesn't correspond
        // to a source location in the source.  Should we be emitting
        // structured info for those when we're processing the class here?

        J.attributeBegin("props");
        J.arrayBegin();
        if (MethodDecl->isStatic()) {
          J.value("static");
        }
        if (MethodDecl->isInstance()) {
          J.value("instance");
        }
        if (MethodDecl->isVirtual()) {
          J.value("virtual");
        }
        if (MethodDecl->isUserProvided()) {
          J.value("user");
        }
        if (MethodDecl->isDefaulted()) {
          J.value("defaulted");
        }
        if (MethodDecl->isDeleted()) {
          J.value("deleted");
        }
        if (MethodDecl->isConstexpr()) {
          J.value("constexpr");
        }
        J.arrayEnd();
        J.attributeEnd();

        J.objectEnd();
      }
      J.arrayEnd();
      J.attributeEnd();
    }

    FileID structFileID = SM.getFileID(Loc);

    J.attributeBegin("fields");
    J.arrayBegin();
    uint64_t iField = 0;
    for (RecordDecl::field_iterator It = decl->field_begin(),
                                    End = decl->field_end();
         It != End; ++It, ++iField) {
      const FieldDecl &Field = **It;
      auto sourceRange =
          SM.getExpansionRange(Field.getSourceRange()).getAsRange();
      uint64_t localOffsetBits = Layout.getFieldOffset(iField);
      CharUnits localOffsetBytes = C.toCharUnitsFromBits(localOffsetBits);

      J.objectBegin();
      J.attribute("lineRange",
                  pathAndLineRangeToString(structFileID, sourceRange));
      J.attribute("pretty", getQualifiedName(&Field));
      J.attribute("sym", getMangledName(CurMangleContext, &Field));

      QualType FieldType = Field.getType();
      QualType CanonicalFieldType = FieldType.getCanonicalType();
      LangOptions langOptions;
      PrintingPolicy Policy(langOptions);
      Policy.PrintCanonicalTypes = true;
      J.attribute("type", CanonicalFieldType.getAsString(Policy));

      const TagDecl *tagDecl = CanonicalFieldType->getAsTagDecl();
      if (!tagDecl) {
        // Try again piercing any pointers/references involved.  Note that our
        // typesym semantics are dubious-ish and right now crossref just does
        // some parsing of "type" itself until we improve this rep.
        CanonicalFieldType = CanonicalFieldType->getPointeeType();
        if (!CanonicalFieldType.isNull()) {
          tagDecl = CanonicalFieldType->getAsTagDecl();
        }
      }
      if (tagDecl) {
        J.attribute("typesym", getMangledName(CurMangleContext, tagDecl));
      }
      J.attribute("offsetBytes", localOffsetBytes.getQuantity());
      if (Field.isBitField()) {
        J.attributeBegin("bitPositions");
        J.objectBegin();

        J.attribute("begin",
                    unsigned(localOffsetBits - C.toBits(localOffsetBytes)));
#if CLANG_VERSION_MAJOR < 20
        J.attribute("width", Field.getBitWidthValue(C));
#else
        J.attribute("width", Field.getBitWidthValue());
#endif

        J.objectEnd();
        J.attributeEnd();
      } else {
        // Try and get the field as a record itself so we can know its size, but
        // we don't actually want to recurse into it.
        if (auto FieldRec = Field.getType()->getAs<RecordType>()) {
          auto const &FieldLayout = C.getASTRecordLayout(FieldRec->getDecl());
          J.attribute("sizeBytes", FieldLayout.getSize().getQuantity());
        } else {
          // We were unable to get it as a record, which suggests it's a normal
          // type, in which case let's just ask for the type size.  (Maybe this
          // would also work for the above case too?)
          uint64_t typeSizeBits = C.getTypeSize(Field.getType());
          CharUnits typeSizeBytes = C.toCharUnitsFromBits(typeSizeBits);
          J.attribute("sizeBytes", typeSizeBytes.getQuantity());
        }
      }
      J.objectEnd();
    }
    J.arrayEnd();
    J.attributeEnd();
  }

  void emitStructuredEnumInfo(llvm::json::OStream &J, const EnumDecl *ED) {
    J.attribute("kind", "enum");
  }

  void emitStructuredEnumConstantInfo(llvm::json::OStream &J,
                                      const EnumConstantDecl *ECD) {
    J.attribute("kind", "enumConstant");
  }

  void emitStructuredFunctionInfo(llvm::json::OStream &J,
                                  const FunctionDecl *decl) {
    emitBindingAttributes(J, *decl);

    J.attributeBegin("args");
    J.arrayBegin();

    for (auto param : decl->parameters()) {
      J.objectBegin();

      J.attribute("name", param->getName());
      QualType ArgType = param->getOriginalType();
      J.attribute("type", ArgType.getAsString());

      QualType CanonicalArgType = ArgType.getCanonicalType();
      const TagDecl *canonDecl = CanonicalArgType->getAsTagDecl();
      if (!canonDecl) {
        // Try again piercing any pointers/references involved.  Note that our
        // typesym semantics are dubious-ish and right now crossref just does
        // some parsing of "type" itself until we improve this rep.
        CanonicalArgType = CanonicalArgType->getPointeeType();
        if (!CanonicalArgType.isNull()) {
          canonDecl = CanonicalArgType->getAsTagDecl();
        }
      }
      if (canonDecl) {
        J.attribute("typesym", getMangledName(CurMangleContext, canonDecl));
      }

      J.objectEnd();
    }

    J.arrayEnd();
    J.attributeEnd();

    auto cxxDecl = dyn_cast<CXXMethodDecl>(decl);

    if (cxxDecl) {
      J.attribute("kind", "method");
      if (auto parentDecl = cxxDecl->getParent()) {
        J.attribute("parentsym", getMangledName(CurMangleContext, parentDecl));
      }

      J.attributeBegin("overrides");
      J.arrayBegin();
      for (const CXXMethodDecl *MethodDecl : cxxDecl->overridden_methods()) {
        J.objectBegin();

        // TODO: Make sure we're doing template traversals appropriately...
        // findOverriddenMethods (now removed) liked to do:
        //   if (Decl->isTemplateInstantiation()) {
        //     Decl =
        //     dyn_cast<CXXMethodDecl>(Decl->getTemplateInstantiationPattern());
        //   }
        // I think our pre-emptive dereferencing/avoidance of templates may
        // protect us from this, but it needs more investigation.

        J.attribute("sym", getMangledName(CurMangleContext, MethodDecl));

        J.objectEnd();
      }
      J.arrayEnd();
      J.attributeEnd();

    } else {
      J.attribute("kind", "function");
    }

    // ## Props
    J.attributeBegin("props");
    J.arrayBegin();
    // some of these are only possible on a CXXMethodDecl, but we want them all
    // in the same array, so condition these first ones.
    if (cxxDecl) {
      if (cxxDecl->isStatic()) {
        J.value("static");
      }
      if (cxxDecl->isInstance()) {
        J.value("instance");
      }
      if (cxxDecl->isVirtual()) {
        J.value("virtual");
      }
      if (cxxDecl->isUserProvided()) {
        J.value("user");
      }
    }
    if (decl->isDefaulted()) {
      J.value("defaulted");
    }
    if (decl->isDeleted()) {
      J.value("deleted");
    }
    if (decl->isConstexpr()) {
      J.value("constexpr");
    }
    J.arrayEnd();
    J.attributeEnd();
  }

  /**
   * Emit structured info for a field.  Right now the intent is for this to just
   * be a pointer to its parent's structured info with this method entirely
   * avoiding getting the ASTRecordLayout.
   *
   * TODO: Give more thought on where to locate the canonical info on fields and
   * how to normalize their exposure over the web.  We could relink the info
   * both at cross-reference time and web-server lookup time.  This is also
   * called out in `analysis.md`.
   */
  void emitStructuredFieldInfo(llvm::json::OStream &J, const FieldDecl *decl) {
    J.attribute("kind", "field");

    // XXX the call to decl::getParent will assert below for ObjCIvarDecl
    // instances because their DecContext is not a RecordDecl.  So just bail
    // for now.
    // TODO: better support ObjC.
    if (!dyn_cast<ObjCIvarDecl>(decl)) {
      if (auto parentDecl = decl->getParent()) {
        J.attribute("parentsym", getMangledName(CurMangleContext, parentDecl));
      }
    }
  }

  /**
   * Emit structured info for a variable if it is a static class member.
   */
  void emitStructuredVarInfo(llvm::json::OStream &J, const VarDecl *decl) {
    const auto *parentDecl =
        dyn_cast_or_null<RecordDecl>(decl->getDeclContext());

    if (parentDecl) {
      J.attribute("kind", "field");
    } else if (llvm::isa<ParmVarDecl>(decl)) {
      J.attribute("kind", "parameter");
    } else if (decl->isLocalVarDecl()) {
      J.attribute("kind", "localVar");
    } else {
      // namespace scope variable
      J.attribute("kind", "variable");
    }

    if (parentDecl) {
      J.attribute("parentsym", getMangledName(CurMangleContext, parentDecl));
    }

    emitBindingAttributes(J, *decl);
  }

  void emitStructuredInfo(SourceLocation Loc, const NamedDecl *decl) {
    std::string json_str;
    llvm::raw_string_ostream ros(json_str);
    llvm::json::OStream J(ros);
    // Start the top-level object.
    J.objectBegin();

    unsigned StartOffset = SM.getFileOffset(Loc);
    unsigned EndOffset =
        StartOffset + Lexer::MeasureTokenLength(Loc, SM, CI.getLangOpts());
    J.attribute("loc", locationToString(Loc, EndOffset - StartOffset));
    J.attribute("structured", 1);
    J.attribute("pretty", getQualifiedName(decl));
    J.attribute("sym", getMangledName(CurMangleContext, decl));

    if (const RecordDecl *RD = dyn_cast<RecordDecl>(decl)) {
      emitStructuredRecordInfo(J, Loc, RD);
    } else if (const EnumDecl *ED = dyn_cast<EnumDecl>(decl)) {
      emitStructuredEnumInfo(J, ED);
    } else if (const EnumConstantDecl *ECD = dyn_cast<EnumConstantDecl>(decl)) {
      emitStructuredEnumConstantInfo(J, ECD);
    } else if (const FunctionDecl *FD = dyn_cast<FunctionDecl>(decl)) {
      emitStructuredFunctionInfo(J, FD);
    } else if (const FieldDecl *FD = dyn_cast<FieldDecl>(decl)) {
      emitStructuredFieldInfo(J, FD);
    } else if (const VarDecl *VD = dyn_cast<VarDecl>(decl)) {
      emitStructuredVarInfo(J, VD);
    }

    // End the top-level object.
    J.objectEnd();

    FileInfo *F = getFileInfo(Loc);
    // we want a newline.
    ros << '\n';
    F->Output.push_back(std::move(ros.str()));
  }

  // XXX Type annotating.
  // QualType is the type class.  It has helpers like TagDecl via getAsTagDecl.
  // ValueDecl exposes a getType() method.
  //
  // Arguably it makes sense to only expose types that Searchfox has definitions
  // for as first-class.  Probably the way to go is like context/contextsym.
  // We expose a "type" which is just a human-readable string which has no
  // semantic purposes and is just a display string, plus then a "typesym" which
  // we expose if we were able to map the type.
  //
  // Other meta-info: field offsets.  Ancestor types.

  // This is the only function that emits analysis JSON data. It should be
  // called for each identifier that corresponds to a symbol.
  void visitIdentifier(const char *Kind, const char *SyntaxKind,
                       llvm::StringRef QualName, SourceRange LocRange,
                       std::string Symbol, QualType MaybeType = QualType(),
                       Context TokenContext = Context(), int Flags = 0,
                       SourceRange PeekRange = SourceRange(),
                       SourceRange NestingRange = SourceRange(),
                       std::vector<SourceRange> *ArgRanges = nullptr) {
    SourceLocation Loc = LocRange.getBegin();

    // Also visit the spelling site.
    SourceLocation SpellingLoc = SM.getSpellingLoc(Loc);
    if (SpellingLoc != Loc) {
      visitIdentifier(Kind, SyntaxKind, QualName, SpellingLoc, Symbol,
                      MaybeType, TokenContext, Flags, PeekRange, NestingRange,
                      ArgRanges);
    }

    SourceLocation ExpansionLoc = SM.getExpansionLoc(Loc);
    normalizeLocation(&ExpansionLoc);

    if (!shouldVisit(ExpansionLoc)) {
      return;
    }

    if (ExpansionLoc != Loc)
      Flags = Flags & ~LocRangeEndValid;

    // Find the file positions corresponding to the token.
    unsigned StartOffset = SM.getFileOffset(ExpansionLoc);
    unsigned EndOffset =
        (Flags & LocRangeEndValid)
            ? SM.getFileOffset(LocRange.getEnd())
            : StartOffset +
                  Lexer::MeasureTokenLength(ExpansionLoc, SM, CI.getLangOpts());

    std::string LocStr =
        locationToString(ExpansionLoc, EndOffset - StartOffset);
    std::string RangeStr =
        locationToString(ExpansionLoc, EndOffset - StartOffset);
    std::string PeekRangeStr;

    if (!(Flags & NotIdentifierToken)) {
      // Get the token's characters so we can make sure it's a valid token.
      const char *StartChars = SM.getCharacterData(ExpansionLoc);
      std::string Text(StartChars, EndOffset - StartOffset);
      if (!isValidIdentifier(Text)) {
        return;
      }
    }

    FileInfo *F = getFileInfo(ExpansionLoc);

    if (!(Flags & NoCrossref)) {
      std::string json_str;
      llvm::raw_string_ostream ros(json_str);
      llvm::json::OStream J(ros);
      // Start the top-level object.
      J.objectBegin();

      J.attribute("loc", LocStr);
      J.attribute("target", 1);
      J.attribute("kind", Kind);
      J.attribute("pretty", QualName.data());
      J.attribute("sym", Symbol);
      if (!TokenContext.Name.empty()) {
        J.attribute("context", TokenContext.Name);
      }
      if (!TokenContext.Symbol.empty()) {
        J.attribute("contextsym", TokenContext.Symbol);
      }
      if (PeekRange.isValid()) {
        PeekRangeStr = lineRangeToString(PeekRange);
        if (!PeekRangeStr.empty()) {
          J.attribute("peekRange", PeekRangeStr);
        }
      }

      if (ArgRanges) {
        J.attributeBegin("argRanges");
        J.arrayBegin();

        for (auto range : *ArgRanges) {
          std::string ArgRangeStr = fullRangeToString(range);
          if (!ArgRangeStr.empty()) {
            J.value(ArgRangeStr);
          }
        }

        J.arrayEnd();
        J.attributeEnd();
      }

      // End the top-level object.
      J.objectEnd();
      // we want a newline.
      ros << '\n';
      F->Output.push_back(std::move(ros.str()));
    }

    // Generate a single "source":1 for all the symbols. If we search from here,
    // we want to union the results for every symbol in `symbols`.
    std::string json_str;
    llvm::raw_string_ostream ros(json_str);
    llvm::json::OStream J(ros);
    // Start the top-level object.
    J.objectBegin();

    J.attribute("loc", RangeStr);
    J.attribute("source", 1);

    if (NestingRange.isValid()) {
      std::string NestingRangeStr = fullRangeToString(NestingRange);
      if (!NestingRangeStr.empty()) {
        J.attribute("nestingRange", NestingRangeStr);
      }
    }

    std::string Syntax;
    if (Flags & NoCrossref) {
      J.attribute("syntax", "");
    } else {
      Syntax = Kind;
      Syntax.push_back(',');
      Syntax.append(SyntaxKind);
      J.attribute("syntax", Syntax);
    }

    if (!MaybeType.isNull()) {
      J.attribute("type", MaybeType.getAsString());
      QualType canonical = MaybeType.getCanonicalType();
      const TagDecl *decl = canonical->getAsTagDecl();
      if (!decl) {
        // Try again piercing any pointers/references involved.  Note that our
        // typesym semantics are dubious-ish and right now crossref just does
        // some parsing of "type" itself until we improve this rep.
        canonical = canonical->getPointeeType();
        if (!canonical.isNull()) {
          decl = canonical->getAsTagDecl();
        }
      }
      if (decl) {
        std::string Mangled = getMangledName(CurMangleContext, decl);
        J.attribute("typesym", Mangled);
      }
    }

    std::string Pretty(SyntaxKind);
    Pretty.push_back(' ');
    Pretty.append(QualName.data());
    J.attribute("pretty", Pretty);

    J.attribute("sym", Symbol);

    if (Flags & NoCrossref) {
      J.attribute("no_crossref", 1);
    }

    if (Flags & Heuristic) {
      J.attributeBegin("confidence");
      J.arrayBegin();
      J.value("cppTemplateHeuristic");
      J.arrayEnd();
      J.attributeEnd();
    }

    if (ArgRanges) {
      J.attributeBegin("argRanges");
      J.arrayBegin();

      for (auto range : *ArgRanges) {
        std::string ArgRangeStr = fullRangeToString(range);
        if (!ArgRangeStr.empty()) {
          J.value(ArgRangeStr);
        }
      }

      J.arrayEnd();
      J.attributeEnd();
    }

    const auto macro = MacroMaps.find(ExpansionLoc);
    if (macro != MacroMaps.end()) {
      const auto &macroInfo = macro->second;
      if (macroInfo.Symbol == Symbol) {
        J.attributeBegin("expandsTo");
        J.objectBegin();
        J.attributeBegin(macroInfo.Key);
        J.objectBegin();
        J.attribute("", macroInfo.Expansion); // "" is the platform key,
                                              // populated by the merge step
        J.objectEnd();
        J.attributeEnd();
        J.objectEnd();
        J.attributeEnd();
      } else {
        const auto it = macroInfo.TokenLocations.find(Loc);
        if (it != macroInfo.TokenLocations.end()) {
          J.attributeBegin("inExpansionAt");
          J.objectBegin();
          J.attributeBegin(macroInfo.Key);
          J.objectBegin();
          J.attributeBegin(
              ""); // "" is the platform key, populated by the merge step
          J.arrayBegin();
          J.value(it->second);
          J.arrayEnd();
          J.attributeEnd();
          J.objectEnd();
          J.attributeEnd();
          J.objectEnd();
          J.attributeEnd();
        }
      }
    }

    // End the top-level object.
    J.objectEnd();

    // we want a newline.
    ros << '\n';
    F->Output.push_back(std::move(ros.str()));
  }

  void normalizeLocation(SourceLocation *Loc) {
    *Loc = SM.getSpellingLoc(*Loc);
  }

  // For cases where the left-brace is not directly accessible from the AST,
  // helper to use the lexer to find the brace.  Make sure you're picking the
  // start location appropriately!
  SourceLocation findLeftBraceFromLoc(SourceLocation Loc) {
    return Lexer::findLocationAfterToken(Loc, tok::l_brace, SM, LO, false);
  }

  // If the provided statement is compound, return its range.
  SourceRange getCompoundStmtRange(Stmt *D) {
    if (!D) {
      return SourceRange();
    }

    CompoundStmt *D2 = dyn_cast<CompoundStmt>(D);
    if (D2) {
      return D2->getSourceRange();
    }

    return SourceRange();
  }

  SourceRange getFunctionPeekRange(FunctionDecl *D) {
    // We always start at the start of the function decl, which may include the
    // return type on a separate line.
    SourceLocation Start = D->getBeginLoc();

    // By default, we end at the line containing the function's name.
    SourceLocation End = D->getLocation();

    std::pair<FileID, unsigned> FuncLoc = SM.getDecomposedLoc(End);

    // But if there are parameters, we want to include those as well.
    for (ParmVarDecl *Param : D->parameters()) {
      std::pair<FileID, unsigned> ParamLoc =
          SM.getDecomposedLoc(Param->getLocation());

      // It's possible there are macros involved or something. We don't include
      // the parameters in that case.
      if (ParamLoc.first == FuncLoc.first) {
        // Assume parameters are in order, so we always take the last one.
        End = Param->getEndLoc();
      }
    }

    return SourceRange(Start, End);
  }

  SourceRange getTagPeekRange(TagDecl *D) {
    SourceLocation Start = D->getBeginLoc();

    // By default, we end at the line containing the name.
    SourceLocation End = D->getLocation();

    std::pair<FileID, unsigned> FuncLoc = SM.getDecomposedLoc(End);

    if (CXXRecordDecl *D2 = dyn_cast<CXXRecordDecl>(D)) {
      // But if there are parameters, we want to include those as well.
      for (CXXBaseSpecifier &Base : D2->bases()) {
        std::pair<FileID, unsigned> Loc = SM.getDecomposedLoc(Base.getEndLoc());

        // It's possible there are macros involved or something. We don't
        // include the parameters in that case.
        if (Loc.first == FuncLoc.first) {
          // Assume parameters are in order, so we always take the last one.
          End = Base.getEndLoc();
        }
      }
    }

    return SourceRange(Start, End);
  }

  SourceRange getCommentRange(NamedDecl *D) {
    const RawComment *RC = AstContext->getRawCommentForDeclNoCache(D);
    if (!RC) {
      return SourceRange();
    }

    return RC->getSourceRange();
  }

  // Sanity checks that all ranges are in the same file, returning the first if
  // they're in different files.  Unions the ranges based on which is first.
  SourceRange combineRanges(SourceRange Range1, SourceRange Range2) {
    if (Range1.isInvalid()) {
      return Range2;
    }
    if (Range2.isInvalid()) {
      return Range1;
    }

    std::pair<FileID, unsigned> Begin1 = SM.getDecomposedLoc(Range1.getBegin());
    std::pair<FileID, unsigned> End1 = SM.getDecomposedLoc(Range1.getEnd());
    std::pair<FileID, unsigned> Begin2 = SM.getDecomposedLoc(Range2.getBegin());
    std::pair<FileID, unsigned> End2 = SM.getDecomposedLoc(Range2.getEnd());

    if (End1.first != Begin2.first) {
      // Something weird is probably happening with the preprocessor. Just
      // return the first range.
      return Range1;
    }

    // See which range comes first.
    if (Begin1.second <= End2.second) {
      return SourceRange(Range1.getBegin(), Range2.getEnd());
    } else {
      return SourceRange(Range2.getBegin(), Range1.getEnd());
    }
  }

  // Given a location and a range, returns the range if:
  // - The location and the range live in the same file.
  // - The range is well ordered (end is not before begin).
  // Returns an empty range otherwise.
  SourceRange validateRange(SourceLocation Loc, SourceRange Range) {
    std::pair<FileID, unsigned> Decomposed = SM.getDecomposedLoc(Loc);
    std::pair<FileID, unsigned> Begin = SM.getDecomposedLoc(Range.getBegin());
    std::pair<FileID, unsigned> End = SM.getDecomposedLoc(Range.getEnd());

    if (Begin.first != Decomposed.first || End.first != Decomposed.first) {
      return SourceRange();
    }

    if (Begin.second >= End.second) {
      return SourceRange();
    }

    return Range;
  }

  bool VisitNamedDecl(NamedDecl *D) {
    SourceLocation Loc = D->getLocation();
    if (!isInterestingLocation(Loc)) {
      return true;
    }

    SourceLocation ExpansionLoc = Loc;
    if (SM.isMacroBodyExpansion(Loc)) {
      ExpansionLoc = SM.getFileLoc(Loc);
    }
    normalizeLocation(&ExpansionLoc);

    if (isa<ParmVarDecl>(D) && !D->getDeclName().getAsIdentifierInfo()) {
      // Unnamed parameter in function proto.
      return true;
    }

    int Flags = 0;
    const char *Kind = "def";
    const char *PrettyKind = "?";
    bool wasTemplate = false;
    SourceRange PeekRange(D->getBeginLoc(), D->getEndLoc());
    // The nesting range identifies the left brace and right brace, which
    // heavily depends on the AST node type.
    SourceRange NestingRange;
    QualType qtype = QualType();
    if (FunctionDecl *D2 = dyn_cast<FunctionDecl>(D)) {
      if (D2->isTemplateInstantiation()) {
        wasTemplate = true;
        D = D2->getTemplateInstantiationPattern();
      }
      // We treat pure virtual declarations as definitions.
      Kind =
          (D2->isThisDeclarationADefinition() || isPure(D2)) ? "def" : "decl";
      PrettyKind = "function";
      PeekRange = getFunctionPeekRange(D2);

      // Only emit the nesting range if:
      // - This is a definition AND
      // - This isn't a template instantiation.  Function templates'
      //   instantiations can end up as a definition with a Loc at their point
      //   of declaration but with the CompoundStmt of the template's
      //   point of definition.  This really messes up the nesting range logic.
      //   At the time of writing this, the test repo's `big_header.h`'s
      //   `WhatsYourVector_impl::forwardDeclaredTemplateThingInlinedBelow` as
      //   instantiated by `big_cpp.cpp` triggers this phenomenon.
      //
      // Note: As covered elsewhere, template processing is tricky and it's
      // conceivable that we may change traversal patterns in the future,
      // mooting this guard.
      if (D2->isThisDeclarationADefinition() &&
          !D2->isTemplateInstantiation()) {
        // The CompoundStmt range is the brace range.
        NestingRange = getCompoundStmtRange(D2->getBody());
      }
    } else if (TagDecl *D2 = dyn_cast<TagDecl>(D)) {
      Kind = D2->isThisDeclarationADefinition() ? "def" : "forward";
      PrettyKind = "type";

      if (D2->isThisDeclarationADefinition() && D2->getDefinition() == D2) {
        PeekRange = getTagPeekRange(D2);
        NestingRange = D2->getBraceRange();
      } else {
        PeekRange = SourceRange();
      }
    } else if (TypedefNameDecl *D2 = dyn_cast<TypedefNameDecl>(D)) {
      Kind = "alias";
      PrettyKind = "type";
      PeekRange = SourceRange(ExpansionLoc, ExpansionLoc);
      qtype = D2->getUnderlyingType();
    } else if (VarDecl *D2 = dyn_cast<VarDecl>(D)) {
      if (D2->isLocalVarDeclOrParm()) {
        Flags = NoCrossref;
      }

      Kind = D2->isThisDeclarationADefinition() == VarDecl::DeclarationOnly
                 ? "decl"
                 : "def";
      PrettyKind = "variable";
    } else if (isa<NamespaceDecl>(D) || isa<NamespaceAliasDecl>(D)) {
      Kind = "def";
      PrettyKind = "namespace";
      PeekRange = SourceRange(ExpansionLoc, ExpansionLoc);
      NamespaceDecl *D2 = dyn_cast<NamespaceDecl>(D);
      if (D2) {
        // There's no exposure of the left brace so we have to find it.
        NestingRange = SourceRange(
            findLeftBraceFromLoc(D2->isAnonymousNamespace() ? D2->getBeginLoc()
                                                            : ExpansionLoc),
            D2->getRBraceLoc());
      }
    } else if (isa<FieldDecl>(D)) {
      Kind = "def";
      PrettyKind = "field";
    } else if (isa<EnumConstantDecl>(D)) {
      Kind = "def";
      PrettyKind = "enum constant";
    } else {
      return true;
    }

    if (ValueDecl *D2 = dyn_cast<ValueDecl>(D)) {
      qtype = D2->getType();
    }

    SourceRange CommentRange = getCommentRange(D);
    PeekRange = combineRanges(PeekRange, CommentRange);
    PeekRange = validateRange(Loc, PeekRange);
    NestingRange = validateRange(Loc, NestingRange);

    std::string Symbol = getMangledName(CurMangleContext, D);

    // In the case of destructors, Loc might point to the ~ character. In that
    // case we want to skip to the name of the class. However, Loc might also
    // point to other places that generate destructors, such as a lambda
    // (apparently clang 8 creates a destructor declaration for at least some
    // lambdas). In that case we'll just drop the declaration.
    if (isa<CXXDestructorDecl>(D)) {
      PrettyKind = "destructor";
      const char *P = SM.getCharacterData(Loc);
      if (*P == '~') {
        // Advance Loc to the class name
        P++;

        unsigned Skipped = 1;
        while (*P == ' ' || *P == '\t' || *P == '\r' || *P == '\n') {
          P++;
          Skipped++;
        }

        Loc = Loc.getLocWithOffset(Skipped);
      } else {
        return true;
      }
    }

    visitIdentifier(Kind, PrettyKind, getQualifiedName(D), SourceRange(Loc),
                    Symbol, qtype, getContext(D), Flags, PeekRange,
                    NestingRange);

    // In-progress structured info emission.
    if (RecordDecl *D2 = dyn_cast<RecordDecl>(D)) {
      if (D2->isThisDeclarationADefinition() &&
          // XXX getASTRecordLayout doesn't work for dependent types, so we
          // avoid calling into emitStructuredInfo for now if there's a
          // dependent type or if we're in any kind of template context.  This
          // should be re-evaluated once this is working for normal classes and
          // we can better evaluate what is useful.
          !D2->isDependentType() && !TemplateStack) {
        if (auto *D3 = dyn_cast<CXXRecordDecl>(D2)) {
          findBindingToJavaClass(*AstContext, *D3);
          findBoundAsJavaClasses(*AstContext, *D3);
        }
        emitStructuredInfo(ExpansionLoc, D2);
      }
    }
    if (EnumDecl *D2 = dyn_cast<EnumDecl>(D)) {
      if (D2->isThisDeclarationADefinition() && !D2->isDependentType() &&
          !TemplateStack) {
        emitStructuredInfo(ExpansionLoc, D2);
      }
    }
    if (EnumConstantDecl *D2 = dyn_cast<EnumConstantDecl>(D)) {
      if (!D2->isTemplated() && !TemplateStack) {
        emitStructuredInfo(ExpansionLoc, D2);
      }
    }
    if (FunctionDecl *D2 = dyn_cast<FunctionDecl>(D)) {
      if ((D2->isThisDeclarationADefinition() || isPure(D2)) &&
          // a clause at the top should have generalized and set wasTemplate so
          // it shouldn't be the case that isTemplateInstantiation() is true.
          !D2->isTemplateInstantiation() && !wasTemplate &&
          !D2->isFunctionTemplateSpecialization() && !TemplateStack) {
        if (auto *D3 = dyn_cast<CXXMethodDecl>(D2)) {
          findBindingToJavaMember(*AstContext, *D3);
        } else {
          findBindingToJavaFunction(*AstContext, *D2);
        }
        emitStructuredInfo(ExpansionLoc, D2);
      }
    }
    if (FieldDecl *D2 = dyn_cast<FieldDecl>(D)) {
      if (!D2->isTemplated() && !TemplateStack) {
        emitStructuredInfo(ExpansionLoc, D2);
      }
    }
    if (VarDecl *D2 = dyn_cast<VarDecl>(D)) {
      if (!D2->isTemplated() && !TemplateStack) {
        findBindingToJavaConstant(*AstContext, *D2);
        emitStructuredInfo(ExpansionLoc, D2);
      }
    }

    return true;
  }

  bool VisitCXXConstructExpr(const CXXConstructExpr *E) {
    // If we are in a template and find a Stmt that was registed in
    // ForwardedTemplateLocations, convert the location to an actual Stmt* in
    // ForwardingTemplates
    if (TemplateStack && !TemplateStack->inGatherMode()) {
      if (ForwardedTemplateLocations.find(E->getBeginLoc().getRawEncoding()) !=
          ForwardedTemplateLocations.end()) {
        if (const auto *currentTemplate =
                getCurrentFunctionTemplateInstantiation()) {
          ForwardingTemplates.insert({currentTemplate, E});
        }
        return true;
      }
    }

    SourceLocation Loc = E->getBeginLoc();
    if (!isInterestingLocation(Loc)) {
      return true;
    }

    return VisitCXXConstructExpr(E, Loc);
  }

  bool VisitCXXConstructExpr(const CXXConstructExpr *E, SourceLocation Loc) {
    SourceLocation SpellingLoc = SM.getSpellingLoc(Loc);

    FunctionDecl *Ctor = E->getConstructor();
    if (Ctor->isTemplateInstantiation()) {
      Ctor = Ctor->getTemplateInstantiationPattern();
    }
    std::string Mangled = getMangledName(CurMangleContext, Ctor);

    // FIXME: Need to do something different for list initialization.

    visitIdentifier("use", "constructor", getQualifiedName(Ctor), Loc, Mangled,
                    QualType(), getContext(SpellingLoc));

    return true;
  }

  CallExpr *CurrentCall = nullptr;
  bool TraverseCallExpr(CallExpr *E) {
    const auto _ = ValueRollback(CurrentCall, E);
    return Super::TraverseCallExpr(E);
  }

  bool VisitCallExpr(CallExpr *E) {
    Expr *CalleeExpr = E->getCallee()->IgnoreParenImpCasts();

    if (TemplateStack) {
      const auto CalleeLocation = [&] {
        if (const auto *Member =
                dyn_cast<CXXDependentScopeMemberExpr>(CalleeExpr)) {
          return Member->getMemberLoc();
        }
        if (const auto *DeclRef =
                dyn_cast<DependentScopeDeclRefExpr>(CalleeExpr)) {
          return DeclRef->getLocation();
        }
        if (const auto *DeclRef = dyn_cast<DeclRefExpr>(CalleeExpr)) {
          return DeclRef->getLocation();
        }

        // Does the right thing for MemberExpr and UnresolvedMemberExpr at
        // least.
        return CalleeExpr->getExprLoc();
      }();

      // If we are in a template:
      // - when in GatherDependent mode and the callee is type-dependent,
      //   register it in ForwardedTemplateLocations
      // - when in AnalyseDependent mode and the callee is in
      //   ForwardedTemplateLocations, convert the location to an actual Stmt*
      //   in ForwardingTemplates
      if (TemplateStack->inGatherMode()) {
        if (CalleeExpr->isTypeDependent()) {
          TemplateStack->visitDependent(CalleeLocation);
          ForwardedTemplateLocations.insert(CalleeLocation.getRawEncoding());
        }
      } else {
        if (ForwardedTemplateLocations.find(CalleeLocation.getRawEncoding()) !=
            ForwardedTemplateLocations.end()) {
          if (const auto *currentTemplate =
                  getCurrentFunctionTemplateInstantiation()) {
            ForwardingTemplates.insert({currentTemplate, E});
          }
        }
      }
    }

    Decl *Callee = E->getCalleeDecl();
    if (!Callee || !FunctionDecl::classof(Callee)) {
      return true;
    }

    const NamedDecl *NamedCallee = dyn_cast<NamedDecl>(Callee);

    SourceLocation Loc;

    const FunctionDecl *F = dyn_cast<FunctionDecl>(NamedCallee);
    if (F->isTemplateInstantiation()) {
      NamedCallee = F->getTemplateInstantiationPattern();
    }

    std::string Mangled = getMangledName(CurMangleContext, NamedCallee);
    int Flags = 0;

    if (CXXOperatorCallExpr::classof(E)) {
      // Just take the first token.
      CXXOperatorCallExpr *Op = dyn_cast<CXXOperatorCallExpr>(E);
      Loc = Op->getOperatorLoc();
      Flags |= NotIdentifierToken;
    } else if (MemberExpr::classof(CalleeExpr)) {
      MemberExpr *Member = dyn_cast<MemberExpr>(CalleeExpr);
      Loc = Member->getMemberLoc();
    } else if (DeclRefExpr::classof(CalleeExpr)) {
      // We handle this in VisitDeclRefExpr.
      return true;
    } else {
      return true;
    }

    if (!isInterestingLocation(Loc)) {
      return true;
    }

    if (F->isTemplateInstantiation()) {
      VisitForwardedStatements(E, Loc);
    }

    SourceLocation SpellingLoc = SM.getSpellingLoc(Loc);

    std::vector<SourceRange> argRanges;
    for (auto argExpr : E->arguments()) {
      argRanges.push_back(argExpr->getSourceRange());
    }

    visitIdentifier("use", "function", getQualifiedName(NamedCallee), Loc,
                    Mangled, E->getCallReturnType(*AstContext),
                    getContext(SpellingLoc), Flags, SourceRange(),
                    SourceRange(), &argRanges);

    return true;
  }

  bool VisitTagTypeLoc(TagTypeLoc L) {
    SourceLocation Loc = L.getBeginLoc();
    if (!isInterestingLocation(Loc)) {
      return true;
    }

    SourceLocation SpellingLoc = SM.getSpellingLoc(Loc);

    TagDecl *Decl = L.getDecl();
    std::string Mangled = getMangledName(CurMangleContext, Decl);
    visitIdentifier("use", "type", getQualifiedName(Decl), Loc, Mangled,
                    L.getType(), getContext(SpellingLoc));
    return true;
  }

  bool VisitTypedefTypeLoc(TypedefTypeLoc L) {
    SourceLocation Loc = L.getBeginLoc();
    if (!isInterestingLocation(Loc)) {
      return true;
    }

    SourceLocation SpellingLoc = SM.getSpellingLoc(Loc);

    NamedDecl *Decl = L.getTypedefNameDecl();
    std::string Mangled = getMangledName(CurMangleContext, Decl);
    visitIdentifier("use", "type", getQualifiedName(Decl), Loc, Mangled,
                    L.getType(), getContext(SpellingLoc));
    return true;
  }

  bool VisitInjectedClassNameTypeLoc(InjectedClassNameTypeLoc L) {
    SourceLocation Loc = L.getBeginLoc();
    if (!isInterestingLocation(Loc)) {
      return true;
    }

    SourceLocation SpellingLoc = SM.getSpellingLoc(Loc);

    NamedDecl *Decl = L.getDecl();
    std::string Mangled = getMangledName(CurMangleContext, Decl);
    visitIdentifier("use", "type", getQualifiedName(Decl), Loc, Mangled,
                    L.getType(), getContext(SpellingLoc));
    return true;
  }

  bool VisitTemplateSpecializationTypeLoc(TemplateSpecializationTypeLoc L) {
    SourceLocation Loc = L.getBeginLoc();
    if (!isInterestingLocation(Loc)) {
      return true;
    }

    SourceLocation SpellingLoc = SM.getSpellingLoc(Loc);

    TemplateDecl *Td = L.getTypePtr()->getTemplateName().getAsTemplateDecl();
    if (ClassTemplateDecl *D = dyn_cast<ClassTemplateDecl>(Td)) {
      NamedDecl *Decl = D->getTemplatedDecl();
      std::string Mangled = getMangledName(CurMangleContext, Decl);
      visitIdentifier("use", "type", getQualifiedName(Decl), Loc, Mangled,
                      QualType(), getContext(SpellingLoc));
    } else if (TypeAliasTemplateDecl *D = dyn_cast<TypeAliasTemplateDecl>(Td)) {
      NamedDecl *Decl = D->getTemplatedDecl();
      std::string Mangled = getMangledName(CurMangleContext, Decl);
      visitIdentifier("use", "type", getQualifiedName(Decl), Loc, Mangled,
                      QualType(), getContext(SpellingLoc));
    }

    return true;
  }

  bool VisitDependentNameTypeLoc(DependentNameTypeLoc L) {
    SourceLocation Loc = L.getNameLoc();
    if (!isInterestingLocation(Loc)) {
      return true;
    }

    for (const NamedDecl *D :
         Resolver->resolveDependentNameType(L.getTypePtr())) {
      visitHeuristicResult(Loc, D);
    }
    return true;
  }

  void VisitForwardedStatements(const Expr *E, SourceLocation Loc) {
    // If Loc itself is forwarded to its callers, do nothing
    if (ForwardedTemplateLocations.find(Loc.getRawEncoding()) !=
        ForwardedTemplateLocations.cend())
      return;

    // If this is a forwarding template (eg MakeUnique), visit the forwarded
    // statements
    auto todo = std::stack{std::vector<const Stmt *>{E}};
    auto seen = std::unordered_set<const Stmt *>{};
    while (!todo.empty()) {
      const auto forwarded = std::move(todo.top());
      todo.pop();
      if (seen.find(forwarded) != seen.end())
        continue;
      seen.insert(forwarded);

      if (const auto *C = dyn_cast<CXXConstructExpr>(forwarded))
        VisitCXXConstructExpr(C, Loc);

      const Decl *Decl = nullptr;
      if (const auto *D = dyn_cast<CallExpr>(forwarded))
        Decl = D->getCalleeDecl();
      if (const auto *D = dyn_cast<DeclRefExpr>(forwarded))
        Decl = D->getDecl();

      if (!Decl)
        continue;
      const auto *F = Decl->getAsFunction();
      if (!F)
        continue;
      if (!F->isTemplateInstantiation())
        continue;
      const auto [ForwardedBegin, ForwardedEnd] =
          ForwardingTemplates.equal_range(F);
      for (auto ForwardedIt = ForwardedBegin; ForwardedIt != ForwardedEnd;
           ++ForwardedIt)
        if (seen.find(ForwardedIt->second) == seen.end())
          todo.push(ForwardedIt->second);
    }
  }

  bool VisitDeclRefExpr(const DeclRefExpr *E) {
    SourceLocation Loc = E->getExprLoc();
    if (!isInterestingLocation(Loc)) {
      return true;
    }

    SourceLocation SpellingLoc = SM.getSpellingLoc(Loc);

    if (E->hasQualifier()) {
      Loc = E->getNameInfo().getLoc();
      SpellingLoc = SM.getSpellingLoc(Loc);
    }

    const NamedDecl *Decl = E->getDecl();
    if (const VarDecl *D2 = dyn_cast<VarDecl>(Decl)) {
      int Flags = 0;
      if (D2->isLocalVarDeclOrParm()) {
        Flags = NoCrossref;
      }
      std::string Mangled = getMangledName(CurMangleContext, Decl);
      visitIdentifier("use", "variable", getQualifiedName(Decl), Loc, Mangled,
                      D2->getType(), getContext(SpellingLoc), Flags);
    } else if (isa<FunctionDecl>(Decl)) {
      const FunctionDecl *F = dyn_cast<FunctionDecl>(Decl);
      if (F->isTemplateInstantiation()) {
        Decl = F->getTemplateInstantiationPattern();
        VisitForwardedStatements(E, Loc);
      }

      std::string Mangled = getMangledName(CurMangleContext, Decl);
      visitIdentifier("use", "function", getQualifiedName(Decl), Loc, Mangled,
                      E->getType(), getContext(SpellingLoc));
    } else if (isa<EnumConstantDecl>(Decl)) {
      std::string Mangled = getMangledName(CurMangleContext, Decl);
      visitIdentifier("use", "enum", getQualifiedName(Decl), Loc, Mangled,
                      E->getType(), getContext(SpellingLoc));
    }

    return true;
  }

  bool VisitCXXConstructorDecl(CXXConstructorDecl *D) {
    if (!isInterestingLocation(D->getLocation())) {
      return true;
    }

    for (CXXConstructorDecl::init_const_iterator It = D->init_begin();
         It != D->init_end(); ++It) {
      const CXXCtorInitializer *Ci = *It;
      if (!Ci->getMember() || !Ci->isWritten()) {
        continue;
      }

      SourceLocation Loc = Ci->getMemberLocation();
      if (!isInterestingLocation(Loc)) {
        continue;
      }

      FieldDecl *Member = Ci->getMember();
      std::string Mangled = getMangledName(CurMangleContext, Member);
      // We want the constructor to be the context of the field use and
      // `getContext(D)` would skip the current context.  An alternate approach
      // would be `getContext(Loc)` but the heuristic to omit a context if we're
      // in a macro body expansion seems incorrect for field initializations; if
      // code is using macros to initialize the fields, we still care.
      visitIdentifier("use", "field", getQualifiedName(Member), Loc, Mangled,
                      Member->getType(), translateContext(D));
    }

    return true;
  }

  bool VisitMemberExpr(MemberExpr *E) {
    SourceLocation Loc = E->getExprLoc();
    if (!isInterestingLocation(Loc)) {
      return true;
    }

    SourceLocation SpellingLoc = SM.getSpellingLoc(Loc);

    ValueDecl *Decl = E->getMemberDecl();
    if (FieldDecl *Field = dyn_cast<FieldDecl>(Decl)) {
      std::string Mangled = getMangledName(CurMangleContext, Field);
      visitIdentifier("use", "field", getQualifiedName(Field), Loc, Mangled,
                      Field->getType(), getContext(SpellingLoc));
    }
    return true;
  }

  // Helper function for producing heuristic results for usages in dependent
  // code. These are distinguished from concrete results (obtained for dependent
  // code using the AutoTemplateContext machinery) by setting the â€œconfidenceâ€
  // property to â€œcppTemplateHeuristicâ€. We don't expect this method to be
  // intentionally called multiple times for a given (Loc, NamedDecl) pair
  // because our callers should be mutually exclusive AST node types. However,
  // it's fine if this method is called multiple time for a given pair because
  // we explicitly de-duplicate records with an identical string representation
  // (which is a good reason to have this helper, as it ensures identical
  // representations).
  void visitHeuristicResult(SourceLocation Loc, const NamedDecl *ND) {
    SourceLocation SpellingLoc = SM.getSpellingLoc(Loc);

    if (const UsingShadowDecl *USD = dyn_cast<UsingShadowDecl>(ND)) {
      ND = USD->getTargetDecl();
    }
    if (const TemplateDecl *TD = dyn_cast<TemplateDecl>(ND)) {
      ND = TD->getTemplatedDecl();
    }
    QualType MaybeType;
    const char *SyntaxKind = nullptr;
    if (const FunctionDecl *F = dyn_cast<FunctionDecl>(ND)) {
      MaybeType = F->getType();
      SyntaxKind = "function";
    } else if (const FieldDecl *F = dyn_cast<FieldDecl>(ND)) {
      MaybeType = F->getType();
      SyntaxKind = "field";
    } else if (const EnumConstantDecl *E = dyn_cast<EnumConstantDecl>(ND)) {
      MaybeType = E->getType();
      SyntaxKind = "enum";
    } else if (const TypedefNameDecl *T = dyn_cast<TypedefNameDecl>(ND)) {
      MaybeType = T->getUnderlyingType();
      SyntaxKind = "type";
    }
    if (SyntaxKind) {
      std::string Mangled = getMangledName(CurMangleContext, ND);
      visitIdentifier("use", SyntaxKind, getQualifiedName(ND), Loc, Mangled,
                      MaybeType, getContext(SpellingLoc), Heuristic);
    }
  }

  bool arityMatchesCurrentCallExpr(const Expr *E, const NamedDecl *Candidate) {
    const auto IsCurrentCallee = CurrentCall && E == CurrentCall->getCallee();
    const auto CallNumArgs =
        IsCurrentCallee ? CurrentCall->getNumArgs() : std::optional<uint>{};

    const FunctionDecl *CandidateFunc;
    if (const auto *UsingDecl = dyn_cast<UsingShadowDecl>(Candidate)) {
      CandidateFunc = UsingDecl->getTargetDecl()->getAsFunction();
    } else {
      CandidateFunc = Candidate->getAsFunction();
    }

    // We try and filter candidates by arity, but be conservative and accept
    // them when we don't know better
    if (!CandidateFunc || !CallNumArgs) {
      return true;
    }

    const auto MinNumArgs = CandidateFunc->getMinRequiredExplicitArguments();
    const auto MaxNumArgs = [&]() -> std::optional<uint> {
      const auto IsVariadic =
          CandidateFunc->isVariadic() ||
          std::any_of(CandidateFunc->param_begin(), CandidateFunc->param_end(),
                      [](const ParmVarDecl *param) {
                        return param->isParameterPack();
                      });

      if (IsVariadic)
        return {};

      return CandidateFunc->getNumNonObjectParams();
    }();

    if (CallNumArgs < MinNumArgs || (MaxNumArgs && CallNumArgs > *MaxNumArgs)) {
      return false;
    }

    return true;
  }

  bool VisitOverloadExpr(OverloadExpr *E) {
    SourceLocation Loc = E->getExprLoc();
    normalizeLocation(&Loc);
    if (!isInterestingLocation(Loc)) {
      return true;
    }

    for (auto *Candidate : E->decls()) {
      if (arityMatchesCurrentCallExpr(E, Candidate))
        visitHeuristicResult(Loc, Candidate);
    }

    // Also record this location so that if we have instantiations, we can
    // gather more accurate results from them.
    if (TemplateStack) {
      TemplateStack->visitDependent(Loc);
    }
    return true;
  }

  bool VisitCXXDependentScopeMemberExpr(CXXDependentScopeMemberExpr *E) {
    SourceLocation Loc = E->getMemberLoc();
    normalizeLocation(&Loc);
    if (!isInterestingLocation(Loc)) {
      return true;
    }

    for (const NamedDecl *Candidate : Resolver->resolveMemberExpr(E)) {
      if (arityMatchesCurrentCallExpr(E, Candidate))
        visitHeuristicResult(Loc, Candidate);
    }

    // Also record this location so that if we have instantiations, we can
    // gather more accurate results from them.
    if (TemplateStack) {
      TemplateStack->visitDependent(Loc);
    }
    return true;
  }

  bool VisitCXXNewExpr(CXXNewExpr *N) {
    // If we are in a template and the new is type-dependent, register it in
    // ForwardedTemplateLocations to forward its uses to the surrounding
    // template call site
    if (TemplateStack && TemplateStack->inGatherMode()) {
      const auto *TypeInfo = N->getAllocatedTypeSourceInfo();
      const auto ConstructExprLoc = TypeInfo->getTypeLoc().getBeginLoc();
      if (N->isTypeDependent()) {
        TemplateStack->visitDependent(ConstructExprLoc);
        ForwardedTemplateLocations.insert(ConstructExprLoc.getRawEncoding());
      }
    }
    return true;
  }

  bool VisitDependentScopeDeclRefExpr(DependentScopeDeclRefExpr *E) {
    SourceLocation Loc = E->getLocation();
    normalizeLocation(&Loc);
    if (!isInterestingLocation(Loc)) {
      return true;
    }

    for (const NamedDecl *Candidate : Resolver->resolveDeclRefExpr(E)) {
      if (arityMatchesCurrentCallExpr(E, Candidate))
        visitHeuristicResult(Loc, Candidate);
    }

    // Also record this location so that if we have instantiations, we can
    // gather more accurate results from them.
    if (TemplateStack) {
      TemplateStack->visitDependent(Loc);

      // Also record the dependent NestedNameSpecifier locations
      for (auto NestedNameLoc = E->getQualifierLoc();
           NestedNameLoc &&
           NestedNameLoc.getNestedNameSpecifier()->isDependent();
           NestedNameLoc = NestedNameLoc.getPrefix()) {
        TemplateStack->visitDependent(NestedNameLoc.getLocalBeginLoc());
      }
    }

    return true;
  }

  bool VisitStringLiteral(StringLiteral *E) {
    if (E->getCharByteWidth() != 1) {
      return true;
    }

    StringRef sref = E->getString();
    std::string s = sref.str();

    bool isMozSrc = stringStartsWith(s, "moz-src:///");

    if (!stringStartsWith(s, "chrome://") &&
        !stringStartsWith(s, "resource://") &&
        !isMozSrc) {
      return true;
    }

    if (!isASCII(s)) {
      return true;
    }

    SourceLocation Loc = E->getStrTokenLoc(0);
    normalizeLocation(&Loc);

    std::string symbol;

    if (isMozSrc) {
      symbol = std::string("FILE_") + mangleFile(s.substr(11), FileType::Source);
    } else {
      symbol = std::string("URL_") + mangleURL(s);
    }

    visitIdentifier("use", "file", StringRef(s), Loc, symbol, QualType(),
                    Context(), NotIdentifierToken | LocRangeEndValid);

    return true;
  }

  void enterSourceFile(SourceLocation Loc) {
    normalizeLocation(&Loc);
    FileInfo *newFile = getFileInfo(Loc);
    if (!newFile->Interesting) {
      return;
    }
    FileType type = newFile->Generated ? FileType::Generated : FileType::Source;
    std::string symbol =
        std::string("FILE_") + mangleFile(newFile->Realname, type);

    // We use an explicit zero-length source range at the start of the file. If
    // we don't set the LocRangeEndValid flag, the visitIdentifier code will use
    // the entire first token, which could be e.g. a long multiline-comment.
    visitIdentifier("def", "file", newFile->Realname, SourceRange(Loc), symbol,
                    QualType(), Context(),
                    NotIdentifierToken | LocRangeEndValid);
  }

  void inclusionDirective(SourceRange FileNameRange, const FileEntry *File) {
    std::string includedFile(File->tryGetRealPathName());
    FileType type = relativizePath(includedFile);
    if (type == FileType::Unknown) {
      return;
    }
    std::string symbol = std::string("FILE_") + mangleFile(includedFile, type);

    // Support the #include MACRO use-case
    // When parsing #include MACRO:
    // - the filename is never passed to onTokenLexed
    // - inclusionDirective is called before endMacroExpansion (which is only
    // called when the following token is parsed) So add the filename here and
    // call endMacroExpansion immediately. This ensures the macro has a correct
    // expansion and it has been added to MacroMaps so the referenced filename
    // knows to populate inExpansionAt.
    if (MacroExpansionState) {
      MacroExpansionState->TokenLocations[FileNameRange.getBegin()] =
          MacroExpansionState->Expansion.length();
      MacroExpansionState->Expansion += '"';
      MacroExpansionState->Expansion += includedFile;
      MacroExpansionState->Expansion += '"';
      endMacroExpansion();
    }

    visitIdentifier("use", "file", includedFile, FileNameRange, symbol,
                    QualType(), Context(),
                    NotIdentifierToken | LocRangeEndValid);
  }

  void macroDefined(const Token &Tok, const MacroDirective *Macro) {
    if (Macro->getMacroInfo()->isBuiltinMacro()) {
      return;
    }
    SourceLocation Loc = Tok.getLocation();
    normalizeLocation(&Loc);
    if (!isInterestingLocation(Loc)) {
      return;
    }

    IdentifierInfo *Ident = Tok.getIdentifierInfo();
    if (Ident) {
      std::string Mangled = std::string("M_") +
                            mangleLocation(Loc, std::string(Ident->getName()));
      visitIdentifier("def", "macro", Ident->getName(), Loc, Mangled);
    }
  }

  void macroUsed(const Token &Tok, const MacroInfo *Macro) {
    if (!Macro) {
      return;
    }
    if (Macro->isBuiltinMacro()) {
      return;
    }
    SourceLocation Loc = Tok.getLocation();
    if (!isInterestingLocation(Loc)) {
      return;
    }

    IdentifierInfo *Ident = Tok.getIdentifierInfo();
    if (Ident) {
      std::string Mangled =
          std::string("M_") + mangleLocation(Macro->getDefinitionLoc(),
                                             std::string(Ident->getName()));
      visitIdentifier("use", "macro", Ident->getName(), Loc, Mangled);
    }
  }

  void beginMacroExpansion(const Token &Tok, const MacroInfo *Macro,
                           SourceRange Range) {
    if (!Macro)
      return;

    if (Macro->isBuiltinMacro())
      return;

    if (!Tok.getIdentifierInfo())
      return;

    auto location = Tok.getLocation();
    normalizeLocation(&location);
    if (!isInterestingLocation(location))
      return;

    if (MacroExpansionState) {
      const auto InMacroArgs = MacroExpansionState->Range.fullyContains(
          SM.getExpansionRange(Range).getAsRange());
      const auto InMacroBody =
          SM.getExpansionLoc(Tok.getLocation()) ==
          SM.getExpansionLoc(MacroExpansionState->MacroNameToken.getLocation());
      if (InMacroArgs || InMacroBody) {
        if (MacroExpansionState->MacroInfo->getDefinitionLoc() !=
            Macro->getDefinitionLoc()) {
          IdentifierInfo *DependencyIdent = Tok.getIdentifierInfo();
          std::string DependencySymbol =
              std::string("M_") +
              mangleLocation(Macro->getDefinitionLoc(),
                             std::string(DependencyIdent->getName()));

          MacroExpansionState->Dependencies.push_back(DependencySymbol);
        }

        macroUsed(Tok, Macro);
        return;
      }

      endMacroExpansion();
    }

    MacroExpansionState = ::MacroExpansionState{
        .MacroNameToken = Tok,
        .MacroInfo = Macro,
        .Expansion = {},
        .TokenLocations = {},
        .Range = Range,
        .PrevPrevTok = {},
        .PrevTok = {},
    };
  }

  void endMacroExpansion() {
    // large macros are too slow to reformat, don't reformat macros larger than
    // those arbitrary thresholds
    static constexpr auto includedFileExpansionReformatThreshold = 20'000;
    static constexpr auto mainFileExpansionReformatThreshold = 200'000;

    const auto expansionLocation =
        SM.getExpansionLoc(MacroExpansionState->MacroNameToken.getLocation());
    const auto expansionFilename = SM.getFilename(expansionLocation);
    const auto includedExtensions =
        std::array{".h", ".hpp", ".hxx", ".inc", ".def"};
    const auto isIncludedFile =
        std::any_of(includedExtensions.begin(), includedExtensions.end(),
                    [&](const auto *extension) {
                      return expansionFilename.ends_with_insensitive(extension);
                    });
    const auto expansionReformatThreshold =
        isIncludedFile ? includedFileExpansionReformatThreshold
                       : mainFileExpansionReformatThreshold;

    if (MacroExpansionState->Expansion.length() < expansionReformatThreshold) {
      // large macros are too memory-hungry to reformat with ColumnLimit != 0
      // see https://github.com/llvm/llvm-project/issues/107434
      auto style = clang::format::getMozillaStyle();
      if (MacroExpansionState->Expansion.length() >
          includedFileExpansionReformatThreshold)
        style.ColumnLimit = 0;

      const auto replacements = clang::format::reformat(
          style, MacroExpansionState->Expansion,
          {tooling::Range(0, MacroExpansionState->Expansion.length())});
      auto formatted = clang::tooling::applyAllReplacements(
          MacroExpansionState->Expansion, replacements);
      if (formatted) {
        for (auto &[k, v] : MacroExpansionState->TokenLocations) {
          v = replacements.getShiftedCodePosition(v);
        }
        MacroExpansionState->Expansion = std::move(formatted.get());
      }
    }

    IdentifierInfo *Ident =
        MacroExpansionState->MacroNameToken.getIdentifierInfo();
    std::string Symbol =
        std::string("M_") +
        mangleLocation(MacroExpansionState->MacroInfo->getDefinitionLoc(),
                       std::string(Ident->getName()));

    const auto dependenciesBegin = MacroExpansionState->Dependencies.begin();
    const auto dependenciesEnd = MacroExpansionState->Dependencies.end();
    std::sort(dependenciesBegin, dependenciesEnd);
    MacroExpansionState->Dependencies.erase(
        std::unique(dependenciesBegin, dependenciesEnd), dependenciesEnd);

    auto Key = Symbol;
    for (const auto &Dependency : MacroExpansionState->Dependencies) {
      Key.push_back(',');
      Key += Dependency;
    }

    MacroMaps.emplace(std::pair{
        MacroExpansionState->MacroNameToken.getLocation(),
        ExpandedMacro{
            std::move(Symbol),
            std::move(Key),
            std::move(MacroExpansionState->Expansion),
            std::move(MacroExpansionState->TokenLocations),
        },
    });

    MacroExpansionState.reset();

    macroUsed(MacroExpansionState->MacroNameToken,
              MacroExpansionState->MacroInfo);
  }

  void onTokenLexed(const Token &Tok) {
    if (!MacroExpansionState)
      return;

    // check if we exited the macro expansion
    SourceLocation SLoc = Tok.getLocation();
    if (!SLoc.isMacroID()) {
      endMacroExpansion();
      return;
    }

    if (ConcatInfo.AvoidConcat(MacroExpansionState->PrevPrevTok,
                               MacroExpansionState->PrevTok, Tok)) {
      MacroExpansionState->Expansion += ' ';
    }

    if (Tok.isAnnotation()) {
      const auto Range = SM.getImmediateExpansionRange(Tok.getLocation());
      const char *Start = SM.getCharacterData(Range.getBegin());
      const char *End = SM.getCharacterData(Range.getEnd()) + 1;
      MacroExpansionState->Expansion += StringRef(Start, End - Start);
    } else {
      const auto spelling = CI.getPreprocessor().getSpelling(Tok);
      if (Tok.isAnyIdentifier()) {
        MacroExpansionState->TokenLocations[SLoc] =
            MacroExpansionState->Expansion.length();
      }
      MacroExpansionState->Expansion += spelling;
    }

    MacroExpansionState->PrevPrevTok = MacroExpansionState->PrevTok;
    MacroExpansionState->PrevTok = Tok;
  }
};

void PreprocessorHook::FileChanged(SourceLocation Loc, FileChangeReason Reason,
                                   SrcMgr::CharacteristicKind FileType,
                                   FileID PrevFID = FileID()) {
  switch (Reason) {
  case PPCallbacks::RenameFile:
  case PPCallbacks::SystemHeaderPragma:
    // Don't care about these, since we want the actual on-disk filenames
    break;
  case PPCallbacks::EnterFile:
    Indexer->enterSourceFile(Loc);
    break;
  case PPCallbacks::ExitFile:
    // Don't care about exiting files
    break;
  }
}

void PreprocessorHook::InclusionDirective(
    SourceLocation HashLoc, const Token &IncludeTok, StringRef FileName,
    bool IsAngled, CharSourceRange FileNameRange,
#if CLANG_VERSION_MAJOR >= 16
    OptionalFileEntryRef File,
#elif CLANG_VERSION_MAJOR >= 15
    Optional<FileEntryRef> File,
#else
    const FileEntry *File,
#endif
    StringRef SearchPath, StringRef RelativePath,
#if CLANG_VERSION_MAJOR >= 19
    const Module *SuggestedModule, bool ModuleImported,
#else
    const Module *Imported,
#endif
    SrcMgr::CharacteristicKind FileType) {
#if CLANG_VERSION_MAJOR >= 15
  if (!File) {
    return;
  }
  Indexer->inclusionDirective(FileNameRange.getAsRange(),
                              &File->getFileEntry());
#else
  Indexer->inclusionDirective(FileNameRange.getAsRange(), File);
#endif
}

void PreprocessorHook::MacroDefined(const Token &Tok,
                                    const MacroDirective *Md) {
  Indexer->macroDefined(Tok, Md);
}

void PreprocessorHook::MacroExpands(const Token &Tok, const MacroDefinition &Md,
                                    SourceRange Range, const MacroArgs *Ma) {
  Indexer->beginMacroExpansion(Tok, Md.getMacroInfo(), Range);
}

void PreprocessorHook::MacroUndefined(const Token &Tok,
                                      const MacroDefinition &Md,
                                      const MacroDirective *Undef) {
  Indexer->macroUsed(Tok, Md.getMacroInfo());
}

void PreprocessorHook::Defined(const Token &Tok, const MacroDefinition &Md,
                               SourceRange Range) {
  Indexer->macroUsed(Tok, Md.getMacroInfo());
}

void PreprocessorHook::Ifdef(SourceLocation Loc, const Token &Tok,
                             const MacroDefinition &Md) {
  Indexer->macroUsed(Tok, Md.getMacroInfo());
}

void PreprocessorHook::Ifndef(SourceLocation Loc, const Token &Tok,
                              const MacroDefinition &Md) {
  Indexer->macroUsed(Tok, Md.getMacroInfo());
}

class IndexAction : public PluginASTAction {
protected:
  std::unique_ptr<ASTConsumer> CreateASTConsumer(CompilerInstance &CI,
                                                 llvm::StringRef F) {
    return make_unique<IndexConsumer>(CI);
  }

  bool ParseArgs(const CompilerInstance &CI,
                 const std::vector<std::string> &Args) {
    if (Args.size() != 3) {
      DiagnosticsEngine &D = CI.getDiagnostics();
      unsigned DiagID = D.getCustomDiagID(
          DiagnosticsEngine::Error,
          "Need arguments for the source, output, and object directories");
      D.Report(DiagID);
      return false;
    }

    // Load our directories
    Srcdir = getAbsolutePath(Args[0]);
    if (Srcdir.empty()) {
      DiagnosticsEngine &D = CI.getDiagnostics();
      unsigned DiagID = D.getCustomDiagID(
          DiagnosticsEngine::Error, "Source directory '%0' does not exist");
      D.Report(DiagID) << Args[0];
      return false;
    }

    ensurePath(Args[1] + PATHSEP_STRING);
    Outdir = getAbsolutePath(Args[1]);
    Outdir += PATHSEP_STRING;

    Objdir = getAbsolutePath(Args[2]);
    if (Objdir.empty()) {
      DiagnosticsEngine &D = CI.getDiagnostics();
      unsigned DiagID = D.getCustomDiagID(DiagnosticsEngine::Error,
                                          "Objdir '%0' does not exist");
      D.Report(DiagID) << Args[2];
      return false;
    }
    Objdir += PATHSEP_STRING;

    printf("MOZSEARCH: %s %s %s\n", Srcdir.c_str(), Outdir.c_str(),
           Objdir.c_str());

    return true;
  }

  void printHelp(llvm::raw_ostream &Ros) {
    Ros << "Help for mozsearch plugin goes here\n";
  }
};

static FrontendPluginRegistry::Add<IndexAction>
    Y("mozsearch-index", "create the mozsearch index database");

```

## clang-plugin/Makefile
```
CC = clang
CXX = clang++
LLVM_CONFIG ?= llvm-config
LLVM_LDFLAGS := $(shell ${LLVM_CONFIG} --ldflags)
CXXFLAGS := $(shell ${LLVM_CONFIG} --cxxflags) -fPIC -Wall -Wno-strict-aliasing \
	$(if $(DEBUG),-O0 -g)
LDFLAGS := -fPIC -g -Wl,-R -Wl,'$$ORIGIN' $(LLVM_LDFLAGS) -shared

build: libclang-index-plugin.so

build_with_version_check: clang-version-guard build

clang-version-guard:
	@$(CXX) --version > clang-version.cache.tmp
	@if [ -f clang-version.cache ]; then \
		if ! diff -q clang-version.cache clang-version.cache.tmp; then \
			echo "Clang version mismatch is detected."; \
			echo "Rebuilding from scratch..."; \
			mv clang-version.cache.tmp clang-version.cache; \
			make clean; \
		else \
			rm clang-version.cache.tmp; \
		fi; \
	else \
		echo "Previous clang version is not found."; \
		echo "Rebuilding from scratch..."; \
		mv clang-version.cache.tmp clang-version.cache; \
		make clean; \
	fi

%.o: %.cpp
	$(CXX) $(CXXFLAGS) -c $^ -o $@

from-clangd/%.o: from-clangd/%.cpp
	mkdir -p from-clangd
	$(CXX) $(CXXFLAGS) -c $^ -o $@

libclang-index-plugin.so: FileOperations.o StringOperations.o BindingOperations.o MozsearchIndexer.o from-clangd/HeuristicResolver.o
	$(CXX) $(LDFLAGS) $^ -o $@ -lclangASTMatchers

check: build
	which clang
	which clang++

clean:
	$(RM) *.o from-clangd/*.o *.dwo libclang-index-plugin.so

dump: build
	-clang++ -c -Xclang -ast-dump testfiles/test.cpp

# When NFS is used on linux under libvirt, the file-locking is a real problem,
# so let's not use an immediate subdirectory of this dir.
ANALYSIS_DIR := /tmp/searchfox-analysis
analyze: build
	@if [ ! -d objdir ]; then mkdir objdir; fi
	-mkdir -p $(ANALYSIS_DIR)
	-$(RM) $(ANALYSIS_DIR)/*
	$(CXX) -c \
		-Xclang -load                       -Xclang ./libclang-index-plugin.so \
		-Xclang -add-plugin                 -Xclang mozsearch-index \
		-Xclang -plugin-arg-mozsearch-index -Xclang testfiles/ \
		-Xclang -plugin-arg-mozsearch-index -Xclang $(ANALYSIS_DIR) \
		-Xclang -plugin-arg-mozsearch-index -Xclang objdir/ \
		testfiles/test.cpp
	@echo "-------------------------------------------"
	@echo "Analysis output is in the ${ANALYSIS_DIR}/ folder"
	@echo "-------------------------------------------"
	@echo "maybe type:"
	@echo "vim ${ANALYSIS_DIR}/test.cpp"

.PHONY: build clean clang-version-guard

```

## clang-plugin/FileOperations.h
```
/* -*- Mode: C++; tab-width: 2; indent-tabs-mode: nil; c-basic-offset: 2 -*- */
/* This Source Code Form is subject to the terms of the Mozilla Public
 * License, v. 2.0. If a copy of the MPL was not distributed with this
 * file, You can obtain one at http://mozilla.org/MPL/2.0/. */

#ifndef FileOperations_h
#define FileOperations_h

#include <stdio.h>
#include <string>

#if defined(_WIN32) || defined(_WIN64)
#include <windows.h>
#define PATHSEP_CHAR '\\'
#define PATHSEP_STRING "\\"
#else
#define PATHSEP_CHAR '/'
#define PATHSEP_STRING "/"
#endif

// Make sure that all directories on path exist, excluding the final element of
// the path.
void ensurePath(std::string Path);

std::string getAbsolutePath(const std::string &Filename);

// Used to synchronize access when writing to an analysis file, so that
// concurrently running clang instances don't clobber each other's data.
// On Windows, we use a named mutex. On POSIX platforms, we use flock on the
// source files. flock is advisory locking, and doesn't interfere with clang's
// own opening of the source files (i.e. to interfere, clang would have to be
// using flock itself, which it does not).
struct AutoLockFile {
  // Absolute path to the analysis file
  std::string Filename;

#if defined(_WIN32) || defined(_WIN64)
  // Handle for the named Mutex
  HANDLE Handle = NULL;
#else
  // fd for the *source* file that corresponds to the analysis file. We use
  // the source file because it doesn't change while the analysis file gets
  // repeatedly replaced by a new version written to a separate tmp file.
  // This fd is used when using flock to synchronize access.
  int FileDescriptor = -1;
#endif

  // SrcFile should be the absolute path to the source code file, and DstFile
  // the absolute path to the corresponding analysis file. This constructor
  // will block until exclusive access has been obtained.
  AutoLockFile(const std::string &SrcFile, const std::string &DstFile);
  ~AutoLockFile();

  // Check after constructing to ensure the mutex was properly set up.
  bool success();

  // There used to be an `openFile` method here but we switched to directly
  // using a std::ifstream for the input file in able to take advantage of its
  // support for variable length lines (as opposed to fgets which takes a fixed
  // size buffer).

  // Open a new tmp file for writing the new analysis data to. Caller is
  // responsible for fclose'ing it.
  FILE *openTmp();
  // Replace the existing analysis file with the new "tmp" one that has the new
  // data. Returns false on error.
  bool moveTmp();
};

#endif

```

## clang-plugin/testfiles/xyz/test.cpp
```
namespace NS {

struct R {
  virtual void v() = 0;
};

struct S : public R {
  S();
  ~S();
  void m();
  void m(int);
  virtual void v();
};

struct S2 {
  virtual void v() = 0;
};

struct T : public S, public S2 {
  virtual void v();
  void m();
  void m(int);
};

void f() {}
void g();

} // namespace NS

int main() {
  NS::f();
  NS::g();
  NS::S s;
  s.m();
  s.m(4);

  return 0;
}

```

## clang-plugin/testfiles/test.cpp
```
#include <stdio.h>

namespace NS {

struct R;

enum { TAG3 };

typedef struct {
  int f;

  bool operator()(int);
} Abc;

struct R {
  enum XYZ { TAG1, TAG2 };

  virtual void v() = 0;
};

struct S : public R {
  S();
  ~S();
  void m();
  void m(int);
  virtual void v();
};

namespace {
int xyz;
};

struct S2 {
  virtual void v() = 0;
};

struct T : public S, public S2 {
  virtual void v();
  void m();
  void m(int);

  int field;
};

template <typename T> struct OtherObj {
  OtherObj(char c) {}
};

template <typename T> struct StackObj {
  StackObj(int x) : mOther('x') {}

  OtherObj<T> mOther;
};

void f() {}
void g();

typedef R OtherR;

template <typename T> class X {
public:
  X() {}

  void f();

  int field;
};

template <typename T> void X<T>::f() {}

template <> void X<int>::f() {}

template <typename T> void templateFunc(const T &arg);

template <> void templateFunc(const char &arg);

struct Dummy {
#define DECL_SOMETHING(Env, Name)                                              \
  static bool Name() { return Env; }

  DECL_SOMETHING(true, Hello);
  DECL_SOMETHING(false, Goodbye);
};
} // namespace NS

#define HELLO s.m

class Q {
  typedef int (Q::*AddressReader)(const char *) const;
};

extern int GLOBAL;

int main() {
  GLOBAL = NS::TAG3;

  NS::OtherR *otherr;

  NS::f();
  NS::g();
  NS::S s;
  s.m();
  HELLO(4);

#ifdef HELLO
  int abc;
#endif

#if defined(HELLO)
  int abc1;
#endif

#undef HELLO

  void (*fp)();
  fp = &NS::f;
  fp();

  NS::S *sp = new NS::S();

  NS::X<char> xx;
  xx.f();
  xx.field = 12;

  NS::X<int> xy;
  xy.f();

  NS::templateFunc(47);

  NS::templateFunc('c');

  NS::Dummy::Hello();

  NS::StackObj<int> stackobj(10);

  return 0;
}

```

## clang-plugin/.clang-format
```
BasedOnStyle: LLVM

```

## clang-plugin/StringOperations.h
```
/* -*- Mode: C++; tab-width: 2; indent-tabs-mode: nil; c-basic-offset: 2 -*- */
/* This Source Code Form is subject to the terms of the Mozilla Public
 * License, v. 2.0. If a copy of the MPL was not distributed with this
 * file, You can obtain one at http://mozilla.org/MPL/2.0/. */

#ifndef StringOperations_h
#define StringOperations_h

#include <memory>
#include <string.h>
#include <string>

std::string hash(const std::string &Str);

template <typename... Args>
inline std::string stringFormat(const std::string &Format, Args... ArgList) {
  size_t Len = snprintf(nullptr, 0, Format.c_str(), ArgList...);
  std::unique_ptr<char[]> Buf(new char[Len + 1]);
  snprintf(Buf.get(), Len + 1, Format.c_str(), ArgList...);
  return std::string(Buf.get(), Buf.get() + Len);
}

std::string toString(int N);

#endif

```

## clang-plugin/BindingOperations.cpp
```
/* -*- Mode: C++; tab-width: 2; indent-tabs-mode: nil; c-basic-offset: 2 -*- */
/* This Source Code Form is subject to the terms of the Mozilla Public
 * License, v. 2.0. If a copy of the MPL was not distributed with this
 * file, You can obtain one at http://mozilla.org/MPL/2.0/. */

#include "BindingOperations.h"

#include <clang/AST/Attr.h>
#include <clang/AST/Expr.h>
#include <clang/AST/RecursiveASTVisitor.h>
#include <clang/Basic/Version.h>

#include <algorithm>
#include <array>
#include <cuchar>
#include <set>
#include <string>
#include <unordered_map>
#include <vector>

#ifdef __cpp_lib_optional
#include <optional>
template <typename T> using optional = std::optional<T>;
#else
#include <llvm/ADT/Optional.h>
template <typename T> using optional = clang::Optional<T>;
#endif

using namespace clang;

namespace {

template <typename InputIt>
bool hasReverseQualifiedName(InputIt first, InputIt last,
                             const NamedDecl &tag) {
  const NamedDecl *currentDecl = &tag;
  InputIt currentName;
  for (currentName = first; currentName != last; currentName++) {
    if (!currentDecl || !currentDecl->getIdentifier() ||
        currentDecl->getName() != *currentName)
      return false;

    currentDecl = dyn_cast<NamedDecl>(currentDecl->getDeclContext());
  }
  if (currentName != last)
    return false;

  if (currentDecl != nullptr)
    return false;

  return true;
}

bool isMozillaJniObjectBase(const CXXRecordDecl &klass) {
  const auto qualifiedName =
      std::array<StringRef, 3>{"mozilla", "jni", "ObjectBase"};
  return hasReverseQualifiedName(qualifiedName.crbegin(), qualifiedName.crend(),
                                 klass);
}

bool isMozillaJniNativeImpl(const CXXRecordDecl &klass) {
  const auto qualifiedName =
      std::array<StringRef, 3>{"mozilla", "jni", "NativeImpl"};
  return hasReverseQualifiedName(qualifiedName.crbegin(), qualifiedName.crend(),
                                 klass);
}

const NamedDecl *fieldNamed(StringRef name, const RecordDecl &strukt) {
  for (const auto *decl : strukt.decls()) {
    const auto *namedDecl = dyn_cast<VarDecl>(decl);
    if (!namedDecl)
      continue;

    if (!namedDecl->getIdentifier() || namedDecl->getName() != name)
      continue;

    return namedDecl;
  }

  return {};
}

optional<StringRef> nameFieldValue(const RecordDecl &strukt) {
  const auto *nameField = dyn_cast_or_null<VarDecl>(fieldNamed("name", strukt));
  if (!nameField)
    return {};

  const auto *def = nameField->getDefinition();
  if (!def)
    return {};

  const auto *name = dyn_cast_or_null<StringLiteral>(def->getInit());
  if (!name)
    return {};

  return name->getString();
}

struct AbstractBinding {
  // Subset of tools/analysis/BindingSlotLang
  enum class Lang {
    Cpp,
    Jvm,
  };
  static constexpr size_t LangLength = 2;
  static constexpr std::array<StringRef, LangLength> langNames = {
      "cpp",
      "jvm",
  };

  static optional<Lang> langFromString(StringRef langName) {
    const auto it = std::find(langNames.begin(), langNames.end(), langName);
    if (it == langNames.end())
      return {};

    return Lang(it - langNames.begin());
  }
  static StringRef stringFromLang(Lang lang) { return langNames[size_t(lang)]; }

  // Subset of tools/analysis/BindingSlotKind
  enum class Kind {
    Class,
    Method,
    Getter,
    Setter,
    Const,
  };
  static constexpr size_t KindLength = 5;
  static constexpr std::array<StringRef, KindLength> kindNames = {
      "class", "method", "getter", "setter", "const",
  };

  static optional<Kind> kindFromString(StringRef kindName) {
    const auto it = std::find(kindNames.begin(), kindNames.end(), kindName);
    if (it == kindNames.end())
      return {};

    return Kind(it - kindNames.begin());
  }
  static StringRef stringFromKind(Kind kind) { return kindNames[size_t(kind)]; }

  Lang lang;
  Kind kind;
  StringRef symbol;
};
constexpr size_t AbstractBinding::KindLength;
constexpr std::array<StringRef, AbstractBinding::KindLength>
    AbstractBinding::kindNames;
constexpr size_t AbstractBinding::LangLength;
constexpr std::array<StringRef, AbstractBinding::LangLength>
    AbstractBinding::langNames;

struct BindingTo : public AbstractBinding {
  BindingTo(AbstractBinding b) : AbstractBinding(std::move(b)) {}
  static constexpr StringRef ANNOTATION = "binding_to";
};
constexpr StringRef BindingTo::ANNOTATION;

struct BoundAs : public AbstractBinding {
  BoundAs(AbstractBinding b) : AbstractBinding(std::move(b)) {}
  static constexpr StringRef ANNOTATION = "bound_as";
};
constexpr StringRef BoundAs::ANNOTATION;

template <typename B>
void setBindingAttr(ASTContext &C, Decl &decl, B binding) {
#if CLANG_VERSION_MAJOR >= 18
  auto utf8 = StringLiteralKind::UTF8;
#else
  auto utf8 = StringLiteral::UTF8;
#endif
  // recent LLVM: CreateImplicit then setDelayedArgs
  Expr *langExpr = StringLiteral::Create(
      C, AbstractBinding::stringFromLang(binding.lang), utf8, false, {}, {});
  Expr *kindExpr = StringLiteral::Create(
      C, AbstractBinding::stringFromKind(binding.kind), utf8, false, {}, {});
  Expr *symbolExpr =
      StringLiteral::Create(C, binding.symbol, utf8, false, {}, {});
  auto **args = new (C, 16) Expr *[3]{langExpr, kindExpr, symbolExpr};
  auto *attr = AnnotateAttr::CreateImplicit(C, B::ANNOTATION, args, 3);
  decl.addAttr(attr);
}

optional<AbstractBinding> readBinding(const AnnotateAttr &attr) {
  if (attr.args_size() != 3)
    return {};

  const auto *langExpr = attr.args().begin()[0];
  const auto *kindExpr = attr.args().begin()[1];
  const auto *symbolExpr = attr.args().begin()[2];
  if (!langExpr || !kindExpr || !symbolExpr)
    return {};

  const auto *langName =
      dyn_cast<StringLiteral>(langExpr->IgnoreUnlessSpelledInSource());
  const auto *kindName =
      dyn_cast<StringLiteral>(kindExpr->IgnoreUnlessSpelledInSource());
  const auto *symbol =
      dyn_cast<StringLiteral>(symbolExpr->IgnoreUnlessSpelledInSource());
  if (!langName || !kindName || !symbol)
    return {};

  const auto lang = AbstractBinding::langFromString(langName->getString());
  const auto kind = AbstractBinding::kindFromString(kindName->getString());

  if (!lang || !kind)
    return {};

  return AbstractBinding{
      .lang = *lang,
      .kind = *kind,
      .symbol = symbol->getString(),
  };
}

optional<BindingTo> getBindingTo(const Decl &decl) {
  for (const auto *attr : decl.specific_attrs<AnnotateAttr>()) {
    if (attr->getAnnotation() != BindingTo::ANNOTATION)
      continue;

    const auto binding = readBinding(*attr);
    if (!binding)
      continue;

    return BindingTo{*binding};
  }
  return {};
}

// C++23: turn into generator
std::vector<BoundAs> getBoundAs(const Decl &decl) {
  std::vector<BoundAs> found;

  for (const auto *attr : decl.specific_attrs<AnnotateAttr>()) {
    if (attr->getAnnotation() != BoundAs::ANNOTATION)
      continue;

    const auto binding = readBinding(*attr);
    if (!binding)
      continue;

    found.push_back(BoundAs{*binding});
  }

  return found;
}

class FindCallCall : private RecursiveASTVisitor<FindCallCall> {
public:
  struct Result {
    using Kind = AbstractBinding::Kind;

    Kind kind;
    StringRef name;
  };

  static optional<Result> search(Stmt *statement) {
    FindCallCall finder;
    finder.TraverseStmt(statement);
    return finder.result;
  }

private:
  optional<Result> result;

  friend RecursiveASTVisitor<FindCallCall>;

  optional<Result> tryParseCallCall(CallExpr *callExpr) {
    const auto *callee =
        dyn_cast_or_null<CXXMethodDecl>(callExpr->getDirectCallee());
    if (!callee)
      return {};

    if (!callee->getIdentifier())
      return {};

    const auto action = callee->getIdentifier()->getName();

    if (action != "Call" && action != "Get" && action != "Set")
      return {};

    const auto *parentClass =
        dyn_cast_or_null<ClassTemplateSpecializationDecl>(callee->getParent());

    if (!parentClass)
      return {};

    const auto *parentTemplate = parentClass->getTemplateInstantiationPattern();

    if (!parentTemplate || !parentTemplate->getIdentifier())
      return {};

    const auto parentName = parentTemplate->getIdentifier()->getName();

    AbstractBinding::Kind kind;
    if (action == "Call") {
      if (parentName == "Constructor" || parentName == "Method") {
        kind = AbstractBinding::Kind::Method;
      } else {
        return {};
      }
    } else if (parentName == "Field") {
      if (action == "Get") {
        kind = AbstractBinding::Kind::Getter;
      } else if (action == "Set") {
        kind = AbstractBinding::Kind::Setter;
      } else {
        return {};
      }
    } else {
      return {};
    }

    const auto *templateArg =
        parentClass->getTemplateArgs().get(0).getAsType()->getAsRecordDecl();

    if (!templateArg)
      return {};

    const auto name = nameFieldValue(*templateArg);
    if (!name)
      return {};

    return Result{
        .kind = kind,
        .name = *name,
    };

    return {};
  }
  bool VisitCallExpr(CallExpr *callExpr) {
    return !(result = tryParseCallCall(callExpr));
  }
};

constexpr StringRef JVM_SCIP_SYMBOL_PREFIX = "S_jvm_";

std::string javaScipSymbol(StringRef prefix, StringRef name,
                           AbstractBinding::Kind kind) {
  auto symbol = (prefix + name).str();

  switch (kind) {
  case AbstractBinding::Kind::Class:
    std::replace(symbol.begin(), symbol.end(), '$', '#');
    symbol += "#";
    break;
  case AbstractBinding::Kind::Method:
    symbol += "().";
    break;
  case AbstractBinding::Kind::Const:
  case AbstractBinding::Kind::Getter:
  case AbstractBinding::Kind::Setter:
    symbol += ".";
    break;
  }

  return symbol;
}

void addSlotOwnerAttribute(llvm::json::OStream &J, const Decl &decl) {
  if (const auto bindingTo = getBindingTo(decl)) {
    J.attributeBegin("slotOwner");
    J.objectBegin();
    J.attribute("slotKind", AbstractBinding::stringFromKind(bindingTo->kind));
    J.attribute("slotLang", "cpp");
    J.attribute("ownerLang", AbstractBinding::stringFromLang(bindingTo->lang));
    J.attribute("sym", bindingTo->symbol);
    J.objectEnd();
    J.attributeEnd();
  }
}
void addBindingSlotsAttribute(llvm::json::OStream &J, const Decl &decl) {
  const auto allBoundAs = getBoundAs(decl);
  if (!allBoundAs.empty()) {
    J.attributeBegin("bindingSlots");
    J.arrayBegin();
    for (const auto boundAs : allBoundAs) {
      J.objectBegin();
      J.attribute("slotKind", AbstractBinding::stringFromKind(boundAs.kind));
      J.attribute("slotLang", AbstractBinding::stringFromLang(boundAs.lang));
      J.attribute("ownerLang", "cpp");
      J.attribute("sym", boundAs.symbol);
      J.objectEnd();
    }
    J.arrayEnd();
    J.attributeEnd();
  }
}

// The mangling scheme is documented at
// https://docs.oracle.com/javase/8/docs/technotes/guides/jni/spec/design.html
// The main takeaways are:
// - _0xxxx is the utf16 code unit xxxx
// - _1 is _
// - _2 is ;
// - _3 is [
// - __ is the separator between function name and overload specification
// - _ is otherwise the separator between packages/classes/methods
//
// This method takes a StringRef & and mutates it and can be called twice on a
// Jnicall function name to get
//  first the demangled name
//  second the demangled overload specification
// But we don't use the later for now because we have no way to map that to how
// SCIP resolves overloads.
optional<std::string> demangleJnicallPart(StringRef &remainder) {
  std::string demangled;

  std::mbstate_t ps = {};

  while (!remainder.empty()) {
    switch (remainder[0]) {
    case '0': {
      remainder = remainder.drop_front(1);

      uint16_t codeUnit;
      const auto ok = remainder.substr(1, 4).getAsInteger(16, codeUnit);
      remainder = remainder.drop_front(4);

      if (!ok) // failed reading xxxx as hexadecimal from _0xxxx
        return {};

      std::array<char, MB_LEN_MAX> codePoint;
      const auto mbLen = std::c16rtomb(codePoint.data(), codeUnit, &ps);

      if (mbLen == -1) // failed converting utf16 to utf8
        return {};

      demangled += StringRef(codePoint.begin(), mbLen);
      break;
    }
    case '1':
      remainder = remainder.drop_front(1);
      ps = {};
      demangled += '_';
      break;
    case '2':
      remainder = remainder.drop_front(1);
      ps = {};
      demangled += ';';
      break;
    case '3':
      remainder = remainder.drop_front(1);
      ps = {};
      demangled += '[';
      break;
    case '_':
      remainder = remainder.drop_front(1);
      ps = {};
      if (remainder.empty()) // the string ends with _
        return {};

      switch (remainder[0]) {
      case '0':
      case '1':
      case '2':
      case '3':
        demangled += '.';
        break;
      default:
        // either:
        // * the string began with _[^0-3], which is not supposed to happen; or
        // * we reached __[^0-3] meaning we finished the first part of the name
        // and remainder holds the overload specification
        return demangled;
      }
    default:
      ps = {};
      demangled += '.';
      break;
    }
    StringRef token;
    std::tie(token, remainder) = remainder.split('_');
    demangled += token;
  }

  return demangled;
}

optional<std::string>
scipSymbolFromJnicallFunctionName(StringRef functionName) {
  if (!functionName.consume_front("Java_"))
    return {};

  const auto demangledName = demangleJnicallPart(functionName);

  if (!demangledName || demangledName->empty())
    return {};

  // demangleJavaName returns something like
  // .some.package.Class$InnerClass.method
  // - prepend S_jvm_
  // - remove the leading dot
  // - replace the last dot with a #
  // - replace the other dots with /
  // - replace $ with #
  // - add the ([+overloadNumber]). suffix
  auto symbol = JVM_SCIP_SYMBOL_PREFIX.str();
  symbol += demangledName->substr(1);
  const auto lastDot = symbol.rfind('.');
  if (lastDot != std::string::npos)
    symbol[lastDot] = '#';
  std::replace(symbol.begin(), symbol.end(), '.', '/');
  std::replace(symbol.begin(), symbol.end(), '$', '#');

  // Keep track of how many times we have seen this method, to build the
  // ([+overloadNumber]). suffix. This assumes this function is called on C
  // function definitions in the same order the matching overloads are declared
  // in Java.
  static std::unordered_map<std::string, uint> jnicallFunctions;
  auto &overloadNumber = jnicallFunctions[symbol];

  symbol += '(';
  if (overloadNumber) {
    symbol += '+';
    symbol += overloadNumber;
    overloadNumber++;
  }
  symbol += ").";

  return symbol;
};

} // anonymous namespace

// class [wrapper] : public mozilla::jni::ObjectBase<[wrapper]>
// {
//   static constexpr char name[] = "[nameFieldValue]";
// }
void findBindingToJavaClass(ASTContext &C, CXXRecordDecl &klass) {
  for (const auto &baseSpecifier : klass.bases()) {
    const auto *base = baseSpecifier.getType()->getAsCXXRecordDecl();
    if (!base)
      continue;

    if (!isMozillaJniObjectBase(*base))
      continue;

    const auto name = nameFieldValue(klass);
    if (!name)
      continue;

    const auto symbol =
        javaScipSymbol(JVM_SCIP_SYMBOL_PREFIX, *name, BindingTo::Kind::Class);
    const auto binding = BindingTo{{
        .lang = BindingTo::Lang::Jvm,
        .kind = BindingTo::Kind::Class,
        .symbol = symbol,
    }};

    setBindingAttr(C, klass, binding);
    return;
  }
}

// When a Java method is marked as native, the JRE looks by default for a
// function named Java_<mangled method name>[__<mangled overload
// specification>].
void findBindingToJavaFunction(ASTContext &C, FunctionDecl &function) {
  const auto *identifier = function.getIdentifier();
  if (!identifier)
    return;

  const auto name = identifier->getName();
  const auto symbol = scipSymbolFromJnicallFunctionName(name);
  if (!symbol)
    return;

  const auto binding = BoundAs{{
      .lang = BindingTo::Lang::Jvm,
      .kind = BindingTo::Kind::Method,
      .symbol = *symbol,
  }};

  setBindingAttr(C, function, binding);
}

// class [parent]
// {
//   struct [methodStruct] {
//     static constexpr char name[] = "[methodNameFieldValue]";
//   }
//   [method]
//   {
//     ...
//     mozilla::jni::{Method,Constructor,Field}<[methodStruct]>::{Call,Get,Set}(...)
//     ...
//   }
// }
void findBindingToJavaMember(ASTContext &C, CXXMethodDecl &method) {
  const auto *parent = method.getParent();
  if (!parent)
    return;
  const auto classBinding = getBindingTo(*parent);
  if (!classBinding)
    return;

  auto *body = method.getBody();
  if (!body)
    return;

  const auto found = FindCallCall::search(body);
  if (!found)
    return;

  const auto symbol =
      javaScipSymbol(classBinding->symbol, found->name, found->kind);
  const auto binding = BindingTo{{
      .lang = BindingTo::Lang::Jvm,
      .kind = found->kind,
      .symbol = symbol,
  }};

  setBindingAttr(C, method, binding);
}

// class [parent]
// {
//   struct [methodStruct] {
//     static constexpr char name[] = "[methodNameFieldValue]";
//   }
//   [method]
//   {
//     ...
//     mozilla::jni::{Method,Constructor,Field}<[methodStruct]>::{Call,Get,Set}(...)
//     ...
//   }
// }
void findBindingToJavaConstant(ASTContext &C, VarDecl &field) {
  const auto *parent = dyn_cast_or_null<CXXRecordDecl>(field.getDeclContext());
  if (!parent)
    return;

  const auto classBinding = getBindingTo(*parent);
  if (!classBinding)
    return;

  const auto symbol = javaScipSymbol(classBinding->symbol, field.getName(),
                                     BindingTo::Kind::Const);
  const auto binding = BindingTo{{
      .lang = BindingTo::Lang::Jvm,
      .kind = BindingTo::Kind::Const,
      .symbol = symbol,
  }};

  setBindingAttr(C, field, binding);
}

// class [klass] : public [wrapper]::Natives<[klass]> {...}
// class [wrapper] : public mozilla::jni::ObjectBase<[wrapper]>
// {
//   static constexpr char name[] = "[nameFieldValue]";
//
//   struct [methodStruct] {
//     static constexpr char name[] = "[methodNameFieldValue]";
//   }
//
//   template<typename T>
//   class [wrapper]::Natives : public mozilla::jni::NativeImpl<[wrapper], T> {
//     static const JNINativeMethod methods[] = {
//       mozilla::jni::MakeNativeMethod<[wrapper]::[methodStruct]>(
//               mozilla::jni::NativeStub<[wrapper]::[methodStruct], Impl>
//               ::template Wrap<&Impl::[method]>),
//     }
//   }
// }
void findBoundAsJavaClasses(ASTContext &C, CXXRecordDecl &klass) {
  for (const auto &baseSpecifier : klass.bases()) {
    const auto *base = baseSpecifier.getType()->getAsCXXRecordDecl();
    if (!base)
      continue;

    for (const auto &baseBaseSpecifier : base->bases()) {
      const auto *baseBase = dyn_cast_or_null<ClassTemplateSpecializationDecl>(
          baseBaseSpecifier.getType()->getAsCXXRecordDecl());
      if (!baseBase)
        continue;

      if (!isMozillaJniNativeImpl(*baseBase))
        continue;

      const auto *wrapper =
          baseBase->getTemplateArgs().get(0).getAsType()->getAsCXXRecordDecl();

      if (!wrapper)
        continue;

      const auto name = nameFieldValue(*wrapper);
      if (!name)
        continue;

      const auto javaClassSymbol =
          javaScipSymbol(JVM_SCIP_SYMBOL_PREFIX, *name, BoundAs::Kind::Class);
      const auto classBinding = BoundAs{{
          .lang = BoundAs::Lang::Jvm,
          .kind = BoundAs::Kind::Class,
          .symbol = javaClassSymbol,
      }};
      setBindingAttr(C, klass, classBinding);

      const auto *methodsDecl =
          dyn_cast_or_null<VarDecl>(fieldNamed("methods", *base));
      if (!methodsDecl)
        continue;

      const auto *methodsDef = methodsDecl->getDefinition();
      if (!methodsDef)
        continue;

      const auto *inits = dyn_cast_or_null<InitListExpr>(methodsDef->getInit());
      if (!inits)
        continue;

      std::set<const CXXMethodDecl *> alreadyBound;

      for (const auto *init : inits->inits()) {
        const auto *call =
            dyn_cast<CallExpr>(init->IgnoreUnlessSpelledInSource());
        if (!call)
          continue;

        const auto *funcDecl = call->getDirectCallee();
        if (!funcDecl)
          continue;

        const auto *templateArgs = funcDecl->getTemplateSpecializationArgs();
        if (!templateArgs)
          continue;

        const auto *strukt = dyn_cast_or_null<RecordDecl>(
            templateArgs->get(0).getAsType()->getAsRecordDecl());
        if (!strukt)
          continue;

        const auto *wrapperRef = dyn_cast_or_null<DeclRefExpr>(
            call->getArg(0)->IgnoreUnlessSpelledInSource());
        if (!wrapperRef)
          continue;

        const auto *boundRef = dyn_cast_or_null<UnaryOperator>(
            wrapperRef->template_arguments().front().getArgument().getAsExpr());
        if (!boundRef)
          continue;

        auto addToBound = [&](CXXMethodDecl &boundDecl, uint overloadNum) {
          const auto methodName = nameFieldValue(*strukt);
          if (!methodName)
            return;

          auto javaMethodSymbol = javaClassSymbol;
          javaMethodSymbol += *methodName;
          javaMethodSymbol += '(';
          if (overloadNum > 0) {
            javaMethodSymbol += '+';
            javaMethodSymbol += std::to_string(overloadNum);
          }
          javaMethodSymbol += ").";

          const auto binding = BoundAs{{
              .lang = BoundAs::Lang::Jvm,
              .kind = BoundAs::Kind::Method,
              .symbol = javaMethodSymbol,
          }};
          setBindingAttr(C, boundDecl, binding);
        };

        if (auto *bound =
                dyn_cast_or_null<DeclRefExpr>(boundRef->getSubExpr())) {
          auto *method = dyn_cast_or_null<CXXMethodDecl>(bound->getDecl());
          if (!method)
            continue;
          addToBound(*method, 0);
        } else if (const auto *bound = dyn_cast_or_null<UnresolvedLookupExpr>(
                       boundRef->getSubExpr())) {
          // XXX This is hackish
          // In case of overloads it's not obvious which one we should use
          // this expects the declaration order between C++ and Java to match
          auto declarations =
              std::vector<Decl *>(bound->decls_begin(), bound->decls_end());
          auto byLocation = [](Decl *a, Decl *b) {
            return a->getLocation() < b->getLocation();
          };
          std::sort(declarations.begin(), declarations.end(), byLocation);

          uint i = 0;
          for (auto *decl : declarations) {
            auto *method = dyn_cast<CXXMethodDecl>(decl);
            if (!method)
              continue;
            if (alreadyBound.find(method) == alreadyBound.end()) {
              addToBound(*method, i);
              alreadyBound.insert(method);
              break;
            }
            i++;
          }
        }
      }
    }
  }
}

void emitBindingAttributes(llvm::json::OStream &J, const Decl &decl) {
  addSlotOwnerAttribute(J, decl);
  addBindingSlotsAttribute(J, decl);
}

```

## clang-plugin/FileOperations.cpp
```
/* -*- Mode: C++; tab-width: 2; indent-tabs-mode: nil; c-basic-offset: 2 -*- */
/* This Source Code Form is subject to the terms of the Mozilla Public
 * License, v. 2.0. If a copy of the MPL was not distributed with this
 * file, You can obtain one at http://mozilla.org/MPL/2.0/. */

#include "FileOperations.h"

#include <stdio.h>
#include <stdlib.h>

#if defined(_WIN32) || defined(_WIN64)
#include "StringOperations.h"
#include <direct.h>
#include <io.h>
#include <windows.h>
#else
#include <sys/file.h>
#include <sys/time.h>
#include <unistd.h>
#endif

#include <fcntl.h>
#include <sys/stat.h>
#include <sys/types.h>

// Make sure that all directories on path exist, excluding the final element of
// the path.
void ensurePath(std::string Path) {
  size_t Pos = 0;
  if (Path[0] == PATHSEP_CHAR) {
    Pos++;
  }

  while ((Pos = Path.find(PATHSEP_CHAR, Pos)) != std::string::npos) {
    std::string Portion = Path.substr(0, Pos);
    if (!Portion.empty()) {
#if defined(_WIN32) || defined(_WIN64)
      int Err = _mkdir(Portion.c_str());
#else
      int Err = mkdir(Portion.c_str(), 0775);
#endif
      if (Err == -1 && errno != EEXIST) {
        perror("mkdir failed");
        exit(1);
      }
    }

    Pos++;
  }
}

#if defined(_WIN32) || defined(_WIN64)
AutoLockFile::AutoLockFile(const std::string &SrcFile,
                           const std::string &DstFile) {
  this->Filename = DstFile;
  std::string Hash = hash(SrcFile);
  std::string MutexName = std::string("Local\\searchfox-") + Hash;
  std::wstring WideMutexName;
  WideMutexName.assign(MutexName.begin(), MutexName.end());
  Handle = CreateMutex(nullptr, false, WideMutexName.c_str());
  if (Handle == NULL) {
    return;
  }

  if (WaitForSingleObject(Handle, INFINITE) != WAIT_OBJECT_0) {
    return;
  }
}

AutoLockFile::~AutoLockFile() {
  ReleaseMutex(Handle);
  CloseHandle(Handle);
}

bool AutoLockFile::success() { return Handle != NULL; }

FILE *AutoLockFile::openTmp() {
  int TmpDescriptor = _open((Filename + ".tmp").c_str(),
                            _O_WRONLY | _O_APPEND | _O_CREAT | _O_BINARY, 0666);
  return _fdopen(TmpDescriptor, "ab");
}

bool AutoLockFile::moveTmp() {
  if (_unlink(Filename.c_str()) == -1) {
    if (errno != ENOENT) {
      return false;
    }
  }
  return rename((Filename + ".tmp").c_str(), Filename.c_str()) == 0;
}

std::string getAbsolutePath(const std::string &Filename) {
  char Full[_MAX_PATH];
  if (!_fullpath(Full, Filename.c_str(), _MAX_PATH)) {
    return std::string("");
  }
  return std::string(Full);
}
#else
AutoLockFile::AutoLockFile(const std::string &SrcFile,
                           const std::string &DstFile) {
  this->Filename = DstFile;
  FileDescriptor = open(SrcFile.c_str(), O_RDONLY);
  if (FileDescriptor == -1) {
    return;
  }

  do {
    int rv = flock(FileDescriptor, LOCK_EX);
    if (rv == 0) {
      break;
    }
  } while (true);
}

AutoLockFile::~AutoLockFile() { close(FileDescriptor); }

bool AutoLockFile::success() { return FileDescriptor != -1; }

FILE *AutoLockFile::openTmp() {
  int TmpDescriptor =
      open((Filename + ".tmp").c_str(), O_WRONLY | O_APPEND | O_CREAT, 0666);
  return fdopen(TmpDescriptor, "ab");
}

bool AutoLockFile::moveTmp() {
  if (unlink(Filename.c_str()) == -1) {
    if (errno != ENOENT) {
      return false;
    }
  }
  return rename((Filename + ".tmp").c_str(), Filename.c_str()) == 0;
}

std::string getAbsolutePath(const std::string &Filename) {
  char Full[4096];
  if (!realpath(Filename.c_str(), Full)) {
    return std::string("");
  }
  return std::string(Full);
}
#endif

```

## clang-plugin/from-clangd/README.md
```
The facilities in this subdirectory are copied over from clangd
(https://clangd.llvm.org/).

The files here are currently copies of the following upstream files:
https://github.com/llvm/llvm-project/blob/2bcbcbefcd0f7432f99cc07bb47d1e1ecb579a3f/clang-tools-extra/clangd/HeuristicResolver.h
https://github.com/llvm/llvm-project/blob/2bcbcbefcd0f7432f99cc07bb47d1e1ecb579a3f/clang-tools-extra/clangd/HeuristicResolver.cpp

If, in the future, these facilities are moved from clangd to
to libclangTooling and exposed in the clang API headers, we can
switch to consuming them from there directly.

```

## clang-plugin/from-clangd/HeuristicResolver.h
```
//===--- HeuristicResolver.h - Resolution of dependent names -----*- C++-*-===//
//
// Part of the LLVM Project, under the Apache License v2.0 with LLVM Exceptions.
// See https://llvm.org/LICENSE.txt for license information.
// SPDX-License-Identifier: Apache-2.0 WITH LLVM-exception
//
//===----------------------------------------------------------------------===//

#ifndef LLVM_CLANG_TOOLS_EXTRA_CLANGD_HEURISTICRESOLVER_H
#define LLVM_CLANG_TOOLS_EXTRA_CLANGD_HEURISTICRESOLVER_H

#include "clang/AST/Decl.h"
#include <vector>

namespace clang {

class ASTContext;
class CallExpr;
class CXXBasePath;
class CXXDependentScopeMemberExpr;
class DeclarationName;
class DependentScopeDeclRefExpr;
class NamedDecl;
class Type;
class UnresolvedUsingValueDecl;

namespace clangd {

// This class heuristic resolution of declarations and types in template code.
//
// As a compiler, clang only needs to perform certain types of processing on
// template code (such as resolving dependent names to declarations, or
// resolving the type of a dependent expression) after instantiation. Indeed,
// C++ language features such as template specialization mean such resolution
// cannot be done accurately before instantiation
//
// However, template code is written and read in uninstantiated form, and clangd
// would like to provide editor features like go-to-definition in template code
// where possible. To this end, clangd attempts to resolve declarations and
// types in uninstantiated code by using heuristics, understanding that the
// results may not be fully accurate but that this is better than nothing.
//
// At this time, the heuristic used is a simple but effective one: assume that
// template instantiations are based on the primary template definition and not
// not a specialization. More advanced heuristics may be added in the future.
class HeuristicResolver {
public:
  HeuristicResolver(ASTContext &Ctx) : Ctx(Ctx) {}

  // Try to heuristically resolve certain types of expressions, declarations, or
  // types to one or more likely-referenced declarations.
  std::vector<const NamedDecl *>
  resolveMemberExpr(const CXXDependentScopeMemberExpr *ME) const;
  std::vector<const NamedDecl *>
  resolveDeclRefExpr(const DependentScopeDeclRefExpr *RE) const;
  std::vector<const NamedDecl *>
  resolveTypeOfCallExpr(const CallExpr *CE) const;
  std::vector<const NamedDecl *>
  resolveCalleeOfCallExpr(const CallExpr *CE) const;
  std::vector<const NamedDecl *>
  resolveUsingValueDecl(const UnresolvedUsingValueDecl *UUVD) const;
  std::vector<const NamedDecl *>
  resolveDependentNameType(const DependentNameType *DNT) const;
  std::vector<const NamedDecl *> resolveTemplateSpecializationType(
      const DependentTemplateSpecializationType *DTST) const;

  // Try to heuristically resolve a dependent nested name specifier
  // to the type it likely denotes. Note that *dependent* name specifiers always
  // denote types, not namespaces.
  const Type *
  resolveNestedNameSpecifierToType(const NestedNameSpecifier *NNS) const;

  // Given the type T of a dependent expression that appears of the LHS of a
  // "->", heuristically find a corresponding pointee type in whose scope we
  // could look up the name appearing on the RHS.
  const Type *getPointeeType(const Type *T) const;

private:
  ASTContext &Ctx;

  // Given a tag-decl type and a member name, heuristically resolve the
  // name to one or more declarations.
  // The current heuristic is simply to look up the name in the primary
  // template. This is a heuristic because the template could potentially
  // have specializations that declare different members.
  // Multiple declarations could be returned if the name is overloaded
  // (e.g. an overloaded method in the primary template).
  // This heuristic will give the desired answer in many cases, e.g.
  // for a call to vector<T>::size().
  std::vector<const NamedDecl *> resolveDependentMember(
      const Type *T, DeclarationName Name,
      llvm::function_ref<bool(const NamedDecl *ND)> Filter) const;

  // Try to heuristically resolve the type of a possibly-dependent expression
  // `E`.
  const Type *resolveExprToType(const Expr *E) const;
  std::vector<const NamedDecl *> resolveExprToDecls(const Expr *E) const;

  // Helper function for HeuristicResolver::resolveDependentMember()
  // which takes a possibly-dependent type `T` and heuristically
  // resolves it to a CXXRecordDecl in which we can try name lookup.
  CXXRecordDecl *resolveTypeToRecordDecl(const Type *T) const;

  // This is a reimplementation of CXXRecordDecl::lookupDependentName()
  // so that the implementation can call into other HeuristicResolver helpers.
  // FIXME: Once HeuristicResolver is upstreamed to the clang libraries
  // (https://github.com/clangd/clangd/discussions/1662),
  // CXXRecordDecl::lookupDepenedentName() can be removed, and its call sites
  // can be modified to benefit from the more comprehensive heuristics offered
  // by HeuristicResolver instead.
  std::vector<const NamedDecl *> lookupDependentName(
      CXXRecordDecl *RD, DeclarationName Name,
      llvm::function_ref<bool(const NamedDecl *ND)> Filter) const;
  bool findOrdinaryMemberInDependentClasses(const CXXBaseSpecifier *Specifier,
                                            CXXBasePath &Path,
                                            DeclarationName Name) const;
};

} // namespace clangd
} // namespace clang

#endif

```

## clang-plugin/from-clangd/HeuristicResolver.cpp
```
//===--- HeuristicResolver.cpp ---------------------------*- C++-*-===//
//
// Part of the LLVM Project, under the Apache License v2.0 with LLVM Exceptions.
// See https://llvm.org/LICENSE.txt for license information.
// SPDX-License-Identifier: Apache-2.0 WITH LLVM-exception
//
//===----------------------------------------------------------------------===//

#include "HeuristicResolver.h"
#include "clang/AST/ASTContext.h"
#include "clang/AST/CXXInheritance.h"
#include "clang/AST/DeclTemplate.h"
#include "clang/AST/ExprCXX.h"
#include "clang/AST/Type.h"

namespace clang {
namespace clangd {

// Convenience lambdas for use as the 'Filter' parameter of
// HeuristicResolver::resolveDependentMember().
const auto NoFilter = [](const NamedDecl *D) { return true; };
const auto NonStaticFilter = [](const NamedDecl *D) {
  return D->isCXXInstanceMember();
};
const auto StaticFilter = [](const NamedDecl *D) {
  return !D->isCXXInstanceMember();
};
const auto ValueFilter = [](const NamedDecl *D) { return isa<ValueDecl>(D); };
const auto TypeFilter = [](const NamedDecl *D) { return isa<TypeDecl>(D); };
const auto TemplateFilter = [](const NamedDecl *D) {
  return isa<TemplateDecl>(D);
};

namespace {

const Type *resolveDeclsToType(const std::vector<const NamedDecl *> &Decls,
                               ASTContext &Ctx) {
  if (Decls.size() != 1) // Names an overload set -- just bail.
    return nullptr;
  if (const auto *TD = dyn_cast<TypeDecl>(Decls[0])) {
    return Ctx.getTypeDeclType(TD).getTypePtr();
  }
  if (const auto *VD = dyn_cast<ValueDecl>(Decls[0])) {
    return VD->getType().getTypePtrOrNull();
  }
  return nullptr;
}

} // namespace

// Helper function for HeuristicResolver::resolveDependentMember()
// which takes a possibly-dependent type `T` and heuristically
// resolves it to a CXXRecordDecl in which we can try name lookup.
CXXRecordDecl *HeuristicResolver::resolveTypeToRecordDecl(const Type *T) const {
  assert(T);

  // Unwrap type sugar such as type aliases.
  T = T->getCanonicalTypeInternal().getTypePtr();

  if (const auto *DNT = T->getAs<DependentNameType>()) {
    T = resolveDeclsToType(resolveDependentNameType(DNT), Ctx);
    if (!T)
      return nullptr;
    T = T->getCanonicalTypeInternal().getTypePtr();
  }

  if (const auto *RT = T->getAs<RecordType>())
    return dyn_cast<CXXRecordDecl>(RT->getDecl());

  if (const auto *ICNT = T->getAs<InjectedClassNameType>())
    T = ICNT->getInjectedSpecializationType().getTypePtrOrNull();
  if (!T)
    return nullptr;

  const auto *TST = T->getAs<TemplateSpecializationType>();
  if (!TST)
    return nullptr;

  const ClassTemplateDecl *TD = dyn_cast_or_null<ClassTemplateDecl>(
      TST->getTemplateName().getAsTemplateDecl());
  if (!TD)
    return nullptr;

  return TD->getTemplatedDecl();
}

const Type *HeuristicResolver::getPointeeType(const Type *T) const {
  if (!T)
    return nullptr;

  if (T->isPointerType())
    return T->castAs<PointerType>()->getPointeeType().getTypePtrOrNull();

  // Try to handle smart pointer types.

  // Look up operator-> in the primary template. If we find one, it's probably a
  // smart pointer type.
  auto ArrowOps = resolveDependentMember(
      T, Ctx.DeclarationNames.getCXXOperatorName(OO_Arrow), NonStaticFilter);
  if (ArrowOps.empty())
    return nullptr;

  // Getting the return type of the found operator-> method decl isn't useful,
  // because we discarded template arguments to perform lookup in the primary
  // template scope, so the return type would just have the form U* where U is a
  // template parameter type.
  // Instead, just handle the common case where the smart pointer type has the
  // form of SmartPtr<X, ...>, and assume X is the pointee type.
  auto *TST = T->getAs<TemplateSpecializationType>();
  if (!TST)
    return nullptr;
  if (TST->template_arguments().size() == 0)
    return nullptr;
  const TemplateArgument &FirstArg = TST->template_arguments()[0];
  if (FirstArg.getKind() != TemplateArgument::Type)
    return nullptr;
  return FirstArg.getAsType().getTypePtrOrNull();
}

std::vector<const NamedDecl *> HeuristicResolver::resolveMemberExpr(
    const CXXDependentScopeMemberExpr *ME) const {
  // If the expression has a qualifier, first try resolving the member
  // inside the qualifier's type.
  // Note that we cannot use a NonStaticFilter in either case, for a couple
  // of reasons:
  //   1. It's valid to access a static member using instance member syntax,
  //      e.g. `instance.static_member`.
  //   2. We can sometimes get a CXXDependentScopeMemberExpr for static
  //      member syntax too, e.g. if `X::static_member` occurs inside
  //      an instance method, it's represented as a CXXDependentScopeMemberExpr
  //      with `this` as the base expression as `X` as the qualifier
  //      (which could be valid if `X` names a base class after instantiation).
  if (NestedNameSpecifier *NNS = ME->getQualifier()) {
    if (const Type *QualifierType = resolveNestedNameSpecifierToType(NNS)) {
      auto Decls =
          resolveDependentMember(QualifierType, ME->getMember(), NoFilter);
      if (!Decls.empty())
        return Decls;
    }
  }

  // If that didn't yield any results, try resolving the member inside
  // the expression's base type.
  const Type *BaseType = ME->getBaseType().getTypePtrOrNull();
  if (ME->isArrow()) {
    BaseType = getPointeeType(BaseType);
  }
  if (!BaseType)
    return {};
  if (const auto *BT = BaseType->getAs<BuiltinType>()) {
    // If BaseType is the type of a dependent expression, it's just
    // represented as BuiltinType::Dependent which gives us no information. We
    // can get further by analyzing the dependent expression.
    Expr *Base = ME->isImplicitAccess() ? nullptr : ME->getBase();
    if (Base && BT->getKind() == BuiltinType::Dependent) {
      BaseType = resolveExprToType(Base);
    }
  }
  return resolveDependentMember(BaseType, ME->getMember(), NoFilter);
}

std::vector<const NamedDecl *> HeuristicResolver::resolveDeclRefExpr(
    const DependentScopeDeclRefExpr *RE) const {
  return resolveDependentMember(RE->getQualifier()->getAsType(),
                                RE->getDeclName(), StaticFilter);
}

std::vector<const NamedDecl *>
HeuristicResolver::resolveTypeOfCallExpr(const CallExpr *CE) const {
  const auto *CalleeType = resolveExprToType(CE->getCallee());
  if (!CalleeType)
    return {};
  if (const auto *FnTypePtr = CalleeType->getAs<PointerType>())
    CalleeType = FnTypePtr->getPointeeType().getTypePtr();
  if (const FunctionType *FnType = CalleeType->getAs<FunctionType>()) {
    if (const auto *D =
            resolveTypeToRecordDecl(FnType->getReturnType().getTypePtr())) {
      return {D};
    }
  }
  return {};
}

std::vector<const NamedDecl *>
HeuristicResolver::resolveCalleeOfCallExpr(const CallExpr *CE) const {
  if (const auto *ND = dyn_cast_or_null<NamedDecl>(CE->getCalleeDecl())) {
    return {ND};
  }

  return resolveExprToDecls(CE->getCallee());
}

std::vector<const NamedDecl *> HeuristicResolver::resolveUsingValueDecl(
    const UnresolvedUsingValueDecl *UUVD) const {
  return resolveDependentMember(UUVD->getQualifier()->getAsType(),
                                UUVD->getNameInfo().getName(), ValueFilter);
}

std::vector<const NamedDecl *> HeuristicResolver::resolveDependentNameType(
    const DependentNameType *DNT) const {
  return resolveDependentMember(
      resolveNestedNameSpecifierToType(DNT->getQualifier()),
      DNT->getIdentifier(), TypeFilter);
}

std::vector<const NamedDecl *>
HeuristicResolver::resolveTemplateSpecializationType(
    const DependentTemplateSpecializationType *DTST) const {
  return resolveDependentMember(
      resolveNestedNameSpecifierToType(DTST->getQualifier()),
      DTST->getIdentifier(), TemplateFilter);
}

std::vector<const NamedDecl *>
HeuristicResolver::resolveExprToDecls(const Expr *E) const {
  if (const auto *ME = dyn_cast<CXXDependentScopeMemberExpr>(E)) {
    return resolveMemberExpr(ME);
  }
  if (const auto *RE = dyn_cast<DependentScopeDeclRefExpr>(E)) {
    return resolveDeclRefExpr(RE);
  }
  if (const auto *OE = dyn_cast<OverloadExpr>(E)) {
    return {OE->decls_begin(), OE->decls_end()};
  }
  if (const auto *CE = dyn_cast<CallExpr>(E)) {
    return resolveTypeOfCallExpr(CE);
  }
  if (const auto *ME = dyn_cast<MemberExpr>(E))
    return {ME->getMemberDecl()};

  return {};
}

const Type *HeuristicResolver::resolveExprToType(const Expr *E) const {
  std::vector<const NamedDecl *> Decls = resolveExprToDecls(E);
  if (!Decls.empty())
    return resolveDeclsToType(Decls, Ctx);

  return E->getType().getTypePtr();
}

const Type *HeuristicResolver::resolveNestedNameSpecifierToType(
    const NestedNameSpecifier *NNS) const {
  if (!NNS)
    return nullptr;

  // The purpose of this function is to handle the dependent (Kind ==
  // Identifier) case, but we need to recurse on the prefix because
  // that may be dependent as well, so for convenience handle
  // the TypeSpec cases too.
  switch (NNS->getKind()) {
  case NestedNameSpecifier::TypeSpec:
  case NestedNameSpecifier::TypeSpecWithTemplate:
    return NNS->getAsType();
  case NestedNameSpecifier::Identifier: {
    return resolveDeclsToType(
        resolveDependentMember(
            resolveNestedNameSpecifierToType(NNS->getPrefix()),
            NNS->getAsIdentifier(), TypeFilter),
        Ctx);
  }
  default:
    break;
  }
  return nullptr;
}

namespace {

bool isOrdinaryMember(const NamedDecl *ND) {
  return ND->isInIdentifierNamespace(Decl::IDNS_Ordinary | Decl::IDNS_Tag |
                                     Decl::IDNS_Member);
}

bool findOrdinaryMember(const CXXRecordDecl *RD, CXXBasePath &Path,
                        DeclarationName Name) {
  Path.Decls = RD->lookup(Name).begin();
  for (DeclContext::lookup_iterator I = Path.Decls, E = I.end(); I != E; ++I)
    if (isOrdinaryMember(*I))
      return true;

  return false;
}

} // namespace

bool HeuristicResolver::findOrdinaryMemberInDependentClasses(
    const CXXBaseSpecifier *Specifier, CXXBasePath &Path,
    DeclarationName Name) const {
  CXXRecordDecl *RD =
      resolveTypeToRecordDecl(Specifier->getType().getTypePtr());
  if (!RD)
    return false;
  return findOrdinaryMember(RD, Path, Name);
}

std::vector<const NamedDecl *> HeuristicResolver::lookupDependentName(
    CXXRecordDecl *RD, DeclarationName Name,
    llvm::function_ref<bool(const NamedDecl *ND)> Filter) const {
  std::vector<const NamedDecl *> Results;

  // Lookup in the class.
  bool AnyOrdinaryMembers = false;
  for (const NamedDecl *ND : RD->lookup(Name)) {
    if (isOrdinaryMember(ND))
      AnyOrdinaryMembers = true;
    if (Filter(ND))
      Results.push_back(ND);
  }
  if (AnyOrdinaryMembers)
    return Results;

  // Perform lookup into our base classes.
  CXXBasePaths Paths;
  Paths.setOrigin(RD);
  if (!RD->lookupInBases(
          [&](const CXXBaseSpecifier *Specifier, CXXBasePath &Path) {
            return findOrdinaryMemberInDependentClasses(Specifier, Path, Name);
          },
          Paths, /*LookupInDependent=*/true))
    return Results;
  for (DeclContext::lookup_iterator I = Paths.front().Decls, E = I.end();
       I != E; ++I) {
    if (isOrdinaryMember(*I) && Filter(*I))
      Results.push_back(*I);
  }
  return Results;
}

std::vector<const NamedDecl *> HeuristicResolver::resolveDependentMember(
    const Type *T, DeclarationName Name,
    llvm::function_ref<bool(const NamedDecl *ND)> Filter) const {
  if (!T)
    return {};
  if (auto *ET = T->getAs<EnumType>()) {
    auto Result = ET->getDecl()->lookup(Name);
    return {Result.begin(), Result.end()};
  }
  if (auto *RD = resolveTypeToRecordDecl(T)) {
    if (!RD->hasDefinition())
      return {};
    RD = RD->getDefinition();
    return lookupDependentName(RD, Name, Filter);
  }
  return {};
}

} // namespace clangd
} // namespace clang

```

## TODO
```
Big tasks:
* Autocomplete
* Display links to nearby methods in sidebar
* Better click UI that includes comments, IDL, etc.
* Multiple panes for looking through every reference in a file
* Searching for declarations

UI:
* Crossref result matches in bold
* "Search substring" context menu option
* Allow "locking on" a given highlighted symbol (from Kris)
* Incrementally load search results for less jank

AWS:
! Automatic indexing
! Error reporting
* Better security
* Multiple web hosts for performance

C++ indexing:
* Index specialized types (templated, etc.)
* Better indexing with inheritance (don't just search from root method)

JS indexing:
* Indexing of test files

Other indexing:
* IPDL
* HTML
* WebIDL?

IDL:
* Need to index the name of the IDL class as a type.

== Searching for declarations ==

Perhaps I'll make an index file that can be used to augment the full text search.
This would be similar to augmenting it with filenames.
When doing the analysis, a given decl would generate a fully-qualified name and a line/column number.
This could be a separate kind of analysis line; maybe it could be called "index". I could
also use this for JS, where you could search for ClassName.PropertyName.
The search feature would probably just grep through this file since I want to allow substring searches.
Not sure if that would be fast enough, but it might not be too bad. It would just be defs.
Or I could put in each level of qualification as a separate thing, and they would have to type
a prefix. That way I could do a binary search, which would be much faster.

There are two ways to make this fast:
1. Use codesearch to search through the file. It would probably go in a separate repo.
2. Use binary search. The file would be sorted. For this, though, I would need to include
   all levels of qualification. I guess the JS analysis already does this.
3. Just grep through the file. It's hard to see how this would be fast enough though.

Actually, though, I don't want just defs. I also want uses. So this will be a very big file.
I'll start out using binary search. It makes sense to have the crossref code generate this
file I think.

ID table: This will be a map from identifiers (with all levels of
qualification) to symbols. Once we have a symbol, we can look up the
symbol in the crossref file.

== C++ ==

For a given (static type, method) combo, I think I should have one
canonical set of results regardless of whether you're searching from a
def or a use. That set will include:
  - all possible method defs that can be dispatched to from that static type
  - all possible call sites that might call those methods
  - related methods up the inheritance chain?

class A {
  virtual void foo() {}
  // Result for def:A#foo
  // Searches for use:A#foo
};

class B : A {
};

class C : B {
  virtual void foo() {}
  // Result for def:A#foo, def:B#foo, def:C#foo
  // Searches for use:A#foo, use:B#foo, use:C#foo, def:A#foo
};

int main() {
  A* a;
  B* b;
  C* c;

  a->foo();
  // Searches for def:A#foo, use:A#foo
  // Result for use:A#foo

  b->foo();
  // Searches for def:B#foo, use:A#foo, use:B#foo
  // Result for def:B#foo

  c->foo();
  // Searches for def:C#foo, use:A#foo, use:B#foo, use:C#foo
  // Result for use:C#foo
}

What sort of pretty names should I use throughout all this?

```

## config_defaults/per-file-info.toml
```
# ## Path Kind Mapping
[pathkind.normal]
name = "Normal"
default = true
decision_order = 0
sort_order = 0

[pathkind.generated]
name = "Generated code"
decision_order = 3
sort_order = 3

[pathkind.generated.heuristics]
path_prefixes = [
    "__GENERATED__/",
]

[pathkind.test]
name = "Test files"
decision_order = 2
sort_order = 2

[pathkind.test.heuristics]
dir_names = [
    "androidTest",
    "crashtest",
    "crashtests",
    "googletest",
    "gtest",
    "gtests",
    "imptests",
    "jsapi-tests",
    "mochitest",
    "reftest",
    "reftests",
    "test",
    "tests",
    "unit",
]
dir_suffixes = [
    "testing",
]
path_prefixes = [
    "LayoutTests/",
]

[pathkind.third_party]
name = "Third-party code"
priority = 2
decision_order = 1
sort_order = 1

[pathkind.third_party.heuristics]
path_prefixes = [
    "third_party/",
]


# ## Input Files

# ### .eslintignore
#
# This is a proof-of-concept for deriving a file label from an in-tree listing.
# In order to decrease the false-positive rate of the glob mechanism, I've added
# a pre-filtering mechanism by file extension that I've hard-coded here, but
# better approaches would obviously involve options like:
# - Have searchfox have a more first class idea of file types available at this
#   moment.  (Right now there are a few ad-hoc hard-coded places.)
# - Actually be reading some kind of eslint output results here that tell us
#   what got linted and what files were ignored rather than having this
#   mechanism attempt to replace what's going on.
[textfile.".eslintignore"]
source = [{ root = "files", file = ".eslintignore" }]
# We filter the list of files we apply the file list to according to these file
# extensions to avoid marking files eslint would not normally consider as
# ignored like markdown files, etc.  (Should HTML files be included here too?)
#
# To reiterate the above, this mechanism here is intended to be a hack and a
# better idea would be to process eslint output, etc. but this is just a baby
# step here.
filter_input_ext = ["js", "mjs", "jsx", "cjs", "jsm", "sjs"]
format = "file-list"
# This is saying add a tag "eslint-ignored" to every file that seems to have
# matched the ignore list glob and that wasn't explicitly reincluded via `!`.
apply_tag = "eslint-ignored"

[textfile."data-review-required.list"]
source = [
    # I made this file up, it doesn't exist yet, but the idea is that if someone
    # made it exist, we'd use it over our hard-coded list!  We can rename this.
    { root = "files", file = "toolkit/components/telemetry/docs/collection/review-required.list" },
    # Although we're expecting this file to be symlinked into each repo, for
    # this hard-coded path we try and use the explicit shared directory for
    # slightly more sanity.
    { root = "config_repo", file = "shared/mc-data-review-required.list" },
]
format = "file-glob-list"
apply_tag = "data-review-required"

# ### bugzilla-components.json
#
# Paths are stored via recursive nesting.
#
# - "components": A dictionary mapping from stringified numeric values to list
#   tuples of the form [product, component].
# - "paths": A tree where internal nodes are dictionaries corresponding to
#   directories.  Each key is a filename and each value is either another
#   directory dictionary or a non-stringified number corresponding to an entry
#   in the `components` top-level dictionary.
[jsonfile."bugzilla-components.json"]
source = [{ root = "index", file = "bugzilla-components.json" }]

[jsonfile."bugzilla-components.json".ingestion]
root = "/paths"
nesting = "hierarchical-dict-dirs-are-dicts-files-are-values"
value_lookup = "/components"

[jsonfile."bugzilla-components.json".concise]
bugzilla_component.pointer = ""
subsystem.liquid = '{{ value[0] }}/{{ value[1] | replace: "&", "" | replace: ":", "/" | compact_pathlike }}'

# ### code-coverage-report.json
#
# Hierarchical file where the root node corresponds to the root of the source
# tree.  Paths are stored via recursive nesting.
#
# Each node can contain the following keys:
# - "children": An object whose keys are file/directory names and whose values
#   are nodes of the self-same type.
# - "coverage": An array where each entry corresponds to a line of the source
#   file with `-1` indicating an uninstrumented line, `0` indicating an
#   instrumented line with no coverage hits, and any positive integers
#   indicating a line with that number of hits.
# - "coveragePercent": Coverage percent in the node and all its children as a
#   floating point value in the range [0, 100] to 2 decimal places.  For a
#   file this is for the file and for a directory this is the average over all
#   of its children.
# - "linesCovered": The number of coverage lines in the node and all its
#   children which are `> 0`.  So for a file this is derived from its
#   "coverage" and for a directory this is the sum of the value in all of its
#   children.
# - "linesMissed": The number of lines in the node and all its children which
#   are `0`.
# - "linesTotal": The number of lines in the node and all its children which
#   aren't `-1` AKA are `>= 0`.  Should be the same as adding up
#   `linesCovered` and `linesMissed`.  There is no summary value for the
#   number of lines that report `-1` because they're presumed to be whitespace
#   or comments or whatever.
# - "name": The same name that is the key that matches this value in its
#   parent's "children" dictionary.  In the case of the root node this is "".
#
# Currently only the "coverage" data is used, going in the detailed per-file
# storage, but it would make a lot of sense to save off the aggregate info
# in the summary file.
[jsonfile."code-coverage-report.json"]
source = [{ root = "index", file = "code-coverage-report.json" }]

[jsonfile."code-coverage-report.json".ingestion]
root = ""
nesting = "hierarchical-dict-explicit-key"
nesting_key = "children"

[jsonfile."code-coverage-report.json".detailed]
coverage_lines.pointer = "/coverage"

# ### test-info-all-tests.json
#
# Paths are flat, with only a single level of clustering by bugzilla
# component.
#
# - "description": A string which conveys the date range and tree that this
#   data corresponds to.
# - "summary": A dictionary with the following keys:
#   - "components"
#   - "failed tests"
#   - "manifests"
#   - "skipped tests"
#   - "tests"
# - "tests": A dictionary whose keys are bugzilla "Product::Component" strings
#   and values are list of objects with the following keys:
#   - NEW: "failure_count": Number
#   - OLD, from Active{Data,Record} "failed runs": Number
#   - "skip-if" (optional): String excerpt of the manifest's skip-if clause.
#   - "skipped runs": Number
#   - "test": Repository-relative path of the test file.
#   - "total run time, seconds": Floating point number.
#   - "total runs": Number
[jsonfile."test-info-all-tests.json"]
source = [{ root = "index", file = "test-info-all-tests.json" }]

[jsonfile."test-info-all-tests.json".ingestion]
root = "/tests"
# The keys in the "tests" dict are stringified bugzilla "product::component"
# reps, but that grouping is irrelevant to us.
nesting = "boring-dict-of-arrays"
# The "test" key in each array is a full path to the test
nesting_key = "test"

[jsonfile."test-info-all-tests.json".concise.info.test.object]
skip_if.pointer = "/skip-if"
failure_count.pointer = "/failure_count"
manifest.pointer = "/manifest"

# ### WPT manifest variants (wpt proper plus "mozilla" wpt)
#
# Hierarchical tree rooted under "items" where the first level groups by the
# test type (https://web-platform-tests.org/test-suite-design.html#test-types)
# like "testharness" (the most common kind), "crashtest", "reftest", etc.  They
# then have a self-similar structure where at each level objects are directories
# and arrays are file values.
#
# The value payloads are an array structure:
# - 0: hash
# - 1+: tuple array
#   - `null` if there are no "test id" variants for this file, otherwise the
#     "test id" string which is a full path.  For example, for the key
#     `"idlharness.https.any.js"` under `"shape-detection"`, we have values like
#     `"shape-detection/idlharness.https.any.html"` and
#     `"shape-detection/idlharness.https.any.serviceworker.html"` and
#     `"shape-detection/idlharness.https.any.sharedworker.html"` and
#     `"shape-detection/idlharness.https.any.worker.html"`
#   - an object which can be empty, but can also have keys:
#     - `"script_metadata"` which is an array of tuple arrays with first value:
#       - `"global"`: happens first and only once? ex: `"window,worker"`
#       - `"script"`: can happen multiple times, ex: `"/resources/WebIDLParser.js"`
#         and `"/resources/idlharness.js"`.

[jsonfile."wpt-manifest.json"]
source = [{ root = "index", file = "wpt-manifest.json" }]

[jsonfile."wpt-manifest.json".ingestion]
root = "/items"
nesting = "hierarchical-dict-dirs-are-dicts-files-are-values"
# As documented at https://web-platform-tests.org/test-suite-design.html#test-types
# there are number of different test types and these partition the file-hierarchies
# by type.  For now we will just consume this value, but in the future we should
# expose this via the context mechanism so that at the file level we can use this
# value.
partitioned_by = "test_type"
# Prefix every path we see with our WPT root.
path_prefix = "testing/web-platform/tests"

[jsonfile."wpt-manifest.json".concise.info.wpt.object]
test_ids.pointer = ""
test_ids.map.first_index = 1
test_ids.map.each.pointer = "/0"
test_ids.map.each.null_fallback.liquid = "{{ context.path | strip_prefix_or_empty: 'testing/web-platform/tests/' }}"

# #### mozilla WPT sub-tree variants
[jsonfile."wpt-mozilla-manifest.json"]
source = [{ root = "index", file = "wpt-mozilla-manifest.json" }]

[jsonfile."wpt-mozilla-manifest.json".ingestion]
root = "/items"
nesting = "hierarchical-dict-dirs-are-dicts-files-are-values"
partitioned_by = "test_type"
# Prefix every path we see with our WPT root.
path_prefix = "testing/web-platform/mozilla/tests"

[jsonfile."wpt-mozilla-manifest.json".concise.info.wpt.object]
test_ids.pointer = ""
test_ids.map.first_index = 1
test_ids.map.each.pointer = "/0"
test_ids.map.each.null_fallback.liquid = "{{ context.path | strip_prefix_or_empty: 'testing/web-platform/mozilla/tests/' }}"

# ### WPT wpt-metadata-summary.json
#
# Paths are flat with only a single level of directory clustering.
#
# Consult
# https://searchfox.org/mozilla-central/source/testing/web-platform/tests/tools/wptrunner/wptrunner/manifestexpected.py
# for detailed info about the schema.
#
# - [directory]: A WPT-root (testing/web-platform/tests) string identifying a
#   directory containing tests.  Value is an object.
#   - "bug": Corresponds to a `bug: NNN` line in a meta-dir `__dir__.ini` file
#     with value payload `[null, "NNN"]`.
#   - "lsan-allowed": Corresponds to a `__dir__.ini`
#     `lsan-allowed: [Alloc, Create, ...]` line and results in `["Alloc",
#     "Create", ...]`.
#   - "_tests": An object whose keys are test file names.
#     - [test file name]: Value is an object which may contain any of the
#       following keys:
#       - "disabled": An array of 2-tuple arrays, where each 2-tuple is of the
#         form [if-predicate contents, bug URL].  So the line
#         `if (os == "win"): https://bugzilla.mozilla.org/show_bug.cgi?id=NNN`
#         under a "disabled" mochitest ini-format header would result in
#         `["os == \"win\"\n", "https://bugzilla.mozilla.org/show_bug.cgi?id=NNN"]
#         and a line like the following directly under the test name
#         `disabled: https://bugzilla.mozilla.org/show_bug.cgi?id=NNN` gives
#         `[null, "https://bugzilla.mozilla.org/show_bug.cgi?id=NNN"]`.
#         - It appears the bug URL's can just be straight bug numbers or
#           string bug aliases.
#       - "_subtests":
#         - [assertion string]: Payload is an object with optional keys:
#           - "intermittent": An array of nested tuples of the form
#             [condition clause, [ expected values ]].  For example, given
#             `if (processor == "x86") and debug: ["PASS", "FAIL"]` indented
#             beneath an `expected:` header results in
#             `["(processor == \"x86\") and debug\n", ["PASS", "FAIL"]]`.
#             - If this key is not present, then it appears this corresponds
#               to an ini entry of `expected: FAIL`, which would be equivalent
#               to `[null, ["FAIL"]]` I guess.
#       - "max-asserts": [condition?, max-asserts value]
[jsonfile."wpt-metadata-summary.json"]
source = [{ root = "index", file = "wpt-metadata-summary.json" }]

[jsonfile."wpt-metadata-summary.json".ingestion]
root = ""
nesting = "flat-dir-dict-files-are-keys"
# The dict of tests keyed by their filename is nested under "_tests".
nesting_key = "_tests"
# Prefix every path we see with our WPT root.
path_prefix = "testing/web-platform/tests"
# NB: this mechanism currently only exists for "flat-dir-dict-files-are-keys"
filename_key = "_filename"

[jsonfile."wpt-metadata-summary.json".concise.info.wpt.object]
disabling_conditions.pointer = "/disabled"
disabled_subtests_count.pointer = "/_subtests"
disabled_subtests_count.aggregation = "length"

```

## config_defaults/source_file_other_tools_panels.liquid
```

```

## config_defaults/ontology-mapping.toml
```
# ### Runnables ###
#
# The mappings here specify rules that annotate all descendant overrides of
# virtual runnable methods so that when we encounter them in the crossref
# process that we are able to generate `RunnableConstructor` slot edges to their
# constructors and corresponding `RunnableMethod` slot edges back from the
# constructors to the runnable methods.  These will be followed by the
# "traverse" command instead of getting into the infrastructure boilerplate
# around runnables.
#
# This heuristic can also be used for general callback interfaces like
# nsIRequestObserver on each of its listener methods as the effects are
# additive.  For example, a reference to a constructor then becomes a reference
# to both OnStartRequest and OnStopRequest assuming both methods were marked
# as runnable.
#
# TODO: Need to deal with situations like
# https://searchfox.org/mozilla-central/rev/6220909421e5cdb2e706a87f77ba7c6f4f21e4d0/dom/quota/ActorsParent.cpp#7064
# where we have an explicit call to Run made from inside the class.  Although we
# want to avoid including the calls to the base class signature of the method,
# we absolutely want to process in-class concrete references to the Run method,
# maybe even all the way up the hierarchy short of the runnable method itself.
# This case will shortly change in https://phabricator.services.mozilla.com/D182182
# so that OriginOperationBase::RunImmediately will directly be invoking Run
# itself.

[pretty."nsIRunnable::Run"]
runnable = "constructor"

[pretty."mozilla::dom::WorkerRunnable::WorkerRun"]
runnable = "constructor"

[pretty."mozilla::dom::WorkerMainThreadRunnable::MainThreadRun"]
runnable = "constructor"

[pretty."mozilla::dom::WorkerThreadProxySyncRunnable::RunOnMainThread"]
runnable = "constructor"

[pretty."nsIProgressEventSink::OnProgress"]
runnable = "constructor"

[pretty."nsIProgressEventSink::OnStatus"]
runnable = "constructor"

[pretty."nsIRequestObserver::OnStartRequest"]
runnable = "constructor"

[pretty."nsIRequestObserver::OnStopRequest"]
runnable = "constructor"

[pretty."nsIStreamListener::OnDataAavailable"]
runnable = "constructor"

[pretty."nsIStreamLoaderObserver::OnStreamComplete"]
runnable = "constructor"

# Treating nsITimerCallback as a runnable is an improvement on the current
# behavior of citing `nsTimerImpl::Fire` plus random implementations that
# directly call the Notify method (like `HTMLCanvasPrintState`), but is mainly
# useful for small one-off classes of a certain era before lambdas would be
# used.  For larger monolithic classes, it would be better to specialize on the
# call to setup the timer, and this would also work for the smaller classes too.
[pretty."nsITimerCallback::Notify"]
runnable = "constructor"

# #### Java / Kotlin Runnable Things ####
#
# These seem like they use reflection to construct the class and so we don't
# actually see the constructor invoked, so our edges here need to be to
# references to the class.  We of course also see references to the classes
# when they are imported, but those thankfully lack a contextsym and so end up
# being moot.
[pretty."androidx::work::Worker::doWork"]
runnable = "class"

[pretty."androidx::work::CoroutineWorker::doWork"]
runnable = "class"

[pretty."kotlin::Function3::invoke"]
runnable = "constructor"

# ### Class Diagram Directives ###
#
# - "class-diagram:stop" - This is used for cases where the type is notable and
#   it's potentially worth displaying, but we definitely do not want to traverse
#   the type because it has so much going on that it will swamp everything else
#   going on.  An exception is made if the class is explicitly a root of the
#   diagram (depth is zero).
#   - It's possible that many uses of this label could be removed by use of
#     heuristics like:
#     - Overload heuristics.  We have some of these already, but the core idea
#       is that sometimes something should not be traversed without explicit
#       user interaction.
#     - Subsystem-crossing heuristics.  In many cases, problematic classes live
#       in other subsystems from what a user is investigating, so a heuristic
#       that doesn't expand or possibly even show the classes would work.
#   - It's also possible that more explicit display of the types of fields
#     either directly in the diagram or contextually via super sidebar/similar
#     could moot the need to display a number of classes that don't contribute
#     to the graph topology.
# - "class-diagram:elide-and-badge:" prefix - A more extensive intervention for
#   cases where including the class in the graph creates an unhelpful layout
#   nexus.  For example, tons of things subclass "nsISupports" and "nsIRunnable"
#   so we use this mechanism to suppress the edge and instead put "nsI" and
#   "nsIRun" labels on the classes when we would have instead added an edge.

[pretty."JSObject"]
labels = ["class-diagram:stop"]

[pretty."nsIEventTarget"]
labels = ["class-diagram:stop"]

[pretty."nsIGlobalObject"]
labels = ["class-diagram:stop"]

[pretty."nsIInterfaceRequestor"]
labels = ["class-diagram:stop", "class-diagram:elide-and-badge:nsIIReq"]

[pretty."nsIObserver"]
labels = ["class-diagram:stop", "class-diagram:elide-and-badge:nsIObs"]

# nsIRunnable is not as bad as nsISupports, but it's sufficiently common that
# it will create a layout cluster that we really don't need/want.
[pretty."nsIRunnable"]
labels = ["class-diagram:stop", "class-diagram:elide-and-badge:nsIRun"]

[pretty."nsISerialEventTarget"]
labels = ["class-diagram:stop"]

# This will get both `T_nsISupports` and `XPIDL_nsISupports`
[pretty."nsISupports"]
labels = ["class-diagram:stop", "class-diagram:elide-and-badge:nsI"]

# calls-between likes to find "NS_TableDrivenQI" calling "nsISupports::AddRef"
[pretty."nsISupports::AddRef"]
labels = ["uses-diagram:stop"]
# adding the release counterpart just in case.
[pretty."nsISupports::Release"]
labels = ["uses-diagram:stop"]
# QueryInterface is only a minor problem but is not helpful.
[pretty."nsISupports::QueryInterface"]
labels = ["uses-diagram:stop"]

[pretty."nsIThreadPool"]
labels = ["class-diagram:stop"]

[pretty."nsSupportsWeakReference"]
labels = ["class-diagram:stop", "class-diagram:elide-and-badge:nsSupWeak"]

[pretty."nsGlobalWindowInner"]
labels = ["class-diagram:stop"]

[pretty."nsPIDOMWindowInner"]
labels = ["class-diagram:stop"]

[pretty."nsWrapperCache"]
labels = ["class-diagram:stop", "class-diagram:elide-and-badge:WC"]

[pretty."mozilla::DOMEventTargetHelper"]
labels = ["class-diagram:stop"]

[pretty."mozilla::dom::WorkerPrivate"]
labels = ["class-diagram:stop"]

[pretty."mozilla::ipc::IProtocol"]
labels = ["class-diagram:stop"]

[pretty."mozilla::ipc::PBackgroundParent"]
labels = ["class-diagram:stop"]

[pretty."mozilla::Runnable"]
labels = ["class-diagram:stop"]

[pretty."mozilla::SupportsWeakPtr"]
labels = ["class-diagram:stop"]

# calls-between WorkerPrivate and nsITimer found an interesting but distracting path
# and nsGlobalWindowInner similarly ended up with noise we don't need.  The
# specific choice of using these impl classes is because they use a lot of
# overloads so we can specify them more easily based on pretty.  These are not
# comprehensive.
#
# This could be mooted by better traversal heuristics, and/or made more compact
# if we still see value in the annotation.
[pretty."TelemetryScalar::Add"]
labels = ["uses-diagram:stop"]
[pretty."TelemetryScalar::Set"]
labels = ["uses-diagram:stop"]
[pretty."TelemetryScalar::SetMaximum"]
labels = ["uses-diagram:stop"]
[pretty."TelemetryHistogram::Accumulate"]
labels = ["uses-diagram:stop"]
[pretty."TelemetryHistogram::AccumulateCategorical"]
labels = ["uses-diagram:stop"]
[pretty."KeyedHistogram::Add"]
labels = ["uses-diagram:stop"]
[pretty."TelemetryEvent::RecordEventNative"]
labels = ["uses-diagram:stop"]

# Until we are able to understand the observer service arguments and refine the
# calls to tuple over the argument, we want uses diagrams to stop at the
# interface because otherwise they entrain impossible control flow paths.
# TODO: Understand observer notification arguments / subscriptions.
[pretty."nsIObserver::Observe"]
labels = ["uses-diagram:stop"]

# stay out of debug spam stuff
[pretty."NS_warn_if_impl"]
labels = ["calls-diagram:stop"]
[pretty."NS_DebugBreak"]
labels = ["calls-diagram:stop"]
[pretty."MOZ_ReportAssertionFailure"]
labels = ["calls-diagram:stop"]
[pretty."MOZ_ReportCrash"]
labels = ["calls-diagram:stop"]
[pretty."NS_IsMainThread"]
labels = ["calls-diagram:stop"]
# the above cases should prevent this, but this has an annoying fan-out if we get here
[pretty."PR_GetEnv"]
labels = ["calls-diagram:stop"]

# some speculative profiler filters
[pretty."mozilla::MarkerStack::Capture"]
labels = ["calls-diagram:stop"]
[pretty."profiler_thread_is_being_profiled_for_markers"]
labels = ["calls-diagram:stop"]

# stay out of various logging related things that have fan out
[pretty."mozilla::ipc::LoggingEnabledFor"]
labels = ["calls-diagram:stop"]
[pretty."mozilla::ipc::LogMessageForProtocol"]
labels = ["calls-diagram:stop"]
# stay out of IPC internals for IPC protocols we don't have IPDL bindings for
[pretty."mozilla::ipc::IProtocol::ChannelSend"]
labels = ["calls-diagram:stop"]

# Avoid some NSPR stuff that comes up; this very much could be handled via subsystem
# heuristics about NSPR always being boring.
[pretty."PR_JoinThread"]
labels = ["calls-diagram:stop"]
[pretty."PR_Now"]
labels = ["calls-diagram:stop"]

# ### Refcounted Types ###

[[pretty."nsAutoRefCnt".label_owning_class.labels]]
label = "rc"

# (Label atomic refcounted things as both "rc" and "arc".)
[[pretty."mozilla::ThreadSafeAutoRefCnt".label_owning_class.labels]]
label = "rc"
[[pretty."mozilla::ThreadSafeAutoRefCnt".label_owning_class.labels]]
label = "arc"

[[pretty."nsCycleCollectingAutoRefCnt".label_owning_class.labels]]
label = "rc"
[[pretty."nsCycleCollectingAutoRefCnt".label_owning_class.labels]]
label = "ccrc"

[[pretty."mozilla::RefCounted".label_subclasses.labels]]
label = "rc"

# (Label atomic refcounted things as both "rc" and "arc".)
[[pretty."mozilla::AtomicRefCounted".label_subclasses.labels]]
label = "rc"
[[pretty."mozilla::AtomicRefCounted".label_subclasses.labels]]
label = "arc"

# #### Webkit

# This is transitive and gets RefCounted too.
[[pretty."WTF::RefCountedBase".label_subclasses.labels]]
label = "rc"

[[pretty."WTF::ThreadSafeRefCounted".label_subclasses.labels]]
label = "rc"
[[pretty."WTF::ThreadSafeRefCounted".label_subclasses.labels]]
label = "arc"

# ### Cycle Collection ###
# We use a containing class rule here because the participant is an inner class
# of the cycle collected class.  Compare with the refcount objects which are
# fields on the refcounted class.
[[pretty."nsCycleCollectionParticipant".label_containing_class.labels]]
label = "cc"

[[pretty."nsCycleCollectionParticipant".label_containing_class_field_uses.labels]]
context_sym_suffix = "::cycleCollection::Trace"
label = "cc-trace"
[[pretty."nsCycleCollectionParticipant".label_containing_class_field_uses.labels]]
context_sym_suffix = "::cycleCollection::TraverseNative"
label = "cc-traverse"
[[pretty."nsCycleCollectionParticipant".label_containing_class_field_uses.labels]]
context_sym_suffix = "::cycleCollection::Unlink"
label = "cc-unlink"

# ### Label Testing ###
#
# Okay, so, uh, these are only for testing.

[[pretty."outerNS::CycleCollectingMagic".label_containing_class.labels]]
label = "cc"

[[pretty."outerNS::CycleCollectingMagic".label_containing_class_field_uses.labels]]
context_sym_suffix = "::unlink"
label = "cc-unlink"

# ### Decorator Types ###
#
# A type like Atomic or IntializedOnce that provides notable semantics and
# so we should apply a label, but where the decorator type itself is not
# the underlying type of interest and we should continue processing its
# arguments like they existed without the decorator.

[types."mozilla::Atomic".decorator]
labels = ["atomic"]

[types."std::atomic".decorator]
labels = ["atomic"]

[types."mozilla::DataMutex".decorator]
labels = ["synchronized"]

# should this have a "static" label, or is it redundant because the field will
# be static?
[types."mozilla::StaticDataMutex".decorator]
labels = ["synchronized"]

[types."mozilla::ThreadBound".decorator]
labels = ["threadbound"]

# #### Webkit

[types."WTF::Atomic".decorator]
labels = ["atomic"]

[types."WTF::DataMutex".decorator]
labels = ["synchronized"]

# ### Value Types ###
#
# Marking something as a value excludes that type itself from being considered
# for pointer_info purposes and thereby not something that "class-diagram" will
# show because there's no potential graph edge to generate.  For example, URIs
# are very interesting properties, but there is nothing interesting about the
# relationship between a document and its URI fields.  The same for strings.
#
# Because the current processing algorithm only potentially looks at a given
# template arg and its parent, marking something as a value doesn't stop us
# from potentially processing nested arguments; like if we marked nsTArray as
# a value, then `nsTArray<RefPtr<Foo> >` would not stop us from seeing the
# contained RefPtr right now.  Note that nsTArray is explicitly now a container
# and one should not mark things as values depending on that behavior, as we
# will potentially fix that problem.
# TODO: fix that problem
#
# Note that we also hope to automatically mark types that only contain value
# types themselves as values, although it's unclear if we should only do that
# for primitive types or also inductively for types we've explicitly marked as
# values here.

[types."nsTString".value]
[types."nsTSubstring".value]

[types."nsPoint".value]

[types."nsIPrincipal".value]
[types."mozilla::ipc::PrincipalInfo".value]
[types."nsIURI".value]
[types."mozilla::ipc::URIParams".value]
[types."nsIReferrerInfo".value]

[types."mozilla::ipc::IPDLStructMemberWrapper".value]

# A mutex itself just a value and not inherently interesting, although I wonder
# if "label_owning_class" should be used to mark a class as having some internal
# synchronization going on.  Although that might be something that's best done
# as a first-class thing so that clicking on it would immediately show what
# fields are guarded and what the guarding relationships are.
[types."mozilla::Mutex".value]

[types."mozilla::TimeStamp".value]

[types."mozilla::dom::IdType".value]

# The refcount and owning thread are boring and usually there.  These get badged
# via `pretty` rules.
[types."nsCycleCollectingAutoRefCnt".value]
[types."nsAutoRefCnt".value]
[types."nsAutoOwningThread".value]
[types."ThreadSafeAutoRefCnt".value]

# #### Webkit

[types."WTF::ApproximateTime".value]
# this is somewhat notable but not useful to be called out
[types."WTF::Lock".value]
[types."WTF::MonotonicTime".value]
[types."WTF::Seconds".value]
[types."WTF::String".value]
[types."WTF::WallTime".value]


# ### Containers ###
#
# Container types are themselves not interesting classes, but their payloads are
# and we assume containers have a multiplicity of >1.  For something like
# "mozilla::Maybe", we currently just model that as a pointer, but we could
# probably do better.

# Maps/Sets are interesting if their values are interesting, so let's mark
# them as values right now.
# TODO: Potentially have this (and arrays) annotate multiplicity.
[types."nsBaseHashtable".container]
[types."nsTBaseHashSet".container]

[types."std::array".container]
[types."std::deque".container]
[types."std::flat_map".container]
[types."std::flat_set".container]
[types."std::flat_multimap".container]
[types."std::flat_multiset".container]
[types."std::forward_list".container]
[types."std::list".container]
[types."std::map".container]
[types."std::multiset".container]
[types."std::multimap".container]
[types."std::priority_queue".container]
[types."std::queue".container]
[types."std::set".container]
# we really need ownership for this one
[types."std::span".container]
[types."std::stack".container]
[types."std::unordered_map".container]
[types."std::unordered_set".container]
[types."std::unordered_multimap".container]
[types."std::unordered_multiset".container]
[types."std::vector".container]


[types."nsTArray".container]
[types."AutoTArray".container]
[types."mozilla::Vector".container]
[types."HashSet".container]

# nsClassHashtable is somewhat unique in terms of baking in a UniquePtr, so we
# define it as a pointer.
# XXX we probably want to be able to specify multiplicity for pointers then.
[types."nsClassHashtable".pointer]
kind = "unique"
arg_index = 1

# ManagedContainer should be a container, but it bakes in retained raw pointers
# that are conceptually strong but structurally raw.  Notionally, these could
# probably be considered weak pointers where the ActorLifecycleProxy holds the
# strong reference, but I think we may be eliding that right now.
#
# TODO: Figure out what's going on with ActorLifecycleProxy
[types."mozilla::ManagedContainer".pointer]
kind = "strong"

# #### Webkit

[types."WTF::Bag".container]
[types."WTF::Deque".container]
[types."WTF::DoublyLinkedList".container]
# the values are the 2nd arg, don't have container support for this yet.
[types."WTF::EnumeratedArray".pointer]
kind = "contains"
arg_index = 1
[types."WTF::HashCountedSet".container]
[types."WTF::HashMap".container]
[types."WTF::HashSet".container]
[types."WTF::HashTable".container]
# the value is the 2nd arg which goes in an underlying Vector
[types."WTF::IndexMap".pointer]
kind = "contains"
arg_index = 1
# IndexSet has no payload; the underlying storage is just a BitVector
[types."WTF::ListHashSet".container]
[types."WTF::LocklessBag".container]
[types."WTF::MessageQueue".container]
[types."WTF::PriorityQueue".container]
[types."WTF::RefCountedFixedVector".container]
[types."WTF::ReferenceWrapperVector".container]
# This is a WTF::Vector of WTF::Refs and so this throws away our pointer info;
# maybe it would be better to not define this and instead heuristically
# understand what it's subclassing?
[types."WTF::RefVector".container]
[types."WTF::SegmentedVector".container]
[types."WTF::SentinelLinkedList".container]
[types."WTF::SinglyLinkedList".container]
[types."WTF::SinglyLinkedListWithTail".container]
[types."WTF::SmallSet".container]
[types."WTF::SortedArrayMap".container]
[types."WTF::ThreadSafeWeakHashSet".container]
[types."WTF::UniqueArray".container]
[types."WTF::UniqueRefVector".container]
[types."WTF::WeakHashCountedSet".container]
[types."WTF::WeakHashMap".container]
[types."WTF::WeakListHashSet".container]

[types."WTF::WeakHashSet".container]
[types."WTF::Vector".container]

# ### Variant Types ###
[types."mozilla::Variant".variant]

# XXX pair is not a variant, but for our purposes we just want all of its types
[types."std::pair".variant]
[types."std::tuple".variant]
[types."std::variant".variant]

# #### Webkit

# like std::pair, we just want both types
[types."WTF::KeyValuePair".variant]


# ### Sentinel Nothing Types ###
[types."mozilla::Nothing".nothing]

# ### Pointer Types ###
#
# These inform our processing of class fields.

# Doing this for Maybe<ServiceWorkerDescriptor> right now.
[types."mozilla::Maybe".pointer]
kind = "contains"

[types."std::optional".pointer]
kind = "contains"

[types."nsCOMPtr".pointer]
kind = "strong"

# explicitly not in the mozilla namespace
[types."RefPtr".pointer]
kind = "strong"

[types."mozilla::SafeRefPtr".pointer]
kind = "strong"

[types."mozilla::ThreadSafeWeakPtr".pointer]
kind = "weak"

[types."mozilla::CheckedUnsafePtr".pointer]
kind = "raw"

[types."mozilla::UniquePtr".pointer]
kind = "unique"

[types."mozilla::WeakPtr".pointer]
kind = "weak"

[types."WeakPtr".pointer]
kind = "weak"

[types."std::shared_ptr".pointer]
kind = "strong"

[types."shared_ptr".pointer]
kind = "strong"

[types."std::unique_ptr".pointer]
kind = "unique"

[types."unique_ptr".pointer]
kind = "unique"

[types."std::weak_ptr".pointer]
kind = "weak"

[types."weak_ptr".pointer]
kind = "weak"

[types."nsRefPtrHashKey".pointer]
kind = "strong"

# ### JS Wrapper Types ###

[types."JS::Result".value]

# ### JS Container Types ###

[types."js::HashMap".container]

[types."js::HashSet".container]

[types."js::ProtectedData".container]

[types."js::ProtectedDataNoCheckArgs".container]

[types."js::ProtectedDataWriteOnce".container]

[types."js::Vector".container]

# ### JS Pointer Types ###

[types."js::AtomicRefCounted".pointer]
kind = "strong"

[types."js::RefCounted".pointer]
kind = "strong"

# ### JS GC Wrapper Types ###

[types."js::GCPtr".pointer]
kind = "gcref"

[types."JS::Handle".pointer]
kind = "gcref"

[types."JS::Heap".pointer]
kind = "gcref"

[types."js::HeapPtr".pointer]
kind = "gcref"

[types."JS::TenuredHeap".pointer]
kind = "gcref"

[types."JS::MutableHandle".pointer]
kind = "gcref"

[types."JS::PersistentRooted".pointer]
kind = "gcref"

[types."JS::Rooted".pointer]
kind = "gcref"

# ### JS GC Collection Types ###

[types."JS::GCHashMap".container]

[types."js::GCRekeyableHashMap".container]

[types."JS::GCVector".container]

[types."JS::PersistentRootedVector".container]

[types."JS::RootedVector".container]

[types."JS::StackGCVector".container]

# #### Webkit
#
# Note that a shocking number of the header files provide no documentation and
# so the kinds are potentially vibe-based.

[types."WTF::Box".pointer]
kind = "strong"

[types."WTF::BoxPtr".pointer]
kind = "strong"

[types."WTF::CagedPtr".pointer]
kind = "raw"

[types."WTF::CagedUniquePtr".pointer]
kind = "unique"

[types."WTF::CheckedPtr".pointer]
kind = "raw"

[types."WTF::CheckedRef".pointer]
kind = "ref"

[types."WTF::CodePtr".pointer]
kind = "raw"

[types."WTF::CompactPtr".pointer]
kind = "raw"

[types."WTF::CompactRefPtr".pointer]
kind = "strong"

[types."WTF::DataRef".pointer]
kind = "strong"

# this doesn't do destruction so it feels wrong to call it unique
[types."WTF::MallocPtr".pointer]
kind = "raw"

[types."WTF::NakedPtr".pointer]
kind = "raw"

[types."WTF::NakedRef".pointer]
kind = "ref"

[types."WTF::Packed".pointer]
kind = "contains"

[types."WTF::RawPointer".pointer]
kind = "raw"

# RefPtr but no support for null
[types."WTF::Ref".pointer]
kind = "strong"

[types."WTF::RefPtr".pointer]
kind = "strong"

[types."WTF::SignedPtr".pointer]
kind = "raw"

[types."WTF::ThreadSafeWeakPtr".pointer]
kind = "weak"

[types."WTF::UniqueRef".pointer]
kind = "unique"

[types."WTF::WeakPtr".pointer]
kind = "weak"

[types."WTF::WeakObjCPtr".pointer]
kind = "weak"

```

## config_defaults/source_file_info_boxes.liquid
```
{% if concise.info contains "test" -%}
  {%- assign test_heading = "Test Info:" -%}
  {%- assign test_severity = "info" -%}
  {%- if concise.info.test contains "failure_count" and concise.info.test.failure_count > 0 -%}
    {%- assign test_heading = "Test Info: Errors" -%}
    {%- assign test_severity = "error" -%}
  {%- elsif concise.info.test contains "skip_if" and concise.info.test.skip_if != empty -%}
    {%- assign test_heading = "Test Info: Warnings" -%}
    {%- assign test_severity = "warning" -%}
  {%- elsif concise.info contains "wpt" -%}
    {%- if concise.info.wpt contains "disabling_conditions" or concise.info.wpt contains "disabled_subtests_count" -%}
      {%- assign test_heading = "Test Info: Warnings" -%}
      {%- assign test_severity = "warning" -%}
    {%- endif -%}
  {%- endif -%}
  {%- if concise.bugzilla_component == empty -%}
    {%- assign bugzillaString = "" %}
  {%- else -%}
    {%- assign bugzillaString = concise.bugzilla_component | join: "::" %}
{%- endif -%}
{%- capture double_newline %}

{% endcapture -%}
{%- capture newline %}
{% endcapture -%}
  <section class="info-box info-box-{{test_severity}}">
    <h4>{{ test_heading }}</h4>
    <div>
      <ul>
{% if concise.info.test contains "skip_if" and concise.info.test.skip_if != empty %}
        <li>This test gets skipped with pattern: <span class="test-skip-info">{{ concise.info.test.skip_if | replace: double_newline, newline | replace: newline, " OR " }}</span></li>
{%- endif -%}
{%- if concise.info.test contains "failure_count" and concise.info.test.failure_count > 0 %}
        <li>This test failed {{ concise.info.test.failure_count }} times in the preceding 30 days. <a href="https://bugzilla.mozilla.org/buglist.cgi?quicksearch={{ path }}">quicksearch this test</a></li>
{% endif -%}
{%- if concise.info contains "wpt" -%}
  {%- capture wpt_meta_url -%}/{{ tree }}/source/{{ path | replace_first: "/tests/", "/meta/" }}.ini{%- endcapture -%}
  {%- if concise.info.wpt contains "disabling_conditions" -%}
                {%- assign linked_meta = true %}
        <li>This test has a <a href="{{ wpt_meta_url }}">WPT meta file</a> that disables it given conditions:
            <ul>{%- for cond_bug in concise.info.wpt.disabling_conditions %}
                <li><span class="test-skip-info">{{ cond_bug[0] | strip }}</span>&nbsp; : <a href="{{ cond_bug[1] | ensure_bug_url }}">{{ cond_bug[1] }}</a></li>
                {%- endfor -%}
            </ul>
        </li>
  {%- endif -%}
  {%- if concise.info.wpt contains "disabled_subtests_count" %}
        <li>This test has a <a href="{{ wpt_meta_url }}">WPT meta file</a> that expects {{ concise.info.wpt.disabled_subtests_count }} subtest issues.</li>
  {% endif -%}
  {%- if concise.info.wpt contains "test_ids" %}
        <li>This WPT test may be referenced by the following <a href="https://firefox-source-docs.mozilla.org/web-platform/index.html#running-tests">Test IDs</a>:
            <ul>{%- for test_id in concise.info.wpt.test_ids %}
                <li>/{{ test_id }} - <a href="https://wpt.fyi/results/{{ test_id }}">WPT Dashboard</a> <a href="https://jgraham.github.io/wptdash/?bugComponent={{ bugzillaString | downcase }}#/{{ test_id }}">Interop Dashboard</a></li>
                {%- endfor -%}
            </ul>
        </li>
  {%- endif -%}
{% else %}
  {%- comment %}WPTs do have a normalized manifest for infrastructure
  purposes but it's not useful to show the user because it's just the
  (implicit) directory.{% endcomment -%}
  {%- if concise.info.test contains "manifest" and concise.info.test.manifest != empty %}
    {%- for aggr_manifest in concise.info.test.manifest -%}
              {%- assign manifests = aggr_manifest | split: ":" %}
        <li>Manifest: {% for manifest in manifests -%}
              {%- unless forloop.first %} includes {% endunless -%}<a href="/{{ tree }}/source/{{ manifest }}">{{ manifest }}</a>
            {%- endfor -%}</li>
    {%- endfor -%}
  {%- endif -%}
{%- endif %}
      </ul>
    </div>
  </section>
{%- endif -%}

```

## tests/tree-list.js
```
var TREE_LIST = [
  [
    {
      name: "Test",
      items: [
        { value: "tests" },
        { value: "searchfox" },
      ],
    },
    {
      name: "Firefox",
      items: [
        { value: "mozilla-central" },
        { value: "mozilla-beta" },
        { value: "mozilla-release" },
        { value: "mozilla-esr128" },
        { value: "mozilla-esr115" },
        { value: "mozilla-esr102" },
        { value: "mozilla-esr91" },
        { value: "mozilla-esr78" },
        { value: "mozilla-esr68" },
        { value: "mozilla-esr60" },
        { value: "mozilla-esr45" },
        { value: "mozilla-esr31" },
        { value: "mozilla-esr17" },
      ],
    },
  ],
  [
    {
      name: "Firefox other",
      items: [
        { value: "mozilla-mobile" },
        { value: "mozilla-elm" },
        { value: "mozilla-cedar" },
        { value: "mozilla-cypress" },
      ],
    },
    {
      name: "Thunderbird",
      items: [
        { value: "comm-central" },
        { value: "comm-esr128" },
        { value: "comm-esr115" },
        { value: "comm-esr102" },
        { value: "comm-esr91" },
        { value: "comm-esr78" },
        { value: "comm-esr68" },
        { value: "comm-esr60" },
      ],
    },
  ],
  [
    {
      name: "Searchfox",
      items: [
        { value: "mozsearch" },
        { value: "mozsearch-mozilla" },
        { value: "mozsearch-tests" },
      ],
    },
    {
      name: "MinGW",
      items: [
        { value: "mingw" },
        { value: "mingw-moz" },
      ],
    },
    {
      name: "Other",
      items: [
        { label: "KaiOS", value: "kaios" },
        { value: "glean" },
        { value: "l10n" },
        { label: "LLVM", value: "llvm" },
        { value: "nss" },
        { label: "Rust", value: "rust" },
        { label: "TC39 ECMA262 spec", value: "ecma262" },
        { value: "version-control-tools" },
        { label: "WHATWG HTML spec", value: "whatwg-html" },
        { value: "wubkat" },
      ]
    }
  ]
];

```

## tests/searchfox/setup
```
#!/usr/bin/env bash

set -x # Show commands
set -eu # Errors/undefined vars are fatal
set -o pipefail # Check all commands in a pipeline

# I think we used to try and delete the whole dir but we were messing it up.
#rm -rf $INDEX_ROOT

pushd $GIT_ROOT
if ! git diff-index --quiet HEAD --; then
    set +x
    echo ""
    echo "!!! CRITICAL !!!: Uncommitted changes found. Please commit them before building."
    echo ""
    git status
    exit 1
fi
popd

rm -rf $BLAME_ROOT
mkdir -p $BLAME_ROOT
git init $BLAME_ROOT
$MOZSEARCH_PATH/tools/target/release/build-blame $GIT_ROOT $BLAME_ROOT

# When actively developing it's nice to purge the existing contents, but the rest
# of the time it's nice to not reprocess it all.
rm -rf $HISTORY_ROOT

mkdir -p $HISTORY_ROOT
mkdir -p $HISTORY_ROOT/syntax
git init $HISTORY_ROOT/syntax
mkdir -p $HISTORY_ROOT/timeline
git init $HISTORY_ROOT/timeline
mkdir -p $HISTORY_ROOT/rev-summaries
$MOZSEARCH_PATH/tools/target/release/build-syntax-token-tree $GIT_ROOT $HISTORY_ROOT/syntax


# Link over the fake metadata and test file information as well.
ln -s -f $CONFIG_REPO/searchfox/metadata/code-coverage-report.json $INDEX_ROOT

```

## tests/webtest-config.json
```
{
  "mozsearch_path": "$MOZSEARCH_PATH",
  "config_repo": "$CONFIG_REPO",

  "default_tree": "tests",

  "trees": {
    "tests": {
      "priority": 100,
      "on_error": "halt",
      "cache": "everything",
      "index_path": "$WORKING/tests",
      "files_path": "$WORKING/tests/files",
      "webidl_binding_local_path": "webidl/bindings",
      "staticprefs_binding_local_path": "staticprefs/bindings",
      "objdir_path": "$WORKING/tests/objdir",
      "wpt_root": "testing/web-platform",
      "codesearch_path": "$WORKING/tests/livegrep.idx",
      "codesearch_port": 8080,
      "scip_subtrees": {}
    },
    "searchfox": {
      "priority": 100,
      "on_error": "halt",
      "cache": "everything",
      "index_path": "$WORKING/searchfox",
      "files_path": "$MOZSEARCH_PATH",
      "objdir_path": "$WORKING/searchfox/objdir",
      "git_path": "$MOZSEARCH_PATH",
      "git_blame_path": "$WORKING/searchfox/blame",
      "history_path": "$WORKING/searchfox/history",
      "codesearch_path": "$WORKING/searchfox/livegrep.idx",
      "codesearch_port": 8081,
      "scip_subtrees": {
        "rust": {
          "scip_index_path": "$WORKING/searchfox/objdir/tools/rust.scip",
          "subtree_root": "tools"
        }
      }
    }
  },

  "allow_webtest": true
}

```

## tests/webtest/head.js
```
"use strict";

/**
 * A class for communication with webtest command.
 */
class TestHarness {
  /**
   * Logs generated by TestHarness.log and wrappers, which are not
   * yet sent to webtest command.
   */
  static pendingLogs = [];

  /**
   * List of test functions added by add_task (or TestHarness.addTest)
   */
  static tests = [];

  /**
   * List of functions added by registerCleanupFunction
   */
  static cleanupFunctions = [];

  /**
   * Add log, this is sent to webtest command and also shown in console.
   *
   * @param {String} type
   *        See tools/src/cmd_pipeline/cmd_webtest.rs for the list.
   * @param {String} msg
   *        The message for the log.
   */
  static log(type, msg) {
    let color = "";
    switch (type) {
      case "INFO":
        // blue
        color = "light-dark(#0000FF, #6060FF)";
        break;
      case "DEBUG":
        // gray
        color = "light-dark(#A0A0A0, #A0A0A0)";
        break;
      case "PASS":
        // green
        color = "light-dark(#00A600, #40D940)";
        break;
      case "FAIL":
      case "STACK":
        // red
        color = "light-dark(#D00000, #FF4040)";
        break;
      case "TEST_START":
      case "TEST_END":
        // yellow
        color = "light-dark(#B0A000, #D0D040)";
        break;
      default:
        // cyan
        color = "light-dark(#00A6B2, #60E5E5)";
        break;
        // unused
        // magenta
        // color = "light-dark(#B200B2, #D040D0)";
    }
    console.log(`%c${type}%c - %s`,
                `color: ${color};`, "", msg);
    this.pendingLogs.push([type, `${msg}`]);
  }

  // Wrappers for logging.

  static pass(msg) {
    this.log("PASS", msg);
  }

  static fail(msg) {
    this.log("FAIL", msg);
    this.log("STACK", new Error().stack);
  }

  static info(msg) {
    this.log("INFO", msg);
  }

  static debug(msg) {
    this.log("DEBUG", msg);
  }

  /**
   * Add a test function which is an asynchronous function.
   *
   * @param {Function} func
   *        An async function to be run.
   */
  static addTest(func) {
    this.tests.push(func);
  }

  /**
   * Add a function to perform at the end of the current subtest.
   *
   * @param {Function} func
   *        An async function to be run.
   */
  static registerCleanupFunction(func) {
    this.cleanupFunctions.push(func);
  }

  static onBodyLoad() {
    const frameLocation = document.querySelector("#frame-location");
    const frameTitle = document.querySelector("#frame-title");
    const frame = document.querySelector("#frame");
    window.frame = frame;

    // Show the frame's location and title.
    const updateLocationAndTitle = () => {
      frameLocation.value = window.frame.contentDocument.location.href;
      frameTitle.value = window.frame.contentDocument.title;
    };
    const onFrameLoad = () => {
      updateLocationAndTitle();

      const originalPushState = window.frame.contentWindow.history.pushState;
      frame.contentWindow.history.pushState = (...args) => {
        setTimeout(updateLocationAndTitle, 10);
        originalPushState.call(frame.contentWindow.history, ...args);
      };
      const originalReplaceState = window.frame.contentWindow.history.replaceState;
      frame.contentWindow.history.replaceState = (...args) => {
        setTimeout(updateLocationAndTitle, 10);
        originalReplaceState.call(frame.contentWindow.history, ...args);
      };

      frame.contentDocument.addEventListener("titlechanged", () => {
        updateLocationAndTitle();
      });
    };
    frame.addEventListener("load", onFrameLoad);
    onFrameLoad();
  }

  /**
   * Run all subtests and clear them.
   */
  static async runTests() {
    try {
      for (const func of this.tests) {
        this.log("SUBTEST", func.name);
        this.info(`Entering test ${func.name}`);
        try {
          await func();
        } finally {
          const cleanupFunctions = this.cleanupFunctions.slice();
          this.cleanupFunctions.length = 0;

          for (const cleanup of cleanupFunctions) {
            await cleanup();
          }
        }
        this.info(`Leaving test ${func.name}`);
      }
    } catch (e) {
      this.log("FAIL", e.toString());
      this.log("STACK", e.stack);
    }

    this.tests.length = 0;
  }

  /**
   * Load a test file specified by `path`.
   * Once the test file is loaded, all subtests added by the file
   * are executed.
   *
   * This is called by webtest.
   *
   * @param {String} path
   *        The full path to the test, starting with "tests/webtest/"
   */
  static loadTest(path) {
    if (!path.startsWith("tests/webtest/")) {
      this.fail(`Unsupported test path ${path}`);
      return;
    }

    this.log("TEST_START", path);

    const script = document.createElement("script");
    script.src = `/${path}`;
    const onError = e => {
      this.log("FAIL", e.message);
    };
    window.addEventListener("error", onError);
    script.addEventListener("load", async () => {
      window.removeEventListener("error", onError);
      await this.runTests();
      this.log("TEST_END", path);
    });
    document.body.append(script);
  }

  /**
   * Returns newly added logs, and clear the log.
   *
   * This is called by webtest.
   *
   * @returns {Array<[String, Sring]>}
   *          List of logs
   */
  static getNewLogs() {
    const logs = this.pendingLogs.slice();
    this.pendingLogs.length = 0;
    return logs;
  }
}
window.TestHarness = TestHarness;

/**
 * A class provides utility functions for each testcase.
 */
class TestUtils {
  /**
   * Search functionality has timeout for reflecting user input to
   * query field and location.
   *
   * Shorten the timeout for the current page, in order to reduce the
   * time taken by the test.
   */
  static shortenSearchTimeouts() {
    const frame = document.querySelector("#frame");
    frame.contentWindow.Dxr.timeouts.search = 10;
    frame.contentWindow.Dxr.timeouts.history = 20;
  }

  /**
   * Load searchfox page and wait for load.
   *
   * @param {String} path
   *        The path part of the URL.
   * @returns {Promise<undefined>}
   *          Resolves when the page is loaded.
   */
  static async loadPath(path) {
    const loadPromise = this.waitForLoad();

    TestHarness.debug(`Loading ${path}`);

    const frame = document.querySelector("#frame");
    frame.src = path;

    await loadPromise;
  }

  /**
   * Wait for the subsequent page load.
   * This also waits for pageshow event, given the context menu
   * relies on the pageshow to be dispatched before starting interacting.
   *
   * @returns {Promise<undefined>}
   *          Resolves when the subsequent page load finishes.
   */
  static async waitForLoad() {
    TestHarness.debug(`Waiting for load`);

    const frame = document.querySelector("#frame");
    const loadEvent = Promise.withResolvers();
    const pageshowEvent = Promise.withResolvers();

    frame.addEventListener("load", () => {
      TestHarness.debug(`Observed load event`);
      frame.contentWindow.addEventListener("pageshow", () => {
        TestHarness.debug(`Observed pageshow event`);
        pageshowEvent.resolve();
      }, { once: true });
      loadEvent.resolve();
    }, { once: true });

    await Promise.all([
      loadEvent.promise,
      pageshowEvent.promise,
    ]);
  }

  /**
   * Emulate setting a text on an input element, with dispating input events.
   *
   * @param {Element} elem
   *        The input element.
   * @param {String} text
   *        The text for the element.
   */
  static setText(elem, text) {
    TestHarness.debug(`Setting text ${text}`);

    elem.value = text;
    const ev = new InputEvent("input", { bubbles: true });
    elem.dispatchEvent(ev);
  }

  /**
   * Emulate clicking a checkbox.
   *
   * @param {Element} elem
   *        The input element.
   */
  static clickCheckbox(elem) {
    TestHarness.debug(`Clicking checkbox`);

    elem.checked = !elem.checked;
    const ev = new Event("change", { bubbles: true });
    elem.dispatchEvent(ev);
  }

  /**
   * Emulate clicking on link etc.
   *
   * @param {Element} elem
   *        The element to be clicked.
   * @param {Object?} options
   *        The options for the mouse event.
   */
  static click(elem, options = { bubbles: true }) {
    TestUtils.dispatchMouseEvent("click", elem, options);
  }

  static dispatchMouseEvent(name, elem, options = { bubbles: true }) {
    TestHarness.debug(`Dispatching MouseEvent ${name}`);

    const ev = new MouseEvent(name, options);
    elem.dispatchEvent(ev);
  }

  /**
   * Emulate keypress event.
   *
   * @param {Element} elem
   *        The element for the event..
   * @param {Object} options
   *        The options for the event.
   */
  static keypress(elem, options) {
    TestHarness.debug(`dispatching keypress event`);

    const ev = new KeyboardEvent("keypress", options);
    elem.dispatchEvent(ev);
  }

  /**
   * Emulate keydown event.
   *
   * @param {Element} elem
   *        The element for the event..
   * @param {Object} options
   *        The options for the event.
   */
  static keydown(elem, options) {
    TestHarness.debug(`dispatching keydown event`);

    const ev = new KeyboardEvent("keydown", options);
    elem.dispatchEvent(ev);
  }

  /**
   * Emulate selecting a select option.
   *
   * @param {Element} elem
   *        The select element.
   * @param {String} value
   *        The value for the option.
   */
  static selectMenu(elem, value) {
    TestHarness.debug(`Selecting value ${value}`);

    elem.value = value;
    const ev = new Event("change", { bubbles: true });
    elem.dispatchEvent(ev);
  }

  /**
   * Returns a promise that resolves when timeout is exceeded.
   *
   * @returns {Promise<undefined>}
   *          Resolves when timeout is exceeded.
   */
  static sleep(timeout) {
    return new Promise(resolve => setTimeout(resolve, timeout));
  }

  /**
   * Will poll a condition function until it returns true.
   *
   * @param {Function} condition
   *        A condition function that must return true or false.
   * @param {String} msg
   *        The message for logging.
   * @param {Number} interval
   *        The time interval to poll the condition function. Defaults
   *        to 100ms.
   * @param {Number} maxTries
   *        The number of times to poll before giving up and rejecting
   *        if the condition has not yet returned true. Defaults to 50
   *        (~5 seconds for 100ms intervals)
   * @return {Promise<undefined>}
   *         Resolves when condition is true.
   *         Rejects if timeout is exceeded or condition ever throws.
   */
  static async waitForCondition(condition, msg, interval=100, maxTries=50) {
    TestHarness.debug(`Waiting for condition: ${msg}`);

    for (let i = 0; i < maxTries; i ++) {
      if (condition()) {
        TestHarness.pass(msg);
        return;
      }

      await this.sleep(interval);
    }

    TestHarness.fail(`${msg} - timed out after ${maxTries} tries.`);
  }

  /**
   * Returns true if the element is set to be shown.
   *
   * @param {Element} elem
   *        The element.
   */
  static isShown(elem) {
    return window.getComputedStyle(elem).display != "none";
  }

  /**
   * Wait until the element becomes shown.
   *
   * @param {Element} elem
   *        The element.
   * @return {Promise<undefined>}
   *         Resolves when the element becomes shown.
   */
  static async waitForShown(elem, ...args) {
    this.waitForCondition(() => {
      return this.isShown(elem);
    }, ...args);
  }

  /**
   * Test if the condition is true.
   *
   * @param {bool} condition
   *        If this is true, this test passes.
   *        Otherwise fails.
   * @param {String} msg
   *        The message for the condition.
   */
  static ok(condition, msg) {
    if (condition) {
      TestHarness.pass(msg);
    } else {
      TestHarness.fail(msg);
    }
  }

  /**
   * Test if the two values are equivalent.
   *
   * @param {any} actual
   *        The actual value.
   * @param {any} expected
   *        The expected value.
   * @param {String} msg
   *        The message for the condition.
   */
  static is(actual, expected, msg) {
    if (Object.is(actual, expected)) {
      TestHarness.pass(msg);
    } else {
      TestHarness.fail(`${msg} - Got ${actual}, expected ${expected}`);
    }
  }

  /**
   * Test if the two values are not equivalent.
   *
   * @param {any} actual
   *        The actual value.
   * @param {any} unexpected
   *        The unexpected value.
   * @param {String} msg
   *        The message for the condition.
   */
  static isnot(actual, unexpected, msg) {
    if (Object.is(actual, unexpected)) {
      TestHarness.fail(`${msg} - Didn't expect ${actual}, but got it`);
    } else {
      TestHarness.pass(msg);
    }
  }

  /**
   * Show informative log.
   *
   * @param {String} msg
   *        The log message.
   */
  static info(msg) {
    TestHarness.info(msg);
  }

  /**
   * Set feature gate value.
   *
   * @param {String} name
   *        The name of the feature gate (e.g. "semanticInfo")
   * @param {String} value
   *        The value of the feature gate.
   *        One fo "", "release", "beta", "alpha".
   */
  static async setFeatureGate(name, value) {
    await this.loadPath("/tests/pages/settings.html");

    const itemName = name.replace(/[A-Z]/g, m => "-" + m.toLowerCase());

    const menu = window.frame.contentDocument.querySelector(`#${itemName}--enabled`);

    this.selectMenu(menu, value);
  }

  static async resetFeatureGate(name) {
    await this.setFeatureGate(name, "");
  }

  /**
   * Set title behavior setting.
   *
   * @param {String} name
   *        The name of the title behavior setting (e.g. "lineSelection")
   * @param {bool} value
   *        The value of the title behavior setting.
   */
  static async setTitleBehavior(name, value) {
    await this.loadPath("/tests/pages/settings.html");

    const itemName = name.replace(/[A-Z]/g, m => "-" + m.toLowerCase());

    const checkbox = window.frame.contentDocument.querySelector(`#page-title--${itemName}`);
    if (checkbox.checked != value) {
      this.clickCheckbox(checkbox);
    }
  }

  /**
   * Click the line number in the source listing to select line(s).
   *
   * @param {Number} lineno
   *        The line number.
   * @param {Object?} options
   *        The options for the click event.
   */
  static selectLine(lineno, options = undefined) {
    const elem = window.frame.contentDocument.querySelector(`div[data-line-number="${lineno}"]`);

    TestUtils.click(elem, options);
  }
}
window.TestUtils = TestUtils;

for (const name of [
  "waitForCondition",
  "waitForShown",
  "ok",
  "is",
  "isnot",
  "info",
]) {
  window[name] = TestUtils[name].bind(TestUtils);
}

function add_task(func) {
  TestHarness.addTest(func);
}
function registerCleanupFunction(func) {
  TestHarness.registerCleanupFunction(func);
}

```

## tests/webtest/test_LineNumberInURL.js
```
"use strict";

add_task(async function test_LinNumberInUrl() {
  await TestUtils.loadPath("/tests/source/webtest/Webtest.cpp");

  is(frame.contentDocument.location.hash,
     "",
     "Hash is empty if no line is selected");

  TestUtils.selectLine(1);
  is(frame.contentDocument.location.hash,
     "#1",
     "Hash contains the selected line");

  TestUtils.selectLine(1, { shiftKey: true, bubbles: true });
  is(frame.contentDocument.location.hash,
     "",
     "Hash is empty if no line is selected");

  TestUtils.selectLine(3);
  TestUtils.selectLine(7, { shiftKey: true, bubbles: true });
  is(frame.contentDocument.location.hash,
     "#3-7",
     "Shift click selects line range");

  TestUtils.selectLine(9, { metaKey: true, bubbles: true });
  is(frame.contentDocument.location.hash,
     "#3-7,9",
     "Meta click adds line");

  TestUtils.selectLine(11, { shiftKey: true, bubbles: true });
  is(frame.contentDocument.location.hash,
     "#3-7,9-11",
     "Shift click adds line range from the last clicked line");

  TestUtils.selectLine(5, { metaKey: true, bubbles: true });
  is(frame.contentDocument.location.hash,
     "#3-4,6-7,9-11",
     "Meta click deselects line");

  TestUtils.selectLine(11);
  is(frame.contentDocument.location.hash,
     "#11",
     "Normal click deselects all lines and select the clicked line");
});

```

## tests/webtest/test_CopyAsMarkdown.js
```
"use strict";

function sanitizeURL(text) {
  return text.replace(/https?:\/\/[^\/]+\//, "BASE_URL/");
}

add_task(async function test_CopyAsMarkdown() {
  await TestUtils.loadPath("/tests/source/webtest/CopyAsMarkdown.cpp");

  // Hook the clipboard API for 2 reasons:
  //   * in order to check the copied text
  //   * given there's no user activity, the original writeText will throw
  let copiedText = null;
  frame.contentWindow.navigator.clipboard.writeText = async function(text) {
    copiedText = text;
  };

  const filenameButton = frame.contentDocument.querySelector(`button[title="Filename Link"]`);
  const symbolButton = frame.contentDocument.querySelector(`button[title="Symbol Link"]`);
  const codeButton = frame.contentDocument.querySelector(`button[title="Code Block"]`);

  ok(!filenameButton.disabled, "Filename Link should always be enabled");
  ok(symbolButton.disabled, "Symbol Link should be disabled if nothing is selected");
  ok(codeButton.disabled, "Code Block should be disabled if nothing is selected");

  TestUtils.click(filenameButton);
  is(sanitizeURL(copiedText),
     "[CopyAsMarkdown.cpp](BASE_URL/tests/source/webtest/CopyAsMarkdown.cpp)",
     "Filename is copied");

  copiedText = "";
  TestUtils.keypress(frame.contentDocument.documentElement, { bubbles: true, key: "F" });
  is(sanitizeURL(copiedText),
     "[CopyAsMarkdown.cpp](BASE_URL/tests/source/webtest/CopyAsMarkdown.cpp)",
     "Filename is copied");

  copiedText = "*unmodified*";
  TestUtils.click(symbolButton);
  is(copiedText, "*unmodified*", "Copy does not happen for disabled Symbol Link button");
  TestUtils.keypress(frame.contentDocument.documentElement, { bubbles: true, key: "S" });
  is(copiedText, "*unmodified*", "Copy does not happen for disabled Symbol Link accel");

  TestUtils.click(codeButton);
  is(copiedText, "*unmodified*", "Copy does not happen for disabled Code Block button");
  TestUtils.keypress(frame.contentDocument.documentElement, { bubbles: true, key: "C" });
  is(copiedText, "*unmodified*", "Copy does not happen for disabled Code Block accel");

  // Select the top level comment.
  TestUtils.selectLine(3);

  ok(!filenameButton.disabled, "Filename Link should always be enabled");
  ok(symbolButton.disabled, "Symbol Link should be disabled if no symbol is selected");
  ok(!codeButton.disabled, "Code Block should be enabled when a line is selected");

  TestUtils.click(codeButton);
  is(copiedText.replace(/https?:\/\/[^\/]+\//, "BASE_URL/"),
     "BASE_URL/tests/source/webtest/CopyAsMarkdown.cpp#3\n" +
     "```cpp\n" +
     "// Comment at the top level.\n" +
     "```",
     "Code block with single line is copied");

  copiedText = "";
  TestUtils.keypress(frame.contentDocument.documentElement, { bubbles: true, key: "C" });
  is(copiedText.replace(/https?:\/\/[^\/]+\//, "BASE_URL/"),
     "BASE_URL/tests/source/webtest/CopyAsMarkdown.cpp#3\n" +
     "```cpp\n" +
     "// Comment at the top level.\n" +
     "```",
     "Code block with single line is copied");

  copiedText = "*unmodified*";
  TestUtils.click(symbolButton);
  is(copiedText, "*unmodified*", "Copy does not happen for disabled Symbol Link button");
  TestUtils.keypress(frame.contentDocument.documentElement, { bubbles: true, key: "S" });
  is(copiedText, "*unmodified*", "Copy does not happen for disabled Symbol Link accel");

  // Select the global variable.
  TestUtils.selectLine(5);

  ok(!filenameButton.disabled, "Filename Link should always be enabled");
  ok(!symbolButton.disabled, "Symbol Link should be enabled when a symbol exists in the selected line");
  ok(!codeButton.disabled, "Code Block should be enabled when a line is selected");

  TestUtils.click(symbolButton);
  is(sanitizeURL(copiedText),
     "[globalVariable](BASE_URL/tests/source/webtest/CopyAsMarkdown.cpp#5)",
     "Global variable symbol is copied");

  copiedText = "";
  TestUtils.keypress(frame.contentDocument.documentElement, { bubbles: true, key: "S" });
  is(sanitizeURL(copiedText),
     "[globalVariable](BASE_URL/tests/source/webtest/CopyAsMarkdown.cpp#5)",
     "Global variable symbol is copied");

  // Select the namespace.

  TestUtils.selectLine(7);

  ok(!filenameButton.disabled, "Filename Link should always be enabled");
  ok(!symbolButton.disabled, "Symbol Link should be enabled when the selected line is inside a nesting line");
  ok(!codeButton.disabled, "Code Block should be enabled when a line is selected");

  TestUtils.click(symbolButton);
  is(sanitizeURL(copiedText),
     "[copy_as_markdown](BASE_URL/tests/source/webtest/CopyAsMarkdown.cpp#7)",
     "Namespace symbol is copied");

  // Select the comment inside namespace.
  TestUtils.selectLine(9);

  ok(!filenameButton.disabled, "Filename Link should always be enabled");
  ok(!symbolButton.disabled, "Symbol Link should be enabled when the selected line is inside a nesting line");
  ok(!codeButton.disabled, "Code Block should be enabled when a line is selected");

  // Select the method
  TestUtils.selectLine(14);

  ok(!filenameButton.disabled, "Filename Link should always be enabled");
  ok(!symbolButton.disabled, "Symbol Link should be enabled when the selected line is inside a nesting line");
  ok(!codeButton.disabled, "Code Block should be enabled when a line is selected");

  TestUtils.click(symbolButton);
  is(sanitizeURL(copiedText),
     "[copy_as_markdown::CopyAsMarkdown::SomeMethod](BASE_URL/tests/source/webtest/CopyAsMarkdown.cpp#14)",
     "Method symbol is copied");

  // Select the local variable
  TestUtils.selectLine(17);

  ok(!filenameButton.disabled, "Filename Link should always be enabled");
  ok(!symbolButton.disabled, "Symbol Link should be enabled when the selected line is inside a nesting line");
  ok(!codeButton.disabled, "Code Block should be enabled when a line is selected");

  TestUtils.click(symbolButton);
  is(sanitizeURL(copiedText),
     "[copy_as_markdown::CopyAsMarkdown::SomeMethod](BASE_URL/tests/source/webtest/CopyAsMarkdown.cpp#17)",
     "Method symbol is copied instead of local variable symbol");

  // Shift-select from the start of the class to the currently selected local variable.
  TestUtils.selectLine(11, { bubbles: true, shiftKey: true });

  ok(!filenameButton.disabled, "Filename Link should always be enabled");
  ok(!symbolButton.disabled, "Symbol Link should be enabled when the selected lines have symbol");
  ok(!codeButton.disabled, "Code Block should be enabled when lines are selected");

  TestUtils.click(codeButton);
  is(copiedText.replace(/https?:\/\/[^\/]+\//, "BASE_URL/"),
     "BASE_URL/tests/source/webtest/CopyAsMarkdown.cpp#11-17\n" +
     "```cpp\n" +
     "class CopyAsMarkdown {\n" +
     "  // Comment inside class.\n" +
     "\n" +
     "  void SomeMethod() {\n" +
     "    // Comment inside method.\n" +
     "\n" +
     "    bool LocalVariable = true;\n" +
     "```",
     "Code block with multiple lines are copied");

  // Select lines inside a block
  TestUtils.selectLine(14);
  TestUtils.selectLine(15, { bubbles: true, metaKey: true });
  TestUtils.selectLine(17, { bubbles: true, metaKey: true });

  TestUtils.click(codeButton);
  is(copiedText.replace(/https?:\/\/[^\/]+\//, "BASE_URL/"),
     "BASE_URL/tests/source/webtest/CopyAsMarkdown.cpp#14-15,17\n" +
     "```cpp\n" +
     "void SomeMethod() {\n" +
     "  // Comment inside method.\n" +
     "...\n" +
     "  bool LocalVariable = true;\n" +
     "```",
     "Code block with multiple lines are copied with dedent");

  // Disable accel

  const accelCheckbox = frame.contentDocument.querySelector("#panel-accel-enable");
  TestUtils.clickCheckbox(accelCheckbox);
  registerCleanupFunction(() => {
    TestUtils.clickCheckbox(accelCheckbox);
  });

  copiedText = "*unmodified*";
  TestUtils.keypress(frame.contentDocument.documentElement, { bubbles: true, key: "F" });
  is(copiedText, "*unmodified*", "Copy does not happen if accel is disabled");
  TestUtils.keypress(frame.contentDocument.documentElement, { bubbles: true, key: "S" });
  is(copiedText, "*unmodified*", "Copy does not happen if accel is disabled");
  TestUtils.keypress(frame.contentDocument.documentElement, { bubbles: true, key: "C" });
  is(copiedText, "*unmodified*", "Copy does not happen if accel is disabled");
});

add_task(async function test_CopyAsMarkdown_clicked() {
  registerCleanupFunction(async () => {
    await TestUtils.resetFeatureGate("fancyBar");
  });

  const tests = [
    { value: "release", enabled: false },
    { value: "beta", enabled: false },
    { value: "alpha", enabled: true },
  ];
  for (const { value, enabled } of tests) {
    await TestUtils.setFeatureGate("fancyBar", value);
    await TestUtils.loadPath("/tests/source/webtest/CopyAsMarkdown.cpp");

    const symbolButton = frame.contentDocument.querySelector(`button[title="Symbol Link"]`);

    let copiedText = null;
    frame.contentWindow.navigator.clipboard.writeText = async function(text) {
      copiedText = text;
    };

    TestUtils.selectLine(5);

    TestUtils.click(symbolButton);
    is(sanitizeURL(copiedText),
       "[globalVariable](BASE_URL/tests/source/webtest/CopyAsMarkdown.cpp#5)",
       "Global variable symbol is copied");

    const methodName = frame.contentDocument.querySelector(`span.syn_def[data-symbols="_ZN16copy_as_markdown14CopyAsMarkdown10SomeMethodEv"]`);
    TestUtils.click(methodName);

    copiedText = "*unmodified*";
    TestUtils.click(symbolButton);
    if (enabled) {
      is(sanitizeURL(copiedText),
         "[copy_as_markdown::CopyAsMarkdown::SomeMethod](BASE_URL/tests/source/webtest/CopyAsMarkdown.cpp#5,14)",
         "Method symbol is copied if clicked-symbol is enabled, with global variable line number in URL");
    } else {
      is(sanitizeURL(copiedText),
         "[globalVariable](BASE_URL/tests/source/webtest/CopyAsMarkdown.cpp#5)",
         "Global variable symbol is copied if clicked-symbol is disabled");
    }
  }
});

```

## tests/webtest/test_FieldLayoutForGenerated.js
```
"use strict";

add_task(async function test_FieldLayoutForGenerated() {
  const sym = "generated::GeneratedStruct";

  await TestUtils.loadPath(`/tests/query/default?q=field-layout:'${sym}'`);

  const symInLine = document.querySelectorAll(`span.syn_def[data-symbols="F_<T_generated::GeneratedStruct>_x"]`);
  ok(!!symInLine, "Symbol in the definition line exists");
});

```

## tests/webtest/test_SymbolSectionInPanel.js
```
"use strict";

add_task(async function test_SymbolSectionInPanel() {
  await TestUtils.resetFeatureGate("fancyBar");
  await TestUtils.loadPath("/tests/source/webtest/CopyAsMarkdown.cpp");

  // Hook the clipboard API for 2 reasons:
  //   * in order to check the copied text
  //   * given there's no user activity, the original writeText will throw
  let copiedText = null;
  frame.contentWindow.navigator.clipboard.writeText = async function(text) {
    copiedText = text;
  };

  const symBox = frame.contentDocument.querySelector(".selected-symbol-box");

  is(symBox.textContent,
     "(no symbol clicked)",
     "No symbol is shown when nothing is selected");

  // Select the top level comment.
  TestUtils.selectLine(3);

  is(symBox.textContent,
     "(no symbol clicked)",
     "No symbol is shown when nothing is selected");

  // Select the global variable.
  TestUtils.selectLine(5);

  is(symBox.textContent,
     "globalVariable",
     "Selected global variable name is shown");

  // Select class name.
  TestUtils.selectLine(11);

  is(symBox.textContent,
     "copy_as_markdown::CopyAsMarkdown",
     "Selected class's qualified name is shown");

  // Click method name.
    const methodName = frame.contentDocument.querySelector(`span.syn_def[data-symbols="_ZN16copy_as_markdown14CopyAsMarkdown10SomeMethodEv"]`);
  TestUtils.click(methodName);

  is(symBox.textContent,
     "copy_as_markdown::CopyAsMarkdown::SomeMethod",
     "Selected method's qualified name is shown");

  const copyButton = frame.contentDocument.querySelector(".copy-box .indicator");
  TestUtils.click(copyButton);
  is(copiedText,
     "copy_as_markdown::CopyAsMarkdown::SomeMethod",
     "The selected symbol is copied");
});

add_task(async function test_SymbolSectionInPanel_gate() {
  registerCleanupFunction(async () => {
    await TestUtils.resetFeatureGate("fancyBar");
  });

  const tests = [
    { value: "release", shown: false },
    { value: "beta", shown: false },
    { value: "alpha", shown: true },
  ];
  for (const { value, shown } of tests) {
    await TestUtils.setFeatureGate("fancyBar", value);
    await TestUtils.loadPath("/tests/source/webtest/CopyAsMarkdown.cpp");

    const symBox = frame.contentDocument.querySelector(".selected-symbol-box");
    if (shown) {
      ok(!!symBox, "Symbol section should be shown if enabled");
    } else {
      ok(!symBox, "Symbol section should be shown if disabled");
    }
  }
});

```

## tests/webtest/test_MacroExpansions.js
```
"use strict";

add_task(async function test_MacroExpansionsContextMenu() {
  // Enabled by default on the test config.
  await TestUtils.resetFeatureGate("expansions");
  await TestUtils.loadPath("/tests/source/macro.cpp");

  const perTargetFunctionExpansionPoint = frame.contentDocument.querySelector("span[data-expansions*=per_target_function]");
  TestUtils.click(perTargetFunctionExpansionPoint);

  const menu = frame.contentDocument.querySelector("#context-menu");
  await waitForShown(menu, "Context menu is shown for macro click");

  const expansions = JSON.parse(perTargetFunctionExpansionPoint.dataset.expansions);
  is(Object.keys(expansions).length, 3, "3 expansions are available");

  const expansionRows = menu.querySelectorAll(".contextmenu-expansion-preview");
  is(expansionRows.length, 3, "3 expansion rows are visible");

  const expectedPlatform = "win64";
  const expansionRow = expansionRows[0];
  // needs to be cancelable because context menu actions are <a href="#> and use preventDefault on click
  TestUtils.click(expansionRow, { bubbles: true, cancelable: true });

  const blamePopup = frame.contentWindow.BlamePopup;
  const blameStripHoverHandler = frame.contentWindow.BlameStripHoverHandler;
  await waitForShown(blamePopup.popup, "BlamePopup is shown");

  ok(blameStripHoverHandler.keepVisible, "BlamePopup won't be dismissed on mouseleave");
  is(blamePopup.popupOwner, perTargetFunctionExpansionPoint, "BlamePopup is related to the macro use");
  ok(blamePopup.popup.innerHTML.includes(expectedPlatform), "BlamePopup shows the right platform");

  const functionDefinition = blamePopup.popup.querySelector("span.syn_def[data-symbols=_Z19per_target_functionv]");
  TestUtils.click(functionDefinition);

  const inCodeMenu = frame.contentDocument.querySelector("#context-menu");
  await waitForShown(inCodeMenu, "Context menu is shown for function definition in macro expansion");

  const macroSpanDefinition = blamePopup.popup.querySelector(`span[data-symbols*="M_"]`);
  TestUtils.click(macroSpanDefinition);

  const inTitleMenu = frame.contentDocument.querySelector("#context-menu");
  await waitForShown(inTitleMenu, "Context menu is shown for the macro name in the title");
});

add_task(async function test_MacroExpansionsContextMenu_gate() {
  registerCleanupFunction(async () => {
    await TestUtils.resetFeatureGate("expansions");
  });

  const tests = [
    { value: "release", shown: false },
    { value: "beta", shown: false },
    { value: "alpha", shown: true },
  ];
  for (const { value, shown } of tests) {
    await TestUtils.setFeatureGate("expansions", value);
    await TestUtils.loadPath("/tests/source/macro.cpp");

    const perTargetFunctionExpansionPoint = frame.contentDocument.querySelector("span[data-expansions*=per_target_function]");
    TestUtils.click(perTargetFunctionExpansionPoint);

    const menu = frame.contentDocument.querySelector("#context-menu");
    await waitForShown(menu, "Context menu is shown for symbol click");

    const expansions = menu.querySelector(".contextmenu-expansion-preview");
    if (shown) {
      ok(!!expansions, `Expansion menu items exist on ${value}`);
    } else {
      ok(!expansions, `Expansion menu items do not exist on ${value}`);
    }
  }
});

```

## tests/webtest/test_SpaceInFilename.js
```
add_task(async function test_SpaceInFilenameInSearch() {
  await TestUtils.loadPath("/");
  TestUtils.shortenSearchTimeouts();

  const query = frame.contentDocument.querySelector("#query");
  TestUtils.setText(query, "SymbolInFilenameWithSpace");

  const content = frame.contentDocument.querySelector("#content");

  await waitForCondition(
    () => content.textContent.includes("Core code (1 lines") &&
      content.textContent.includes("Definitions (SymbolInFilenameWithSpace) (1 lines") &&
      content.textContent.includes("var SymbolInFilenameWithSpace"),
    "symbol in file with space in filename matches as definition");

  const links = frame.contentDocument.querySelectorAll(".result-head a");
  is(links.length, 2);
  is(links[1].getAttribute("href"), "/tests/source/js/with%20space.js",
     "The space in the href should be escaped");
});

add_task(async function test_SpaceInFilenameInFileView() {
  await TestUtils.loadPath("/tests/source/js/with%20space.js");

  const links = frame.contentDocument.querySelectorAll(".breadcrumbs a");

  is(links.length, 3);
  is(links[2].getAttribute("href"), "/tests/source/js/with%20space.js",
     "The space in the href should be escaped");
});

add_task(async function test_SpaceInFilenameInNavigationPanel() {
  await TestUtils.loadPath("/searchfox/source/tests/tests/files/js/with%20space.js");

  const panel = frame.contentDocument.getElementById("panel");
  const permalink = panel.querySelector(`.item[title="Permalink"]`);

  ok(permalink.getAttribute("href").includes("/js/with%20space.js"),
     "The space in the href should be escaped");
});

add_task(async function test_SpaceInFilenameInBlameAndOldRevision() {
  await TestUtils.loadPath("/searchfox/source/tests/tests/files/js/with%20space.js");

  // Test the blame popup

  const blameStrip = frame.contentDocument.querySelector(`#line-2 .blame-strip`);

  TestUtils.dispatchMouseEvent("mouseenter", blameStrip);

  function getLinks() {
    return frame.contentDocument.querySelectorAll(`#blame-popup a`);
  }

  await waitForCondition(() => getLinks().length > 0);

  const links = getLinks();
  is(links.length, 4);
  is(links[1].textContent, "annotated diff");
  ok(links[1].getAttribute("href").includes("/js/with%20space.js"),
     "The space in the href should be escaped");

  const annotatedDiffURL = links[1].href;

  is(links[2].textContent, "Show latest version without this line");
  ok(links[2].getAttribute("href").includes("/js/with%20space.js"),
     "The space in the href should be escaped");

  is(links[3].textContent, "Show earliest version with this line");
  ok(links[3].getAttribute("href").includes("/js/with%20space.js"),
     "The space in the href should be escaped");

  TestUtils.click(links[2]);

  await waitForCondition(
    () => frame.contentDocument.location.href.includes("/searchfox/rev"),
    "Navigates to the previous version");

  // In order to avoid hard-coding the old version's hash, continue testing
  // the UI part of the old version page here.

  // Test breadcrumbs.
  {
    const links = frame.contentDocument.querySelectorAll(".breadcrumbs a");

    is(links.length, 6);
    is(links[5].getAttribute("href"), "/searchfox/source/tests/tests/files/js/with%20space.js",
       "The space in the href should be escaped");
  }

  // Test navigation panel.
  {
    const panel = frame.contentDocument.getElementById("panel");
    const goToLatestLink = panel.querySelector(`.item[title="Go to latest version"]`);

    ok(goToLatestLink.getAttribute("href").includes("/js/with%20space.js"),
       "The space in the href should be escaped");
  }
});

add_task(async function test_SpaceInFilenameInAnnotatedDiffAndChangeset() {
  await TestUtils.loadPath("/searchfox/source/tests/tests/files/js/with%20space.js");

  const blameStrip = frame.contentDocument.querySelector(`#line-2 .blame-strip`);

  TestUtils.dispatchMouseEvent("mouseenter", blameStrip);

  function getLinks() {
    return frame.contentDocument.querySelectorAll(`#blame-popup a`);
  }

  await waitForCondition(() => getLinks().length > 0);

  const links = getLinks();
  is(links.length, 4);
  is(links[1].textContent, "annotated diff");
  ok(links[1].getAttribute("href").includes("/js/with%20space.js"),
     "The space in the href should be escaped");

  TestUtils.click(links[1]);

  await waitForCondition(
    () => frame.contentDocument.location.href.includes("/searchfox/diff"),
    "Navigates to the previous version");

  // Test breadcrumbs.
  {
    const links = frame.contentDocument.querySelectorAll(".breadcrumbs a");

    is(links.length, 6);
    is(links[5].getAttribute("href"), "/searchfox/source/tests/tests/files/js/with%20space.js",
       "The space in the href should be escaped");
  }

  // Test navigation panel.
  {
    const panel = frame.contentDocument.getElementById("panel");
    const goToLatestLink = panel.querySelector(`.item[title="Go to latest version"]`);

    ok(goToLatestLink.getAttribute("href").includes("/js/with%20space.js"),
       "The space in the href should be escaped");

    // In order to avoid hard-coding the old version's hash, continue testing
    // the UI part of the changeset view.

    const changesetLink = panel.querySelector(`.item[title="Show changeset"]`);

    TestUtils.click(changesetLink);
  }

  await waitForCondition(
    () => frame.contentDocument.location.href.includes("/searchfox/commit"),
    "Navigates to the changeset view");

  // Test file listing.
  {
    const link = frame.contentDocument.querySelector(`#content ul li a[href*="space.js"]`);

    ok(link.getAttribute("href").includes("/js/with%20space.js"),
       "The space in the href should be escaped");
  }
});

```

## tests/webtest/test_UTF8InFilename.js
```
add_task(async function test_UTF8InFilename_Search() {
  await TestUtils.loadPath("/");
  TestUtils.shortenSearchTimeouts();

  const query = frame.contentDocument.querySelector("#query");
  TestUtils.setText(query, "SymbolInFilenameWithUTF8");

  const content = frame.contentDocument.querySelector("#content");

  await waitForCondition(
    () => content.textContent.includes("Core code (1 lines") &&
      content.textContent.includes("Definitions (SymbolInFilenameWithUTF8) (1 lines") &&
      content.textContent.includes("var SymbolInFilenameWithUTF8"),
    "symbol in file with space in filename matches as definition");
});

add_task(async function test_UTF8InFilename_DirAndFile() {
  await TestUtils.loadPath("/tests/source/js");

  const link = frame.contentDocument.querySelector(`.folder-content a[href*="with-UTF8-"]`);
  ok(!!link, "UTF-8 filename is listed");
  is(link.getAttribute("href"), "/tests/source/js/with-UTF8-ãƒ•ã‚¡ã‚¤ãƒ«.js",
     "UTF-8 path is used with raw text");

  const loadPromise = TestUtils.waitForLoad();
  TestUtils.click(link);
  await loadPromise;

  ok(frame.contentDocument.location.href.includes(
       "with-UTF8-%E3%83%95%E3%82%A1%E3%82%A4%E3%83%AB.js"),
     "Navigated to the file page with URL-encoded path");

  const breadcrumbs = frame.contentDocument.querySelector(`.breadcrumbs`);
  ok(breadcrumbs.textContent.includes("/js/with-UTF8-ãƒ•ã‚¡ã‚¤ãƒ«.js"),
     "UTF-8 path is written in breadcrumbs with raw text");
});

```

## tests/webtest/test_FieldLayoutContextMenu.js
```
"use strict";

function findClassLayoutMenuItem(menu) {
  for (const row of menu.querySelectorAll(".contextmenu-row")) {
    if (row.textContent.startsWith("Class layout of ")) {
      return row;
    }
  }

  return null;
}

add_task(async function test_FieldLayoutContextMenu() {
  // Enabled by default on the test config.
  await TestUtils.resetFeatureGate("semanticInfo");
  await TestUtils.loadPath("/tests/source/field-layout/field-type.cpp");

  const className = frame.contentDocument.querySelector(`span.syn_def[data-symbols="T_field_layout::field_type::S"]`);
  TestUtils.click(className);

  const menu = frame.contentDocument.querySelector("#context-menu");
  await waitForShown(menu, "Context menu is shown for symbol click");

  let layoutRow = findClassLayoutMenuItem(menu);
  ok(!!layoutRow, "Class layout menu item exists");
  is(layoutRow.textContent, "Class layout of field_layout::field_type::S",
     "Menu item shows the qualified class name");

  const loadPromise = TestUtils.waitForLoad();
  const link = layoutRow.querySelector(".contextmenu-link");
  TestUtils.click(link);
  await loadPromise;
  ok(frame.contentDocument.location.href.includes("/query/"),
     "Navigated to query page");

  const query = frame.contentDocument.querySelector("#query");
  is(query.value, "field-layout:'field_layout::field_type::S'",
     "Query for field layout is set");
});

add_task(async function test_FieldLayoutContextMenu_gate() {
  registerCleanupFunction(async () => {
    await TestUtils.resetFeatureGate("semanticInfo");
  });

  const tests = [
    { value: "release", shown: false },
    { value: "beta", shown: false },
    { value: "alpha", shown: true },
  ];
  for (const { value, shown } of tests) {
    await TestUtils.setFeatureGate("semanticInfo", value);
    await TestUtils.loadPath("/tests/source/field-layout/field-type.cpp");

    const className = frame.contentDocument.querySelector(`span.syn_def[data-symbols="T_field_layout::field_type::S"]`);
    TestUtils.click(className);

    const menu = frame.contentDocument.querySelector("#context-menu");
    await waitForShown(menu, "Context menu is shown for symbol click");

    let layoutRow = findClassLayoutMenuItem(menu);
    if (shown) {
      ok(!!layoutRow, `Class layout menu item exists on ${value}`);
    } else {
      ok(!layoutRow, `Class layout menu item does not exist on ${value}`);
    }
  }
});

add_task(async function test_FieldLayoutContextMenu_emptySubClass() {
  // Enabled by default on the test config.
  await TestUtils.resetFeatureGate("semanticInfo");
  await TestUtils.loadPath("/tests/source/field-layout/empty-subclass.cpp");

  const className = frame.contentDocument.querySelector(`span.syn_def[data-symbols="T_field_layout::empty_subclass::T"]`);
  TestUtils.click(className);

  const menu = frame.contentDocument.querySelector("#context-menu");
  await waitForShown(menu, "Context menu is shown for symbol click");

  let layoutRow = findClassLayoutMenuItem(menu);
  ok(!!layoutRow, "Class layout menu item exists for empty subclass");
  is(layoutRow.textContent, "Class layout of field_layout::empty_subclass::T",
     "Menu item shows the qualified class name");

  const loadPromise = TestUtils.waitForLoad();
  const link = layoutRow.querySelector(".contextmenu-link");
  TestUtils.click(link);
  await loadPromise;
  ok(frame.contentDocument.location.href.includes("/query/"),
     "Navigated to query page");

  const query = frame.contentDocument.querySelector("#query");
  is(query.value, "field-layout:'field_layout::empty_subclass::T'",
     "Query for field layout is set");
});

```

## tests/webtest/test_WebIDLBindings.js
```
"use strict";

function findMenuItem(menu, text) {
  for (const row of menu.querySelectorAll(".contextmenu-row")) {
    if (row.textContent.includes(text)) {
      return row;
    }
  }

  return null;
}

add_task(async function test_WebIDLBindings() {
  await TestUtils.loadPath("/tests/source/webidl/BindingTest.webidl");

  const tests = [
    {
      sym: "WEBIDL_BindingTest",
      items: [
        "mozilla::dom::BindingTest_Binding",
      ],
    },
    {
      sym: "WEBIDL_BindingTest_constructor",
      items: [
        "mozilla::dom::BindingTest_Binding::_constructor",
      ],
    },
    {
      sym: "WEBIDL_BindingTest_CONST_1",
      items: [
        "mozilla::dom::BindingTest_Binding::CONST_1",
      ],
    },
    {
      sym: "WEBIDL_BindingTest_attr1",
      items: [
        "mozilla::dom::BindingTest_Binding::get_attr1",
        "mozilla::dom::BindingTest_Binding::set_attr1",
        "mozilla::dom::BindingTest::GetAttr1",
        "mozilla::dom::BindingTest::SetAttr1",
      ],
    },
    {
      sym: "WEBIDL_BindingTest_method1",
      items: [
        "mozilla::dom::BindingTest_Binding::method1",
        "mozilla::dom::BindingTest::Method1",
      ],
    },
    {
      sym: "WEBIDL_BindingTestDict",
      items: [
        "mozilla::dom::BindingTestDict",
      ],
    },
    {
      sym: "WEBIDL_BindingTestDict_prop1",
      items: [
        "mozilla::dom::BindingTestDict::mProp1",
      ],
    },
    {
      sym: "WEBIDL_BindingTestEnum",
      items: [
        "mozilla::dom::BindingTestEnum",
      ],
    },
  ];
  for (const { sym, items } of tests) {
    const elem = frame.contentDocument.querySelector(`span.syn_def[data-symbols="${sym}"]`);
    ok(!!elem, `Symbol element exists for ${sym}`);

    TestUtils.click(elem);

    const menu = frame.contentDocument.querySelector("#context-menu");
    await waitForShown(menu, `Context menu is shown when clicking ${sym} symbol`);

    for (const item of items) {
      const row = findMenuItem(menu, item);
      ok(!!row, `Menu item for ${item} exists`);
    }
  }
});

add_task(async function test_WebIDLBindingsWithMixin() {
  await TestUtils.loadPath("/tests/source/webidl/BindingTestMixin.webidl");

  const tests = [
    {
      sym: "WEBIDL_BindingTestMixin_MIXIN_CONST",
      items: [
        "mozilla::dom::BindingTestMixed1_Binding::MIXIN_CONST",
        "mozilla::dom::BindingTestMixed2_Binding::MIXIN_CONST",
      ],
    },
    {
      sym: "WEBIDL_BindingTestMixin_mixinAttr",
      items: [
        "mozilla::dom::BindingTestMixed1_Binding::get_mixinAttr",
        "mozilla::dom::BindingTestMixed1_Binding::set_mixinAttr",
        "mozilla::dom::BindingTestMixed1::GetMixinAttr",
        "mozilla::dom::BindingTestMixed1::SetMixinAttr",
        "mozilla::dom::BindingTestMixed2_Binding::get_mixinAttr",
        "mozilla::dom::BindingTestMixed2_Binding::set_mixinAttr",
        "mozilla::dom::BindingTestMixed2::GetMixinAttr",
        "mozilla::dom::BindingTestMixed2::SetMixinAttr",
      ],
    },
    {
      sym: "WEBIDL_BindingTestMixin_mixinMethod",
      items: [
        "mozilla::dom::BindingTestMixed1_Binding::mixinMethod",
        "mozilla::dom::BindingTestMixed1::MixinMethod",
        "mozilla::dom::BindingTestMixed2_Binding::mixinMethod",
        "mozilla::dom::BindingTestMixed2::MixinMethod",
      ],
    },
  ];
  for (const { sym, items } of tests) {
    const elem = frame.contentDocument.querySelector(`span.syn_def[data-symbols="${sym}"]`);
    ok(!!elem, `Symbol element exists for ${sym}`);

    TestUtils.click(elem);

    const menu = frame.contentDocument.querySelector("#context-menu");
    await waitForShown(menu, `Context menu is shown when clicking ${sym} symbol`);

    for (const item of items) {
      const row = findMenuItem(menu, item);
      ok(!!row, `Menu item for ${item} exists`);
    }
  }
});

```

## tests/webtest/webtest.html
```
<!DOCTYPE html>
<html lang="en-US" class="">
<head>
  <meta charset="utf-8" />
  <link href="/tests/static/icons/search.png" rel="shortcut icon">
  <title>webtest - mozsearch</title>
<style>
body {
  margin: 0;
}
#frame-location-and-title {
  width: 100vw;
  height: 24px;
  display: grid;
  grid-template-columns: 50vw 50vw;
}
#frame-location {
  height: 24px;
  box-sizing: border-box;
}
#frame-title {
  height: 24px;
  box-sizing: border-box;
}
#frame {
  width: calc(100vw);
  height: calc(100vh - 8px - 24px);
  box-sizing: border-box;
}
</style>
<script src="./head.js"></script>
</head>

<body onload="TestHarness.onBodyLoad()">
<div id="frame-location-and-title"><input id="frame-location" type="text" readonly><input id="frame-title" type="text" readonly></div>
<iframe id="frame" src="/"></iframe>
</body>

</html>

```

## tests/webtest/test_Titler.js
```
"use strict";

add_task(async function test_Titler() {
  registerCleanupFunction(async () => {
    await TestUtils.setTitleBehavior("lineSelection", true);
    await TestUtils.setTitleBehavior("stickySymbol", true);
  });
  const tests = [
    { lineSelection: false, stickySymbol: false },
    { lineSelection: false, stickySymbol: true },
    { lineSelection: true, stickySymbol: false },
    { lineSelection: true, stickySymbol: true },
  ];

  for (const { lineSelection, stickySymbol } of tests) {
    await TestUtils.setTitleBehavior("lineSelection", lineSelection);
    await TestUtils.setTitleBehavior("stickySymbol", stickySymbol);
    await TestUtils.loadPath("/tests/source/webtest/Titler.cpp");

    is(frame.contentDocument.title,
       "Titler.cpp - mozsearch",
       "Filename is shown in the title");

    TestUtils.selectLine(1);
    if (lineSelection) {
      is(frame.contentDocument.title,
         "globalVariable (Titler.cpp - mozsearch)",
         "Symbol in the selected line is shown in the title if enabled");
    } else {
      is(frame.contentDocument.title,
         "Titler.cpp - mozsearch",
         "Symbol in the selected line is not shown in the title if disabled");
    }

    TestUtils.selectLine(1, { shiftKey: true, bubbles: true });
    is(frame.contentDocument.title,
       "Titler.cpp - mozsearch",
       "Symbol disappears after unselecting the line.");

    const scrolling = frame.contentDocument.querySelector("#scrolling");
    scrolling.scrollTop = scrolling.scrollHeight;

    if (stickySymbol) {
      await waitForCondition(
        () => frame.contentDocument.title == "pagetitler (Titler.cpp - mozsearch)",
        "Sticky symbol is shown in the title");
    } else {
      await TestUtils.sleep(100);
      is(frame.contentDocument.title,
         "Titler.cpp - mozsearch",
         "Sticky symbol is not shown in the title if disabled");
    }

    TestUtils.selectLine(190);
    if (lineSelection) {
      is(frame.contentDocument.title,
         "p:PageTitler (Titler.cpp - mozsearch)",
         "Symbol in the selected line is shown with shortened namespace in the title if enabled");
    } else if (stickySymbol) {
      await waitForCondition(
        () => frame.contentDocument.title == "pagetitler (Titler.cpp - mozsearch)",
        "Sticky symbol is shown in the title but symbol in the selected line is not shown if disabled");
    } else {
      is(frame.contentDocument.title,
         "Titler.cpp - mozsearch",
         "Symbol in the selected line is not shown in the title if disabled");
    }
  }
});

```

## tests/webtest/test_Panel.js
```
"use strict";

add_task(async function test_PanelOnLoad() {
  const tests = [
    {
      path: "/",
      expanded: false,
      empty: true,
    },
    {
      path: "/tests/pages/settings.html",
      expanded: false,
      empty: true,
    },
    {
      path: "/tests/source/",
      expanded: false,
      empty: true,
    },
    {
      path: "/tests/source/webtest",
      expanded: false,
      empty: true,
    },
    {
      path: "/tests/source/.gitignore",
      expanded: true,
      empty: false,
    },
    {
      path: "/tests/source/webtest/Webtest.cpp",
      expanded: true,
      empty: false,
    },
    {
      path: "/tests/search?q=webtest&path=&case=false&regexp=false",
      expanded: false,
      empty: true,
    },
    {
      path: "/tests/query/default?q=webtest",
      expanded: false,
      // Debug items are added on tests repository.
      empty: false,
    },
    {
      path: "/searchfox/diff/4e266f75295afe5f94d14eb9b72445c830c095ef/.eslintrc.js",
      expanded: true,
      empty: false,
    },
    {
      path: "/searchfox/commit/4e266f75295afe5f94d14eb9b72445c830c095ef",
      expanded: false,
      empty: true,
    },
    {
      path: "/searchfox/rev/e6ff7d3798a68e41c1166524be276fac4a8dfeb2/.gitignore",
      expanded: true,
      empty: false,
    },
  ];

  for (const { path, expanded, empty } of tests) {
    await TestUtils.loadPath(path);

    const panel = frame.contentDocument.querySelector("#panel");
    ok(!!panel, `Navigation panel node exists on ${path}`);

    const content = frame.contentDocument.querySelector("#panel-content");
    if (expanded) {
      is(content.getAttribute("aria-expanded"), "true",
         `Navigation panel is expanded on ${path}`);
    } else {
      is(content.getAttribute("aria-expanded"), "false",
         `Navigation panel is collapsed on ${path}`);
    }

    if (empty) {
      is(content.children.length, 1,
         `Navigation panel has only keyboard shortcut checkbox on ${path}`);
    } else {
      is(content.children.length > 1, true,
         `Navigation panel has multiple items on ${path}`);
    }
  }
});

add_task(async function test_PanelAfterSearch() {
  const tests = [
    {
      path: "/",
    },
    {
      path: "/tests/pages/settings.html",
    },
    {
      path: "/tests/source/",
    },
    {
      path: "/tests/source/webtest",
    },
    {
      path: "/tests/source/.gitignore",
    },
    {
      path: "/tests/source/webtest/Webtest.cpp",
    },
    {
      path: "/tests/search?q=webtest&path=&case=false&regexp=false",
    },
    {
      path: "/searchfox/diff/4e266f75295afe5f94d14eb9b72445c830c095ef/.eslintrc.js",
    },
    {
      path: "/searchfox/commit/4e266f75295afe5f94d14eb9b72445c830c095ef",
    },
    {
      path: "/searchfox/rev/e6ff7d3798a68e41c1166524be276fac4a8dfeb2/.gitignore",
    },
  ];

  for (const { path, expanded, empty } of tests) {
    await TestUtils.loadPath(path);
    TestUtils.shortenSearchTimeouts();

    const query = frame.contentDocument.querySelector("#query");
    TestUtils.setText(query, "SimpleSearch");

    const content = frame.contentDocument.querySelector("#content");
    await waitForCondition(
      () => content.textContent.includes("Number of results:"),
      "Search result is shown");

    const panel = frame.contentDocument.querySelector("#panel");
    ok(!!panel, `Navigation panel node exists on the search result from ${path}`);

    const panelContent = frame.contentDocument.querySelector("#panel-content");
    is(panelContent.getAttribute("aria-expanded"), "false",
       `Navigation panel is collapsed on the search result from ${path}`);
    is(panelContent.children.length, 1,
       `Navigation panel has only keyboard shortcut checkbox on the search result from ${path}`);
  }
});

```

## tests/webtest/test_SearchSection.js
```
"use strict";

add_task(async function test_CollapseSearchSection() {
  await TestUtils.loadPath("/");
  TestUtils.shortenSearchTimeouts();

  const query = frame.contentDocument.querySelector("#query");
  TestUtils.setText(query, "SimpleSearch");

  const content = frame.contentDocument.querySelector("#content");

  await waitForCondition(
    () => content.textContent.includes("Core code (1 lines") &&
      content.textContent.includes("class SimpleSearch"),
    "1 class matches");

  const expando = frame.contentDocument.querySelector(".expando");
  ok(!!expando, "Expando button exists");
  ok(expando.classList.contains("open"), "Expando button is opened");

  const resultHead = frame.contentDocument.querySelector(".result-head");
  ok(!!resultHead, "Result head exists");
  ok(TestUtils.isShown(resultHead), "Result head is expanded");

  TestUtils.click(expando);

  ok(!expando.classList.contains("open"), "Expando button is not open");
  ok(!TestUtils.isShown(resultHead), "Result head is collapsed");
});

```

## tests/webtest/test_Breadcrumbs.js
```
"use strict";

add_task(async function test_BreadcrumbsOnLoad() {
  const tests = [
    {
      path: "/",
      hidden: true,
    },
    {
      path: "/tests/pages/settings.html",
      hidden: true,
    },
    {
      path: "/tests/source/",
      hidden: false,
      text: "tests",
    },
    {
      path: "/tests/source/webtest",
      hidden: false,
      text: "tests/webtest",
    },
    {
      path: "/tests/source/.gitignore",
      hidden: false,
      text: "tests/.gitignore",
    },
    {
      path: "/tests/source/webtest/Webtest.cpp",
      hidden: false,
      text: "tests/webtest/Webtest.cpp  (file symbol)",
    },
    {
      path: "/tests/search?q=webtest&path=&case=false&regexp=false",
      hidden: false,
      text: "tests",
    },
    {
      path: "/tests/query/default?q=webtest",
      hidden: false,
      text: "tests",
    },
    {
      path: "/searchfox/diff/4e266f75295afe5f94d14eb9b72445c830c095ef/.eslintrc.js",
      hidden: false,
      text: "searchfox/.eslintrc.js",
    },
    {
      path: "/searchfox/commit/4e266f75295afe5f94d14eb9b72445c830c095ef",
      hidden: false,
      text: "searchfox",
    },
    {
      path: "/searchfox/rev/e6ff7d3798a68e41c1166524be276fac4a8dfeb2/.gitignore",
      hidden: false,
      text: "searchfox/.gitignore",
    },
  ];

  for (const { path, hidden, text } of tests) {
    await TestUtils.loadPath(path);

    const breadcrumbs = frame.contentDocument.querySelector(".breadcrumbs");
    ok(!!breadcrumbs, `Breadcrumbs node exists on ${path}`);
    if (hidden) {
      is(breadcrumbs.style.display, "none", `Breadcrumbs is hidden on ${path}`);
    } else {
      isnot(breadcrumbs.style.display, "none", `Breadcrumbs is shown on ${path}`);
      is(breadcrumbs.textContent.trim(), text,
         `Breadcrumbs shows the correct path on ${path}`);
    }

    const treeSwitcher = breadcrumbs.querySelector("#tree-switcher");
    ok(!!treeSwitcher, `Tree switcher node exists on ${path}`);
    const treeSwitcherMenu = breadcrumbs.querySelector("#tree-switcher-menu");
    ok(!!treeSwitcherMenu, `Tree switcher menu node exists on ${path}`);
    is(treeSwitcherMenu.style.display, "none",
       `Tree switcher menu is hidden on ${path}`);

    if (!hidden) {
      TestUtils.click(treeSwitcher);

      waitForShown(treeSwitcherMenu,
                   `Tree switcher menu is shown after clicking switcher`);

      const href = frame.contentDocument.location.href;

      const loadPromise = TestUtils.waitForLoad();
      const links = treeSwitcherMenu.querySelectorAll("a[href]");
      is(links[0].textContent, "tests",
         "The first item should be tests");
      is(links[1].textContent, "searchfox",
         "The first item should be searchfox");
      const isTests = frame.contentDocument.location.href.includes("/tests/");
      if (isTests) {
        TestUtils.click(links[1]);
      } else {
        TestUtils.click(links[0]);
      }
      await loadPromise;

      if (isTests) {
        is(frame.contentDocument.location.href,
           href.replace(/tests/, "searchfox"),
           "Tree should be switched to searchfox");
      } else {
        is(frame.contentDocument.location.href,
           href.replace(/searchfox/, "tests"),
           "Tree should be switched to tests");
      }
    }
  }
});

add_task(async function test_BreadcrumbsAfterSearch() {
  // Search result is shown without navigation.
  // Breadcrumbs should be preserved across the search result display.

  const tests = [
    {
      path: "/",
      tree: "tests",
    },
    {
      path: "/tests/pages/settings.html",
      tree: "tests",
    },

    {
      path: "/tests/source/",
      tree: "tests",
    },
    {
      path: "/tests/source/webtest",
      tree: "tests",
    },
    {
      path: "/tests/source/.gitignore",
      tree: "tests",
    },
    {
      path: "/tests/source/webtest/Webtest.cpp",
      tree: "tests",
    },
    {
      path: "/tests/search?q=webtest&path=&case=false&regexp=false",
      tree: "tests",
    },

    {
      path: "/searchfox/diff/4e266f75295afe5f94d14eb9b72445c830c095ef/.eslintrc.js",
      tree: "searchfox",
    },
    {
      path: "/searchfox/commit/4e266f75295afe5f94d14eb9b72445c830c095ef",
      tree: "searchfox",
    },
    {
      path: "/searchfox/rev/e6ff7d3798a68e41c1166524be276fac4a8dfeb2/.gitignore",
      tree: "searchfox",
    },
  ];

  for (const { path, tree } of tests) {
    await TestUtils.loadPath(path);
    TestUtils.shortenSearchTimeouts();

    const query = frame.contentDocument.querySelector("#query");
    TestUtils.setText(query, "SimpleSearch");

    const panelContent = frame.contentDocument.querySelector("#content");
    await waitForCondition(
      () => panelContent.textContent.includes("Number of results:"),
      "Search result is shown");

    const breadcrumbs = frame.contentDocument.querySelector(".breadcrumbs");
    ok(!!breadcrumbs, `Breadcrumbs node exists on the search result from ${path}`);
    isnot(breadcrumbs.style.display, "none",
          `Breadcrumbs is shown on the search result from ${path}`);
    is(breadcrumbs.textContent.trim(), tree,
       `Breadcrumbs shows the tree name on the search result from ${path}`);

    const treeSwitcher = breadcrumbs.querySelector("#tree-switcher");
    ok(!!treeSwitcher, `Tree switcher node exists on the search result from ${path}`);
    const treeSwitcherMenu = breadcrumbs.querySelector("#tree-switcher-menu");
    ok(!!treeSwitcherMenu, `Tree switcher menu node exists on the search result from ${path}`);
  }
});

```

## tests/webtest/test_Search.js
```
"use strict";

add_task(async function test_Search() {
  await TestUtils.loadPath("/");
  TestUtils.shortenSearchTimeouts();

  const query = frame.contentDocument.querySelector("#query");
  TestUtils.setText(query, "SimpleSearch");

  const content = frame.contentDocument.querySelector("#content");

  await waitForCondition(
    () => content.textContent.includes("Core code (1 lines") &&
      content.textContent.includes("class SimpleSearch"),
    "1 class matches");

  await waitForCondition(
    () => frame.contentDocument.location.href.includes("SimpleSearch"),
    "URL is updated");
});

add_task(async function test_SearchCase() {
  await TestUtils.loadPath("/");
  TestUtils.shortenSearchTimeouts();

  const query = frame.contentDocument.querySelector("#query");
  TestUtils.setText(query, "CaseSensitiveness");

  const content = frame.contentDocument.querySelector("#content");

  await waitForCondition(
    () => content.textContent.includes("Core code (2 lines") &&
      content.textContent.includes("class CaseSensitiveness1") &&
      content.textContent.includes("class casesensitiveness2"),
    "2 classes match with case==false");

  await waitForCondition(
    () => frame.contentDocument.location.href.includes("CaseSensitiveness") &&
      frame.contentDocument.location.href.includes("case=false"),
    "URL is updated");

  const caseCheckbox = frame.contentDocument.querySelector("#case");
  is(caseCheckbox.checked, false, "case checkbox is unchecked by default");

  TestUtils.clickCheckbox(caseCheckbox);

  await waitForCondition(
    () => content.textContent.includes("Core code (1 lines") &&
      content.textContent.includes("class CaseSensitiveness1") &&
      !content.textContent.includes("class casesensitiveness2"),
    "1 class matches with case==true");

  await waitForCondition(
    () => frame.contentDocument.location.href.includes("CaseSensitiveness") &&
      frame.contentDocument.location.href.includes("case=true"),
    "URL is updated");

  TestUtils.clickCheckbox(caseCheckbox);

  await waitForCondition(
    () => content.textContent.includes("Core code (2 lines") &&
      content.textContent.includes("class CaseSensitiveness1") &&
      content.textContent.includes("class casesensitiveness2"),
    "2 classes match with case==false");

  await waitForCondition(
    () => frame.contentDocument.location.href.includes("CaseSensitiveness") &&
      frame.contentDocument.location.href.includes("case=false"),
    "URL is updated");
});

add_task(async function test_SearchRegExp() {
  await TestUtils.loadPath("/");
  TestUtils.shortenSearchTimeouts();

  const query = frame.contentDocument.querySelector("#query");
  TestUtils.setText(query, "Simpl.Search");

  const content = frame.contentDocument.querySelector("#content");

  await waitForCondition(
    () => content.textContent.includes("No results for current query"),
    "Nothing matches with regexp==false");

  await waitForCondition(
    () => frame.contentDocument.location.href.includes("Simpl.Search") &&
      frame.contentDocument.location.href.includes("regexp=false"),
    "URL is updated");

  const regExpCheckbox = frame.contentDocument.querySelector("#regexp");
  is(regExpCheckbox.checked, false, "regexp checkbox is unchecked by default");

  TestUtils.clickCheckbox(regExpCheckbox);

  await waitForCondition(
    () => content.textContent.includes("Core code (1 lines") &&
      content.textContent.includes("class SimpleSearch"),
    "1 class matches with regexp==true");

  await waitForCondition(
    () => frame.contentDocument.location.href.includes("Simpl.Search") &&
      frame.contentDocument.location.href.includes("regexp=true"),
    "URL is updated");

  TestUtils.clickCheckbox(regExpCheckbox);

  await waitForCondition(
    () => content.textContent.includes("No results for current query"),
    "Nothing matches with regexp==false");

  await waitForCondition(
    () => frame.contentDocument.location.href.includes("Simpl.Search") &&
      frame.contentDocument.location.href.includes("regexp=false"),
    "URL is updated");
});

add_task(async function test_SearchPathFilter() {
  await TestUtils.loadPath("/");
  TestUtils.shortenSearchTimeouts();

  const query = frame.contentDocument.querySelector("#query");
  TestUtils.setText(query, "PathFilter");

  const content = frame.contentDocument.querySelector("#content");

  await waitForCondition(
    () => content.textContent.includes("Core code (2 lines") &&
      content.textContent.includes("class PathFilter") &&
      content.textContent.includes("Webtest.cpp") &&
      content.textContent.includes("WebtestPathFilter.cpp"),
    "2 classes match without path filter");

  await waitForCondition(
    () => frame.contentDocument.location.href.includes("PathFilter") &&
      frame.contentDocument.location.href.includes("path=&"),
    "URL is updated");

  const pathFilter = frame.contentDocument.querySelector("#path");
  TestUtils.setText(pathFilter, "Filter.cpp");

  await waitForCondition(
    () => content.textContent.includes("Core code (1 lines") &&
      content.textContent.includes("class PathFilter") &&
      !content.textContent.includes("Webtest.cpp") &&
      content.textContent.includes("WebtestPathFilter.cpp"),
    "1 class matches without path filter");

  await waitForCondition(
    () => frame.contentDocument.location.href.includes("PathFilter") &&
      frame.contentDocument.location.href.includes("path=Filter.cpp&"),
    "URL is updated");

  TestUtils.setText(pathFilter, "");

  await waitForCondition(
    () => content.textContent.includes("Core code (2 lines") &&
      content.textContent.includes("class PathFilter") &&
      content.textContent.includes("Webtest.cpp") &&
      content.textContent.includes("WebtestPathFilter.cpp"),
    "2 classes match without path filter");

  await waitForCondition(
    () => frame.contentDocument.location.href.includes("PathFilter") &&
      frame.contentDocument.location.href.includes("path=&"),
    "URL is updated");
});

```

## tests/webtest/test_DebugUI.js
```
"use strict";

add_task(async function test_RawAnalysisLink() {
  await TestUtils.loadPath("/tests/source/webtest/Webtest.cpp");

  let rawAnalysisLink = null;
  const panelContent = frame.contentDocument.querySelector("#panel-content");
  for (const link of panelContent.querySelectorAll("a")) {
    if (link.textContent == "Raw analysis records") {
      rawAnalysisLink = link;
      break;
    }
  }
  ok(rawAnalysisLink, "Raw analysis records link exists");

  const loadPromise = TestUtils.waitForLoad();
  TestUtils.click(rawAnalysisLink);
  await loadPromise;

  is(frame.contentDocument.location.pathname,
     "/tests/raw-analysis/webtest/Webtest.cpp",
     "Raw analysis page is opened");
});

add_task(async function test_QueryDebugLog() {
  await TestUtils.loadPath("/tests/query/default?q=field-layout%3A%27field_layout%3A%3Aholes%3A%3ASub%27");

  {
    const logs = frame.contentDocument.querySelector("#query-debug-logs");
    ok(!logs, "Debug logs are not shown by default");

    let debugLogsLink = null;
    const panelContent = frame.contentDocument.querySelector("#panel-content");
    for (const link of panelContent.querySelectorAll("a")) {
      if (link.textContent == "Show debug log") {
        debugLogsLink = link;
        break;
      }
    }
    ok(debugLogsLink, "Debug log link exists");

    is(panelContent.getAttribute("aria-expanded"), "false",
       `Navigation panel is collapsed`);
    const toggle = frame.contentDocument.querySelector("#panel-toggle");
    TestUtils.click(toggle);
    is(panelContent.getAttribute("aria-expanded"), "true",
       `Navigation panel is expanded`);

    const loadPromise = TestUtils.waitForLoad();
    TestUtils.click(debugLogsLink);
    await loadPromise;
  }

  is(frame.contentDocument.location.pathname + frame.contentDocument.location.search,
     "/tests/query/default?q=field-layout%3A%27field_layout%3A%3Aholes%3A%3ASub%27&debug=true",
     "Query with debug log is opened");

  {
    const logs = frame.contentDocument.querySelector("#query-debug-logs");
    ok(!!logs, "Debug logs are shown");
    ok(logs.textContent.includes("logged_span"),
       "log JSON is shown");

    let debugLogsLink = null;
    const panelContent = frame.contentDocument.querySelector("#panel-content");
    for (const link of panelContent.querySelectorAll("a")) {
      if (link.textContent == "Hide debug log") {
        debugLogsLink = link;
        break;
      }
    }
    ok(debugLogsLink, "Debug log link exists");

    is(panelContent.getAttribute("aria-expanded"), "false",
       `Navigation panel is collapsed`);
    const toggle = frame.contentDocument.querySelector("#panel-toggle");
    TestUtils.click(toggle);
    is(panelContent.getAttribute("aria-expanded"), "true",
       `Navigation panel is expanded`);

    const loadPromise = TestUtils.waitForLoad();
    TestUtils.click(debugLogsLink);
    await loadPromise;
  }

  is(frame.contentDocument.location.pathname + frame.contentDocument.location.search,
     "/tests/query/default?q=field-layout%3A%27field_layout%3A%3Aholes%3A%3ASub%27",
     "Query without debug log is opened");
});

add_task(async function test_QueryResultsJSON() {
  await TestUtils.loadPath("/tests/query/default?q=field-layout%3A%27field_layout%3A%3Aholes%3A%3ASub%27");

  const box = frame.contentDocument.querySelector("#query-debug-results-json");
  ok(!!box, "results JSON node exists");
  ok(!TestUtils.isShown(box),
     "results JSON node is hidden by default");

  let resultsJSONButtton = null;
  const panelContent = frame.contentDocument.querySelector("#panel-content");
  for (const button of panelContent.querySelectorAll("button")) {
    if (button.textContent == "Show results JSON") {
      resultsJSONButtton = button;
      break;
    }
  }
  ok(resultsJSONButtton, "Results JSON button exists");

  is(panelContent.getAttribute("aria-expanded"), "false",
     `Navigation panel is collapsed`);
  const toggle = frame.contentDocument.querySelector("#panel-toggle");
  TestUtils.click(toggle);
  is(panelContent.getAttribute("aria-expanded"), "true",
     `Navigation panel is expanded`);

  TestUtils.click(resultsJSONButtton);

  is(resultsJSONButtton.textContent, "Hide results JSON",
     "Button text is updated");

  ok(TestUtils.isShown(box),
     "results JSON node is shown");
  ok(box.textContent.includes("SymbolTreeTableList"),
     "results JSON is shown");

  TestUtils.click(resultsJSONButtton);

  ok(!TestUtils.isShown(box),
     "results JSON node is hidden");

  is(resultsJSONButtton.textContent, "Show results JSON",
     "Button text is updated");
});

```

## tests/webtest/test_OverloadInTemplate.js
```
"use strict";

function goToDefinitionMenuItems(menu) {
  const items = []

  for (const row of menu.querySelectorAll(".contextmenu-row")) {
    if (row.textContent.startsWith("Go to definition of ")) {
      items.push(row)
    }
  }

  return items;
}

add_task(async function test_OverloadedFunctionInTemplateContextMenuHasMultipleDefs() {
  await TestUtils.loadPath("/tests/source/templates6.cpp");

  const overloaded = frame.contentDocument.querySelector("span[data-symbols*=overloaded]");
  TestUtils.click(overloaded);

  const menu = frame.contentDocument.querySelector("#context-menu");
  await waitForShown(menu, "Context menu is shown");

  const symbols = overloaded.dataset.symbols.split(',');
  is(symbols.length, 2, "2 symbols are available");

  const goToRows = goToDefinitionMenuItems(menu);
  is(goToRows.length, 2, "2 go to rows are available");
});

```

## tests/webtest/test_TreeSwitcherKeys.js
```
"use strict";

add_task(async function test_TreeSwitcherKeyboardNavigation() {
  await TestUtils.loadPath("/tests/source/");

  // Use minimal tree list.
  frame.contentWindow.TREE_LIST = [
    [
      {
        name: "Test",
        items: [
          { value: "tests" },
          { value: "searchfox" },
        ],
      },
      {
        name: "Firefox",
        items: [
          { value: "mozilla-central" },
          { value: "mozilla-beta" },
          { value: "mozilla-release" },
        ],
      },
    ],
    [
      {
        name: "Firefox other",
        items: [
          { value: "mozilla-mobile" },
        ],
      },
      {
        name: "Thunderbird",
        items: [
          { value: "comm-central" },
        ],
      },
    ],
    [
      {
        name: "MinGW",
        items: [
          { value: "mingw" },
          { value: "mingw-moz" },
        ],
      },
      {
        name: "Other",
        items: [
          { value: "wubkat" },
        ]
      }
    ]
  ];

  const breadcrumbs = frame.contentDocument.querySelector(".breadcrumbs");

  // Given the focus event requires user interaction, use test-only events
  // to detect the focus handling.
  const focusEvents = (async function* () {
    while (true) {
      const event = await new Promise(resolve => {
        frame.contentDocument.addEventListener("focusmenuitem", event => {
          resolve(event);
        }, { once: true });
      });

      yield event;
    }
  })();

  class KeyboardNavigationTester {
    constructor(currentItem) {
      this.currentItem = currentItem;

      // Set to true to track the navigation.
      this.debug = false;

      if (this.debug) {
        this.currentItem.style.outline = "1px dashed red";
      }
    }

    async keydown(key) {
      if (this.debug) {
        this.currentItem.style.outline = "";
      }

      const eventPromise = focusEvents.next();
      TestUtils.keydown(this.currentItem, { key });
      const event = (await eventPromise).value;
      this.currentItem = event.targetItem;

      if (this.debug) {
        this.currentItem.style.outline = "1px dashed red";
        await TestUtils.sleep(100);
      }

      return this.currentItem.textContent;
    }
  };

  const treeSwitcher = breadcrumbs.querySelector("#tree-switcher");
  const treeSwitcherMenu = breadcrumbs.querySelector("#tree-switcher-menu");

  const eventPromise = focusEvents.next();
  const shownPromise = waitForCondition(() => treeSwitcherMenu.style.display == "flex");
  treeSwitcher.click();
  await shownPromise;

  const event = (await eventPromise).value;
  is(event.targetItem.textContent, "tests",
     "The current tree should be focused");

  const tester = new KeyboardNavigationTester(event.targetItem);

  is(await tester.keydown("Down"), "searchfox",
     "The next tree should be focused");
  is(await tester.keydown("Down"), "mozilla-central",
     "The next tree should be focused, skipping the group label");

  is(await tester.keydown("Up"), "searchfox",
     "The previous tree should be focused, skipping the group label");
  is(await tester.keydown("Up"), "tests",
     "The previous tree should be focused");

  is(await tester.keydown("Up"), "tests",
     "Moving up from the first item keeps the focus");

  is(await tester.keydown("ArrowDown"), "searchfox",
     "The next tree should be focused");
  is(await tester.keydown("ArrowUp"), "tests",
     "The previous tree should be focused");

  is(await tester.keydown("Right"), "mozilla-mobile",
     "The next column's tree should be focused");
  is(await tester.keydown("Right"), "mingw",
     "The next column's tree should be focused");
  is(await tester.keydown("Right"), "mingw",
     "Moving right from the last column keeps the focus");

  is(await tester.keydown("Left"), "mozilla-mobile",
     "The previous column's tree should be focused");
  is(await tester.keydown("Left"), "tests",
     "The previous column's tree should be focused");

  is(await tester.keydown("Left"), "tests",
     "Moving left from the first column keeps the focus");

  is(await tester.keydown("ArrowRight"), "mozilla-mobile",
     "The next column's tree should be focused");
  is(await tester.keydown("ArrowLeft"), "tests",
     "The previous column's tree should be focused");

  is(await tester.keydown("ArrowLeft"), "tests",
     "The previous column's tree should be focused");

  is(await tester.keydown("Down"), "searchfox",
     "The next tree should be focused");
  is(await tester.keydown("Right"), "comm-central",
     "The next column's tree should be focused, skipping the group label to down");
  is(await tester.keydown("Right"), "wubkat",
     "The next column's tree should be focused, skipping the group label to down");
  is(await tester.keydown("Up"), "mingw-moz",
     "The previous tree should be focused, skipping the group label");
  is(await tester.keydown("Left"), "comm-central",
     "The previous column's tree should be focused, skipping the group label to down");
  is(await tester.keydown("Left"), "mozilla-central",
     "The previous column's tree should be focused, skipping the group label to down");

  is(await tester.keydown("PageDown"), "mozilla-release",
     "The last tree in the column should be focused");
  is(await tester.keydown("Down"), "mozilla-mobile",
     "The first tree in the next column should be focused");
  is(await tester.keydown("PageDown"), "comm-central",
     "The last tree in the column should be focused");
  is(await tester.keydown("Down"), "mingw",
     "The first tree in the next column should be focused");
  is(await tester.keydown("PageDown"), "wubkat",
     "The last tree in the column should be focused");
  is(await tester.keydown("Down"), "wubkat",
     "Moving down from the last item keeps the focus");

  is(await tester.keydown("PageUp"), "mingw",
     "The first tree in the column should be focused");
  is(await tester.keydown("Up"), "comm-central",
     "The last tree in the previous column should be focused");
  is(await tester.keydown("PageUp"), "mozilla-mobile",
     "The first tree in the column should be focused");
  is(await tester.keydown("Up"), "mozilla-release",
     "The last tree in the previous column should be focused");
  is(await tester.keydown("PageUp"), "tests",
     "The last tree in the column should be focused");

  is(await tester.keydown("End"), "wubkat",
     "The last tree in the last column should be focused");
  is(await tester.keydown("Home"), "tests",
     "The first tree in the first column should be focused");

  const hidePromise = waitForCondition(() => treeSwitcherMenu.style.display == "none");
  TestUtils.keydown(tester.currentItem, { key: "Escape" });
  await hidePromise;

  {
    const shownPromise = waitForCondition(() => treeSwitcherMenu.style.display == "flex");
    treeSwitcher.click();
    await shownPromise;

    const hidePromise = waitForCondition(() => treeSwitcherMenu.style.display == "none");
    TestUtils.keydown(tester.currentItem, { key: "Esc" });
    await hidePromise;
  }
});

```

## tests/webtest/test_ContextMenuKeys.js
```
"use strict";

add_task(async function test_TreeSwitcherKeyboardNavigation() {
  await TestUtils.loadPath("/tests/source/webtest/Webtest.cpp");

  // Given the focus event requires user interaction, use test-only events
  // to detect the focus handling.
  const focusEvents = (async function* () {
    while (true) {
      const event = await new Promise(resolve => {
        frame.contentDocument.addEventListener("focusmenuitem", event => {
          resolve(event);
        }, { once: true });
      });

      yield event;
    }
  })();

  class KeyboardNavigationTester {
    constructor(currentItem) {
      this.currentItem = currentItem;

      // Set to true to track the navigation.
      this.debug = false;

      if (this.debug) {
        this.currentItem.style.outline = "1px dashed red";
      }
    }

    async keydown(key) {
      if (this.debug) {
        this.currentItem.style.outline = "";
      }

      const eventPromise = focusEvents.next();
      TestUtils.keydown(this.currentItem, { key });
      const event = (await eventPromise).value;
      this.currentItem = event.targetItem;

      if (this.debug) {
        this.currentItem.style.outline = "1px dashed red";
        await TestUtils.sleep(100);
      }

      return this.currentItem;
    }
  };

  const word = frame.contentDocument.querySelector(`span[data-symbols="T_webtest::SimpleSearch"]`);

  const menu = frame.contentDocument.querySelector("#context-menu");

  {
    const shownPromise = waitForCondition(() => menu.style.display != "none");
    word.click();
    await shownPromise;

    const items = [...menu.querySelectorAll("a")];
    ok(items.length >= 4, "There should be at least 4 items in the menu");

    const firstItem = items[0];
    const nextItem = items[1];

    const lastItem = items[items.length - 1];
    const lastPrevItem = items[items.length - 2];

    const tester = new KeyboardNavigationTester(menu);

    is(await tester.keydown("Down"), firstItem,
       "The first item should be focused when down key is pressed on the menu");
    is(await tester.keydown("Down"), nextItem,
       "The next item should be focused");

    is(await tester.keydown("Up"), firstItem,
       "The previous item should be focused");
    is(await tester.keydown("Up"), firstItem,
       "Moving up from the first item keeps the focus");

    is(await tester.keydown("ArrowDown"), nextItem,
       "The next item should be focused");

    is(await tester.keydown("ArrowUp"), firstItem,
       "The previous item should be focused");
    is(await tester.keydown("ArrowUp"), firstItem,
       "Moving up from the first item keeps the focus");

    is(await tester.keydown("PageDown"), lastItem,
       "The last item should be focused");

    is(await tester.keydown("Up"), lastPrevItem,
       "The previous item should be focused");
    is(await tester.keydown("Down"), lastItem,
       "The next item should be focused");
    is(await tester.keydown("Down"), lastItem,
       "Moving down from the last item keeps the focus");

    is(await tester.keydown("ArrowUp"), lastPrevItem,
       "The previous item should be focused");
    is(await tester.keydown("ArrowDown"), lastItem,
       "The next item should be focused");
    is(await tester.keydown("ArrowDown"), lastItem,
       "Moving down from the last item keeps the focus");

    is(await tester.keydown("PageUp"), firstItem,
       "The first item should be focused");

    is(await tester.keydown("End"), lastItem,
       "The last item should be focused");

    is(await tester.keydown("Home"), firstItem,
       "The first item should be focused");

    is(await tester.keydown("Left"), firstItem,
       "Moving left keeps the focus");
    is(await tester.keydown("ArrowLeft"), firstItem,
       "Moving left keeps the focus");
    is(await tester.keydown("Right"), firstItem,
       "Moving right keeps the focus");
    is(await tester.keydown("ArrowRight"), firstItem,
       "Moving right keeps the focus");

    const hidePromise = waitForCondition(() => menu.style.display == "none");
    TestUtils.keydown(tester.currentItem, { key: "Escape" });
    await hidePromise;
  }

  {
    const shownPromise = waitForCondition(() => menu.style.display != "none");
    word.click();
    await shownPromise;

    const items = [...menu.querySelectorAll("a")];
    const firstItem = items[0];

    const tester = new KeyboardNavigationTester(menu);

    is(await tester.keydown("ArrowDown"), firstItem,
       "The first item should be focused when down key is pressed on the menu");

    const hidePromise = waitForCondition(() => menu.style.display == "none");
    TestUtils.keydown(tester.currentItem, { key: "Esc" });
    await hidePromise;
  }

  {
    const shownPromise = waitForCondition(() => menu.style.display != "none");
    word.click();
    await shownPromise;

    const items = [...menu.querySelectorAll("a")];
    const lastItem = items[items.length - 1];

    const tester = new KeyboardNavigationTester(menu);

    is(await tester.keydown("Up"), lastItem,
       "The last item should be focused when down key is pressed on the menu");

    const hidePromise = waitForCondition(() => menu.style.display == "none");
    TestUtils.keydown(tester.currentItem, { key: "Escape" });
    await hidePromise;
  }

  {
    const shownPromise = waitForCondition(() => menu.style.display != "none");
    word.click();
    await shownPromise;

    const items = [...menu.querySelectorAll("a")];
    const lastItem = items[items.length - 1];

    const tester = new KeyboardNavigationTester(menu);

    is(await tester.keydown("ArrowUp"), lastItem,
       "The last item should be focused when down key is pressed on the menu");

    const hidePromise = waitForCondition(() => menu.style.display == "none");
    TestUtils.keydown(tester.currentItem, { key: "Esc" });
    await hidePromise;
  }
});

```

## tests/webtest/test_FieldLayoutColumns.js
```
"use strict";

add_task(async function test_FieldLayoutColumns() {
  const sym = "field_layout::field_type::S";

  await TestUtils.loadPath(`/tests/query/default?q=field-layout:'${sym}'`);
  TestUtils.shortenSearchTimeouts();

  const query = frame.contentDocument.querySelector("#query");
  is(query.value, "field-layout:'field_layout::field_type::S'",
     "Query for field layout is set");

  const nameCheckbox = frame.contentDocument.querySelector("#col-show-name");
  const typeCheckbox = frame.contentDocument.querySelector("#col-show-type");
  const lineCheckbox = frame.contentDocument.querySelector("#col-show-line");

  is(nameCheckbox.checked, true,
     "name checkbox is checked by default");
  is(typeCheckbox.checked, false,
     "type checkbox is not checked by default");
  is(lineCheckbox.checked, true,
     "line checkbox is checked by default");

  const nameCell = frame.contentDocument.querySelector(".name-cell");
  is(TestUtils.isShown(nameCell), true,
     "name cells are shown by default");
  const typeCell = frame.contentDocument.querySelector(".type-cell");
  is(TestUtils.isShown(typeCell), false,
     "type cells are hidden by default");
  const lineCell = frame.contentDocument.querySelector(".line-cell");
  is(TestUtils.isShown(lineCell), true,
     "line cells are shown by default");

  TestUtils.clickCheckbox(nameCheckbox);

  is(TestUtils.isShown(nameCell), false,
     "name cells are hidden");
  is(TestUtils.isShown(typeCell), false,
     "type cells are hidden");
  is(TestUtils.isShown(lineCell), true,
     "line cells are shown");

  await waitForCondition(
    () => frame.contentDocument.location.href.includes(encodeURIComponent("hide-cols:name")),
    "URL is updated");

  is(query.value, "field-layout:'field_layout::field_type::S' hide-cols:name",
     "Query is updated");

  TestUtils.clickCheckbox(lineCheckbox);

  is(TestUtils.isShown(nameCell), false,
     "name cells are hidden");
  is(TestUtils.isShown(typeCell), false,
     "type cells are hidden");
  is(TestUtils.isShown(lineCell), false,
     "line cells are hidden");

  await waitForCondition(
    () => frame.contentDocument.location.href.includes(encodeURIComponent("hide-cols:name,line")),
    "URL is updated");

  is(query.value, "field-layout:'field_layout::field_type::S' hide-cols:name,line",
     "Query is updated");

  TestUtils.clickCheckbox(typeCheckbox);

  is(TestUtils.isShown(nameCell), false,
     "name cells are hidden");
  is(TestUtils.isShown(typeCell), true,
     "type cells are shown");
  is(TestUtils.isShown(lineCell), false,
     "line cells are hidden");

  await waitForCondition(
    () => frame.contentDocument.location.href.includes(encodeURIComponent("show-cols:type")) &&
      frame.contentDocument.location.href.includes(encodeURIComponent("hide-cols:name,line")),
    "URL is updated");

  is(query.value, "field-layout:'field_layout::field_type::S' show-cols:type hide-cols:name,line",
     "Query is updated");

  TestUtils.clickCheckbox(nameCheckbox);

  is(TestUtils.isShown(nameCell), true,
     "name cells are shown");
  is(TestUtils.isShown(typeCell), true,
     "type cells are shown");
  is(TestUtils.isShown(lineCell), false,
     "line cells are hidden");

  await waitForCondition(
    () => frame.contentDocument.location.href.includes(encodeURIComponent("show-cols:type")) &&
      frame.contentDocument.location.href.includes(encodeURIComponent("hide-cols:line")),
    "URL is updated");

  is(query.value, "field-layout:'field_layout::field_type::S' show-cols:type hide-cols:line",
     "Query is updated");

  TestUtils.clickCheckbox(lineCheckbox);

  is(TestUtils.isShown(nameCell), true,
     "name cells are shown");
  is(TestUtils.isShown(typeCell), true,
     "type cells are shown");
  is(TestUtils.isShown(lineCell), true,
     "line cells are shown");

  await waitForCondition(
    () => frame.contentDocument.location.href.includes(encodeURIComponent("show-cols:type")) &&
      !frame.contentDocument.location.href.includes(encodeURIComponent("hide-cols:")),
    "URL is updated");

  is(query.value, "field-layout:'field_layout::field_type::S' show-cols:type",
     "Query is updated");

  TestUtils.clickCheckbox(typeCheckbox);

  is(TestUtils.isShown(nameCell), true,
     "name cells are shown");
  is(TestUtils.isShown(typeCell), false,
     "type cells are hidden");
  is(TestUtils.isShown(lineCell), true,
     "line cells are shown");

  await waitForCondition(
    () => !frame.contentDocument.location.href.includes(encodeURIComponent("show-cols:")) &&
      !frame.contentDocument.location.href.includes(encodeURIComponent("hide-cols:")),
    "URL is updated");

  is(query.value, "field-layout:'field_layout::field_type::S'",
     "Query is updated");
});

```

## tests/config.json
```
{
  "mozsearch_path": "$MOZSEARCH_PATH",
  "config_repo": "$CONFIG_REPO",

  "default_tree": "tests",

  "trees": {
    "tests": {
      "priority": 100,
      "on_error": "halt",
      "cache": "everything",
      "index_path": "$WORKING/tests",
      "files_path": "$WORKING/tests/files",
      "webidl_binding_local_path": "webidl/bindings",
      "staticprefs_binding_local_path": "staticprefs/bindings",
      "objdir_path": "$WORKING/tests/objdir",
      "wpt_root": "testing/web-platform",
      "codesearch_path": "$WORKING/tests/livegrep.idx",
      "codesearch_port": 8080,
      "scip_subtrees": {}
    }
  },

  "allow_webtest": true
}

```

## tests/searchfox-config.json
```
{
  "mozsearch_path": "$MOZSEARCH_PATH",
  "config_repo": "$CONFIG_REPO",

  "default_tree": "searchfox",

  "trees": {
    "searchfox": {
      "priority": 100,
      "on_error": "halt",
      "cache": "everything",
      "index_path": "$WORKING/searchfox",
      "files_path": "$MOZSEARCH_PATH",
      "objdir_path": "$WORKING/searchfox/objdir",
      "git_path": "$MOZSEARCH_PATH",
      "git_blame_path": "$WORKING/searchfox/blame",
      "github_repo": "https://github.com/mozsearch/mozsearch",
      "history_path": "$WORKING/searchfox/history",
      "codesearch_path": "$WORKING/searchfox/livegrep.idx",
      "codesearch_port": 8081,
      "scip_subtrees": {
        "rust": {
          "scip_index_path": "$WORKING/searchfox/objdir/tools/rust.scip",
          "subtree_root": "tools"
        }
      }
    }
  }
}

```

## tests/tests/checks/src/dummy-test-package.rs
```
#[allow(dead_code)]
fn main() {}

```

## tests/tests/checks/inputs/fancy/diagram/traverse/lots_of_calls.cpp/paths_between__four_left_one_right__json
```
search-identifiers CallerFour::four_left CallerOne::one_calls_two_right | crossref-lookup | traverse --paths-between

```

## tests/tests/checks/inputs/fancy/diagram/traverse/big_cpp.cpp/uses__thing_takeDamage__json
```
search-identifiers outerNS::Thing::takeDamage | crossref-lookup | traverse --edge=uses

```

## tests/tests/checks/inputs/fancy/diagram/traverse/big_cpp.cpp/uses__thing_takeDamage__hier__dot
```
search-identifiers outerNS::Thing::takeDamage | crossref-lookup | traverse --edge=uses | graph --format=raw-dot --hier=pretty

```

## tests/tests/checks/inputs/fancy/diagram/traverse/big_cpp.cpp/callees__outercat_meet__json
```
search-identifiers outerNS::OuterCat::meet | crossref-lookup | traverse

```

## tests/tests/checks/inputs/fancy/diagram/traverse/big_cpp.cpp/callees__outercat_meet__dot
```
search-identifiers outerNS::OuterCat::meet | crossref-lookup | traverse | graph --format=raw-dot --hier=flat

```

## tests/tests/checks/inputs/fancy/diagram/traverse/big_cpp.cpp/callees__colorize__outercat_meet__dot
```
search-identifiers outerNS::OuterCat::meet | crossref-lookup | traverse --retain-all-symbol-data | graph --colorize-callees=takeDamage --format=raw-dot --hier=flat

```

## tests/tests/checks/inputs/fancy/diagram/traverse/big_cpp.cpp/uses__thing_takeDamage__hier__json
```
search-identifiers outerNS::Thing::takeDamage | crossref-lookup | traverse --edge=uses | graph --format=json --hier=pretty

```

## tests/tests/checks/inputs/fancy/format-symbol/field-layout/platform_specific_field.cpp/field_layout__platform_specific_field_1__json
```
search-identifiers field_layout::platform_specific_field::S3 | crossref-lookup | format-symbols --mode="field-layout"

```

## tests/tests/checks/inputs/fancy/format-symbol/field-layout/platform_specific_field.cpp/field_layout__platform_specific_field_2__json
```
search-identifiers field_layout::platform_specific_field::T3 | crossref-lookup | format-symbols --mode="field-layout"

```

## tests/tests/checks/inputs/fancy/format-symbol/field-layout/empty.cpp/field_layout__empty__json
```
search-identifiers field_layout::empty::S | crossref-lookup | format-symbols --mode="field-layout"

```

## tests/tests/checks/inputs/fancy/format-symbol/field-layout/platform_specific_size.cpp/field_layout__platform_specific_size__json
```
search-identifiers field_layout::platform_specific_size::S | crossref-lookup | format-symbols --mode="field-layout"

```

## tests/tests/checks/inputs/fancy/format-symbol/field-layout/non-struct.cpp/field_layout__non_struct__json
```
search-identifiers field_layout::non_struct::Proxy | crossref-lookup | format-symbols --mode="field-layout"

```

## tests/tests/checks/inputs/fancy/format-symbol/field-layout/bitfields.cpp/field_layout__bitfields__json
```
search-identifiers field_layout::bitfields::S | crossref-lookup | format-symbols --mode="field-layout"

```

## tests/tests/checks/inputs/fancy/format-symbol/field-layout/field-type.cpp/field_show_hide__name__json
```
search-identifiers field_layout::field_type::S | crossref-lookup | format-symbols --mode="field-layout" --hide-cols=name,line

```

## tests/tests/checks/inputs/fancy/format-symbol/field-layout/field-type.cpp/field_layout__field_type__json
```
search-identifiers field_layout::field_type::S | crossref-lookup | format-symbols --mode="field-layout"

```

## tests/tests/checks/inputs/fancy/format-symbol/field-layout/holes.cpp/field_layout__holes__json
```
search-identifiers field_layout::holes::Sub | crossref-lookup | format-symbols --mode="field-layout"

```

## tests/tests/checks/inputs/fancy/format-symbol/field-layout/big_cpp.cpp/field_layout__outercat__json
```
search-identifiers outerNS::OuterCat | crossref-lookup | format-symbols --mode="field-layout"

```

## tests/tests/checks/inputs/fancy/format-symbol/field-layout/multiple_inheritance.cpp/field_layout__multiple_inheritance__json
```
search-identifiers field_layout::multiple_inheritance::SubSubSubA | crossref-lookup | format-symbols --mode="field-layout"

```

## tests/tests/checks/inputs/fancy/format-symbol/field-layout/vtable.cpp/field_layout__vtable__json
```
search-identifiers field_layout::vtable::SubSub | crossref-lookup | format-symbols --mode="field-layout"

```

## tests/tests/checks/inputs/analysis/html/script.html/def_func__json
```
filter-analysis html/script.html -k def -i TestFunc

```

## tests/tests/checks/inputs/analysis/html/script.html/def_func_for_attribute__json
```
filter-analysis html/script.html -k def -i onDivClicked

```

## tests/tests/checks/inputs/analysis/html/script.html/use_func__json
```
filter-analysis html/script.html -k use -i TestFunc

```

## tests/tests/checks/inputs/analysis/html/script.html/use_func_in_attribute__json
```
filter-analysis html/script.html -k use -i onDivClicked

```

## tests/tests/checks/inputs/analysis/html/script.html/unknown_script__json
```
filter-analysis html/script.html -k def -i DoNotIndexMe

```

## tests/tests/checks/inputs/analysis/webidl/external-interface.webidl/webidl_external_interface__json
```
filter-analysis webidl/external-interface.webidl

```

## tests/tests/checks/inputs/analysis/webidl/dictionary.webidl/webidl_dictionary__json
```
filter-analysis webidl/dictionary.webidl

```

## tests/tests/checks/inputs/analysis/webidl/partial-interface.webidl/webidl_partial_interface__json
```
filter-analysis webidl/partial-interface.webidl

```

## tests/tests/checks/inputs/analysis/webidl/const.webidl/webidl_const__json
```
filter-analysis webidl/const.webidl

```

## tests/tests/checks/inputs/analysis/webidl/interface-mixin.webidl/webidl_interface_mixin__json
```
filter-analysis webidl/interface-mixin.webidl

```

## tests/tests/checks/inputs/analysis/webidl/maplike.webidl/webidl_maplike__json
```
filter-analysis webidl/maplike.webidl

```

## tests/tests/checks/inputs/analysis/webidl/overload.webidl/webidl_overload__json
```
filter-analysis webidl/overload.webidl

```

## tests/tests/checks/inputs/analysis/webidl/asynciterable.webidl/webidl_asynciterable__json
```
filter-analysis webidl/asynciterable.webidl

```

## tests/tests/checks/inputs/analysis/webidl/typedef.webidl/webidl_typedef__json
```
filter-analysis webidl/typedef.webidl

```

## tests/tests/checks/inputs/analysis/webidl/includes.webidl/webidl_asynciterable__json
```
filter-analysis webidl/includes.webidl

```

## tests/tests/checks/inputs/analysis/webidl/iterable.webidl/webidl_iterable__json
```
filter-analysis webidl/iterable.webidl

```

## tests/tests/checks/inputs/analysis/webidl/partial-dictionary.webidl/webidl_partial_dictionary__json
```
filter-analysis webidl/partial-dictionary.webidl

```

## tests/tests/checks/inputs/analysis/webidl/enum.webidl/webidl_enum__json
```
filter-analysis webidl/enum.webidl

```

## tests/tests/checks/inputs/analysis/webidl/callback.webidl/webidl_callback__json
```
filter-analysis webidl/callback.webidl

```

## tests/tests/checks/inputs/analysis/webidl/namespace.webidl/webidl_namespace__json
```
filter-analysis webidl/namespace.webidl

```

## tests/tests/checks/inputs/analysis/webidl/setlike.webidl/webidl_setlike__json
```
filter-analysis webidl/setlike.webidl

```

## tests/tests/checks/inputs/analysis/webidl/method.webidl/webidl_method__json
```
filter-analysis webidl/method.webidl

```

## tests/tests/checks/inputs/analysis/webidl/attribute.webidl/webidl_attribute__json
```
filter-analysis webidl/attribute.webidl

```

## tests/tests/checks/inputs/analysis/webidl/super-dictionary.webidl/webidl_super_dictionary__json
```
filter-analysis webidl/super-dictionary.webidl

```

## tests/tests/checks/inputs/analysis/webidl/super-interface.webidl/webidl_super_interface__json
```
filter-analysis webidl/super-interface.webidl

```

## tests/tests/checks/inputs/analysis/webidl/partial-namespace.webidl/webidl_partial_namespace__json
```
filter-analysis webidl/partial-namespace.webidl

```

## tests/tests/checks/inputs/analysis/webidl/type.webidl/webidl_type__json
```
filter-analysis webidl/type.webidl

```

## tests/tests/checks/inputs/analysis/rust/test_rust_dependency/MyType__html
```
filter-analysis test_rust_dependency/src/lib.rs -k def -i MyType::do_foo | show-html

```

## tests/tests/checks/inputs/analysis/java/KotlinTest.kt/external_dependency__json
```
search-identifiers kotlin::test::Test

```

## tests/tests/checks/inputs/analysis/rust/test_rust_dependency/MyType__json
```
filter-analysis test_rust_dependency/src/lib.rs -k def -i MyType::do_foo

```

## tests/tests/checks/inputs/analysis/java/InlineObject.kt/method_with_return_type__json
```
filter-analysis src/main/kotlin/sample/InlineObject.kt -r structured -s "S_jvm_sample/<anonymous_object_at_137>#someOtherFunction()."

```

## tests/tests/checks/inputs/analysis/rust/simple.rs/simple_Loader_new__html
```
filter-analysis simple.rs -k def -i Loader::new | show-html

```

## tests/tests/checks/inputs/analysis/java/InlineObject.kt/method__json
```
filter-analysis src/main/kotlin/sample/InlineObject.kt -r structured -s "S_jvm_sample/<anonymous_object_at_137>#someFunction()."

```

## tests/tests/checks/inputs/analysis/rust/simple.rs/simple_Loader_new__json
```
filter-analysis simple.rs -k def -i Loader::new

```

## tests/tests/checks/inputs/analysis/js/export4.mjs/exported_decls__json
```
filter-analysis js/export4.mjs -k def

```

## tests/tests/checks/inputs/analysis/js/imported-module.mjs/def_ModuleClass__json
```
filter-analysis imported-module.mjs -k def -i ModuleClass

```

## tests/tests/checks/inputs/analysis/js/imported-module.mjs/def_ModuleClass_error__json
```
filter-analysis imported-module.mjs -k def -i ModuleClass.#error

```

## tests/tests/checks/inputs/analysis/js/imported-module.mjs/def_moduleFunc__json
```
filter-analysis imported-module.mjs -k def -i moduleFunc

```

## tests/tests/checks/inputs/analysis/js/imported-module.mjs/def_moduleConst__json
```
filter-analysis imported-module.mjs -k def -i moduleConst

```

## tests/tests/checks/inputs/analysis/js/export6.mjs/exported_decls__json
```
filter-analysis js/export6.mjs -k def

```

## tests/tests/checks/inputs/analysis/js/export5.mjs/exported_decls__json
```
filter-analysis js/export5.mjs -k def

```

## tests/tests/checks/inputs/analysis/js/import.mjs/imported_symbols__json
```
filter-analysis js/import.mjs -k def

```

## tests/tests/checks/inputs/analysis/js/export.mjs/exported_use__json
```
filter-analysis js/export.mjs -k use

```

## tests/tests/checks/inputs/analysis/js/import.mjs/imported_symbols_use__json
```
filter-analysis js/import.mjs -k use

```

## tests/tests/checks/inputs/analysis/js/export.mjs/exported_decls__json
```
filter-analysis js/export.mjs -k def

```

## tests/tests/checks/inputs/analysis/urlmap/root.xhtml/url_def_in_xhtml__json
```
filter-analysis urlmap/root.xhtml -k def --symbol-prefix FILE_

```

## tests/tests/checks/inputs/analysis/urlmap/root.xhtml/url_use_in_xhtml__json
```
filter-analysis urlmap/root.xhtml -k use --symbol-prefix FILE_

```

## tests/tests/checks/inputs/analysis/js/secret-madjewel.js/def_secretMadjewelConst__json
```
filter-analysis secret-madjewel.js -k def -i secretMadjewelConst

```

## tests/tests/checks/inputs/analysis/js/root-module.mjs/def_rootModuleConst__json
```
filter-analysis root-module.mjs -k def -i rootModuleConst

```

## tests/tests/checks/inputs/analysis/urlmap/chrome1.mjs/url_use_with_relative__json
```
filter-analysis urlmap/chrome1.mjs -k use --symbol-prefix FILE_

```

## tests/tests/checks/inputs/analysis/urlmap/chrome1.mjs/no_stray_relpath__json
```
filter-analysis urlmap/chrome1.mjs -k def -s RELPATH

```

## tests/tests/checks/inputs/analysis/js/export3.mjs/exported_decls__json
```
filter-analysis js/export3.mjs -k def

```

## tests/tests/checks/inputs/analysis/js/export2.mjs/exported_use__json
```
filter-analysis js/export2.mjs -k use

```

## tests/tests/checks/inputs/analysis/js/export2.mjs/exported_decls__json
```
filter-analysis js/export2.mjs -k def

```

## tests/tests/checks/inputs/analysis/urlmap/subdir/sub.mjs/url_use_with_relative_parent__json
```
filter-analysis urlmap/subdir/sub.mjs -k use --symbol-prefix FILE_

```

## tests/tests/checks/inputs/analysis/js/export8.mjs/ignore_anonymous_function_default__json
```
filter-analysis js/export8.mjs -k def

```

## tests/tests/checks/inputs/analysis/urlmap/root.js/no_stray_url__json
```
filter-analysis urlmap/root.js --symbol-prefix URL_

```

## tests/tests/checks/inputs/analysis/urlmap/root.js/url_def_in_script__json
```
filter-analysis urlmap/root.js -k def --symbol-prefix FILE_

```

## tests/tests/checks/inputs/analysis/urlmap/root.js/url_use_in_script__json
```
filter-analysis urlmap/root.js -k use --symbol-prefix FILE_

```

## tests/tests/checks/inputs/analysis/urlmap/root.mjs/url_use_in_module__json
```
filter-analysis urlmap/root.mjs -k use --symbol-prefix FILE_

```

## tests/tests/checks/inputs/analysis/js/some_javascript.js/all__priv_field_num__json
```
filter-analysis some_javascript.js -i '#priv_field_num'

```

## tests/tests/checks/inputs/analysis/js/some_javascript.js/all__priv_field_num__html
```
filter-analysis some_javascript.js -i '#priv_field_num' | show-html

```

## tests/tests/checks/inputs/analysis/urlmap/root.mjs/url_def_in_module__json
```
filter-analysis urlmap/root.mjs -k def --symbol-prefix FILE_

```

## tests/tests/checks/inputs/analysis/js/export7.mjs/exported_decls__json
```
filter-analysis js/export7.mjs -k def

```

## tests/tests/checks/inputs/analysis/urlmap/root.html/url_use_in_html__json
```
filter-analysis urlmap/root.html -k use --symbol-prefix FILE_

```

## tests/tests/checks/inputs/analysis/urlmap/root.html/url_def_in_html__json
```
filter-analysis urlmap/root.html -k def --symbol-prefix FILE_

```

## tests/tests/checks/inputs/analysis/urlmap/resource1.mjs/url_use_with_relative__json
```
filter-analysis urlmap/resource1.mjs -k use --symbol-prefix FILE_

```

## tests/tests/checks/inputs/analysis/cpp/template_shapes.cpp/Foo_Project
```
filter-analysis template_shapes.cpp -i DrawingContext::draw

```

## tests/tests/checks/inputs/analysis/cpp/ForwardingTemplates.cpp/TypeIndependentNewInTemplateReportedInTemplate
```
filter-analysis ForwardingTemplates.cpp -i "StructUsedInTypeIndependentNew::StructUsedInTypeIndependentNew"

```

## tests/tests/checks/inputs/analysis/cpp/ForwardingTemplates.cpp/TypeDependentNewInMethodReportedAtCallSite
```
filter-analysis ForwardingTemplates.cpp -i "StructUsedInEmplace::StructUsedInEmplace"

```

## tests/tests/checks/inputs/analysis/cpp/big_header.h/def_big_header__json
```
filter-analysis big_header.h -s FILE_big_header@2Eh

```

## tests/tests/checks/inputs/analysis/cpp/ForwardingTemplates.cpp/TypeDependentNewInTemplateReportedAtCallSite
```
filter-analysis ForwardingTemplates.cpp -i "StructUsedInTypeDependentNew0::StructUsedInTypeDependentNew0"

```

## tests/tests/checks/inputs/analysis/cpp/big_header.h/def_big_header__html
```
filter-analysis big_header.h -s FILE_big_header@2Eh | show-html

```

## tests/tests/checks/inputs/analysis/cpp/template_specialization.cpp/some_function_called_in_partial_specialization
```
filter-analysis template_specialization.cpp -i SomeStruct::some_function

```

## tests/tests/checks/inputs/analysis/cpp/URL_sym.cpp/URL_sym__json
```
filter-analysis cpp/URL_sym.cpp -i url_sym::S::scriptURL_

```

## tests/tests/checks/inputs/analysis/cpp/lambdas.cpp/Struct0UsedInLambda
```
filter-analysis lambdas.cpp -i "Struct0::method"

```

## tests/tests/checks/inputs/analysis/cpp/lambdas.cpp/LambdaFields
```
filter-analysis lambdas.cpp -r structured -i "test::(anonymous)::(anonymous)"

```

## tests/tests/checks/inputs/analysis/cpp/overs.cpp/def__DoubleBase_doublePure__html
```
filter-analysis overs.cpp -k def -i "DoubleBase::doublePure" | show-html

```

## tests/tests/checks/inputs/analysis/cpp/lambdas.cpp/Struct1UsedInLambda
```
filter-analysis lambdas.cpp -i "Struct1::method"

```

## tests/tests/checks/inputs/analysis/cpp/macro.cpp/TEST_MACRO_INCLUDE
```
filter-analysis macro.cpp -i TEST_MACRO_INCLUDE

```

## tests/tests/checks/inputs/analysis/cpp/macro.cpp/MULTI_LINE_MACRO
```
filter-analysis macro.cpp -i MULTI_LINE_MACRO

```

## tests/tests/checks/inputs/analysis/cpp/atom_list.h/YO_ATOM
```
filter-analysis atom_list.h -i YO_ATOM

```

## tests/tests/checks/inputs/analysis/cpp/macro.cpp/EMPTY_MACRO__json
```
filter-analysis macro.cpp -i EMPTY_MACRO

```

## tests/tests/checks/inputs/analysis/cpp/atom_list.h/NESTED_YO_ATOM
```
filter-analysis atom_list.h -i NESTED_YO_ATOM

```

## tests/tests/checks/inputs/analysis/cpp/macro.cpp/NESTED_MACRO_WITH_ARG
```
filter-analysis macro.cpp -i NESTED_MACRO_WITH_ARG

```

## tests/tests/checks/inputs/analysis/cpp/atom_list.h/structured_yo_atoms_foo_string__json
```
filter-analysis atom_list.h -r structured -i YoAtoms::Foo_string

```

## tests/tests/checks/inputs/analysis/cpp/macro.cpp/CONST_MACRO__json
```
filter-analysis macro.cpp -i CONST_MACRO

```

## tests/tests/checks/inputs/analysis/cpp/macro.cpp/PER_TARGET_FUNCTION
```
filter-analysis macro.cpp -i PER_TARGET_FUNCTION

```

## tests/tests/checks/inputs/analysis/cpp/macro.cpp/IDENT_MACRO__json
```
filter-analysis macro.cpp -i IDENT_MACRO

```

## tests/tests/checks/inputs/analysis/cpp/macro.cpp/NESTED_MACRO
```
filter-analysis macro.cpp -i NESTED_MACRO

```

## tests/tests/checks/inputs/analysis/cpp/bug1781178.cpp/Foo_Typedef
```
filter-analysis bug1781178.cpp -i Foo::Typedef

```

## tests/tests/checks/inputs/analysis/ipdl/PTestBasic.ipdl/def_PTestBasic__Hello__html
```
filter-analysis ipdl/PTestBasic.ipdl -k idl -i "mozilla__ipdltest::PTestBasic::Hello" | show-html

```

## tests/tests/checks/inputs/analysis/ipdl/PTestBasic.ipdl/def_PTestBasic__Hello__json
```
filter-analysis ipdl/PTestBasic.ipdl -k idl -i "mozilla__ipdltest::PTestBasic::Hello"

```

## tests/tests/checks/inputs/analysis/cpp/bug1781178.cpp/Point_IsThereOne
```
filter-analysis bug1781178.cpp -i Point::IsThereOne

```

## tests/tests/checks/inputs/analysis/cpp/templates7.cpp/def_OutOfLineTemplateShouldntHaveContextSym
```
filter-analysis templates7.cpp -k def -i "Theme::OutOfLineTemplateShouldntHaveContextSym"

```

## tests/tests/checks/inputs/analysis/cpp/bug1781178.cpp/Foo_Static
```
filter-analysis bug1781178.cpp -i Foo::Static

```

## tests/tests/checks/inputs/analysis/cpp/bug1781178.cpp/Foo_Simple
```
filter-analysis bug1781178.cpp -i Foo::Simple

```

## tests/tests/checks/inputs/analysis/cpp/bug1781178.cpp/Foo_Nested_field
```
filter-analysis bug1781178.cpp -i Foo::Nested::field

```

## tests/tests/checks/inputs/analysis/cpp/bug1781178.cpp/WithOverloads_Overloaded
```
filter-analysis bug1781178.cpp -i WithOverloads::Overloaded

```

## tests/tests/checks/inputs/analysis/cpp/bug1781178.cpp/Foo_Project
```
filter-analysis bug1781178.cpp -i Foo::Project

```

## tests/tests/checks/inputs/analysis/cpp/bug1781178.cpp/Foo_E_Waldo
```
filter-analysis bug1781178.cpp -i Foo::E::Waldo

```

## tests/tests/checks/inputs/analysis/cpp/templates7.cpp/def_OutOfLineShouldntHaveContextSym
```
filter-analysis templates7.cpp -k def -i "Theme::OutOfLineShouldntHaveContextSym"

```

## tests/tests/checks/inputs/analysis/cpp/bug1781178.cpp/Foo_E
```
filter-analysis bug1781178.cpp -i Foo::E

```

## tests/tests/checks/inputs/analysis/cpp/bug1781178.cpp/internal_Read
```
filter-analysis bug1781178.cpp -i internal::Read

```

## tests/tests/checks/inputs/analysis/cpp/templates7.cpp/def_InlineShouldHaveContextSym
```
filter-analysis templates7.cpp -k def -i "Theme::InlineShouldHaveContextSym"

```

## tests/tests/checks/inputs/analysis/cpp/templates7.cpp/def_InlineTemplateShouldHaveContextSym
```
filter-analysis templates7.cpp -k def -i "Theme::InlineTemplateShouldHaveContextSym"

```

## tests/tests/checks/inputs/analysis/cpp/big_cpp.cpp/structured_thing__json
```
filter-analysis big_cpp.cpp -r structured -i outerNS::Thing

```

## tests/tests/checks/inputs/analysis/cpp/big_cpp.cpp/structured_practicalart_beart__json
```
filter-analysis big_cpp.cpp -r structured -i outerNS::PracticalArt::beArt

```

## tests/tests/checks/inputs/analysis/xpidl/xpctest_params.idl/use_testOctet__json
```
filter-analysis xpidl_cpp_consumer.cpp -i "nsIXPCTestParams::TestOctet"

```

## tests/tests/checks/inputs/analysis/cpp/big_cpp.cpp/structured_thing_takedamage__json
```
filter-analysis big_cpp.cpp -r structured -i outerNS::Thing::takeDamage

```

## tests/tests/checks/inputs/analysis/xpidl/xpctest_params.idl/def_testOctet__json
```
filter-analysis xpidl/xpctest_params.idl -k idl -i "nsIXPCTestParams::testOctet"

```

## tests/tests/checks/inputs/analysis/cpp/big_cpp.cpp/structured_superhero__json
```
filter-analysis big_cpp.cpp -r structured -i outerNS::Superhero

```

## tests/tests/checks/inputs/analysis/cpp/big_cpp.cpp/structured_lessglobalcontext_decidewhethertodecide__json
```
filter-analysis big_cpp.cpp -r structured -i GlobalContext::LessGlobalContext::decideWhetherToDecide

```

## tests/tests/checks/inputs/analysis/xpidl/xpctest_params.idl/def_testOctet__html
```
filter-analysis xpidl/xpctest_params.idl -k idl -i "nsIXPCTestParams::testOctet" | show-html

```

## tests/tests/checks/inputs/analysis/cpp/big_cpp.cpp/structured_abstractart_beart__json
```
filter-analysis big_cpp.cpp -r structured -i outerNS::AbstractArt::beArt

```

## tests/tests/checks/inputs/analysis/cpp/big_cpp.cpp/structured_globalcontext__json
```
filter-analysis big_cpp.cpp -r structured -i GlobalContext

```

## tests/tests/checks/inputs/analysis/cpp/big_cpp.cpp/include_big_header__json
```
filter-analysis big_cpp.cpp -s FILE_big_header@2Eh

```

## tests/tests/checks/inputs/analysis/cpp/big_cpp.cpp/structured_human__json
```
filter-analysis big_cpp.cpp -r structured -i outerNS::Human

```

## tests/tests/checks/inputs/analysis/cpp/big_cpp.cpp/include_big_header__prodjson
```
filter-analysis big_cpp.cpp -s FILE_big_header@2Eh | production-filter

```

## tests/tests/checks/inputs/analysis/cpp/big_cpp.cpp/structured_superhero_takedamage__json
```
filter-analysis big_cpp.cpp -r structured -i outerNS::Superhero::takeDamage

```

## tests/tests/checks/inputs/analysis/cpp/big_cpp.cpp/include_big_header__prodhtml
```
filter-analysis big_cpp.cpp -s FILE_big_header@2Eh | show-html | production-filter

```

## tests/tests/checks/inputs/analysis/cpp/big_cpp.cpp/def_thing__json
```
# it turns out comments already work!
filter-analysis big_cpp.cpp -k def -i outerNS::Thing

```

## tests/tests/checks/inputs/analysis/cpp/big_cpp.cpp/structured_lessglobalcontext__json
```
filter-analysis big_cpp.cpp -r structured -i GlobalContext::LessGlobalContext

```

## tests/tests/checks/inputs/analysis/cpp/big_cpp.cpp/structured_outercat__json
```
filter-analysis big_cpp.cpp -r structured -i outerNS::OuterCat

```

## tests/tests/checks/inputs/analysis/cpp/big_cpp.cpp/include_big_header__html
```
filter-analysis big_cpp.cpp -s FILE_big_header@2Eh | show-html

```

## tests/tests/checks/inputs/analysis/xpidl/xpctest_attributes.idl/idl_stringProperty__html
```
filter-analysis xpidl/xpctest_attributes.idl -k idl -i "nsIXPCTestObjectReadWrite::stringProperty" | show-html

```

## tests/tests/checks/inputs/analysis/staticprefs/staticprefs__json
```
filter-analysis staticprefs/StaticPrefList.yaml

```

## tests/tests/checks/inputs/analysis/xpidl/xpctest_attributes.idl/idl_stringProperty__json
```
filter-analysis xpidl/xpctest_attributes.idl -k idl -i "nsIXPCTestObjectReadWrite::stringProperty"

```

## tests/tests/checks/inputs/blame/syntax-token-tokenize/big_cpp.cpp__tokenize__file
```
tokenize-source big_cpp.cpp

```

## tests/tests/checks/inputs/analysis/css/embed_css.html/custom_prop__json
```
filter-analysis css/embed_css.html --symbol-prefix 'CSSPROP_@2D@2D'

```

## tests/tests/checks/inputs/blame/syntax-token-tokenize/big_cpp.cpp__tokenize_outline__json
```
tokenize-source --mode outline big_cpp.cpp

```

## tests/tests/checks/inputs/analysis/css/embed_css.html/url__json
```
filter-analysis css/embed_css.html --symbol-prefix 'FILE_'

```

## tests/tests/checks/inputs/analysis/css/test.css/custom_prop__json
```
filter-analysis css/test.css --symbol-prefix 'CSSPROP_@2D@2D'

```

## tests/tests/checks/inputs/analysis/css/test.css/url__json
```
filter-analysis css/test.css --symbol-prefix 'FILE_'

```

## tests/tests/checks/inputs/web/files/nav-panel/wpt__test_ima_weird_meta_wpt__navpanel__html
```
cat-html testing/web-platform/tests/fake-standard/test_ima_weird_meta_wpt.js --select "#panel"

```

## tests/tests/checks/inputs/web/files/nav-panel/wpt__some_cross_global_test__navpanel__html
```
cat-html testing/web-platform/tests/complex-tests/some_cross_global_test.any.js --select "#panel"

```

## tests/tests/checks/inputs/web/files/test-info-boxes/wpt__some_cross_global_test__html
```
cat-html testing/web-platform/tests/complex-tests/some_cross_global_test.any.js --select "section.info-boxes"

```

## tests/tests/checks/inputs/web/files/urlmap/links_in_comment__html
```
cat-html urlmap/root.cpp --select ".syn_comment span"

```

## tests/tests/checks/inputs/web/files/test-info-boxes/test_many_manifest_permutations__html
```
cat-html test_many_manifest_permutations.js --select "section.info-boxes"

```

## tests/tests/checks/inputs/web/files/test-info-boxes/text_custom_element_base_xul__infobox__html
```
cat-html test_custom_element_base.xul --select "section.info-boxes"

```

## tests/tests/checks/inputs/web/files/test-info-boxes/wpt__test_ima_sad_subtests_wpt__html
```
cat-html testing/web-platform/tests/fake-standard/test_ima_sad_subtests_wpt.js --select "section.info-boxes"

```

## tests/tests/checks/inputs/web/files/test-info-boxes/wpt__test_ima_disabled_wpt__html
```
cat-html testing/web-platform/tests/fake-standard/test_ima_disabled_wpt.js --select "section.info-boxes"

```

## tests/tests/checks/inputs/web/files/urlmap/links_in_string__html
```
cat-html urlmap/root.cpp --select ".syn_string"

```

## tests/tests/checks/inputs/web/docs/docs_md__html
```
cat-html docs/md_test/index.md --select "#panel a[href*=\"firefox-source-docs\"]"
```

## tests/tests/checks/inputs/web/docs/docs_rst__html
```
cat-html docs/rst_test/index.rst --select "#panel a[href*=\"firefox-source-docs\"]"
```

## tests/tests/checks/inputs/web/files/contextual-keyword/contextual-keyword_webidl__html
```
cat-html webidl/contextual-keyword.webidl --select "span[data-symbols]"

```

## tests/tests/checks/inputs/web/files/contextual-keyword/contextual-keyword_py__html
```
cat-html python/contextual-keyword.py --select "span[data-symbols]"

```

## tests/tests/checks/inputs/web/templates/search_template__html
```
cat-html --template "search.html"

```

## tests/tests/checks/inputs/web/files/contextual-keyword/contextual-keyword_rs__html
```
cat-html rust/weak_keyword.rs --select "span[data-symbols]"

```

## tests/tests/checks/inputs/web/templates/help_file__html
```
cat-html --template "help.html"

```

## tests/tests/checks/inputs/web/files/contextual-keyword/contextual-keyword_cpp__html
```
cat-html cpp/contextual-keyword.cpp --select "span[data-symbols]"

```

## tests/tests/checks/inputs/web/files/contextual-keyword/contextual-keyword_js__html
```
cat-html js/contextual-keyword.js --select "span[data-symbols]"

```

## tests/tests/checks/inputs/search-text/global/searchfox_plain__json
```
search-text searchfox

```

## tests/tests/checks/inputs/web/search/overs.cpp/doublePure__json
```
search DoubleBase::doublePure

```

## tests/tests/checks/inputs/web/search/overs.cpp/triplePure__json
```
search TripleBase::triplePure

```

## tests/tests/checks/inputs/web/query/parsing/simple/default__unquoted_word__json
```
query --dump-pipeline 'foo'

```

## tests/tests/checks/inputs/web/query/parsing/simple/default__quoted_dashed__json
```
# Although not strictly necessary because of our quote characters, we use
# `--` to be consistent with the unquoted case and in case people use this as
# the basis for other checks.  Specifically, we actually want to test the
# graph building logic of query, not this first-level pipeline parsing, so we
# need to pass `--` to avoid this first-level from getting tricked by dashes.
query --dump-pipeline -- '"--tricky"'

```

## tests/tests/checks/inputs/web/query/execution/field-layout/query__field_layout__outercat__json
```
query "field-layout:'outerNS::OuterCat'"

```

## tests/tests/checks/inputs/web/query/parsing/simple/default__needs_regexp_escape__json
```
query --dump-pipeline foo.bar+hats()

```

## tests/tests/checks/inputs/web/query/parsing/simple/default__unquoted_dashed__json
```
# We have to pass our own "--" here in order to get our payload string to not
# be interpreted at our first level here.  This matches what we would get
# if we provided this to the pipeline-server.
query --dump-pipeline -- '--tricky'

```

## tests/tests/checks/inputs/web/query/parsing/simple/default__quoted_phrase__json
```
query --dump-pipeline '"foo bar"'

```

## tests/tests/checks/inputs/web/query/parsing/simple/lots_of_calls.cpp/query__parse__calls_to__four_left_and_right__svg
```
query --dump-pipeline "calls-to:'CallerFour::four_left' calls-to:'CallerFour::four_right'"

```

## tests/tests/checks/inputs/web/query/parsing/simple/overs.cpp/query__parse__doublePure__context4__json
```
query --dump-pipeline "context:4 'DoubleBase::doublePure'"

```

## tests/tests/checks/inputs/web/query/parsing/simple/overs.cpp/query__parse__doublePure__json
```
query --dump-pipeline "'DoubleBase::doublePure'"

```

## tests/tests/checks/inputs/web/query/parsing/simple/lots_of_calls.cpp/query__parse__calls_between__four_left_one_right__svg
```
query --dump-pipeline "calls-between:'CallerFour::four_left' calls-between:'CallerOne::one_calls_two_right'"

```

## tests/tests/checks/inputs/web/query/parsing/simple/big_cpp.cpp/query__parse_calls_to__graoh_format__takeDamage__json
```
query --dump-pipeline "calls-to:'outerNS::Thing::takeDamage' graph-format:rawdot"

```

## tests/tests/checks/inputs/web/query/parsing/simple/big_cpp.cpp/query__parse__calls_to__takeDamage__json
```
query --dump-pipeline "calls-to:'outerNS::Thing::takeDamage'"

```

## tests/tests/checks/inputs/web/query/parsing/simple/big_cpp.cpp/query__parse__calls_to__colorize__takeDamage__json
```
query --dump-pipeline "calls-to:'outerNS::Thing::takeDamage' colorize-callees:'::isFriendlyCat'"

```

## tests/tests/checks/inputs/web/query/execution/simple/overs.cpp/query__doublePure__context4__json
```
query "context:4 'DoubleBase::doublePure'"

```

## tests/tests/checks/inputs/web/query/execution/simple/overs.cpp/query__doublePure__json
```
query "'DoubleBase::doublePure'"

```

## tests/tests/checks/inputs/web/query/execution/simple/overs.cpp/query__doublePure__context_alias4__json
```
query "C:4 'DoubleBase::doublePure'"

```

## tests/tests/checks/inputs/web/dirs/generated_listing__html
```
cat-html --dir "__GENERATED__" --select ".folder-content tbody tr:nth-child(2)"

```

## tests/tests/checks/inputs/web/search/big_cpp.cpp/practicalart_beart__json
```
search outerNS::PracticalArt::beArt

```

## tests/tests/checks/inputs/web/search/big_cpp.cpp/human__json
```
search outerNS::Human

```

## tests/tests/checks/inputs/web/search/big_cpp.cpp/practicalart__json
```
search outerNS::PracticalArt

```

## tests/tests/checks/inputs/web/dirs/root_listing__html
```
cat-html --dir "/"

```

## tests/tests/checks/inputs/web/search/big_cpp.cpp/abstractart_beart__json
```
search outerNS::AbstractArt::beArt

```

## tests/tests/checks/inputs/web/search/big_cpp.cpp/abstractart__json
```
search outerNS::AbstractArt

```

## tests/tests/checks/inputs/search-text/global/searchfox_boring_re__json
```
search-text --re=searchfox

```

## tests/tests/checks/inputs/search-text/global/searchfox_re_multi_o__json
```
search-text --re=searchfo+x

```

## tests/tests/checks/inputs/search-files/anchored_doublestar_html__json
```
search-files "^**.html"

```

## tests/tests/checks/inputs/search-files/anchored_fakewpt_doublestar_combi__json
```
search-files "^testing/**.{js,ini}"

```

## tests/tests/checks/inputs/search-files/anchored_star_html__json
```
search-files "^*.html"

```

## tests/tests/checks/inputs/search-files/file_list_ingestion_check__json
```
# Let's check that our .eslintignore processing worked out by
# snapshotting the metadata.
search-files "ignored-js/"

```

## tests/tests/checks/inputs/jumpref/jumpref/LightweightThemeManager.jsm/ADDON_TYPE__lookup__json
```
search-identifiers ADDON_TYPE | jumpref-lookup

```

## tests/tests/checks/inputs/crossref/java/KotlinLibrary.kt/links_to_kotlin_library_constructor__json
```
search-identifiers sample::KotlinLibrary::<init> | crossref-lookup

```

## tests/tests/checks/inputs/crossref/merge/merge__big_cpp
```
merge-analyses big_cpp.cpp big_cpp.cpp -p foo-plat -p bar-plat

```

## tests/tests/checks/inputs/crossref/java/package__json
```
search-identifiers sample | crossref-lookup

```

## tests/tests/checks/inputs/crossref/java/JavaLibrary.java/no_inverse_relationships__json
```
search-identifiers sample::JavaLibrary::A | crossref-lookup

```

## tests/tests/checks/inputs/crossref/java/JavaLibrary.java/links_to_java_library_constructor__json
```
search-identifiers sample::JavaLibrary::<init> | crossref-lookup

```

## tests/tests/checks/inputs/crossref/java/JavaLibrary.java/multiple_implements__json
```
search-identifiers sample::JavaLibrary::C | crossref-lookup

```

## tests/tests/checks/inputs/crossref/identifiers/big_cpp.cpp/abstractart_beart__json
```
search-identifiers outerNS::AbstractArt::beArt

```

## tests/tests/checks/inputs/crossref/identifiers/big_cpp.cpp/thing__json
```
search-identifiers outerNS::Thing

```

## tests/tests/checks/inputs/crossref/crossref/big_cpp.cpp/thing_mHP__json
```
search-identifiers outerNS::Thing::mHP | crossref-lookup

```

## tests/tests/checks/inputs/crossref/crossref/xpctest_params.idl/testOctet__json
```
search-identifiers nsIXPCTestParams::TestOctet | crossref-lookup

```

## tests/tests/checks/inputs/crossref/crossref/big_cpp.cpp/abstractart__beart__json
```
search-identifiers outerNS::AbstractArt::beArt | crossref-lookup

```

## tests/tests/checks/inputs/crossref/crossref/big_cpp.cpp/stackartholder__json
```
search-identifiers outerNS::StackArtHolder | crossref-lookup

```

## tests/tests/checks/inputs/crossref/crossref/big_cpp.cpp/thing__json
```
search-identifiers outerNS::Thing | crossref-lookup

```

## tests/tests/checks/inputs/crossref/expand/big_cpp.cpp/outercat__json
```
search-identifiers outerNS::OuterCat | crossref-lookup | crossref-expand

```

## tests/tests/checks/inputs/jumpref/jumpref/simple.rs/loader__needs_hard_reload__lookup__json
```
search-identifiers Loader::needs_hard_reload | jumpref-lookup

```

## tests/tests/checks/inputs/crossref/crossref/xpctest_attributes.idl/booleanProperty_getter__json
```
search-identifiers nsIXPCTestObjectReadWrite::GetBooleanProperty | crossref-lookup

```

## tests/tests/checks/inputs/crossref/crossref/xpctest_attributes.idl/booleanProperty_setter__json
```
search-identifiers nsIXPCTestObjectReadWrite::SetBooleanProperty | crossref-lookup

```

## tests/tests/checks/Cargo.toml
```
# This is a dummy file to make `cargo insta review` okay with this being a root.
[package]
name = "dummy-test-package"
version = "0.1.0"
authors = ["Tests!"]
edition = "2018"

[lib]
name = "dummy_test_package"
path = "src/dummy-test-package.rs"

```

## tests/tests/checks/inputs/jumpref/jumpref/big_cpp.cpp/practicalart__beart__lookup__json
```
search-identifiers outerNS::PracticalArt::beArt | jumpref-lookup

```

## tests/tests/checks/inputs/jumpref/jumpref/big_cpp.cpp/big_header__h__lookup__json
```
jumpref-lookup "FILE_big_cpp@2Ecpp"

```

## tests/tests/checks/inputs/jumpref/jumpref/big_cpp.cpp/header_declared_func__lookup__json
```
search-identifiers "i_was_declared_in_the_header" | jumpref-lookup

```

## tests/tests/checks/inputs/jumpref/jumpref/big_cpp.cpp/abstractart__beart__lookup__json
```
search-identifiers outerNS::AbstractArt::beArt | jumpref-lookup

```

## tests/tests/setup
```
#!/usr/bin/env bash

set -x # Show commands
set -eu # Errors/undefined vars are fatal
set -o pipefail # Check all commands in a pipeline

# Note that this symlink potentially creates edge-cases related to logic that
# canonicalizes paths and thereby resolves symlinks.  So far this has only been
# an issue with ipdl-analyze and we addressed that by running the FILES_ROOT
# through `realpath`.  If we see more problems like this, that's probably the
# most practical course of action, because usually the scripts just want to have
# a prefix they can strip off and so it's just a case of making sure the prefix
# is consistent.
ln -s -f $CONFIG_REPO/tests/files $INDEX_ROOT

# Link over the fake metadata and test file information as well.
ln -s -f $CONFIG_REPO/tests/metadata/bugzilla-components.json $INDEX_ROOT
ln -s -f $CONFIG_REPO/tests/metadata/test-info-all-tests.json $INDEX_ROOT
ln -s -f $CONFIG_REPO/tests/metadata/wpt-manifest.json $INDEX_ROOT
ln -s -f $CONFIG_REPO/tests/metadata/wpt-mozilla-manifest.json $INDEX_ROOT
ln -s -f $CONFIG_REPO/tests/metadata/wpt-metadata-summary.json $INDEX_ROOT
ln -s -f $CONFIG_REPO/tests/metadata/code-coverage-report.json $INDEX_ROOT
ln -s -f $CONFIG_REPO/tests/metadata/test.chrome-map.json $INDEX_ROOT
ln -s -f $CONFIG_REPO/tests/metadata/test2.chrome-map.json $INDEX_ROOT
ln -s -f $CONFIG_REPO/tests/metadata/doc-trees.json $INDEX_ROOT

```

## tests/tests/metadata/wpt-manifest.json
```
{
  "items": {
    "testharness": {
      "complex-tests": {
        "some_cross_global_test.any.js": [
          "33b5f88f75b8052f202394c4f53bcc2770631202",
          [
            "complex-tests/some_cross_global_test.any.html",
            {
              "script_metadata": [
                [
                  "global",
                  "window,worker"
                ],
                [
                  "script",
                  "/some/fake/script.js"
                ]
              ]
            }
          ],
          [
            "complex-tests/some_cross_global_test.any.serviceworker.html",
            {
              "script_metadata": [
                [
                  "global",
                  "window,worker"
                ],
                [
                  "script",
                  "/some/fake/script.js"
                ]
              ]
            }
          ],
          [
            "complex-tests/some_cross_global_test.any.sharedworker.html",
            {
              "script_metadata": [
                [
                  "global",
                  "window,worker"
                ],
                [
                  "script",
                  "/some/fake/script.js"
                ]
              ]
            }
          ],
          [
            "complex-tests/some_cross_global_test.any.worker.html",
            {
              "script_metadata": [
                [
                  "global",
                  "window,worker"
                ],
                [
                  "script",
                  "/some/fake/script.js"
                ]
              ]
            }
          ]
        ]
      },
      "fake-standard": {
        "test_ima_disabled_wpt.js": [
          "bf7c0680412b627b9c8ca5932ffd84e8139eba27",
          [
            null,
            {}
          ]
        ],
        "test_ima_sad_subtests_wpt.js": [
          "bf7c0680412b627b9c8ca5932ffd84e8139eba27",
          [
            null,
            {}
          ]
        ],
        "test_ima_weird_meta_wpt.js": [
          "bf7c0680412b627b9c8ca5932ffd84e8139eba27",
          [
            null,
            {}
          ]
        ]
      }
    }
  }
}

```

## tests/tests/metadata/README.md
```
This directory contains one-off manually generated versions of metadata
that we download from taskcluster for mozilla-central and other gecko
repositories.

Right now it's somewhat of a feature that the files only cover a small
subset of the files in the tests repo, each with a distinct permutation
(ex: different test information configurations).  If we attempt to make
these files more comprehensive or dynamically generated, it would probably
be appropriate to create specific test files that somehow self-describe to
the auto-generation machinery what sort of data they want in their source.
(Embedding the description in the source makes it easier to sanity check
when viewing the searchfox page for the file.)

```

## tests/tests/metadata/test2.chrome-map.json
```
[
  {
    "chrome://global/content/test/": [
      "dist/bin/chrome/test"
    ]
  },
  {
  },
  {
    "dist/bin/chrome/test/chrome1.mjs": [
      "urlmap/chrome1b.mjs",
      null
    ]
  },
  {
    "MOZ_APP_NAME": "firefox",
    "MOZ_MACBUNDLE_NAME": "NightlyDebug.app",
    "OMNIJAR_NAME": "omni.ja",
    "topobjdir": "/builds/worker/workspace/obj-build"
  }
]

```

## tests/tests/metadata/wpt-mozilla-manifest.json
```
{
  "items": {
    "testharness": {
      "random-standard": {
        "mozilla_specific_test.any.js": [
          "33b5f88f75b8052f202394c4f53bcc2770631202",
          [
            "complex-tests/mozilla_specific_test.any.html",
            {
              "script_metadata": [
                [
                  "global",
                  "window,worker"
                ],
                [
                  "script",
                  "/some/fake/script.js"
                ]
              ]
            }
          ],
          [
            "complex-tests/mozilla_specific_test.any.serviceworker.html",
            {
              "script_metadata": [
                [
                  "global",
                  "window,worker"
                ],
                [
                  "script",
                  "/some/fake/script.js"
                ]
              ]
            }
          ],
          [
            "complex-tests/mozilla_specific_test.any.sharedworker.html",
            {
              "script_metadata": [
                [
                  "global",
                  "window,worker"
                ],
                [
                  "script",
                  "/some/fake/script.js"
                ]
              ]
            }
          ],
          [
            "complex-tests/mozilla_specific_test.any.worker.html",
            {
              "script_metadata": [
                [
                  "global",
                  "window,worker"
                ],
                [
                  "script",
                  "/some/fake/script.js"
                ]
              ]
            }
          ]
        ]
      }
    }
  }
}

```

## tests/tests/metadata/test.chrome-map.json
```
[
  {
    "chrome://global/content/test/": [
      "dist/bin/chrome/test"
    ],
    "resource://test/": [
      "dist/bin/resource/test"
    ]
  },
  {
    "chrome://global/content/test/chrome4.mjs": "chrome://global/content/test/chrome1.mjs",
    "resource://test/resource4.mjs": "resource://test/resource1.mjs"
  },
  {
    "dist/bin/chrome/test/chrome1.mjs": [
      "urlmap/chrome1.mjs",
      null
    ],
    "dist/bin/chrome/test/chrome2.mjs": [
      "urlmap/chrome2.mjs",
      null
    ],
    "dist/bin/chrome/test/chrome3.mjs": [
      "urlmap/chrome3.mjs",
      null
    ],
    "dist/bin/chrome/test/subdir/sub.mjs": [
      "urlmap/subdir/sub.mjs",
      null
    ],
    "dist/bin/chrome/test/chrome1.css": [
      "urlmap/chrome1.css",
      null
    ],
    "dist/bin/chrome/test/chrome1.html": [
      "urlmap/chrome1.html",
      null
    ],
    "dist/bin/chrome/test/chrome1.png": [
      "urlmap/chrome1.png",
      null
    ],
    "dist/bin/resource/test/resource1.mjs": [
      "urlmap/resource1.mjs",
      null
    ],
    "dist/bin/resource/test/resource2.mjs": [
      "urlmap/resource2.mjs",
      null
    ],
    "dist/bin/resource/test/resource3.mjs": [
      "urlmap/resource3.mjs",
      null
    ],
    "dist/bin/resource/test/resource1.css": [
      "urlmap/resource1.css",
      null
    ],
    "dist/bin/resource/test/resource1.html": [
      "urlmap/resource1.html",
      null
    ],
    "dist/bin/resource/test/resource1.png": [
      "urlmap/resource1.png",
      null
    ]
  },
  {
    "MOZ_APP_NAME": "firefox",
    "MOZ_MACBUNDLE_NAME": "NightlyDebug.app",
    "OMNIJAR_NAME": "omni.ja",
    "topobjdir": "/builds/worker/workspace/obj-build"
  }
]

```

## tests/tests/metadata/test-info-all-tests.json
```
{
  "description": "This imitates the taskcluster `Linting opt Test manifest skip/fail information source-test-file-metadata-test-info-disabled-by-os disabled-by-os` job",
  "summary": {
    "components": 1,
    "failed tests": 3,
    "manifests": 3,
    "skipped tests": 10,
    "tests": 100
  },
  "tests": {
    "Product::Component": [
      {
        "manifest": ["mochitest.ini"],
        "failure_count": 0,
        "skip-if": "toolkit == 'android'\n\nverify\napple_silicon",
        "test": "test_custom_element_base.xul"
      },
      {
        "manifest": ["chrome.ini"],
        "failure_count": 5,
        "test": "test_DOMWindowCreated_chromeonly.html"
      },
      {
        "manifest": ["browser.ini"],
        "failure_count": 0,
        "skip-if": "(os == \"win\" && processor == \"aarch64\") || (os == \"mac\") || (os == \"linux\" && !debug)",
        "test": "test_talosconfig_browser_config.json"
      },
      {
        "manifest": [
          "mochitest.ini:mochitest-common.ini",
          "mochitest-alt-pref.ini:mochitest-common.ini"
        ],
        "failure_count": 0,
        "test": "test_many_manifest_permutations.js"
      }
    ],
    "OtherProduct::OtherComponent": [
      {
        "searchfox-test-repo-note": "this file is not actually a test but I wanted a subdir",
        "failure_count": 0,
        "test": "test_rust_dependency/src/lib.rs"
      }
    ],
    "Web-Platform::Tests": [
      {
        "manifest": ["/complex-tests"],
        "failure_count": 0,
        "test": "testing/web-platform/tests/complex-tests/some_cross_global_test.any.js"
      },
      {
        "manifest": ["/fake-standard"],
        "failure_count": 0,
        "test": "testing/web-platform/tests/fake-standard/test_ima_disabled_wpt.js"
      },
      {
        "manifest": ["/fake-standard"],
        "failure_count": 0,
        "test": "testing/web-platform/tests/fake-standard/test_ima_sad_subtests_wpt.js"
      },
      {
        "manifest": ["/fake-standard"],
        "failure_count": 0,
        "test": "testing/web-platform/tests/fake-standard/test_ima_weird_meta_wpt.js"
      }
    ]
  }
}

```

## tests/tests/metadata/doc-trees.json
```
{"md_test": "docs/md_test", "rst_test": "docs/rst_test"}

```

## tests/tests/metadata/bugzilla-components.json
```
{
  "components": {
    "0": [
      "Core",
      "Big"
    ],
    "1": [
      "Core",
      "Simple"
    ],
    "2": [
      "Atoms",
      "General"
    ],
    "3": [
      "Data",
      "Images: SVG"
    ],
    "4": [
      "Data",
      "Text Files: Markdown"
    ],
    "5": [
      "Core",
      "Templates"
    ],
    "6": [
      "Core",
      "XPCOM"
    ],
    "7": [
      "Core",
      "DOM: Service Workers"
    ]
  },
  "paths": {
    "README.md": 4,
    "atom_list.h": 2,
    "atom_magic.h": 2,
    "big_header.h": 0,
    "big_cpp.cpp": 0,
    "circle.svg": 3,
    "overs.cpp": 1,
    "simple.cpp": 1,
    "simple.rs": 1,
    "templates1.cpp": 5,
    "templates2.cpp": 5,
    "templates3.cpp": 5,
    "templates4.cpp": 5,
    "templates5.cpp": 5,
    "templates5.h": 5,
    "templates_nsTArray.cpp": 5,
    "testing": {
      "web-platform": {
        "tests": {
          "complex-tests": {
            "some_cross_global_test.any.js": 7
          },
          "fake-standard": {
            "test_ima_disabled_wpt.js": 7,
            "test_ima_sad_subtests_wpt.js": 7,
            "test_ima_weird_meta_wpt.js": 7
          }
        }
      }
    },
    "test_rust_dependency": {
      "src": {
        "lib.rs": 1
      }
    },
    "tiger.svg": 3,
    "using.cpp": 1,
    "xpidl_cpp_consumer.cpp": 6
  }
}

```

## tests/tests/metadata/wpt-metadata-summary.json
```
{
  "fake-standard": {
    "_tests": {
      "test_ima_disabled_wpt.js": {
        "_filename": "test_ima_disabled_wpt.js",
        "disabled": [
          [null, "12345"],
          ["os == \"win\"\n", "https://bugzilla.mozilla.org/show_bug.cgi?id=234567"],
          ["os == \"linux\"\n", "https://bugzilla.mozilla.org/234567"]
        ]
      },
      "test_ima_sad_subtests_wpt.js": {
        "_filename": "test_ima_sad_subtests_wpt.js",
        "_subtests": {
          "foo": {},
          "bar": {},
          "baz": {}
        }
      },
      "test_ima_weird_meta_wpt.html": {
        "_filename": "test_ima_weird_meta_wpt.js"
      }
    }
  },
  "complex-tests": {
    "_tests": {
      "some_cross_global_test.any.html": {
        "_filename": "some_cross_global_test.any.js",
        "_subtests": {
          "foo": {}
        }
      }
    }
  }
}

```

## tests/tests/files/enummacro.h
```
PRES_ARENA_OBJECT(nsLineBox)
PRES_ARENA_OBJECT(nsRuleNode)
PRES_ARENA_OBJECT(DisplayItemData)

```

## tests/tests/files/some_javascript.js
```
// random bits of javascript

const str_computed_field_num = 'computed_field_num';

class ClassWithProperties {

  pub_field_undefined;

  pub_field_num = 1;

  pub_field_dict = {
    field_sub_prop: 2,
    field_sub_dict: {
      field_sub_sub_prop: 3
    }
  };

  pub_field_func = function(b) {
    return b * 2;
  };

  pub_field_arrow_func = b => {
    return b * 2;
  };

  // Computed property name syntax.
  [str_computed_field_num] = 3;

  // PRIVATE FIELDS AREN'T SUPPORTED YET.
  // These next 2 simply won't emit anything in the AST right now.
  #priv_field_undefined;

  #priv_field_num = 10;

  // This generates a syntax error even if I wrap the object initializer in
  // parens.  The spec is very dry and I'm having trouble figuring out if this
  // is actually legal or not.
  #priv_field_dict = {
    sub_prop: 12
  };

  #priv_method() {
    // Let's have a few lines so we can verify position: sticky.

    // BEEP BEEP BEEP BEEP
    // BEEP           BEEP
    // BEEP           BEEP
    // BEEP           BEEP
    // BEEP           BEEP
    // BEEP           BEEP
    // BEEP           BEEP
    // BEEP           BEEP
    // BEEP           BEEP
    // BEEP BEEP BEEP BEEP
    return this.#priv_field_undefined;
  }

  // This also doesn't work.
  #priv_field_func = b => {
    // Let's have a few lines so we can verify position: sticky.

    // BEEP BEEP BEEP BEEP
    // BEEP           BEEP
    // BEEP           BEEP
    // BEEP           BEEP
    // BEEP           BEEP
    // BEEP           BEEP
    // BEEP           BEEP
    // BEEP           BEEP
    // BEEP           BEEP
    // BEEP BEEP BEEP BEEP
    return b * 2;
  };

  consumes_priv_field_num() {
    return this.#priv_field_num;
  }
}

class ClassWithStaticMethods {
  static theStaticMethod(baz) {
    return baz;
  }
}

let obj_dict = {
  obj_prop: 2,
  obj_sub_dict: {
    obj_sub_prop: 3,
    obj_sub_sub_dict: {
      obj_sub_sub_prop: 4
    }
  },
  "<select><keygen>": 2,
};

function Laser() {
}
Laser.prototype = {
  propertyNamedFunction: function(arg1) {

  },

  coolFunctionSyntax(arg1) {

  },

  get getterSyntax() {

  }
};

Laser.prototype.separateAssignedFunc = function(arg1) {
  return 5;
};

Laser.prototype.randoObj = {
  nestedObj: {},

  baz: function() {
  }
};

Laser.prototype.randoObj.nestedObj.foo = function(arg1) {

};

Laser.prototype.randoObj.addedOn.bar = function(arg1) {

};

var multiline_backtick = `here \
  is a thing`;

function destructure_with_spreadexpression(foo) {
  const {
    contextmenu,
    ...options
  } = foo;
}

```

## tests/tests/files/xpidl_cpp_consumer.cpp
```
#include "dist/include/xpctest_attributes.h"
#include "dist/include/xpctest_params.h"

void consume_xpidl(nsIXPCTestParams* params) {
  uint8_t b = 0;
  uint8_t out;
  params->TestOctet(1, &b, &out);
}

void consume_attr(nsIXPCTestObjectReadWrite* attrs) {
  bool string_was_too_hard;

  attrs->GetBooleanProperty(&string_was_too_hard);

  // Yup, now put it back!
  attrs->SetBooleanProperty(string_was_too_hard);
}

```

## tests/tests/files/some_python.py
```
"""Some absurdly long doc-comment.
Spread over multiple lines.
Spread over multiple lines.
Spread over multiple lines.
Spread over multiple lines.
Spread over multiple lines.
Spread over multiple lines.
Spread over multiple lines.
Spread over multiple lines.
Spread over multiple lines.
Spread over multiple lines.
Spread over multiple lines.
Spread over multiple lines.
Spread over multiple lines.
Spread over multiple lines.
Spread over multiple lines.
Spread over multiple lines.
"""

def myFunction():
    """This is the function documentation.
    As per https://bugzil.la/1449606 (i.e. bug 1449606) we want
    URLs and bug numbers here to be linkified."""
    pass

```

## tests/tests/files/gzip-colliding-file.gz
```
ï¿½ï¿½w_ gzip-colliding-file -ï¿½ï¿½ï¿½0sï¿½ØŒï¿½eï¿½ï¿½ï¿½dlï¿½;ï¿½Eï¿½Xï¿½t43$ï¿½Dï¿½ï¿½ï¿½p	ï¿½6b>ï¿½[keï¿½%ï¿½ï¿½ï¿½ï¿½æ¯†ï¿½ï¿½%ï¿½ï¿½k*ï¿½Mcï¿½\ï¿½Ú²uÙ†ï¿½Tï¿½ï¿½Gï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ ï¿½Şµï¿½|   
```

## tests/tests/files/.clang-format
```
BasedOnStyle: Google
ColumnLimit: 80

# Prevent the loss of indentation with these macros
MacroBlockBegin: "^\
JS_BEGIN_MACRO|\
NS_INTERFACE_MAP_BEGIN|\
NS_INTERFACE_TABLE_HEAD|\
NS_INTERFACE_MAP_BEGIN_CYCLE_COLLECTION|\
NS_IMPL_CYCLE_COLLECTION_.*_BEGIN|\
NS_INTERFACE_TABLE_HEAD_CYCLE_COLLECTION_INHERITED|\
NS_INTERFACE_TABLE_BEGIN|\
NS_IMPL_CYCLE_COLLECTION_TRAVERSE_BEGIN_INHERITED|\
NS_IMPL_CYCLE_COLLECTION_UNLINK_BEGIN_INHERITED|\
NS_QUERYFRAME_HEAD$"
MacroBlockEnd: "^\
JS_END_MACRO|\
NS_INTERFACE_MAP_END|\
NS_IMPL_CYCLE_COLLECTION_.*_END|\
NS_INTERFACE_TABLE_END|\
NS_INTERFACE_TABLE_TAIL.*|\
NS_INTERFACE_MAP_END_.*|\
NS_IMPL_CYCLE_COLLECTION_TRAVERSE_END_INHERITED|\
NS_IMPL_CYCLE_COLLECTION_UNLINK_END_INHERITED|\
NS_QUERYFRAME_TAIL.*$"

SortIncludes: false
IndentPPDirectives: AfterHash
StatementMacros: [MARKUPMAP, ASSERT_TRUE, ASSERT_FALSE, TEST, CHECK]

# The Google coding style states:
# You should do this consistently within a single file, so, when modifying an
# existing file, use the style in that file.
# Let's be more prescriptive and default to the one used in the Mozilla
# coding style
DerivePointerAlignment: false
PointerAlignment: Left

```

## tests/tests/files/test_talosconfig_browser_config.json
```
{'deviceroot': '', 'dirs': {}, 'repository': 'http://hg.mozilla.org/releases/mozilla-release', 'buildid': '20131205075310', 'results_log': 'pathtoresults_log', 'symbols_path': None, 'bcontroller_config': 'pathtobcontroller', 'host': '', 'browser_name': 'Firefox', 'sourcestamp': '39faf812aaec', 'remote': False, 'child_process': 'plugin-container', 'branch_name': '', 'browser_version': '26.0', 'extra_args': '', 'develop': True, 'preferences': {'browser.display.overlaynavbuttons': False, 'extensions.getAddons.get.url': 'http://127.0.0.1/extensions-dummy/repositoryGetURL', 'dom.max_chrome_script_run_time': 0, 'network.proxy.type': 1, 'extensions.update.background.url': 'http://127.0.0.1/extensions-dummy/updateBackgroundURL', 'network.proxy.http': 'localhost', 'plugins.update.url': 'http://127.0.0.1/plugins-dummy/updateCheckURL', 'dom.max_script_run_time': 0, 'extensions.update.enabled': False, 'browser.safebrowsing.keyURL': 'http://127.0.0.1/safebrowsing-dummy/newkey', 'media.navigator.permission.disabled': True, 'app.update.enabled': False, 'extensions.blocklist.url': 'http://127.0.0.1/extensions-dummy/blocklistURL', 'browser.EULA.override': True, 'extensions.checkCompatibility': False, 'talos.logfile': 'pathtofile', 'browser.safebrowsing.gethashURL': 'http://127.0.0.1/safebrowsing-dummy/gethash', 'extensions.hotfix.url': 'http://127.0.0.1/extensions-dummy/hotfixURL', 'dom.disable_window_move_resize': True, 'network.proxy.http_port': 80, 'browser.dom.window.dump.enabled': True, 'extensions.update.url': 'http://127.0.0.1/extensions-dummy/updateURL', 'browser.chrome.dynamictoolbar': False,  'browser.link.open_newwindow': 2, 'extensions.getAddons.search.url': 'http://127.0.0.1/extensions-dummy/repositorySearchURL', 'browser.cache.disk.smart_size.first_run': False, 'security.turn_off_all_security_so_that_viruses_can_take_over_this_computer': True, 'dom.disable_open_during_load': False, 'extensions.getAddons.search.browseURL': 'http://127.0.0.1/extensions-dummy/repositoryBrowseURL', 'browser.cache.disk.smart_size.enabled': False, 'extensions.getAddons.getWithPerformance.url': 'http://127.0.0.1/extensions-dummy/repositoryGetWithPerformanceURL', 'hangmonitor.timeout': 0, 'extensions.getAddons.maxResults': 0, 'dom.send_after_paint_to_content': True, 'security.fileuri.strict_origin_policy': False, 'media.capturestream_hints.enabled': True, 'extensions.update.notifyUser': False, 'extensions.blocklist.enabled': False, 'browser.bookmarks.max_backups': 0, 'browser.shell.checkDefaultBrowser': False, 'media.peerconnection.enabled': True, 'dom.disable_window_flip': True, 'security.enable_java': False, 'browser.warnOnQuit': False, 'media.navigator.enabled': True, 'browser.safebrowsing.updateURL': 'http://127.0.0.1/safebrowsing-dummy/update', 'dom.allow_scripts_to_close_windows': True, 'extensions.webservice.discoverURL': 'http://127.0.0.1/extensions-dummy/discoveryURL'}, 'test_timeout': 1200, 'title': 'qm-pxp01', 'error_filename': 'pathtoerrorfile', 'webserver': 'localhost:15707', 'browser_path':ffox_path, 'port': 20701, 'browser_log': 'browser_output.txt', 'process': 'firefox.exe', 'xperf_path': 'C:/Program Files/Microsoft Windows Performance Toolkit/xperf.exe', 'extensions': ['pathtopageloader'], 'fennecIDs': '', 'env': {'NO_EM_RESTART': '1'}, 'init_url': 'http://localhost:15707/getInfo.html', 'browser_wait': 5}

```

## tests/tests/files/templates4.cpp
```
class Base {};

/*
 */
template <typename T>
class Template : public Base {};

template class Template<int>;

```

## tests/tests/files/root-module.mjs
```
import { moduleConst, moduleFunc, ModuleClass } from "./imported-module.mjs";
import { default as aliasedDefault } from "./imported-module.mjs";
import * as importedModule from "./imported-module.mjs";

function exportedAsDefaultAsReference() {
    return 5;
}

// This should generate a failure.
export default exportedAsDefaultAsReference;

const rootModuleConst = 10;

```

## tests/tests/files/mochitest-common.ini
```
[test_many_manifest_permutations.js]

```

## tests/tests/files/test_rust_dependency/src/lib.rs
```
//! A description of this very interesting module.
//!
//! This is an overview of yada yada yada...

#[derive(Copy, Clone, Default)]
pub struct MyType(());

impl MyType {
    pub fn new() -> Self {
        MyType(())
    }

    pub fn do_foo(self) -> MyType {
        unimplemented!()
    }
}

pub mod my_mod {
    pub struct MyOtherType;
}

pub trait MyTrait {
    fn do_bar() -> i32;

    fn do_foo() -> MyType {
        MyType::new()
    }
}

extern "C" {
    pub fn ExternFunctionImplementedInCpp();
}

impl MyTrait for MyType {
    fn do_bar() -> i32 {
        100
    }
}

#[cfg(test)]
mod tests {
    #[test]
    fn it_works() {
        assert_eq!(2 + 2, 4);
    }
}

```

## tests/tests/files/chrome.ini
```
[test_DOMWindowCreated_chromeonly.html]

```

## tests/tests/files/bug1781178.cpp
```
template <typename>
struct Point {
  bool IsThereOne();
};

template <typename T>
struct Foo {
  struct Nested {
    int field;
  };
  Nested nested;

  enum E { Waldo };

  void Simple();
  static void Static();

  template <typename F>
  void Project(Point<F>) {}

  template <typename F>
  void Project(Point<F>, Point<F>) {}

  void Bar() {
    Point<float> p;
    Project(p);

    Point<T> tp;
    this->Project(tp);

    this->Simple();

    (void)nested.field;

    (void)E::Waldo;
  }

  template <typename Other>
  void Baz() {
    Foo<Other>::Static();
  }

  using Typedef = int;
};

namespace internal {
template <typename T>
void Read();
}

template <typename T>
void TemplateFunc(typename Foo<T>::Typedef) {
  Point<T> p;
  p.IsThereOne();

  using internal::Read;
  Read(p);
  Read();
}

template <typename T>
using Pint = Point<T>;

template <typename T>
struct DerivedPoint : Pint<T> {
  void Foo() { this->IsThereOne(); }
};

template <typename T>
void func() {
  const auto _ = T::E::Waldo;
}

void test() { func<Foo<int>>(); }

struct WithOverloads {
  static void Overloaded(int);
  static void Overloaded(float);
  static void Overloaded(bool);

  template <typename T>
  static void Caller() {
    Overloaded(T());
  }
};

void test_overload() { WithOverloads::Caller<int>(); }

```

## tests/tests/files/cpp/URL_sym.cpp
```
#include <stdint.h>

namespace url_sym {

struct S {
  int32_t scriptURL_;
};

}  // namespace url_sym

```

## tests/tests/files/cpp/contextual-keyword.cpp
```
int final() { return 10; }

int import() { return 20; }

int module() { return 30; }

int override() { return 40; }

int f() { return final() + import() + module() + override(); }

class Base {
 public:
  virtual void Foo();
};

class Sub final : public Base {
 public:
  void Foo() override;
};

```

## tests/tests/files/bug1588908.js
```
/**
 * This tests the regexp literal arrow function from
 * BrowserTestUtils.waitForDocLoadAndStopIt, which was breaking syntax
 * highlighting in bug 1588908.
 */

let isHttp = url => /^https?:/.test(url);

function f(x) {
  // Not sure why anyone would ever do this, but it's valid JavaScript...
  return x < /^https?:/;
}

```

## tests/tests/files/LightweightThemeManager.jsm
```
/* This Source Code Form is subject to the terms of the Mozilla Public
 * License, v. 2.0. If a copy of the MPL was not distributed with this
 * file, You can obtain one at http://mozilla.org/MPL/2.0/. */

"use strict";

this.EXPORTED_SYMBOLS = ["LightweightThemeManager"];

const Cc = Components.classes;
const Ci = Components.interfaces;

Components.utils.import("resource://gre/modules/XPCOMUtils.jsm");
Components.utils.import("resource://gre/modules/AddonManager.jsm");
/* globals AddonManagerPrivate*/
Components.utils.import("resource://gre/modules/Services.jsm");

const ID_SUFFIX              = "@personas.mozilla.org";
const PREF_LWTHEME_TO_SELECT = "extensions.lwThemeToSelect";
const PREF_GENERAL_SKINS_SELECTEDSKIN = "general.skins.selectedSkin";
const PREF_EM_DSS_ENABLED    = "extensions.dss.enabled";
const ADDON_TYPE             = "theme";

const URI_EXTENSION_STRINGS  = "chrome://mozapps/locale/extensions/extensions.properties";

const STRING_TYPE_NAME       = "type.%ID%.name";

const DEFAULT_MAX_USED_THEMES_COUNT = 30;

const MAX_PREVIEW_SECONDS = 30;

const MANDATORY = ["id", "name", "headerURL"];
const OPTIONAL = ["footerURL", "textcolor", "accentcolor", "iconURL",
                  "previewURL", "author", "description", "homepageURL",
                  "updateURL", "version"];

const PERSIST_ENABLED = true;
const PERSIST_BYPASS_CACHE = false;
const PERSIST_FILES = {
  headerURL: "lightweighttheme-header",
  footerURL: "lightweighttheme-footer"
};

XPCOMUtils.defineLazyModuleGetter(this, "LightweightThemeImageOptimizer",
  "resource://gre/modules/addons/LightweightThemeImageOptimizer.jsm");
XPCOMUtils.defineLazyModuleGetter(this, "ServiceRequest",
  "resource://gre/modules/ServiceRequest.jsm");


XPCOMUtils.defineLazyGetter(this, "_prefs", () => {
  return Services.prefs.getBranch("lightweightThemes.");
});

Object.defineProperty(this, "_maxUsedThemes", {
  get() {
    delete this._maxUsedThemes;
    try {
      this._maxUsedThemes = _prefs.getIntPref("maxUsedThemes");
    } catch (e) {
      this._maxUsedThemes = DEFAULT_MAX_USED_THEMES_COUNT;
    }
    return this._maxUsedThemes;
  },

  set(val) {
    delete this._maxUsedThemes;
    return this._maxUsedThemes = val;
  },
  configurable: true,
});

// Holds the ID of the theme being enabled or disabled while sending out the
// events so cached AddonWrapper instances can return correct values for
// permissions and pendingOperations
var _themeIDBeingEnabled = null;
var _themeIDBeingDisabled = null;

// Convert from the old storage format (in which the order of usedThemes
// was combined with isThemeSelected to determine which theme was selected)
// to the new one (where a selectedThemeID determines which theme is selected).
(function() {
  let wasThemeSelected = false;
  try {
    wasThemeSelected = _prefs.getBoolPref("isThemeSelected");
  } catch (e) { }

  if (wasThemeSelected) {
    _prefs.clearUserPref("isThemeSelected");
    let themes = [];
    try {
      themes = JSON.parse(_prefs.getComplexValue("usedThemes",
                                                 Ci.nsISupportsString).data);
    } catch (e) { }

    if (Array.isArray(themes) && themes[0]) {
      _prefs.setCharPref("selectedThemeID", themes[0].id);
    }
  }
})();

this.LightweightThemeManager = {
  get name() {
    return "LightweightThemeManager";
  },

  // Themes that can be added for an application.  They can't be removed, and
  // will always show up at the top of the list.
  _builtInThemes: new Map(),

  get usedThemes() {
    let themes = [];
    try {
      themes = JSON.parse(_prefs.getComplexValue("usedThemes",
                                                 Ci.nsISupportsString).data);
    } catch (e) { }

    themes.push(...this._builtInThemes.values());
    return themes;
  },

  get currentTheme() {
    let selectedThemeID = null;
    try {
      selectedThemeID = _prefs.getCharPref("selectedThemeID");
    } catch (e) {}

    let data = null;
    if (selectedThemeID) {
      data = this.getUsedTheme(selectedThemeID);
    }
    return data;
  },

  get currentThemeForDisplay() {
    var data = this.currentTheme;

    if (data && PERSIST_ENABLED) {
      for (let key in PERSIST_FILES) {
        try {
          if (data[key] && _prefs.getBoolPref("persisted." + key))
            data[key] = _getLocalImageURI(PERSIST_FILES[key]).spec
                        + "?" + data.id + ";" + _version(data);
        } catch (e) {}
      }
    }

    return data;
  },

  set currentTheme(aData) {
    return _setCurrentTheme(aData, false);
  },

  setLocalTheme(aData) {
    _setCurrentTheme(aData, true);
  },

  getUsedTheme(aId) {
    var usedThemes = this.usedThemes;
    for (let usedTheme of usedThemes) {
      if (usedTheme.id == aId)
        return usedTheme;
    }
    return null;
  },

  forgetUsedTheme(aId) {
    let theme = this.getUsedTheme(aId);
    if (!theme || LightweightThemeManager._builtInThemes.has(theme.id))
      return;

    let wrapper = new AddonWrapper(theme);
    AddonManagerPrivate.callAddonListeners("onUninstalling", wrapper, false);

    var currentTheme = this.currentTheme;
    if (currentTheme && currentTheme.id == aId) {
      this.themeChanged(null);
      AddonManagerPrivate.notifyAddonChanged(null, ADDON_TYPE, false);
    }

    _updateUsedThemes(_usedThemesExceptId(aId));
    AddonManagerPrivate.callAddonListeners("onUninstalled", wrapper);
  },

  addBuiltInTheme(theme) {
    if (!theme || !theme.id || this.usedThemes.some(t => t.id == theme.id)) {
      throw new Error("Trying to add invalid builtIn theme");
    }

    this._builtInThemes.set(theme.id, theme);

    if (_prefs.getCharPref("selectedThemeID") == theme.id) {
      this.currentTheme = theme;
    }
  },

  forgetBuiltInTheme(id) {
    if (!this._builtInThemes.has(id)) {
      let currentTheme = this.currentTheme;
      if (currentTheme && currentTheme.id == id) {
        this.currentTheme = null;
      }
    }
    return this._builtInThemes.delete(id);
  },

  clearBuiltInThemes() {
    for (let id of this._builtInThemes.keys()) {
      this.forgetBuiltInTheme(id);
    }
  },

  previewTheme(aData) {
    let cancel = Cc["@mozilla.org/supports-PRBool;1"].createInstance(Ci.nsISupportsPRBool);
    cancel.data = false;
    Services.obs.notifyObservers(cancel, "lightweight-theme-preview-requested",
                                 JSON.stringify(aData));
    if (cancel.data)
      return;

    if (_previewTimer)
      _previewTimer.cancel();
    else
      _previewTimer = Cc["@mozilla.org/timer;1"].createInstance(Ci.nsITimer);
    _previewTimer.initWithCallback(_previewTimerCallback,
                                   MAX_PREVIEW_SECONDS * 1000,
                                   _previewTimer.TYPE_ONE_SHOT);

    _notifyWindows(aData);
  },

  resetPreview() {
    if (_previewTimer) {
      _previewTimer.cancel();
      _previewTimer = null;
      _notifyWindows(this.currentThemeForDisplay);
    }
  },

  parseTheme(aString, aBaseURI) {
    try {
      return _sanitizeTheme(JSON.parse(aString), aBaseURI, false);
    } catch (e) {
      return null;
    }
  },

  updateCurrentTheme() {
    try {
      if (!_prefs.getBoolPref("update.enabled"))
        return;
    } catch (e) {
      return;
    }

    var theme = this.currentTheme;
    if (!theme || !theme.updateURL)
      return;

    var req = new ServiceRequest();

    req.mozBackgroundRequest = true;
    req.overrideMimeType("text/plain");
    req.open("GET", theme.updateURL, true);
    // Prevent the request from reading from the cache.
    req.channel.loadFlags |= Ci.nsIRequest.LOAD_BYPASS_CACHE;
    // Prevent the request from writing to the cache.
    req.channel.loadFlags |= Ci.nsIRequest.INHIBIT_CACHING;

    req.addEventListener("load", () => {
      if (req.status != 200)
        return;

      let newData = this.parseTheme(req.responseText, theme.updateURL);
      if (!newData ||
          newData.id != theme.id ||
          _version(newData) == _version(theme))
        return;

      var currentTheme = this.currentTheme;
      if (currentTheme && currentTheme.id == theme.id)
        this.currentTheme = newData;
    }, false);

    req.send(null);
  },

  /**
   * Switches to a new lightweight theme.
   *
   * @param  aData
   *         The lightweight theme to switch to
   */
  themeChanged(aData) {
    if (_previewTimer) {
      _previewTimer.cancel();
      _previewTimer = null;
    }

    if (aData) {
      let usedThemes = _usedThemesExceptId(aData.id);
      usedThemes.unshift(aData);
      _updateUsedThemes(usedThemes);
      if (PERSIST_ENABLED) {
        LightweightThemeImageOptimizer.purge();
        _persistImages(aData, function() {
          _notifyWindows(this.currentThemeForDisplay);
        }.bind(this));
      }
    }

    if (aData)
      _prefs.setCharPref("selectedThemeID", aData.id);
    else
      _prefs.setCharPref("selectedThemeID", "");

    _notifyWindows(aData);
    Services.obs.notifyObservers(null, "lightweight-theme-changed", null);
  },

  /**
   * Starts the Addons provider and enables the new lightweight theme if
   * necessary.
   */
  startup() {
    if (Services.prefs.prefHasUserValue(PREF_LWTHEME_TO_SELECT)) {
      let id = Services.prefs.getCharPref(PREF_LWTHEME_TO_SELECT);
      if (id)
        this.themeChanged(this.getUsedTheme(id));
      else
        this.themeChanged(null);
      Services.prefs.clearUserPref(PREF_LWTHEME_TO_SELECT);
    }

    _prefs.addObserver("", _prefObserver, false);
  },

  /**
   * Shuts down the provider.
   */
  shutdown() {
    _prefs.removeObserver("", _prefObserver);
  },

  /**
   * Called when a new add-on has been enabled when only one add-on of that type
   * can be enabled.
   *
   * @param  aId
   *         The ID of the newly enabled add-on
   * @param  aType
   *         The type of the newly enabled add-on
   * @param  aPendingRestart
   *         true if the newly enabled add-on will only become enabled after a
   *         restart
   */
  addonChanged(aId, aType, aPendingRestart) {
    if (aType != ADDON_TYPE)
      return;

    let id = _getInternalID(aId);
    let current = this.currentTheme;

    try {
      let next = Services.prefs.getCharPref(PREF_LWTHEME_TO_SELECT);
      if (id == next && aPendingRestart)
        return;

      Services.prefs.clearUserPref(PREF_LWTHEME_TO_SELECT);
      if (next) {
        AddonManagerPrivate.callAddonListeners("onOperationCancelled",
                                               new AddonWrapper(this.getUsedTheme(next)));
      } else if (id == current.id) {
        AddonManagerPrivate.callAddonListeners("onOperationCancelled",
                                               new AddonWrapper(current));
        return;
      }
    } catch (e) {
    }

    if (current) {
      if (current.id == id)
        return;
      _themeIDBeingDisabled = current.id;
      let wrapper = new AddonWrapper(current);
      if (aPendingRestart) {
        Services.prefs.setCharPref(PREF_LWTHEME_TO_SELECT, "");
        AddonManagerPrivate.callAddonListeners("onDisabling", wrapper, true);
      } else {
        AddonManagerPrivate.callAddonListeners("onDisabling", wrapper, false);
        this.themeChanged(null);
        AddonManagerPrivate.callAddonListeners("onDisabled", wrapper);
      }
      _themeIDBeingDisabled = null;
    }

    if (id) {
      let theme = this.getUsedTheme(id);
      _themeIDBeingEnabled = id;
      let wrapper = new AddonWrapper(theme);
      if (aPendingRestart) {
        AddonManagerPrivate.callAddonListeners("onEnabling", wrapper, true);
        Services.prefs.setCharPref(PREF_LWTHEME_TO_SELECT, id);

        // Flush the preferences to disk so they survive any crash
        Services.prefs.savePrefFile(null);
      } else {
        AddonManagerPrivate.callAddonListeners("onEnabling", wrapper, false);
        this.themeChanged(theme);
        AddonManagerPrivate.callAddonListeners("onEnabled", wrapper);
      }
      _themeIDBeingEnabled = null;
    }
  },

  /**
   * Called to get an Addon with a particular ID.
   *
   * @param  aId
   *         The ID of the add-on to retrieve
   * @param  aCallback
   *         A callback to pass the Addon to
   */
  getAddonByID(aId, aCallback) {
    let id = _getInternalID(aId);
    if (!id) {
      aCallback(null);
      return;
     }

    let theme = this.getUsedTheme(id);
    if (!theme) {
      aCallback(null);
      return;
    }

    aCallback(new AddonWrapper(theme));
  },

  /**
   * Called to get Addons of a particular type.
   *
   * @param  aTypes
   *         An array of types to fetch. Can be null to get all types.
   * @param  aCallback
   *         A callback to pass an array of Addons to
   */
  getAddonsByTypes(aTypes, aCallback) {
    if (aTypes && aTypes.indexOf(ADDON_TYPE) == -1) {
      aCallback([]);
      return;
    }

    aCallback(this.usedThemes.map(a => new AddonWrapper(a)));
  },
};

const wrapperMap = new WeakMap();
let themeFor = wrapper => wrapperMap.get(wrapper);

/**
 * The AddonWrapper wraps lightweight theme to provide the data visible to
 * consumers of the AddonManager API.
 */
function AddonWrapper(aTheme) {
  wrapperMap.set(this, aTheme);
}

AddonWrapper.prototype = {
  get id() {
    return themeFor(this).id + ID_SUFFIX;
  },

  get type() {
    return ADDON_TYPE;
  },

  get isActive() {
    let current = LightweightThemeManager.currentTheme;
    if (current)
      return themeFor(this).id == current.id;
    return false;
  },

  get name() {
    return themeFor(this).name;
  },

  get version() {
    let theme = themeFor(this);
    return "version" in theme ? theme.version : "";
  },

  get creator() {
    let theme = themeFor(this);
    return "author" in theme ? new AddonManagerPrivate.AddonAuthor(theme.author) : null;
  },

  get screenshots() {
    let url = themeFor(this).previewURL;
    return [new AddonManagerPrivate.AddonScreenshot(url)];
  },

  get pendingOperations() {
    let pending = AddonManager.PENDING_NONE;
    if (this.isActive == this.userDisabled)
      pending |= this.isActive ? AddonManager.PENDING_DISABLE : AddonManager.PENDING_ENABLE;
    return pending;
  },

  get operationsRequiringRestart() {
    // If a non-default theme is in use then a restart will be required to
    // enable lightweight themes unless dynamic theme switching is enabled
    if (Services.prefs.prefHasUserValue(PREF_GENERAL_SKINS_SELECTEDSKIN)) {
      try {
        if (Services.prefs.getBoolPref(PREF_EM_DSS_ENABLED))
          return AddonManager.OP_NEEDS_RESTART_NONE;
      } catch (e) {
      }
      return AddonManager.OP_NEEDS_RESTART_ENABLE;
    }

    return AddonManager.OP_NEEDS_RESTART_NONE;
  },

  get size() {
    // The size changes depending on whether the theme is in use or not, this is
    // probably not worth exposing.
    return null;
  },

  get permissions() {
    let permissions = 0;

    // Do not allow uninstall of builtIn themes.
    if (!LightweightThemeManager._builtInThemes.has(themeFor(this).id))
      permissions = AddonManager.PERM_CAN_UNINSTALL;
    if (this.userDisabled)
      permissions |= AddonManager.PERM_CAN_ENABLE;
    else
      permissions |= AddonManager.PERM_CAN_DISABLE;
    return permissions;
  },

  get userDisabled() {
    let id = themeFor(this).id;
    if (_themeIDBeingEnabled == id)
      return false;
    if (_themeIDBeingDisabled == id)
      return true;

    try {
      let toSelect = Services.prefs.getCharPref(PREF_LWTHEME_TO_SELECT);
      return id != toSelect;
    } catch (e) {
      let current = LightweightThemeManager.currentTheme;
      return !current || current.id != id;
    }
  },

  set userDisabled(val) {
    if (val == this.userDisabled)
      return val;

    if (val)
      LightweightThemeManager.currentTheme = null;
    else
      LightweightThemeManager.currentTheme = themeFor(this);

    return val;
  },

  // Lightweight themes are never disabled by the application
  get appDisabled() {
    return false;
  },

  // Lightweight themes are always compatible
  get isCompatible() {
    return true;
  },

  get isPlatformCompatible() {
    return true;
  },

  get scope() {
    return AddonManager.SCOPE_PROFILE;
  },

  get foreignInstall() {
    return false;
  },

  uninstall() {
    LightweightThemeManager.forgetUsedTheme(themeFor(this).id);
  },

  cancelUninstall() {
    throw new Error("Theme is not marked to be uninstalled");
  },

  findUpdates(listener, reason, appVersion, platformVersion) {
    AddonManagerPrivate.callNoUpdateListeners(this, listener, reason, appVersion, platformVersion);
  },

  // Lightweight themes are always compatible
  isCompatibleWith(appVersion, platformVersion) {
    return true;
  },

  // Lightweight themes are always securely updated
  get providesUpdatesSecurely() {
    return true;
  },

  // Lightweight themes are never blocklisted
  get blocklistState() {
    return Ci.nsIBlocklistService.STATE_NOT_BLOCKED;
  }
};

["description", "homepageURL", "iconURL"].forEach(function(prop) {
  Object.defineProperty(AddonWrapper.prototype, prop, {
    get() {
      let theme = themeFor(this);
      return prop in theme ? theme[prop] : null;
    },
    enumarable: true,
  });
});

["installDate", "updateDate"].forEach(function(prop) {
  Object.defineProperty(AddonWrapper.prototype, prop, {
    get() {
      let theme = themeFor(this);
      return prop in theme ? new Date(theme[prop]) : null;
    },
    enumarable: true,
  });
});

/**
 * Converts the ID used by the public AddonManager API to an lightweight theme
 * ID.
 *
 * @param   id
 *          The ID to be converted
 *
 * @return  the lightweight theme ID or null if the ID was not for a lightweight
 *          theme.
 */
function _getInternalID(id) {
  if (!id)
    return null;
  let len = id.length - ID_SUFFIX.length;
  if (len > 0 && id.substring(len) == ID_SUFFIX)
    return id.substring(0, len);
  return null;
}

function _setCurrentTheme(aData, aLocal) {
  aData = _sanitizeTheme(aData, null, aLocal);

  let needsRestart = (ADDON_TYPE == "theme") &&
                     Services.prefs.prefHasUserValue(PREF_GENERAL_SKINS_SELECTEDSKIN);

  let cancel = Cc["@mozilla.org/supports-PRBool;1"].createInstance(Ci.nsISupportsPRBool);
  cancel.data = false;
  Services.obs.notifyObservers(cancel, "lightweight-theme-change-requested",
                               JSON.stringify(aData));

  if (aData) {
    let theme = LightweightThemeManager.getUsedTheme(aData.id);
    let isInstall = !theme || theme.version != aData.version;
    if (isInstall) {
      aData.updateDate = Date.now();
      if (theme && "installDate" in theme)
        aData.installDate = theme.installDate;
      else
        aData.installDate = aData.updateDate;

      var oldWrapper = theme ? new AddonWrapper(theme) : null;
      var wrapper = new AddonWrapper(aData);
      AddonManagerPrivate.callInstallListeners("onExternalInstall", null,
                                               wrapper, oldWrapper, false);
      AddonManagerPrivate.callAddonListeners("onInstalling", wrapper, false);
    }

    let current = LightweightThemeManager.currentTheme;
    let usedThemes = _usedThemesExceptId(aData.id);
    if (current && current.id != aData.id)
      usedThemes.splice(1, 0, aData);
    else
      usedThemes.unshift(aData);
    _updateUsedThemes(usedThemes);

    if (isInstall)
      AddonManagerPrivate.callAddonListeners("onInstalled", wrapper);
  }

  if (cancel.data)
    return null;

  AddonManagerPrivate.notifyAddonChanged(aData ? aData.id + ID_SUFFIX : null,
                                         ADDON_TYPE, needsRestart);

  return LightweightThemeManager.currentTheme;
}

function _sanitizeTheme(aData, aBaseURI, aLocal) {
  if (!aData || typeof aData != "object")
    return null;

  var resourceProtocols = ["http", "https", "resource"];
  if (aLocal)
    resourceProtocols.push("file");
  var resourceProtocolExp = new RegExp("^(" + resourceProtocols.join("|") + "):");

  function sanitizeProperty(prop) {
    if (!(prop in aData))
      return null;
    if (typeof aData[prop] != "string")
      return null;
    let val = aData[prop].trim();
    if (!val)
      return null;

    if (!/URL$/.test(prop))
      return val;

    try {
      val = _makeURI(val, aBaseURI ? _makeURI(aBaseURI) : null).spec;
      if ((prop == "updateURL" ? /^https:/ : resourceProtocolExp).test(val))
        return val;
      return null;
    } catch (e) {
      return null;
    }
  }

  let result = {};
  for (let mandatoryProperty of MANDATORY) {
    let val = sanitizeProperty(mandatoryProperty);
    if (!val)
      throw Components.results.NS_ERROR_INVALID_ARG;
    result[mandatoryProperty] = val;
  }

  for (let optionalProperty of OPTIONAL) {
    let val = sanitizeProperty(optionalProperty);
    if (!val)
      continue;
    result[optionalProperty] = val;
  }

  return result;
}

function _usedThemesExceptId(aId) {
  return LightweightThemeManager.usedThemes.filter(function(t) {
      return "id" in t && t.id != aId;
    });
}

function _version(aThemeData) {
  return aThemeData.version || "";
}

function _makeURI(aURL, aBaseURI) {
  return Services.io.newURI(aURL, null, aBaseURI);
}

function _updateUsedThemes(aList) {
  // Remove app-specific themes before saving them to the usedThemes pref.
  aList = aList.filter(theme => !LightweightThemeManager._builtInThemes.has(theme.id));

  // Send uninstall events for all themes that need to be removed.
  while (aList.length > _maxUsedThemes) {
    let wrapper = new AddonWrapper(aList[aList.length - 1]);
    AddonManagerPrivate.callAddonListeners("onUninstalling", wrapper, false);
    aList.pop();
    AddonManagerPrivate.callAddonListeners("onUninstalled", wrapper);
  }

  var str = Cc["@mozilla.org/supports-string;1"]
              .createInstance(Ci.nsISupportsString);
  str.data = JSON.stringify(aList);
  _prefs.setComplexValue("usedThemes", Ci.nsISupportsString, str);

  Services.obs.notifyObservers(null, "lightweight-theme-list-changed", null);
}

function _notifyWindows(aThemeData) {
  Services.obs.notifyObservers(null, "lightweight-theme-styling-update",
                               JSON.stringify(aThemeData));
}

var _previewTimer;
var _previewTimerCallback = {
  notify() {
    LightweightThemeManager.resetPreview();
  }
};

/**
 * Called when any of the lightweightThemes preferences are changed.
 */
function _prefObserver(aSubject, aTopic, aData) {
  switch (aData) {
    case "maxUsedThemes":
      try {
        _maxUsedThemes = _prefs.getIntPref(aData);
      } catch (e) {
        _maxUsedThemes = DEFAULT_MAX_USED_THEMES_COUNT;
      }
      // Update the theme list to remove any themes over the number we keep
      _updateUsedThemes(LightweightThemeManager.usedThemes);
      break;
  }
}

function _persistImages(aData, aCallback) {
  function onSuccess(key) {
    return function() {
      let current = LightweightThemeManager.currentTheme;
      if (current && current.id == aData.id) {
        _prefs.setBoolPref("persisted." + key, true);
      }
      if (--numFilesToPersist == 0 && aCallback) {
        aCallback();
      }
    };
  }

  let numFilesToPersist = 0;
  for (let key in PERSIST_FILES) {
    _prefs.setBoolPref("persisted." + key, false);
    if (aData[key]) {
      numFilesToPersist++;
      _persistImage(aData[key], PERSIST_FILES[key], onSuccess(key));
    }
  }
}

function _getLocalImageURI(localFileName) {
  var localFile = Services.dirsvc.get("ProfD", Ci.nsIFile);
  localFile.append(localFileName);
  return Services.io.newFileURI(localFile);
}

function _persistImage(sourceURL, localFileName, successCallback) {
  if (/^(file|resource):/.test(sourceURL))
    return;

  var targetURI = _getLocalImageURI(localFileName);
  var sourceURI = _makeURI(sourceURL);

  var persist = Cc["@mozilla.org/embedding/browser/nsWebBrowserPersist;1"]
                  .createInstance(Ci.nsIWebBrowserPersist);

  persist.persistFlags =
    Ci.nsIWebBrowserPersist.PERSIST_FLAGS_REPLACE_EXISTING_FILES |
    Ci.nsIWebBrowserPersist.PERSIST_FLAGS_AUTODETECT_APPLY_CONVERSION |
    (PERSIST_BYPASS_CACHE ?
       Ci.nsIWebBrowserPersist.PERSIST_FLAGS_BYPASS_CACHE :
       Ci.nsIWebBrowserPersist.PERSIST_FLAGS_FROM_CACHE);

  persist.progressListener = new _persistProgressListener(successCallback);

  persist.saveURI(sourceURI, null,
                  null, Ci.nsIHttpChannel.REFERRER_POLICY_UNSET,
                  null, null, targetURI, null);
}

function _persistProgressListener(successCallback) {
  this.onLocationChange = function() {};
  this.onProgressChange = function() {};
  this.onStatusChange   = function() {};
  this.onSecurityChange = function() {};
  this.onStateChange    = function(aWebProgress, aRequest, aStateFlags, aStatus) {
    if (aRequest &&
        aStateFlags & Ci.nsIWebProgressListener.STATE_IS_NETWORK &&
        aStateFlags & Ci.nsIWebProgressListener.STATE_STOP) {
      try {
        if (aRequest.QueryInterface(Ci.nsIHttpChannel).requestSucceeded) {
          // success
          successCallback();
          return;
        }
      } catch (e) { }
      // failure
    }
  };
}

AddonManagerPrivate.registerProvider(LightweightThemeManager, [
  new AddonManagerPrivate.AddonType("theme", URI_EXTENSION_STRINGS,
                                    STRING_TYPE_NAME,
                                    AddonManager.VIEW_TYPE_LIST, 5000)
]);

```

## tests/tests/files/Cargo.toml
```
[package]
name = "files"
version = "0.1.0"
authors = ["Emilio Cobos Ãlvarez <emilio@crisal.io>"]
build = "build.rs"
edition = "2018"

[lib]
name = "simple"
path = "simple.rs"
doctest = false

[dependencies]
test_rust_dependency = { path = "./test_rust_dependency" }

```

## tests/tests/files/bug1449291.cpp
```
#define NS_GENERIC_FACTORY_CONSTRUCTOR_INIT(_InstanceClass, _InitMethod) \
  static _InstanceClass* _InstanceClass##Constructor() {                 \
    _InstanceClass* inst = new _InstanceClass();                         \
    inst->_InitMethod();                                                 \
    return inst;                                                         \
  }

class NullPrincipal {
 public:
  void Init() {}
};

NS_GENERIC_FACTORY_CONSTRUCTOR_INIT(NullPrincipal, Init)

int main() {
  NullPrincipal* p = NullPrincipalConstructor();
  delete p;
  return 0;
}

```

## tests/tests/files/tricky_symbol_names.cpp
```
#include <cstddef>
#include <cstdio>

constexpr auto operator""_argggh(const char* aStr, std::size_t aLen) {
  return aStr;
}

// This can't be named main() or we'll mess up an existing graph test, whoops.
int use_the_argggh_operator() {
  const char* blah = "blah"_argggh;
  printf("%s\n", blah);

  return 0;
}

```

## tests/tests/files/bug1446220_unicode.html
```
<!DOCTYPE HTML>
<html>
<head>
  <meta charset=utf-8>
  <title>Font family name parsing tests</title>
  <link rel="author" title="John Daggett" href="mailto:jdaggett@mozilla.com">
  <link rel="help" href="http://www.w3.org/TR/css3-fonts/#font-family-prop" />
  <link rel="help" href="http://www.w3.org/TR/css3-fonts/#font-prop" />
  <meta name="assert" content="tests that valid font family names parse and invalid ones don't" />
  <script type="text/javascript" src="/resources/testharness.js"></script>
  <script type="text/javascript" src="/resources/testharnessreport.js"></script>
  <style type="text/css">
  </style>
</head>
<body>
<div id="log"></div>
<pre id="display"></pre>
<style type="text/css" id="testbox"></style>

<script type="text/javascript">

function fontProp(n, size, s1, s2) { return (s1 ? s1 + " " : "") + (s2 ? s2 + " " : "") + size + " " + n; }
function font(n, size, s1, s2) { return "font: " + fontProp(n, size, s1, s2); }

// testrules
//   namelist - font family list
//   invalid  - true if declarations won't parse in either font-family or font
//   fontonly - only test with the 'font' property
//   single   - namelist includes only a single name (@font-face rules only allow a single name)

var testFontFamilyLists = [

  /* basic syntax */
  { namelist: "simple", single: true },
  { namelist: "'simple'", single: true },
  { namelist: '"simple"', single: true },
  { namelist: "-simple", single: true },
  { namelist: "_simple", single: true },
  { namelist: "quite simple", single: true },
  { namelist: "quite _simple", single: true },
  { namelist: "quite -simple", single: true },
  { namelist: "0simple", invalid: true, single: true },
  { namelist: "simple!", invalid: true, single: true },
  { namelist: "simple()", invalid: true, single: true },
  { namelist: "quite@simple", invalid: true, single: true },
  { namelist: "#simple", invalid: true, single: true },
  { namelist: "quite 0simple", invalid: true, single: true },
  { namelist: "ç´è±†å«Œã„", single: true },
  { namelist: "ç´è±†å«Œã„, ick, patooey" },
  { namelist: "ick, patooey, ç´è±†å«Œã„" },
  { namelist: "ç´è±†å«Œã„, ç´è±†å¤§å«Œã„" },
  { namelist: "ç´è±†å«Œã„, ç´è±†å¤§å«Œã„, ç´è±†æœ¬å½“ã«å«Œã„" },
  { namelist: "ç´è±†å«Œã„, ç´è±†å¤§å«Œã„, ç´è±†æœ¬å½“ã«å«Œã„, ç´è±†ã¯å¥½ã¿ã§ã¯ãªã„" },
  { namelist: "arial, helvetica, sans-serif" },
  { namelist: "arial, helvetica, 'times' new roman, sans-serif", invalid: true },
  { namelist: "arial, helvetica, \"times\" new roman, sans-serif", invalid: true },

  { namelist: "arial, helvetica, \"\\\"times new roman\", sans-serif" },
  { namelist: "arial, helvetica, '\\\"times new roman', sans-serif" },
  { namelist: "arial, helvetica, times 'new' roman, sans-serif", invalid: true },
  { namelist: "arial, helvetica, times \"new\" roman, sans-serif", invalid: true },
  { namelist: "\"simple", single: true },
  { namelist: "\\\"simple", single: true },
  { namelist: "\"\\\"simple\"", single: true },
  { namelist: "Ä°simple", single: true },
  { namelist: "ÃŸsimple", single: true },
  { namelist: "áº™simple", single: true },

  /* escapes */
  { namelist: "\\s imple", single: true },
  { namelist: "\\073 imple", single: true },

  { namelist: "\\035 simple", single: true },
  { namelist: "sim\\035 ple", single: true },
  { namelist: "simple\\02cinitial", single: true },
  { namelist: "simple, \\02cinitial" },
  { namelist: "sim\\020 \\035 ple", single: true },
  { namelist: "sim\\020 5ple", single: true },
  { namelist: "\\@simple", single: true },
  { namelist: "\\@simple\\;", single: true },
  { namelist: "\\@font-face", single: true },
  { namelist: "\\@font-face\\;", single: true },
  { namelist: "\\031 \\036 px", single: true },
  { namelist: "\\031 \\036 px", single: true },
  { namelist: "\\1f4a9", single: true },
  { namelist: "\\01f4a9", single: true },
  { namelist: "\\0001f4a9", single: true },
  { namelist: "\\AbAb", single: true },

  /* keywords */
  { namelist: "italic", single: true },
  { namelist: "bold", single: true },
  { namelist: "bold italic", single: true },
  { namelist: "italic bold", single: true },
  { namelist: "larger", single: true },
  { namelist: "smaller", single: true },
  { namelist: "bolder", single: true },
  { namelist: "lighter", single: true },
  { namelist: "default", invalid: true, fontonly: true, single: true },
  { namelist: "initial", invalid: true, fontonly: true, single: true },
  { namelist: "inherit", invalid: true, fontonly: true, single: true },
  { namelist: "normal", single: true },
  { namelist: "default, simple", invalid: true },
  { namelist: "initial, simple", invalid: true },
  { namelist: "inherit, simple", invalid: true },
  { namelist: "normal, simple" },
  { namelist: "simple, default", invalid: true },
  { namelist: "simple, initial", invalid: true },
  { namelist: "simple, inherit", invalid: true },
  { namelist: "simple, default bongo" },
  { namelist: "simple, initial bongo" },
  { namelist: "simple, inherit bongo" },
  { namelist: "simple, bongo default" },
  { namelist: "simple, bongo initial" },
  { namelist: "simple, bongo inherit" },
  { namelist: "simple, normal" },
  { namelist: "simple default", single: true },
  { namelist: "simple initial", single: true },
  { namelist: "simple inherit", single: true },
  { namelist: "simple normal", single: true },
  { namelist: "default simple", single: true },
  { namelist: "initial simple", single: true },
  { namelist: "inherit simple", single: true },
  { namelist: "normal simple", single: true },
  { namelist: "caption", single: true }, // these are keywords for the 'font' property but only when in the first position
  { namelist: "icon", single: true },
  { namelist: "menu", single: true }

];

if (window.SpecialPowers && SpecialPowers.getBoolPref("layout.css.unset-value.enabled")) {
  testFontFamilyLists.push(
    { namelist: "unset", invalid: true, fontonly: true, single: true },
    { namelist: "unset, simple", invalid: true },
    { namelist: "simple, unset", invalid: true },
    { namelist: "simple, unset bongo" },
    { namelist: "simple, bongo unset" },
    { namelist: "simple unset", single: true },
    { namelist: "unset simple", single: true });
}

var gTest = 0;

/* strip out just values */
function extractDecl(rule)
{
  var t = rule.replace(/[ \n]+/g, " ");
  t = t.replace(/.*{[ \n]*/, "");
  t = t.replace(/[ \n]*}.*/, "");
  return t;
}


function testStyleRuleParse(decl, invalid) {
  var sheet = document.styleSheets[1];
  var rule = ".test" + gTest++ + " { " + decl + "; }";

  while(sheet.cssRules.length > 0) {
    sheet.deleteRule(0);
  }

  // shouldn't throw but improper handling of punctuation may cause some parsers to throw
  try {
    sheet.insertRule(rule, 0);
  } catch (e) {
    assert_unreached("unexpected error with " + decl + " ==> " + e.name);
  }

  assert_equals(sheet.cssRules.length, 1,
    "strange number of rules (" + sheet.cssRules.length + ") with " + decl);

  var s = extractDecl(sheet.cssRules[0].cssText);

  if (invalid) {
    assert_equals(s, "", "rule declaration shouldn't parse - " + rule + " ==> " + s);
  } else {
    assert_not_equals(s, "", "rule declaration should parse - " + rule);

    // check that the serialization also parses
    var r = ".test" + gTest++ + " { " + s + " }";
    while(sheet.cssRules.length > 0) {
      sheet.deleteRule(0);
    }
    try {
      sheet.insertRule(r, 0);
    } catch (e) {
      assert_unreached("exception occurred parsing serialized form of rule - " + rule + " ==> " + r + " " + e.name);
    }
    var s2 = extractDecl(sheet.cssRules[0].cssText);
    assert_not_equals(s2, "", "serialized form of rule should also parse - " + rule + " ==> " + r);
  }
}

var kDefaultFamilySetting = "onelittlepiggywenttomarket";

function testFontFamilySetterParse(namelist, invalid) {
  var el = document.getElementById("display");

  el.style.fontFamily = kDefaultFamilySetting;
  var def = el.style.fontFamily;
  el.style.fontFamily = namelist;
  if (!invalid) {
    assert_not_equals(el.style.fontFamily, def, "fontFamily setter should parse - " + namelist);
    var parsed = el.style.fontFamily;
    el.style.fontFamily = kDefaultFamilySetting;
    el.style.fontFamily = parsed;
    assert_equals(el.style.fontFamily, parsed, "fontFamily setter should parse serialized form to identical serialization - " + parsed + " ==> " + el.style.fontFamily);
  } else {
    assert_equals(el.style.fontFamily, def, "fontFamily setter shouldn't parse - " + namelist);
  }
}

var kDefaultFontSetting = "16px onelittlepiggywenttomarket";

function testFontSetterParse(n, size, s1, s2, invalid) {
  var el = document.getElementById("display");

  el.style.font = kDefaultFontSetting;
  var def = el.style.font;
  var fp = fontProp(n, size, s1, s2);
  el.style.font = fp;
  if (!invalid) {
    assert_not_equals(el.style.font, def, "font setter should parse - " + fp);
    var parsed = el.style.font;
    el.style.font = kDefaultFontSetting;
    el.style.font = parsed;
    assert_equals(el.style.font, parsed, "font setter should parse serialized form to identical serialization - " + parsed + " ==> " + el.style.font);
  } else {
    assert_equals(el.style.font, def, "font setter shouldn't parse - " + fp);
  }
}

var testFontVariations = [
  { size: "16px" },
  { size: "900px" },
  { size: "900em" },
  { size: "35%" },
  { size: "7832.3%" },
  { size: "xx-large" },
  { size: "larger", s1: "lighter" },
  { size: "16px", s1: "italic" },
  { size: "16px", s1: "italic", s2: "bold" },
  { size: "smaller", s1: "normal" },
  { size: "16px", s1: "normal", s2: "normal" },
  { size: "16px", s1: "400", s2: "normal" },
  { size: "16px", s1: "bolder", s2: "oblique" }
];

function testFamilyNameParsing() {
  var i;
  for (i = 0; i < testFontFamilyLists.length; i++) {
    var tst = testFontFamilyLists[i];
    var n = tst.namelist;
    var t;

    if (!tst.fontonly) {
      t = "font-family: " + n;
      test(function() { testStyleRuleParse(t, tst.invalid); }, t);
      test(function() { testFontFamilySetterParse(n, tst.invalid); }, t + " (setter)");
    }

    var v;
    for (v = 0; v < testFontVariations.length; v++) {
      var f = testFontVariations[v];
      t = font(n, f.size, f.s1, f.s2);
      test(function() { testStyleRuleParse(t, tst.invalid); }, t);
      test(function() { testFontSetterParse(n, f.size, f.s1, f.s2, tst.invalid); }, t + " (setter)");
    }
  }
}

testFamilyNameParsing();

</script>
</body>
</html>

```

## tests/tests/files/secret-madjewel.js
```
// This file is secretly a mod-ule but with a name that is intended to defeat
// any filename heuristics we might introduce.

import { moduleConst, moduleFunc, moduleClass } from "./imported-module.mjs";
import { default as aliasedDefault } from "./imported-module.mjs";
import * as importedModule from "./imported-module.mjs";

const secretMadjewelConst = 11;

```

## tests/tests/files/atom_list.h
```
// This file is attempting to imitate nsGkAtomList.h and is intended to be
// included by `atom_magic.h`

YO_ATOM(Foo, "foo")
YO_ATOM(Bar, "bar")

#define NESTED_YO_ATOM(A, B) YO_ATOM(A, B)

NESTED_YO_ATOM(Baz, "baz")

```

## tests/tests/files/test_many_manifest_permutations.js
```
// I am pretending to be a JS test.  It's fun.
// I'm not a real test though.

function test() {
  // This is a real good test!
}

```

## tests/tests/files/gzip-colliding-file
```
I have a file that's me but gzipped which helps test that we don't explode when
this happens, which it can happen for tests.

```

## tests/tests/files/test_rust_dependency/Cargo.toml
```
[package]
name = "test_rust_dependency"
version = "0.1.0"
authors = ["vagrant"]

[dependencies]

```

## tests/tests/files/implicit.cpp
```
class Base {
 public:
  Base() {}

  explicit Base(int foo) {}
};

class Derived : public Base {
 public:
  Derived() : Base() {}
};

class Implicit : public Base {
 public:
  Implicit() {}
};

```

## tests/tests/files/some_ini.ini
```
[section]
  testing = bug 1451742

```

## tests/tests/files/templates2.cpp
```
class Foo {
 public:
  void Method();
};

template <class T>
class Bar {
 public:
  void Function1();

  void Function2() { field->Method(); }

 private:
  T* field;
};

template <class T>
inline void Bar<T>::Function1() {
  return field->Method();
}

template class Bar<Foo>;

class Baz {
 public:
  void Method();
};

int main() {
  Foo* f;
  Bar<Baz>* b;
  b->Function1();
  b->Function2();
  return 0;
}

```

## tests/tests/files/templates_nsTArray.cpp
```
#include "nsTArray.h"

struct ServoAttrSnapshot {};

class ServoElementSnapshot {
  nsTArray<ServoAttrSnapshot> mAttrs;
};

```

## tests/tests/files/subdir/header@with,many^strange~chars.h
```
// This is just a file that gets #included, but
// has lots of strange (non-alphanumeric) in the
// filename. This ensures the file name mangling
// code in MozsearchIndexer.cpp produces good
// results even with pathological filenames.

```

## tests/tests/files/nsISupports.h
```
// This file is just a stub to let xpctest_params.h compile!
//
// As it's the first file it includes and is the most obvious places to look for
// dummying definitions, that's where we put them.
//
// No attempt is currently made to actually resemble the real nsISupports
// infrastructure!

#ifndef nsISupports_h__
#define nsISupports_h__

#include <stdint.h>

#define NS_NO_VTABLE
#define NS_DECLARE_STATIC_IID_ACCESSOR(foo)
#define JS_HAZ_CAN_RUN_SCRIPT
#define NS_IMETHOD virtual uint32_t
#define NS_DEFINE_STATIC_IID_ACCESSOR(foo, bar)

class nsISupports {};

class nsAString;
class nsString;

class nsACString;
class nsCString;

class nsIID;

template <class E>
class RefPtr {};

struct PRTime;

#endif  // nsISupports_h__

```

## tests/tests/files/README.md
```
This is a README
================

Warning
-------
This should not be rendered by searchfox.
Instead we should get the normal blame view like regular text files do.

Adding some fodder for text search:
- This software is never known as searchfoox.
- This software is also never known as searchfoox.
- But maybe it could be known as searchfx!

```

## tests/tests/files/nsTArray.h
```
// This file is a combination of a stub to let xpctest_params.h compile and a
// recognition that we already had `templates_nsTArray.cpp` that defined an
// nsTArray and so to avoid symbol-space collisions, that needed to be
// abstracted up into here.

#ifndef nsTArray_h__
#define nsTArray_h__

template <class T>
class Span {
 private:
  T* mRawPtr;
};

template <class E>
class nsTArray {
  template <class Item>
  E* AppendElements(const Item* aArray, unsigned aArrayLen) {
    return nullptr;
  }

  template <class Item>
  E* AppendElements(Span<const Item> aSpan) {
    return AppendElements<Item>(aSpan.Elements(), aSpan.Length());
  }
};

#endif  // nsTArray_h__

```

## tests/tests/files/field-layout/field-type.h
```
class JSObject {};

namespace JS {

class TestAllocPolicy {};

template <typename T>
class Rooted {};

template <typename T, size_t MinInlineCapacity = 0,
          typename AllocPolicy = TestAllocPolicy>
class GCVector {};

template <typename T, typename AllocPolicy = TestAllocPolicy>
class StackGCVector : public GCVector<T, 8, AllocPolicy> {};

template <typename T>
class RootedVector : public Rooted<StackGCVector<T>> {};

class alignas(8) Value {
 private:
  uint64_t asBits_;
};

template <typename Key, typename Value>
class GCHashMap {};

}  // namespace JS

namespace js {

class TestCheck {};

template <typename Check, typename T>
class ProtectedDataNoCheckArgs {};

template <typename T>
using TestLockData = ProtectedDataNoCheckArgs<TestCheck, T>;

}  // namespace js

```

## tests/tests/files/field-layout/empty.cpp
```
#include <stdint.h>

namespace field_layout {

namespace empty {

struct S {};

S f() {
  S s;
  return s;
}

}  // namespace empty

}  // namespace field_layout

```

## tests/tests/files/field-layout/field-type-include.h
```
Type1 other_file_field1;

Container1<Type1> other_file_field2;

std::vector<Type1> other_file_field3;

```

## tests/tests/files/field-layout/empty-subclass.cpp
```
#include <stdint.h>

namespace field_layout {

namespace empty_subclass {

struct S {
  uint32_t x;
};

struct T : public S {};

T f() {
  T t;
  return t;
}

}  // namespace empty_subclass

}  // namespace field_layout

```

## tests/tests/files/field-layout/platform_specific_field.cpp
```
#include <stdint.h>

namespace field_layout {

namespace platform_specific_field {

struct S1 {
  uint32_t f1;

#if defined(TARGET_linux64) || defined(TARGET_macosx64)
  uint32_t f2;
#endif

#ifdef TARGET_win64
  uint8_t f3;
#endif
};

struct S2 : public S1 {
  uint32_t f4;

#ifdef TARGET_linux64
  uint8_t f5;
#endif
};

struct S3 : public S2 {
  uint8_t f6;
};

struct T1 {
  uint8_t f1;
};

struct T2 : public T1 {
  uint32_t f2;

#ifdef TARGET_macosx64
  uint8_t f3;
#else
  uint8_t f3b;
#endif
};

struct T3 : public T2 {
  uint32_t f4;

#if defined(TARGET_linux64) || defined(TARGET_macosx64)
  uint32_t f5;
#endif

#ifdef TARGET_win64
  uint8_t f6;
#endif
};

S3 f() {
  S3 s;
  T3 t;
  return s;
}

}  // namespace platform_specific_field

}  // namespace field_layout

```

## tests/tests/files/field-layout/platform_specific_size.cpp
```
#include <stdint.h>

namespace field_layout {

namespace platform_specific_size {

#ifdef TARGET_linux64
using T1 = uint32_t;
#endif

#ifdef TARGET_macosx64
using T1 = uint32_t;
#endif

#ifdef TARGET_win64
using T1 = uint64_t;
#endif

struct S {
  T1 f1;
};

S f() {
  S s;
  return s;
}

}  // namespace platform_specific_size

}  // namespace field_layout

```

## tests/tests/files/docs/rst_test/index.rst
```
reST Test
=========

This is a test document with reST.

```

## tests/tests/files/field-layout/non-struct.cpp
```
#include <stdint.h>

namespace field_layout {

namespace non_struct {

void proxy() {}

class Proxy {
  int8_t x;
};

}  // namespace non_struct

}  // namespace field_layout

```

## tests/tests/files/field-layout/bitfields.cpp
```
#include <stdint.h>

namespace field_layout {

namespace bitfields {

struct S {
  uint32_t b1 : 1;
  uint32_t b2 : 3;
  uint32_t b3 : 7;
  uint32_t b4 : 4;
  uint8_t b5 : 3;
  uint16_t b6 : 2;
};

S f() {
  S s;
  return s;
}

}  // namespace bitfields

}  // namespace field_layout

```

## tests/tests/files/field-layout/field-type.cpp
```
#include <vector>
#include <stddef.h>
#include <stdint.h>
#include "field-type.h"

namespace field_layout {

namespace field_type {

struct Type1 {
  uint8_t a;
};

template <typename T>
struct Container1 {
  T a;
};

enum class Enum1 : uint8_t {
  No,
  Yes,
};

template <typename T, Enum1 e>
struct Container2 {
  T a;
};

#define DEFINE_FIELDS   \
  Type1 macro_fields_1; \
  Enum1 macro_fields_2; \
  Container1<Type1> macro_fields_3;

#define SOME_ANNOTATION __attribute__((annotate("some_annotation")))

struct S {
  Type1 value_field;
  const Type1* pointer_field;
  Container1<Type1> template_field_1;
  Container2<Type1, Enum1::No> template_field_2;
  std::vector<Type1> vector_field;
  JS::Rooted<JS::Value> rooted_field;
  JS::RootedVector<JS::Value> rooted_vec_field;
  JS::GCHashMap<JS::Value, JSObject*> hash_map_field;
  js::TestLockData<JS::Value> protected_field;
  DEFINE_FIELDS
  js::TestLockData<Type1> multiline_field SOME_ANNOTATION;
#include "field-type-include.h"
};

S f() {
  S s;
  return s;
}

}  // namespace field_type

}  // namespace field_layout

```

## tests/tests/files/field-layout/holes.cpp
```
#include <stdint.h>

namespace field_layout {

namespace holes {

struct Base {
  uint8_t a;
  uint16_t b;
  uint32_t c;
  char d;
};

struct Sub : public Base {
  uint8_t x;
  uint32_t y;
};

Sub f(int32_t n) {
  Sub s;
  s.y = n;

  return s;
}

}  // namespace holes

}  // namespace field_layout

```

## tests/tests/files/field-layout/multiple_inheritance.cpp
```
#include <stdint.h>

namespace field_layout {

namespace multiple_inheritance {

struct BaseEmpty {};

struct SubA : public BaseEmpty {
  int32_t sub_a_1;
  int16_t sub_a_2;
};

struct SubB : public BaseEmpty {
  int32_t sub_b_1;
  int8_t sub_b_2;
#ifdef TARGET_win64
  int64_t sub_b_3;
#endif
};

struct SubC : public BaseEmpty {
  int8_t sub_c_1;
  int32_t sub_c_2;
};

struct SubD : public SubC {
  int32_t sub_d_1;
  int8_t sub_d_2;
};

struct SubE : public BaseEmpty {
  int64_t sub_e_1;
  int32_t sub_e_2;
};

struct SubSubA : public SubA, public SubB, public SubD {
  int32_t sub_sub_c_1;
};

struct SubSubB : public SubE {
  int32_t sub_sub_b_1;
};

struct SubSubSubA : public SubSubA, public SubSubB {
  int32_t sub_sub_sub_a_1;
};

SubSubSubA f() {
  SubSubSubA s;
  return s;
}

}  // namespace multiple_inheritance

}  // namespace field_layout

```

## tests/tests/files/field-layout/vtable.cpp
```
#include <stdint.h>

namespace field_layout {

namespace vtable {

class Base1 {
 public:
  virtual ~Base1() {}
};

class Base2 {
 public:
  virtual ~Base2() {}
};

class Base3 {
 public:
};

class Sub1a : public Base1 {
 public:
  virtual ~Sub1a() {}
  uint8_t x;
};

class Sub1b : public Base1 {
 public:
  virtual ~Sub1b() {}
  uint8_t y;
};

class Sub2 : public Base2 {
 public:
  virtual ~Sub2() {}
  uint8_t w;
};

class Sub3 : public Base3 {
 public:
  virtual ~Sub3() {}
  uint8_t v;
};

class SubSub : public Sub1a, public Sub1b, public Sub2, public Sub3 {
 public:
  virtual ~SubSub() {}
  uint8_t z;
};

SubSub f(int8_t n) {
  SubSub s;
  s.x = n;

  return s;
}

}  // namespace vtable

}  // namespace field_layout

```

## tests/tests/files/mozilla/_ipdltest/TestBasicParent.h
```
/* -*- Mode: C++; tab-width: 8; indent-tabs-mode: nil; c-basic-offset: 2 -*- */
/* vim: set ts=8 sts=2 et sw=2 tw=80: */
/* This Source Code Form is subject to the terms of the Mozilla Public
 * License, v. 2.0. If a copy of the MPL was not distributed with this file,
 * You can obtain one at http://mozilla.org/MPL/2.0/. */

#ifndef mozilla__ipdltest_TestBasicParent_h
#define mozilla__ipdltest_TestBasicParent_h

#include "mozilla/_ipdltest/PTestBasicParent.h"

namespace mozilla::_ipdltest {

class TestBasicParent : public PTestBasicParent {
  NS_INLINE_DECL_THREADSAFE_REFCOUNTING(TestBasicParent, override)

 private:
  ~TestBasicParent() = default;
};

}  // namespace mozilla::_ipdltest

#endif  // mozilla__ipdltest_TestBasicParent_h

```

## tests/tests/files/mochitest-alt-pref.ini
```
[DEFAULT]
prefs =
  some.fake.pref=5

[include:mochitest-common.ini]

```

## tests/tests/files/mozilla/_ipdltest/TestBasicChild.h
```
/* -*- Mode: C++; tab-width: 8; indent-tabs-mode: nil; c-basic-offset: 2 -*- */
/* vim: set ts=8 sts=2 et sw=2 tw=80: */
/* This Source Code Form is subject to the terms of the Mozilla Public
 * License, v. 2.0. If a copy of the MPL was not distributed with this file,
 * You can obtain one at http://mozilla.org/MPL/2.0/. */

#ifndef mozilla__ipdltest_TestBasicChild_h
#define mozilla__ipdltest_TestBasicChild_h

#include "mozilla/_ipdltest/PTestBasicChild.h"

namespace mozilla::_ipdltest {

class TestBasicChild : public PTestBasicChild {
  NS_INLINE_DECL_THREADSAFE_REFCOUNTING(TestBasicChild, override)

 public:
  mozilla::ipc::IPCResult RecvHello();

 private:
  ~TestBasicChild() = default;
};

}  // namespace mozilla::_ipdltest

#endif  // mozilla__ipdltest_TestBasicChild_h

```

## tests/tests/files/docs/md_test/index.md
```
# Markdown Test

This is a test document with Markdown.

```

## tests/tests/files/mochitest.ini
```
[DEFAULT]
skip-if = toolkit == 'android'

[include:mochitest-common.ini]

[test_custom_element_base.xul]
skip-if =
  verify
  apple_silicon

```

## tests/tests/files/ForwardingTemplates.cpp
```
#include <memory>
#include <vector>

struct StructUsedInTypeDependentNew0 {
  StructUsedInTypeDependentNew0() {}
};

struct StructUsedInTypeDependentNew1 {
  StructUsedInTypeDependentNew1() {}
};

struct StructUsedInTypeIndependentNew {
  StructUsedInTypeIndependentNew() {}
};

template <typename T, typename... Args>
std::unique_ptr<T> MakeUniqueWithIndex(int i, Args&&... args) {
  const auto _ = std::unique_ptr<StructUsedInTypeIndependentNew>{
      new StructUsedInTypeIndependentNew()};
  return std::unique_ptr<T>{new T(std::forward<Args>(args)...)};
}

template <typename T, typename... Args>
std::unique_ptr<T> MakeUnique(Args&&... args) {
  const auto _ = std::unique_ptr<StructUsedInTypeIndependentNew>{
      new StructUsedInTypeIndependentNew()};
  return MakeUniqueWithIndex<T>(0, std::forward<Args>(args)...);
}

template <typename T, typename... Args>
std::unique_ptr<T> RecursiveMakeUnique(Args&&... args) {
  const auto _ = RecursiveMakeUnique<T>(std::forward<Args>(args)...);
  return MakeUnique<T>(std::forward<Args>(args)...);
}

template <typename T, typename... Args>
std::unique_ptr<T> MakeUniqueWithLambda(Args&&... args) {
  return std::unique_ptr<T>{[t = new T()] { return t; }()};
}

void test() {
  const auto a = MakeUniqueWithIndex<StructUsedInTypeDependentNew0>(0);
  const auto b = MakeUniqueWithIndex<StructUsedInTypeDependentNew0>(0);
  const auto c = MakeUniqueWithIndex<StructUsedInTypeDependentNew1>(0);
  const auto d = MakeUniqueWithIndex<StructUsedInTypeDependentNew1>(0);
  const auto e = MakeUnique<StructUsedInTypeDependentNew0>();
  const auto f = MakeUnique<StructUsedInTypeDependentNew0>();
  const auto g = MakeUnique<StructUsedInTypeDependentNew1>();
  const auto h = MakeUnique<StructUsedInTypeDependentNew1>();
  const auto i = RecursiveMakeUnique<StructUsedInTypeDependentNew0>();
  const auto j = RecursiveMakeUnique<StructUsedInTypeDependentNew0>();
  const auto k = RecursiveMakeUnique<StructUsedInTypeDependentNew1>();
  const auto l = RecursiveMakeUnique<StructUsedInTypeDependentNew1>();
  const auto m = MakeUniqueWithLambda<StructUsedInTypeDependentNew0>();
  const auto n = MakeUniqueWithLambda<StructUsedInTypeDependentNew0>();
  const auto o = MakeUniqueWithLambda<StructUsedInTypeDependentNew1>();
  const auto p = MakeUniqueWithLambda<StructUsedInTypeDependentNew1>();

  const auto stl = std::make_unique<StructUsedInTypeDependentNew0>();
}

template <typename T>
struct Maybe;

template <typename T>
struct Maybe {
  char storage[sizeof(T)];

  template <typename... Args>
  void emplace_inline(Args&&... args) {
    new (storage) T(std::forward<Args>(args)...);
  }

  template <typename... Args>
  void emplace_out_of_line(Args&&... args);
};

template <typename T>
template <typename... Args>
void Maybe<T>::emplace_out_of_line(Args&&... args) {
  new (storage) T(std::forward<Args>(args)...);
}

struct StructUsedInEmplace {
  StructUsedInEmplace() {}
};

void use_maybe() {
  Maybe<StructUsedInEmplace> m;
  m.emplace_inline();
  m.emplace_out_of_line();

  std::vector<StructUsedInEmplace> v;
  v.emplace_back();
}

```

## tests/tests/files/src/main/java/sample/JavaLibrary.java
```
/*
 * This Java source file was generated by the Gradle 'init' task.
 */
package sample;

public class JavaLibrary {
    public boolean someLibraryMethod() {
        return true;
    }

    interface B extends A {}
    interface A {}

    interface I0 {}
    interface I1 {}
    final class C implements I0, I1 {}
}

```

## tests/tests/files/src/test/java/sample/JavaTest.java
```
/*
 * This Java source file was generated by the Gradle 'init' task.
 */
package sample;

import org.junit.jupiter.api.Test;
import static org.junit.jupiter.api.Assertions.*;

class JavaTest {
    @Test void someJavaLibraryMethodReturnsTrue() {
        JavaLibrary classUnderTest = new JavaLibrary();
        assertTrue(classUnderTest.someLibraryMethod(), "someLibraryMethod should return 'true'");
    }
    @Test void someKotlinLibraryMethodReturnsTrue() {
        KotlinLibrary classUnderTest = new KotlinLibrary();
        assertTrue(classUnderTest.someLibraryMethod(), "someLibraryMethod should return 'true'");
    }
}

```

## tests/tests/files/src/main/java/sample/Jni.java
```
package sample;

public class Jni {
    public static native void autoNativeStaticMethod();
    public native void autoNativeMethod();

    public static native void nativeStaticMethod();
    public native void nativeMethod();

    public static void javaStaticMethod() {}
    public void javaMethod() {}
    public int javaField;
    public static final int javaConst = 5;

    private void user() {
        nativeStaticMethod();
        nativeMethod();

        javaStaticMethod();
        javaMethod();
        javaField = javaConst;
    }
}

```

## tests/tests/files/src/test/kotlin/sample/KotlinTest.kt
```
/*
 * This Kotlin source file was generated by the Gradle 'init' task.
 */
package sample

import kotlin.test.Test
import kotlin.test.assertTrue

class KotlinTest {
    @Test fun someJavaLibraryMethodReturnsTrue() {
        val classUnderTest = JavaLibrary()
        assertTrue(classUnderTest.someLibraryMethod(), "someLibraryMethod should return 'true'")
    }
    @Test fun someKotlinLibraryMethodReturnsTrue() {
        val classUnderTest = KotlinLibrary()
        assertTrue(classUnderTest.someLibraryMethod(), "someLibraryMethod should return 'true'")
    }
}

```

## tests/tests/files/src/main/kotlin/sample/KotlinLibrary.kt
```
/*
 * This Kotlin source file was generated by the Gradle 'init' task.
 */
package sample

class KotlinLibrary {
    fun someLibraryMethod(): Boolean {
        return true
    }
}

```

## tests/tests/files/src/main/kotlin/sample/InlineObject.kt
```
package sample

interface Interface {
    fun someFunction()
    fun someOtherFunction(): Boolean
}

class JavaLibraryTest {
    val a = object : Interface {
        override fun someFunction() {}
        override fun someOtherFunction(): Boolean {
            return true;
        }
    }
    val b = object : Interface {
        override fun someFunction() {}
        override fun someOtherFunction() = false
    }
}

```

## tests/tests/files/html/script.html
```
<!DOCTYPE html>
<html>
  <head>
    <meta charset="utf-8">
    <title>script</title>
  </head>
  <body>
    <script>
function onDivClicked() {
}
function TestFunc() {
}
TestFunc();
    </script>
    <script type="text/javascript">
TestFunc();
    </script>

    <script type="text/something">
function DoNotIndexMe() {
}
    </script>
    <div onclick="onDivClicked()">Click Me</div>
  </body>
</html>

```

## tests/tests/files/html/invalid-entity-ref.html
```
<!DOCTYPE html>
<html>
  <head>
    <meta charset="utf-8">
    <title>Parser shouldn't fail with invalid entity references</title>
  </head>
  <body>
    &#xFFFFFF;
    &#x00FFFFFF;
    &#16777215;
    &something;
  </body>
</html>

```

## tests/tests/files/html/script.xhtml
```
<?xml version="1.0" encoding="UTF-8"?>
<!DOCTYPE html>
<html>
  <head>
    <meta charset="utf-8" />
    <title>script</title>
  </head>
  <body>
    <script>
      <![CDATA[
function onDivClicked() {
}
function TestFunc() {
}
TestFunc();
      ]]>
    </script>
    <script type="text/javascript">
      <![CDATA[
TestFunc();
      ]]>
    </script>
    <script type="text/something">
      <![CDATA[
function DoNotIndexMe() {
}
      ]]>
    </script>
    <div onclick="onDivClicked()">Click Me</div>
  </body>
</html>

```

## tests/tests/files/ignored-js/reignored-after-reinclusion.js
```
console.log("I get ignored then unignored then reignored.");

```

## tests/tests/files/ignored-js/wildcard-ignored.js
```
console.log("I am a file that is wildcard ignored.");

```

## tests/tests/files/ignored-js/explicit-ignored.js
```
console.log("I am a file that is explicitly ignored.");

```

## tests/tests/files/ignored-js/not-ignored-after-reinclusion.js
```
console.log("I am a file that would have been ignored but then is un-ignored.");

```

## tests/tests/files/ignored-js/doublestar-ignored/ignored-by-doublestar.js
```
console.log("I get ignored by a doublestar of my parent directory.");

```

## tests/tests/files/browser.ini
```
# actually, what am I doing?  A JSON file can't be a browser test.
[test_talosconfig_browser_config.json]
skip-if = (os == "win" && processor == "aarch64") || (os == "mac") || (os == "linux" && !debug)

```

## tests/tests/files/mobile/androidTest/asserts/inputs.html
```
<html>
    <head>
        <meta charset="utf-8">
        <title>Inputs</title>
    </head>
    <body>
        <div id="text">lorem</div>
        <input type="text" id="input" value="ipsum">
        <textarea id="textarea">dolor</textarea>
        <div id="contenteditable" contenteditable="true">sit</div>
        <iframe id="iframe" src="selectionAction_frame.html"></iframe>
        <iframe id="designmode" src="selectionAction_frame.html"></iframe>
    </body>
    <script>
        addEventListener("load", function() {
            document.getElementById("iframe").contentDocument.body.textContent = "amet";
            var designmode = document.getElementById("designmode");
            designmode.contentDocument.body.textContent = "consectetur";
            designmode.contentDocument.designMode = "on";
        });
    </script>
</html>

```

## tests/tests/files/webidl/iterable.webidl
```
interface TestInterfaceForIterable {
  iterable<KeyType, ValueType>;
};

```

## tests/tests/files/webidl/contextual-keyword.webidl
```
interface Test {
  any argumentNameKeyword(any async,
                          any attribute,
                          any callback,
                          any const,
                          any constructor,
                          any deleter,
                          any dictionary,
                          any enum,
                          any getter,
                          any includes,
                          any inherit,
                          any interface,
                          any iterable,
                          any maplike,
                          any mixin,
                          any namespace,
                          any partial,
                          any readonly,
                          any required,
                          any setlike,
                          any setter,
                          any static,
                          any stringifier,
                          any typedef,
                          any unrestricted);

  attribute any async;
  attribute any required;

  getter any(unsigned long arg);

  // WebIDL.py doesn't support includes as operation name.
  any _includes();
};

```

## tests/tests/files/webidl/bindings/src/BindingTestMixed2Binding.cpp
```
#include "../include/BindingTestMixed2Binding.h"
#include "./BindingTestMixed2.h"

namespace mozilla {
namespace dom {

namespace BindingTestMixed2_Binding {

static bool _constructor() { return true; }

static bool ownedMethod2(void* void_self) {
  auto* self = static_cast<mozilla::dom::BindingTestMixed2*>(void_self);
  self->OwnedMethod2();
  return true;
}

static bool get_mixinAttr(void* void_self) {
  auto* self = static_cast<mozilla::dom::BindingTestMixed2*>(void_self);
  self->GetMixinAttr();
  return true;
}

static bool set_mixinAttr(void* void_self) {
  auto* self = static_cast<mozilla::dom::BindingTestMixed2*>(void_self);
  self->SetMixinAttr();
  return true;
}

static bool mixinMethod(void* void_self) {
  auto* self = static_cast<mozilla::dom::BindingTestMixed2*>(void_self);
  self->MixinMethod();
  return true;
}

}  // namespace BindingTestMixed2_Binding

}  // namespace dom
}  // namespace mozilla

```

## tests/tests/files/webidl/bindings/src/BindingTestMixed1Binding.cpp
```
#include "../include/BindingTestMixed1Binding.h"
#include "./BindingTestMixed1.h"

namespace mozilla {
namespace dom {

namespace BindingTestMixed1_Binding {

static bool _constructor() { return true; }

static bool ownedMethod1(void* void_self) {
  auto* self = static_cast<mozilla::dom::BindingTestMixed1*>(void_self);
  self->OwnedMethod1();
  return true;
}

static bool get_mixinAttr(void* void_self) {
  auto* self = static_cast<mozilla::dom::BindingTestMixed1*>(void_self);
  self->GetMixinAttr();
  return true;
}

static bool set_mixinAttr(void* void_self) {
  auto* self = static_cast<mozilla::dom::BindingTestMixed1*>(void_self);
  self->SetMixinAttr();
  return true;
}

static bool mixinMethod(void* void_self) {
  auto* self = static_cast<mozilla::dom::BindingTestMixed1*>(void_self);
  self->MixinMethod();
  return true;
}

}  // namespace BindingTestMixed1_Binding

}  // namespace dom
}  // namespace mozilla

```

## tests/tests/files/webidl/bindings/src/BindingTestMixed2.h
```
namespace mozilla {
namespace dom {

class BindingTestMixed2 {
 public:
  void OwnedMethod2() {}

  void GetMixinAttr() {}
  void SetMixinAttr() {}

  void MixinMethod() {}
};

}  // namespace dom
}  // namespace mozilla

```

## tests/tests/files/webidl/external-interface.webidl
```
interface TestExternalInterface;

```

## tests/tests/files/webidl/bindings/src/BindingTestMixed1.h
```
namespace mozilla {
namespace dom {

class BindingTestMixed1 {
 public:
  void OwnedMethod1() {}

  void GetMixinAttr() {}
  void SetMixinAttr() {}

  void MixinMethod() {}
};

}  // namespace dom
}  // namespace mozilla

```

## tests/tests/files/webidl/dictionary.webidl
```
dictionary TestDictionary {
  MemberType1 member1;
};

```

## tests/tests/files/webidl/bindings/src/BindingTest.h
```
namespace mozilla {
namespace dom {

class BindingTest {
 public:
  void GetAttr1() {}
  void SetAttr1() {}

  void Method1() {}
};

}  // namespace dom
}  // namespace mozilla

```

## tests/tests/files/webidl/bindings/src/BindingTestBinding.cpp
```
#include "../include/BindingTestBinding.h"
#include "./BindingTest.h"

namespace mozilla {
namespace dom {

namespace BindingTest_Binding {

static bool _constructor() { return true; }

static bool get_attr1(void* void_self) {
  auto* self = static_cast<mozilla::dom::BindingTest*>(void_self);
  self->GetAttr1();
  return true;
}

static bool set_attr1(void* void_self) {
  auto* self = static_cast<mozilla::dom::BindingTest*>(void_self);
  self->SetAttr1();
  return true;
}

static bool method1(void* void_self) {
  auto* self = static_cast<mozilla::dom::BindingTest*>(void_self);
  self->Method1();
  return true;
}

}  // namespace BindingTest_Binding

}  // namespace dom
}  // namespace mozilla

```

## tests/tests/files/webidl/partial-interface.webidl
```
partial interface TestInterface {
  void PartialMethod();
};

```

## tests/tests/files/webidl/BindingTestMixin.webidl
```
interface mixin BindingTestMixin {
  const unsigned long MIXIN_CONST = 10;

  attribute any mixinAttr;

  any mixinMethod();
};

```

## tests/tests/files/webidl/const.webidl
```
interface TestInterfaceForConst {
  const ConstType1 const1 = 1;
};

```

## tests/tests/files/webidl/interface-mixin.webidl
```
interface mixin TestInterfaceMixin {
  void MixinMethod();
};

```

## tests/tests/files/webidl/maplike.webidl
```
interface TestInterfaceForMaplike {
  maplike<KeyType, ValueType>;
};

```

## tests/tests/files/webidl/overload.webidl
```
interface TestInterfaceForOverload {
  RetType1 OverloadFunc(ArgType1 arg1);
  RetType2 OverloadFunc(ArgType2_1 arg1, ArgType2_2 arg2);
  RetType3 OverloadFunc(ArgType3_1 arg1, ArgType3_2 arg2, ArgType3_3 arg3);
};

```

## tests/tests/files/webidl/asynciterable.webidl
```
interface TestInterfaceForAsyncIterable {
  async iterable<KeyType, ValueType>(optional ArgType1 arg1);
};

```

## tests/tests/files/webidl/typedef.webidl
```
typedef FromType ToType;

```

## tests/tests/files/webidl/includes.webidl
```
TestInterface includes TestInterfaceMixin;

```

## tests/tests/files/webidl/enum.webidl
```
enum TestEnum {
  "variant1",
  "variant2",
  "variant3",
};

```

## tests/tests/files/webidl/partial-dictionary.webidl
```
partial dictionary TestDictionary {
  unsigned long Member2;
};

```

## tests/tests/files/webidl/BindingTestMixed2.webidl
```
interface BindingTestMixed2 {
  constructor();

  any ownedMethod2();
};

BindingTestMixed2 includes BindingTestMixin;

```

## tests/tests/files/webidl/BindingTest.webidl
```
interface BindingTest {
  constructor();

  const unsigned long CONST_1 = 10;

  attribute any attr1;

  any method1();
};

dictionary BindingTestDict {
  unsigned long prop1;
};

enum BindingTestEnum {
  "variant1",
  "variant2",
};

```

## tests/tests/files/webidl/callback.webidl
```
callback TestCallback = RetType (ArgType1 arg1, ArgType2 arg2);

```

## tests/tests/files/webidl/namespace.webidl
```
namespace TestNamespace {
  void Func1();
};

```

## tests/tests/files/webidl/setlike.webidl
```
interface TestInterfaceForSetlike {
  setlike<ValueType>;
};

```

## tests/tests/files/webidl/method.webidl
```
interface TestInterfaceForMethod {
  RetType MethodName(ArgType1 arg1, ArgType2 arg2);
};

```

## tests/tests/files/webidl/attribute.webidl
```
interface TestInterfaceForAttribute {
  attribute AttrType1 attr1;
  readonly attribute AttrType2 attr2;
};

```

## tests/tests/files/rust/weak_keyword.rs
```
#[allow(dead_code)]
pub fn f() -> i32 {
    let macro_rules = 1;
    let union = 2;

    macro_rules + union
}

```

## tests/tests/files/webidl/BindingTestMixed1.webidl
```
interface BindingTestMixed1 {
  constructor();

  any ownedMethod1();
};

BindingTestMixed1 includes BindingTestMixin;

```

## tests/tests/files/rust/mod.rs
```
pub mod weak_keyword;

```

## tests/tests/files/webidl/super-interface.webidl
```
interface TestSubInterface : TestSuperInterface {
};

```

## tests/tests/files/webidl/super-dictionary.webidl
```
dictionary TestSubDictionary : TestSuperDictionary {
};

```

## tests/tests/files/webidl/type.webidl
```
interface TestInterfaceForType {
  void Func1(NullableType? arg1,
             sequence<ItemType1> arg2,
             record<DOMString, ValueType> arg3,
             ObservableArray<ItemType2> arg4,
             Promise<PromiseValueType> arg5,
             (UnionItemType1 or UnionItemType2 or UnionItemType3) arg5,
             unsigned long arg6);
};

```

## tests/tests/files/webidl/partial-namespace.webidl
```
partial namespace TestNamespace {
  void PartialFunc();
};

```

## tests/tests/files/webidl/bindings/include/BindingTestMixed2Binding.h
```
#include <stdint.h>

namespace mozilla {
namespace dom {

namespace BindingTestMixed2_Binding {

static const uint32_t MIXIN_CONST = 10;

}  // namespace BindingTestMixed2_Binding

}  // namespace dom
}  // namespace mozilla

```

## tests/tests/files/webidl/bindings/include/BindingTestMixed1Binding.h
```
#include <stdint.h>

namespace mozilla {
namespace dom {

namespace BindingTestMixed1_Binding {

static const uint32_t MIXIN_CONST = 10;

}  // namespace BindingTestMixed1_Binding

}  // namespace dom
}  // namespace mozilla

```

## tests/tests/files/webidl/bindings/include/BindingTestBinding.h
```
#include <stdint.h>

namespace mozilla {
namespace dom {

namespace BindingTest_Binding {

static const uint32_t CONST_1 = 10;

}  // namespace BindingTest_Binding

struct BindingTestDict {
  unsigned long mProp1;
};

enum class BindingTestEnum {
  Variant1,
  Variant2,
};

}  // namespace dom
}  // namespace mozilla

```

## tests/tests/files/big_header.h
```
// I am a header file.

#ifndef big_header_h__
#define big_header_h__

extern "C" void i_was_declared_in_the_header();

void header_foo();

int inline_header_definition() {
  int i = 0;
  i++;
  i--;
  i++;
  i--;
  return i;
}

namespace namey {
namespace inner_namey {
void name_name_name();

int nom_nom_nom(int foo);
}  // namespace inner_namey
}  // namespace namey

class CoolAlloc {
 public:
  static void ChewGumAndAllocateMemory() {
    // nom nom nom
  }
};

template <class T>
class JokeBase {
  int walkIntoBar() {
    int i = 0;
    return i--;
  }
};

template <class T, class Alloc>
class WhatsYourVector_impl : public JokeBase<T> {
 public:
  typedef T* elem_type;

  elem_type mStorage;

  WhatsYourVector_impl(elem_type thing) : mStorage(thing) {}

  int forwardDeclaredThingInlinedBelow();

  template <class Item, typename ActualAlloc = Alloc>
  elem_type forwardDeclaredTemplateThingInlinedBelow(const Item* aThing);
};

template <class T, class Alloc>
int WhatsYourVector_impl<T, Alloc>::forwardDeclaredThingInlinedBelow() {
  int i = 0;
  i++;
  i--;
  return i;
}

template <typename T, class Alloc>
template <class Item, typename ActualAlloc>
auto WhatsYourVector_impl<T, Alloc>::forwardDeclaredTemplateThingInlinedBelow(
    const Item* aThing) -> elem_type {
  int i = 0;
  if (aThing) {
    i++;
  }
  i--;
  ActualAlloc::ChewGumAndAllocateMemory();
  return mStorage;
}

template <class T>
class WhatsYourVector : public WhatsYourVector_impl<T, CoolAlloc> {
 public:
  typedef T* elem_type;

  WhatsYourVector(elem_type thing)
      : WhatsYourVector_impl<T, CoolAlloc>(thing) {}
};

/* pad out the end of the file so we can more easily ensure that the
 * position: sticky stuff works even on fancy big screen monitors without
 * resizing the window to be tiny.
 *
 _                    __          _                  __          _
| |__   ___   ___  _ _\ \   _ __ | |__   ___  ___ _ _\ \   _ __ | |__   ___
| '_ \ / _ \ / _ \| '_ \ \ | '_ \| '_ \ / _ \/ _ \ '_ \ \ | '_ \| '_ \ / _ \
| |_) | (_) | (_) | |_) \ \| | | | |_) |  __/  __/ |_) \ \| | | | |_) | (_) |
|_.__/ \___/ \___/| .__/ \_\_| |_|_.__/ \___|\___| .__/ \_\_| |_|_.__/ \___/
                  |_|                            |_|
          __          _              __          _     _
  ___  _ _\ \   _ __ | |__   ___  _ _\ \   _ __ | |__ | | ___   ___  _ __
 / _ \| '_ \ \ | '_ \| '_ \ / _ \| '_ \ \ | '_ \| '_ \| |/ _ \ / _ \| '_ \
| (_) | |_) \ \| | | | |_) | (_) | |_) \ \| | | | |_) | | (_) | (_) | |_) |
 \___/| .__/ \_\_| |_|_.__/ \___/| .__/ \_\_| |_|_.__/|_|\___/ \___/| .__/
      |_|                        |_|                                |_|
 _                    __          _                  __          _
| |__   ___   ___  _ _\ \   _ __ | |__   ___  ___ _ _\ \   _ __ | |__   ___
| '_ \ / _ \ / _ \| '_ \ \ | '_ \| '_ \ / _ \/ _ \ '_ \ \ | '_ \| '_ \ / _ \
| |_) | (_) | (_) | |_) \ \| | | | |_) |  __/  __/ |_) \ \| | | | |_) | (_) |
|_.__/ \___/ \___/| .__/ \_\_| |_|_.__/ \___|\___| .__/ \_\_| |_|_.__/ \___/
                  |_|                            |_|
          __          _              __          _     _
  ___  _ _\ \   _ __ | |__   ___  _ _\ \   _ __ | |__ | | ___   ___  _ __
 / _ \| '_ \ \ | '_ \| '_ \ / _ \| '_ \ \ | '_ \| '_ \| |/ _ \ / _ \| '_ \
| (_) | |_) \ \| | | | |_) | (_) | |_) \ \| | | | |_) | | (_) | (_) | |_) |
 \___/| .__/ \_\_| |_|_.__/ \___/| .__/ \_\_| |_|_.__/|_|\___/ \___/| .__/
      |_|                        |_|                                |_|
 _                    __          _                  __          _
| |__   ___   ___  _ _\ \   _ __ | |__   ___  ___ _ _\ \   _ __ | |__   ___
| '_ \ / _ \ / _ \| '_ \ \ | '_ \| '_ \ / _ \/ _ \ '_ \ \ | '_ \| '_ \ / _ \
| |_) | (_) | (_) | |_) \ \| | | | |_) |  __/  __/ |_) \ \| | | | |_) | (_) |
|_.__/ \___/ \___/| .__/ \_\_| |_|_.__/ \___|\___| .__/ \_\_| |_|_.__/ \___/
                  |_|                            |_|
          __          _              __          _     _
  ___  _ _\ \   _ __ | |__   ___  _ _\ \   _ __ | |__ | | ___   ___  _ __
 / _ \| '_ \ \ | '_ \| '_ \ / _ \| '_ \ \ | '_ \| '_ \| |/ _ \ / _ \| '_ \
| (_) | |_) \ \| | | | |_) | (_) | |_) \ \| | | | |_) | | (_) | (_) | |_) |
 \___/| .__/ \_\_| |_|_.__/ \___/| .__/ \_\_| |_|_.__/|_|\___/ \___/| .__/
      |_|                        |_|                                |_|
 _                    __          _                  __          _
| |__   ___   ___  _ _\ \   _ __ | |__   ___  ___ _ _\ \   _ __ | |__   ___
| '_ \ / _ \ / _ \| '_ \ \ | '_ \| '_ \ / _ \/ _ \ '_ \ \ | '_ \| '_ \ / _ \
| |_) | (_) | (_) | |_) \ \| | | | |_) |  __/  __/ |_) \ \| | | | |_) | (_) |
|_.__/ \___/ \___/| .__/ \_\_| |_|_.__/ \___|\___| .__/ \_\_| |_|_.__/ \___/
                  |_|                            |_|
          __          _              __          _     _
  ___  _ _\ \   _ __ | |__   ___  _ _\ \   _ __ | |__ | | ___   ___  _ __
 / _ \| '_ \ \ | '_ \| '_ \ / _ \| '_ \ \ | '_ \| '_ \| |/ _ \ / _ \| '_ \
| (_) | |_) \ \| | | | |_) | (_) | |_) \ \| | | | |_) | | (_) | (_) | |_) |
 \___/| .__/ \_\_| |_|_.__/ \___/| .__/ \_\_| |_|_.__/|_|\___/ \___/| .__/
      |_|                        |_|                                |_|
 _                    __          _                  __          _
| |__   ___   ___  _ _\ \   _ __ | |__   ___  ___ _ _\ \   _ __ | |__   ___
| '_ \ / _ \ / _ \| '_ \ \ | '_ \| '_ \ / _ \/ _ \ '_ \ \ | '_ \| '_ \ / _ \
| |_) | (_) | (_) | |_) \ \| | | | |_) |  __/  __/ |_) \ \| | | | |_) | (_) |
|_.__/ \___/ \___/| .__/ \_\_| |_|_.__/ \___|\___| .__/ \_\_| |_|_.__/ \___/
                  |_|                            |_|
          __          _              __          _     _
  ___  _ _\ \   _ __ | |__   ___  _ _\ \   _ __ | |__ | | ___   ___  _ __
 / _ \| '_ \ \ | '_ \| '_ \ / _ \| '_ \ \ | '_ \| '_ \| |/ _ \ / _ \| '_ \
| (_) | |_) \ \| | | | |_) | (_) | |_) \ \| | | | |_) | | (_) | (_) | |_) |
 \___/| .__/ \_\_| |_|_.__/ \___/| .__/ \_\_| |_|_.__/|_|\___/ \___/| .__/
      |_|                        |_|                                |_|
 _                    __          _                  __          _
| |__   ___   ___  _ _\ \   _ __ | |__   ___  ___ _ _\ \   _ __ | |__   ___
| '_ \ / _ \ / _ \| '_ \ \ | '_ \| '_ \ / _ \/ _ \ '_ \ \ | '_ \| '_ \ / _ \
| |_) | (_) | (_) | |_) \ \| | | | |_) |  __/  __/ |_) \ \| | | | |_) | (_) |
|_.__/ \___/ \___/| .__/ \_\_| |_|_.__/ \___|\___| .__/ \_\_| |_|_.__/ \___/
                  |_|                            |_|
          __          _              __          _     _
  ___  _ _\ \   _ __ | |__   ___  _ _\ \   _ __ | |__ | | ___   ___  _ __
 / _ \| '_ \ \ | '_ \| '_ \ / _ \| '_ \ \ | '_ \| '_ \| |/ _ \ / _ \| '_ \
| (_) | |_) \ \| | | | |_) | (_) | |_) \ \| | | | |_) | | (_) | (_) | |_) |
 \___/| .__/ \_\_| |_|_.__/ \___/| .__/ \_\_| |_|_.__/|_|\___/ \___/| .__/
      |_|                        |_|                                |_|
 _                    __          _                  __          _
| |__   ___   ___  _ _\ \   _ __ | |__   ___  ___ _ _\ \   _ __ | |__   ___
| '_ \ / _ \ / _ \| '_ \ \ | '_ \| '_ \ / _ \/ _ \ '_ \ \ | '_ \| '_ \ / _ \
| |_) | (_) | (_) | |_) \ \| | | | |_) |  __/  __/ |_) \ \| | | | |_) | (_) |
|_.__/ \___/ \___/| .__/ \_\_| |_|_.__/ \___|\___| .__/ \_\_| |_|_.__/ \___/
                  |_|                            |_|
          __          _              __          _     _
  ___  _ _\ \   _ __ | |__   ___  _ _\ \   _ __ | |__ | | ___   ___  _ __
 / _ \| '_ \ \ | '_ \| '_ \ / _ \| '_ \ \ | '_ \| '_ \| |/ _ \ / _ \| '_ \
| (_) | |_) \ \| | | | |_) | (_) | |_) \ \| | | | |_) | | (_) | (_) | |_) |
 \___/| .__/ \_\_| |_|_.__/ \___/| .__/ \_\_| |_|_.__/|_|\___/ \___/| .__/
      |_|                        |_|                                |_|
 _                    __          _                  __          _
| |__   ___   ___  _ _\ \   _ __ | |__   ___  ___ _ _\ \   _ __ | |__   ___
| '_ \ / _ \ / _ \| '_ \ \ | '_ \| '_ \ / _ \/ _ \ '_ \ \ | '_ \| '_ \ / _ \
| |_) | (_) | (_) | |_) \ \| | | | |_) |  __/  __/ |_) \ \| | | | |_) | (_) |
|_.__/ \___/ \___/| .__/ \_\_| |_|_.__/ \___|\___| .__/ \_\_| |_|_.__/ \___/
                  |_|                            |_|
          __          _              __          _     _
  ___  _ _\ \   _ __ | |__   ___  _ _\ \   _ __ | |__ | | ___   ___  _ __
 / _ \| '_ \ \ | '_ \| '_ \ / _ \| '_ \ \ | '_ \| '_ \| |/ _ \ / _ \| '_ \
| (_) | |_) \ \| | | | |_) | (_) | |_) \ \| | | | |_) | | (_) | (_) | |_) |
 \___/| .__/ \_\_| |_|_.__/ \___/| .__/ \_\_| |_|_.__/|_|\___/ \___/| .__/
      |_|                        |_|                                |_|
 _                    __          _                  __          _
| |__   ___   ___  _ _\ \   _ __ | |__   ___  ___ _ _\ \   _ __ | |__   ___
| '_ \ / _ \ / _ \| '_ \ \ | '_ \| '_ \ / _ \/ _ \ '_ \ \ | '_ \| '_ \ / _ \
| |_) | (_) | (_) | |_) \ \| | | | |_) |  __/  __/ |_) \ \| | | | |_) | (_) |
|_.__/ \___/ \___/| .__/ \_\_| |_|_.__/ \___|\___| .__/ \_\_| |_|_.__/ \___/
                  |_|                            |_|
          __          _              __          _     _
  ___  _ _\ \   _ __ | |__   ___  _ _\ \   _ __ | |__ | | ___   ___  _ __
 / _ \| '_ \ \ | '_ \| '_ \ / _ \| '_ \ \ | '_ \| '_ \| |/ _ \ / _ \| '_ \
| (_) | |_) \ \| | | | |_) | (_) | |_) \ \| | | | |_) | | (_) | (_) | |_) |
 \___/| .__/ \_\_| |_|_.__/ \___/| .__/ \_\_| |_|_.__/|_|\___/ \___/| .__/
      |_|                        |_|                                |_|
 */

#endif

```

## tests/tests/files/testing/web-platform/meta/fake-standard/test_ima_weird_meta_wpt.js.ini
```
# This isn't a real ini file and we don't parse it to get the JSON that
# referenced this.  That's hardcoded JSON of hardcoded lies.

```

## tests/tests/files/testing/web-platform/meta/fake-standard/test_ima_disabled_wpt.js.ini
```
# This isn't a real ini file and we don't parse it to get the JSON that
# referenced this.  That's hardcoded JSON of hardcoded lies.

```

## tests/tests/files/testing/web-platform/meta/fake-standard/test_ima_sad_subtests_wpt.js.ini
```
# This isn't a real ini file and we don't parse it to get the JSON that
# referenced this.  That's hardcoded JSON of hardcoded lies.

```

## tests/tests/files/testing/web-platform/tests/fake-standard/test_ima_weird_meta_wpt.js
```
console.log("I'm not really a WPT test.");

```

## tests/tests/files/testing/web-platform/tests/fake-standard/test_ima_sad_subtests_wpt.js
```
console.log("I'm not really a WPT test.");

```

## tests/tests/files/testing/web-platform/tests/fake-standard/test_ima_disabled_wpt.js
```
console.log("I'm not really a WPT test.");

```

## tests/tests/files/template_specialization.cpp
```
struct SomeStruct {
  static void some_function();
};

template <typename T, typename U>
struct SomeTemplate {};

template <typename U>
struct SomeTemplate<int, U> {
  static void call_function() { U::some_function(); }
};

void test() { SomeTemplate<int, SomeStruct>::call_function(); }

```

## tests/tests/files/enummacro.cpp
```
// https://bugzilla.mozilla.org/show_bug.cgi?id=1282172
enum ArenaObjectID {
  eArenaObjectID_DummyBeforeFirstObjectID = 0,

#define PRES_ARENA_OBJECT(name_) eArenaObjectID_##name_,
#include "enummacro.h"
#undef PRES_ARENA_OBJECT

  eArenaObjectID_COUNT
};

void someFunction() {
  ArenaObjectID useTheEnum = ArenaObjectID::eArenaObjectID_nsRuleNode;
}

```

## tests/tests/files/.eslintignore
```
# I am a comment line
ignored-js/explicit-ignored.js
# The things this match will be re-included.
ignored-js/*reinclusion*
ignored-js/*wildcard*
!ignored-js/*reinclusion*
ignored-js/reignored-after-reinclusion.js
**/doublestar-ignored/

```

## tests/tests/files/templates3.cpp
```
class Foo {};
class Baz {};

template <class T>
class Traits {
 public:
  static void Method() {}
};

template <>
class Traits<Foo> {
 public:
  static void Method() {}
};

class Bar {
 public:
  template <class T>
  void Function(T* t);
};

template <class T>
inline void Bar::Function(T* t) {
  Traits<T>::Method();
}

int main() {
  Foo* f;
  Baz* z;
  Bar* b;
  b->Function(f);
  b->Function(z);
  return 0;
}

```

## tests/tests/files/overs.cpp
```
/**
 * This file is intended to create interesting test cases for searches against
 * a base class with a limited number of overrides.
 **/

class DoubleBase {
 public:
  virtual void doublePure() = 0;
};

class DoubleSubOne : public DoubleBase {
 public:
  void doublePure() override {
    // Sub one.
  }
};

class DoubleSubTwo : public DoubleBase {
 public:
  void doublePure() override {
    // Sub two.
  }
};

class TripleBase {
 public:
  virtual void triplePure() = 0;
};

class TripleSubOne : public TripleBase {
 public:
  void triplePure() override {
    // Triple sub one.
  }
};

class TripleSubTwo : public TripleBase {
 public:
  void triplePure() override {
    // Triple sub two.
  }
};

class TripleSubThree : public TripleBase {
 public:
  void triplePure() override {
    // Triple sub three.
  }
};

void generateDoubleUses(void) {
  DoubleBase* subOne = new DoubleSubOne();
  DoubleBase* subTwo = new DoubleSubTwo();
  DoubleSubOne explicitOne;
  DoubleSubTwo explicitTwo;

  subOne->doublePure();
  subTwo->doublePure();

  explicitOne.doublePure();
  explicitTwo.doublePure();
}

void generateTripleUses(void) {
  TripleBase* subOne = new TripleSubOne();
  TripleBase* subTwo = new TripleSubTwo();
  TripleBase* subThree = new TripleSubThree();
  TripleSubOne explicitOne;
  TripleSubTwo explicitTwo;
  TripleSubThree explicitThree;

  subOne->triplePure();
  subTwo->triplePure();
  subThree->triplePure();

  explicitOne.triplePure();
  explicitTwo.triplePure();
  explicitThree.triplePure();
}

```

## tests/tests/files/bug1432300.cpp
```
/** This simulates the code from
 * JS::ProfilingFrameIterator::getPhysicalFrameAndEntry which was highlighting
 * incorrectly in bug 1432300 */

namespace mozilla {

template <class T>
class Maybe {
 public:
  Maybe(T x) : mX(x) {}

  Maybe(const Maybe<T>& x) : mX(x.mX) {}

  T mX;
};

}  // namespace mozilla

mozilla::Maybe<int> getAThing() { return mozilla::Maybe<int>(42); }

void useAThing() { mozilla::Maybe<int> thing = getAThing(); }

```

## tests/tests/files/js/with space.js
```
var SymbolInFilenameWithSpace = 10;

// A comment to create a previous revision.

```

## tests/tests/files/js/export4.mjs
```
export default class defaultClassDecl {}

```

## tests/tests/files/js/export5.mjs
```
export { default } from "./mod1.mjs";

```

## tests/tests/files/js/README.md
```
This directory currently just contains stubs to get `xpctest_params.h` to be
able to sucessfully compile.

```

## tests/tests/files/js/export6.mjs
```
export default function () {}

```

## tests/tests/files/js/import.mjs
```
import ns from "./mod1.mjs";
import * as nsAs from "./mod2.mjs";
import { exportedName1, exportedName2 } from "./mod3.mjs";
import { exportedName3 as importAs } from "./mod4.mjs";
import { default as defaultAs } from "./mod5.mjs";
import { "string name" as stringAs } from "./mod6.mjs";
import "./mod7.mjs";

```

## tests/tests/files/js/export.mjs
```
export let letDecl;
export const constDecl = 1;
export function funDecl() {}
export class classDecl {}
let dummy = 0, local1 = 1, local2 = 2;
export { local1, local2 };
export { dummy as exportAs1 };
export { dummy as "export as string" };
export { dummy as default };

export * from "./mod11.mjs";
export * as exportNSAs from "./mod12.mjs";
export { exportedName1, exportedName2 } from "./mod13.mjs";
export { exportedName3 as exportAs2 } from "./mod14.mjs";
export { default as defaultAs } from "./mod15.mjs";

```

## tests/tests/files/js/GCAnnotations.h
```
// This file is just a stub to let xpctest_params.h compile!

```

## tests/tests/files/js/Value.h
```
// This file is just a stub to let xpctest_params.h compile!

namespace JS {
class Value;
template <typename T>
class Handle;
template <typename T>
class HandleValue;
template <typename T>
class MutableHandle;
template <typename T>
class MutableHandleValue;
}  // namespace JS

```

## tests/tests/files/js/export3.mjs
```
export default function defaultFunDecl() {}

```

## tests/tests/files/js/export2.mjs
```
let dummy = 0;
export default dummy;

```

## tests/tests/files/js/with-UTF8-ãƒ•ã‚¡ã‚¤ãƒ«.js
```
var SymbolInFilenameWithUTF8 = 20;

```

## tests/tests/files/js/export8.mjs
```
export default () => {};
let something = 0;

```

## tests/tests/files/js/contextual-keyword.js
```
var await = 0;
var yield = 0;
var let = 0;
var static = 0;
var implements = 0;
var interface = 0;
var package = 0;
var private = 0;
var protected = 0;
var public = 0;
var as = 0;
var async = 0;
var from = 0;
var get = 0;
var meta = 0;
var of = 0;
var set = 0;
var target = 0;

async function* f() {
  await 10;
  yield 10;
  let x = 10;
}

for (var x of []) {
}

class C {
  constructor() {
    new.target;
  }

  static foo() {}
  get prop() {}
  set prop(v) {}
}

```

## tests/tests/files/js/export7.mjs
```
export default class {}

```

## tests/tests/files/imported-module.mjs
```
export default function moduleDefaultFunc() {
    return "I'm the default!";
}

export const moduleConst = "I am a constant";

export function moduleFunc(moduleFuncArg1) {
    console.log("My argument was:", moduleFuncArg1);
}

export class ModuleClass {
    // Our preprocessor logic should not freak out about this.
    #error = null;

    constructor() {
        this.moduleClassField = 1;
    }
}

```

## tests/tests/files/lambdas.cpp
```
struct Struct0 {
  void method() {}
};

struct Struct1 {
  void method() const {}
};

void test() {
  const auto lambda = [](auto&& t) { t.method(); };

  lambda(Struct0{});
  lambda(Struct1{});

  const auto capture_all_by_reference = [&] { (void)lambda; };
  const auto capture_all_by_value = [=] { (void)lambda; };
  const auto capture_one_by_reference = [&lambda] { (void)lambda; };
  const auto capture_one_by_value = [lambda] { (void)lambda; };
  const auto capture_by_named_reference = [&lambda = lambda] { (void)lambda; };
  const auto capture_by_named_value = [lambda = lambda] { (void)lambda; };
}

```

## tests/tests/files/GeckoApp.java
```
/* -*- Mode: Java; c-basic-offset: 4; tab-width: 4; indent-tabs-mode: nil; -*-
 * This Source Code Form is subject to the terms of the Mozilla Public
 * License, v. 2.0. If a copy of the MPL was not distributed with this
 * file, You can obtain one at http://mozilla.org/MPL/2.0/. */

package org.mozilla.gecko;

import org.mozilla.gecko.AppConstants.Versions;
import org.mozilla.gecko.GeckoProfileDirectories.NoMozillaDirectoryException;
import org.mozilla.gecko.annotation.RobocopTarget;
import org.mozilla.gecko.annotation.WrapForJNI;
import org.mozilla.gecko.db.BrowserDB;
import org.mozilla.gecko.gfx.BitmapUtils;
import org.mozilla.gecko.gfx.FullScreenState;
import org.mozilla.gecko.gfx.LayerView;
import org.mozilla.gecko.health.HealthRecorder;
import org.mozilla.gecko.health.SessionInformation;
import org.mozilla.gecko.health.StubbedHealthRecorder;
import org.mozilla.gecko.home.HomeConfig.PanelType;
import org.mozilla.gecko.menu.GeckoMenu;
import org.mozilla.gecko.menu.GeckoMenuInflater;
import org.mozilla.gecko.menu.MenuPanel;
import org.mozilla.gecko.mma.MmaDelegate;
import org.mozilla.gecko.notifications.NotificationHelper;
import org.mozilla.gecko.util.IntentUtils;
import org.mozilla.gecko.mozglue.SafeIntent;
import org.mozilla.gecko.mozglue.GeckoLoader;
import org.mozilla.gecko.permissions.Permissions;
import org.mozilla.gecko.preferences.ClearOnShutdownPref;
import org.mozilla.gecko.preferences.GeckoPreferences;
import org.mozilla.gecko.prompts.PromptService;
import org.mozilla.gecko.restrictions.Restrictions;
import org.mozilla.gecko.tabqueue.TabQueueHelper;
import org.mozilla.gecko.text.FloatingToolbarTextSelection;
import org.mozilla.gecko.text.TextSelection;
import org.mozilla.gecko.updater.UpdateServiceHelper;
import org.mozilla.gecko.util.ActivityResultHandler;
import org.mozilla.gecko.util.ActivityUtils;
import org.mozilla.gecko.util.BundleEventListener;
import org.mozilla.gecko.util.EventCallback;
import org.mozilla.gecko.util.FileUtils;
import org.mozilla.gecko.util.GeckoBundle;
import org.mozilla.gecko.util.HardwareUtils;
import org.mozilla.gecko.util.PrefUtils;
import org.mozilla.gecko.util.ThreadUtils;
import org.mozilla.gecko.util.ViewUtil;
import org.mozilla.gecko.widget.ActionModePresenter;
import org.mozilla.gecko.widget.AnchoredPopup;

import android.Manifest;
import android.animation.Animator;
import android.animation.ObjectAnimator;
import android.annotation.TargetApi;
import android.app.Activity;
import android.app.AlertDialog;
import android.content.Context;
import android.content.DialogInterface;
import android.content.Intent;
import android.content.SharedPreferences;
import android.content.pm.PackageManager.NameNotFoundException;
import android.content.res.Configuration;
import android.graphics.Bitmap;
import android.graphics.BitmapFactory;
import android.net.Uri;
import android.os.Build;
import android.os.Bundle;
import android.os.Environment;
import android.os.Handler;
import android.os.StrictMode;
import android.provider.ContactsContract;
import android.provider.MediaStore.Images.Media;
import android.support.annotation.NonNull;
import android.support.annotation.WorkerThread;
import android.support.design.widget.Snackbar;
import android.text.TextUtils;
import android.util.AttributeSet;
import android.util.Base64;
import android.util.Log;
import android.util.SparseBooleanArray;
import android.util.SparseIntArray;
import android.view.KeyEvent;
import android.view.Menu;
import android.view.MenuInflater;
import android.view.MenuItem;
import android.view.MotionEvent;
import android.view.OrientationEventListener;
import android.view.View;
import android.view.ViewTreeObserver;
import android.view.Window;
import android.widget.AdapterView;
import android.widget.Button;
import android.widget.ListView;
import android.widget.RelativeLayout;
import android.widget.SimpleAdapter;
import android.widget.TextView;
import android.widget.Toast;

import org.json.JSONArray;
import org.json.JSONException;
import org.json.JSONObject;

import java.io.ByteArrayOutputStream;
import java.io.File;
import java.io.IOException;
import java.io.InputStream;
import java.net.URL;
import java.util.ArrayList;
import java.util.HashMap;
import java.util.HashSet;
import java.util.Iterator;
import java.util.Locale;
import java.util.Map;
import java.util.Set;
import java.util.concurrent.TimeUnit;

import static org.mozilla.gecko.Tabs.INTENT_EXTRA_SESSION_UUID;
import static org.mozilla.gecko.Tabs.INTENT_EXTRA_TAB_ID;
import static org.mozilla.gecko.Tabs.INVALID_TAB_ID;
import static org.mozilla.gecko.mma.MmaDelegate.DOWNLOAD_MEDIA_SAVED_IMAGE;
import static org.mozilla.gecko.mma.MmaDelegate.READER_AVAILABLE;

public abstract class GeckoApp extends GeckoActivity
                               implements AnchoredPopup.OnVisibilityChangeListener,
                                          BundleEventListener,
                                          GeckoMenu.Callback,
                                          GeckoMenu.MenuPresenter,
                                          GeckoView.ContentListener,
                                          ScreenOrientationDelegate,
                                          Tabs.OnTabsChangedListener,
                                          ViewTreeObserver.OnGlobalLayoutListener {

    private static final String LOGTAG = "GeckoApp";
    private static final long ONE_DAY_MS = TimeUnit.MILLISECONDS.convert(1, TimeUnit.DAYS);

    public static final String ACTION_ALERT_CALLBACK       = "org.mozilla.gecko.ALERT_CALLBACK";
    public static final String ACTION_HOMESCREEN_SHORTCUT  = "org.mozilla.gecko.BOOKMARK";
    public static final String ACTION_WEBAPP               = "org.mozilla.gecko.WEBAPP";
    public static final String ACTION_DEBUG                = "org.mozilla.gecko.DEBUG";
    public static final String ACTION_LAUNCH_SETTINGS      = "org.mozilla.gecko.SETTINGS";
    public static final String ACTION_LOAD                 = "org.mozilla.gecko.LOAD";
    public static final String ACTION_INIT_PW              = "org.mozilla.gecko.INIT_PW";
    public static final String ACTION_SWITCH_TAB           = "org.mozilla.gecko.SWITCH_TAB";
    public static final String ACTION_SHUTDOWN             = "org.mozilla.gecko.SHUTDOWN";

    public static final String INTENT_REGISTER_STUMBLER_LISTENER = "org.mozilla.gecko.STUMBLER_REGISTER_LOCAL_LISTENER";

    public static final String EXTRA_STATE_BUNDLE          = "stateBundle";

    public static final String PREFS_ALLOW_STATE_BUNDLE    = "allowStateBundle";
    public static final String PREFS_FLASH_USAGE           = "playFlashCount";
    public static final String PREFS_VERSION_CODE          = "versionCode";
    public static final String PREFS_WAS_STOPPED           = "wasStopped";
    public static final String PREFS_CRASHED_COUNT         = "crashedCount";
    public static final String PREFS_CLEANUP_TEMP_FILES    = "cleanupTempFiles";

    // Used with SharedPreferences, per profile, to determine if this is the first run of
    // the application. When accessing SharedPreferences, the default value of true should be used.
    //
    // Originally, this was only used for the telemetry core ping logic. To avoid
    // having to write custom migration logic, we just keep the original pref key.
    public static final String PREFS_IS_FIRST_RUN = "telemetry-isFirstRun";

    public static final String SAVED_STATE_IN_BACKGROUND   = "inBackground";
    public static final String SAVED_STATE_PRIVATE_SESSION = "privateSession";

    // Delay before running one-time "cleanup" tasks that may be needed
    // after a version upgrade.
    private static final int CLEANUP_DEFERRAL_SECONDS = 15;

    // Length of time in ms during which crashes are classified as startup crashes
    // for crash loop detection purposes.
    private static final int STARTUP_PHASE_DURATION_MS = 30 * 1000;

    private static boolean sAlreadyLoaded;

    protected RelativeLayout mRootLayout;
    protected RelativeLayout mMainLayout;

    protected RelativeLayout mGeckoLayout;
    private OrientationEventListener mCameraOrientationEventListener;
    protected MenuPanel mMenuPanel;
    protected Menu mMenu;
    protected boolean mIsRestoringActivity;

    /** Tells if we're aborting app launch, e.g. if this is an unsupported device configuration. */
    protected boolean mIsAbortingAppLaunch;

    private PromptService mPromptService;
    protected TextSelection mTextSelection;

    protected DoorHangerPopup mDoorHangerPopup;
    protected FormAssistPopup mFormAssistPopup;

    protected GeckoView mLayerView;

    protected boolean mLastSessionCrashed;
    protected boolean mShouldRestore;
    private boolean mSessionRestoreParsingFinished = false;

    private boolean foregrounded = false;

    private static final class LastSessionParser extends SessionParser {
        private JSONArray tabs;
        private JSONObject windowObject;
        private boolean loadingExternalURL;

        private boolean selectNextTab;
        private boolean tabsWereSkipped;
        private boolean tabsWereProcessed;

        private SparseIntArray tabIdMap;

        /**
         * @param loadingExternalURL Pass true if we're going to open an additional tab to load an
         *                           URL received through our launch intent.
         */
        public LastSessionParser(JSONArray tabs, JSONObject windowObject, boolean loadingExternalURL) {
            this.tabs = tabs;
            this.windowObject = windowObject;
            this.loadingExternalURL = loadingExternalURL;

            tabIdMap = new SparseIntArray();
        }

        public boolean allTabsSkipped() {
            return tabsWereSkipped && !tabsWereProcessed;
        }

        public int getNewTabId(int oldTabId) {
            return tabIdMap.get(oldTabId, INVALID_TAB_ID);
        }

        @Override
        public void onTabRead(final SessionTab sessionTab) {
            if (sessionTab.isAboutHomeWithoutHistory()) {
                // This is a tab pointing to about:home with no history. We won't restore
                // this tab. If we end up restoring no tabs then the browser will decide
                // whether it needs to open about:home or a different 'homepage'. If we'd
                // always restore about:home only tabs then we'd never open the homepage.
                // See bug 1261008.

                if (!loadingExternalURL && sessionTab.isSelected()) {
                    // Unfortunately this tab is the selected tab. Let's just try to select
                    // the first tab. If we haven't restored any tabs so far then remember
                    // to select the next tab that gets restored.

                    if (!Tabs.getInstance().selectLastTab()) {
                        selectNextTab = true;
                    }
                }

                // Do not restore this tab.
                tabsWereSkipped = true;
                return;
            }

            tabsWereProcessed = true;

            JSONObject tabObject = sessionTab.getTabObject();

            int flags = Tabs.LOADURL_NEW_TAB;
            flags |= ((loadingExternalURL || !sessionTab.isSelected()) ? Tabs.LOADURL_DELAY_LOAD : 0);
            flags |= (tabObject.optBoolean("desktopMode") ? Tabs.LOADURL_DESKTOP : 0);
            flags |= (tabObject.optBoolean("isPrivate") ? Tabs.LOADURL_PRIVATE : 0);

            final Tab tab = Tabs.getInstance().loadUrl(sessionTab.getUrl(), flags);

            if (selectNextTab) {
                // We did not restore the selected tab previously. Now let's select this tab.
                Tabs.getInstance().selectTab(tab.getId());
                selectNextTab = false;
            }

            ThreadUtils.postToUiThread(new Runnable() {
                @Override
                public void run() {
                    tab.updateTitle(sessionTab.getTitle());
                }
            });

            try {
                int oldTabId = tabObject.optInt("tabId", INVALID_TAB_ID);
                int newTabId = tab.getId();
                tabObject.put("tabId", newTabId);
                if  (oldTabId >= 0) {
                    tabIdMap.put(oldTabId, newTabId);
                }
            } catch (JSONException e) {
                Log.e(LOGTAG, "JSON error", e);
            }
            tabs.put(tabObject);
        }

        @Override
        public void onClosedTabsRead(final JSONArray closedTabData) throws JSONException {
            windowObject.put("closedTabs", closedTabData);
        }

        /**
         * Updates stored parent tab IDs in the session store data to match the new tab IDs
         * that have been allocated during startup session restore.
         *
         * @param tabData A JSONArray containg stored session store tabs.
         */
        public void updateParentId(final JSONArray tabData) {
            if (tabData == null) {
                return;
            }

            for (int i = 0; i < tabData.length(); i++) {
                try {
                    JSONObject tabObject = tabData.getJSONObject(i);

                    int parentId = tabObject.getInt("parentId");
                    int newParentId = getNewTabId(parentId);

                    tabObject.put("parentId", newParentId);
                } catch (JSONException ex) {
                    // Tabs are not guaranteed to have a parentId,
                    // so just skip the tab and try the next one.
                }
            }
        }
    };

    protected boolean mInitialized;
    protected boolean mWindowFocusInitialized;
    private Telemetry.Timer mJavaUiStartupTimer;
    private Telemetry.Timer mGeckoReadyStartupTimer;

    private String mPrivateBrowsingSession;

    private volatile HealthRecorder mHealthRecorder;
    private volatile Locale mLastLocale;

    private boolean mShutdownOnDestroy;
    private boolean mRestartOnShutdown;

    private boolean mWasFirstTabShownAfterActivityUnhidden;

    abstract public int getLayout();

    abstract public View getDoorhangerOverlay();

    protected void processTabQueue() {};

    protected void openQueuedTabs() {};

    @SuppressWarnings("serial")
    class SessionRestoreException extends Exception {
        public SessionRestoreException(Exception e) {
            super(e);
        }

        public SessionRestoreException(String message) {
            super(message);
        }
    }

    void toggleChrome(final boolean aShow) { }

    void focusChrome() { }

    public SharedPreferences getSharedPreferences() {
        return GeckoSharedPrefs.forApp(this);
    }

    public SharedPreferences getSharedPreferencesForProfile() {
        return GeckoSharedPrefs.forProfile(this);
    }

    @Override
    public void onTabChanged(Tab tab, Tabs.TabEvents msg, String data) {
        // When a tab is closed, it is always unselected first.
        // When a tab is unselected, another tab is always selected first.
        switch (msg) {
            case UNSELECTED:
                break;

            case LOCATION_CHANGE:
                // We only care about location change for the selected tab.
                if (Tabs.getInstance().isSelectedTab(tab)) {
                    resetOptionsMenu();
                    resetFormAssistPopup();
                }
                break;

            case SELECTED:
                resetOptionsMenu();
                resetFormAssistPopup();
                break;

            case DESKTOP_MODE_CHANGE:
                if (Tabs.getInstance().isSelectedTab(tab))
                    resetOptionsMenu();
                break;
        }
    }

    private void resetOptionsMenu() {
        if (mInitialized) {
            invalidateOptionsMenu();
        }
    }

    private void resetFormAssistPopup() {
        if (mInitialized && mFormAssistPopup != null) {
            mFormAssistPopup.hide();
        }
    }

    /**
     * Called on tab selection and tab close - return true to allow updating of this activity's
     * last selected tab.
     */
    protected boolean saveAsLastSelectedTab(Tab tab) {
        return false;
    }

    public void refreshChrome() {
    }

    public void invalidateOptionsMenu() {
        if (mMenu == null) {
            return;
        }

        onPrepareOptionsMenu(mMenu);

        super.invalidateOptionsMenu();
    }

    @Override
    public boolean onCreateOptionsMenu(Menu menu) {
        mMenu = menu;

        MenuInflater inflater = getMenuInflater();
        inflater.inflate(R.menu.gecko_app_menu, mMenu);
        return true;
    }

    @Override
    public MenuInflater getMenuInflater() {
        return new GeckoMenuInflater(this);
    }

    public MenuPanel getMenuPanel() {
        if (mMenuPanel == null || mMenu == null) {
            onCreatePanelMenu(Window.FEATURE_OPTIONS_PANEL, null);
            invalidateOptionsMenu();
        }
        return mMenuPanel;
    }

    @Override
    public boolean onMenuItemClick(MenuItem item) {
        return onOptionsItemSelected(item);
    }

    @Override
    public boolean onMenuItemLongClick(MenuItem item) {
        return false;
    }

    @Override
    public void openMenu() {
        openOptionsMenu();
    }

    @Override
    public void showMenu(final View menu) {
        // On devices using the custom menu, focus is cleared from the menu when its tapped.
        // Close and then reshow it to avoid these issues. See bug 794581 and bug 968182.
        closeMenu();

        // Post the reshow code back to the UI thread to avoid some optimizations Android
        // has put in place for menus that hide/show themselves quickly. See bug 985400.
        ThreadUtils.postToUiThread(new Runnable() {
            @Override
            public void run() {
                mMenuPanel.removeAllViews();
                mMenuPanel.addView(menu);
                openOptionsMenu();
            }
        });
    }

    @Override
    public void closeMenu() {
        closeOptionsMenu();
    }

    @Override
    public View onCreatePanelView(int featureId) {
        if (featureId == Window.FEATURE_OPTIONS_PANEL) {
            if (mMenuPanel == null) {
                mMenuPanel = new MenuPanel(this, null);
            } else {
                // Prepare the panel every time before showing the menu.
                onPreparePanel(featureId, mMenuPanel, mMenu);
            }

            return mMenuPanel;
        }

        return super.onCreatePanelView(featureId);
    }

    @Override
    public boolean onCreatePanelMenu(int featureId, Menu menu) {
        if (featureId == Window.FEATURE_OPTIONS_PANEL) {
            if (mMenuPanel == null) {
                mMenuPanel = (MenuPanel) onCreatePanelView(featureId);
            }

            GeckoMenu gMenu = new GeckoMenu(this, null);
            gMenu.setCallback(this);
            gMenu.setMenuPresenter(this);
            menu = gMenu;
            mMenuPanel.addView(gMenu);

            return onCreateOptionsMenu(menu);
        }

        return super.onCreatePanelMenu(featureId, menu);
    }

    @Override
    public boolean onPreparePanel(int featureId, View view, Menu menu) {
        if (featureId == Window.FEATURE_OPTIONS_PANEL) {
            return onPrepareOptionsMenu(menu);
        }

        return super.onPreparePanel(featureId, view, menu);
    }

    @Override
    public boolean onMenuOpened(int featureId, Menu menu) {
        // exit full-screen mode whenever the menu is opened
        if (mLayerView != null && mLayerView.isFullScreen()) {
            EventDispatcher.getInstance().dispatch("FullScreen:Exit", null);
        }

        if (featureId == Window.FEATURE_OPTIONS_PANEL) {
            if (mMenu == null) {
                // getMenuPanel() will force the creation of the menu as well
                MenuPanel panel = getMenuPanel();
                onPreparePanel(featureId, panel, mMenu);
            }

            // Scroll custom menu to the top
            if (mMenuPanel != null)
                mMenuPanel.scrollTo(0, 0);

            return true;
        }

        return super.onMenuOpened(featureId, menu);
    }

    @Override
    public boolean onOptionsItemSelected(MenuItem item) {
        if (item.getItemId() == R.id.quit) {
            // Make sure the Guest Browsing notification goes away when we quit.
            GuestSession.hideNotification(this);

            final SharedPreferences prefs = getSharedPreferencesForProfile();
            final Set<String> clearSet = PrefUtils.getStringSet(
                    prefs, ClearOnShutdownPref.PREF, new HashSet<String>());

            final GeckoBundle clearObj = new GeckoBundle(clearSet.size());
            for (final String clear : clearSet) {
                clearObj.putBoolean(clear, true);
            }

            final GeckoBundle res = new GeckoBundle(2);
            res.putBundle("sanitize", clearObj);

            // If the user wants to clear open tabs, or else has opted out of session
            // restore and does want to clear history, we also want to prevent the current
            // session info from being saved.
            if (clearObj.containsKey("private.data.openTabs")) {
                res.putBoolean("dontSaveSession", true);
            } else if (clearObj.containsKey("private.data.history")) {

                final String sessionRestore =
                        getSessionRestorePreference(getSharedPreferences());
                res.putBoolean("dontSaveSession", "quit".equals(sessionRestore));
            }

            EventDispatcher.getInstance().dispatch("Browser:Quit", res);

            // We don't call shutdown here because this creates a race condition which
            // can cause the clearing of private data to fail. Instead, we shut down the
            // UI only after we're done sanitizing.
            return true;
        }

        return super.onOptionsItemSelected(item);
    }

    @Override
    public void onOptionsMenuClosed(Menu menu) {
        mMenuPanel.removeAllViews();
        mMenuPanel.addView((GeckoMenu) mMenu);
    }

    @Override
    public boolean onKeyDown(int keyCode, KeyEvent event) {
        // Handle hardware menu key presses separately so that we can show a custom menu in some cases.
        if (keyCode == KeyEvent.KEYCODE_MENU) {
            openOptionsMenu();
            return true;
        }

        return super.onKeyDown(keyCode, event);
    }

    @Override
    protected void onSaveInstanceState(Bundle outState) {
        super.onSaveInstanceState(outState);

        outState.putBoolean(SAVED_STATE_IN_BACKGROUND, isApplicationInBackground());
        outState.putString(SAVED_STATE_PRIVATE_SESSION, mPrivateBrowsingSession);
    }

    public void addTab() { }

    public void addPrivateTab() { }

    public void showNormalTabs() { }

    public void showPrivateTabs() { }

    public void hideTabs() { }

    /**
     * Close the tab UI indirectly (not as the result of a direct user
     * action).  This does not force the UI to close; for example in Firefox
     * tablet mode it will remain open unless the user explicitly closes it.
     *
     * @return True if the tab UI was hidden.
     */
    public boolean autoHideTabs() { return false; }

    @Override
    public void handleMessage(final String event, final GeckoBundle message,
                              final EventCallback callback) {
        if (event.equals("Gecko:Ready")) {
            mGeckoReadyStartupTimer.stop();
            geckoConnected();

            // This method is already running on the background thread, so we
            // know that mHealthRecorder will exist. That doesn't stop us being
            // paranoid.
            // This method is cheap, so don't spawn a new runnable.
            final HealthRecorder rec = mHealthRecorder;
            if (rec != null) {
              rec.recordGeckoStartupTime(mGeckoReadyStartupTimer.getElapsed());
            }

            ((GeckoApplication) getApplicationContext()).onDelayedStartup();

            // Reset the crash loop counter if we remain alive for at least half a minute.
            ThreadUtils.postDelayedToBackgroundThread(new Runnable() {
                @Override
                public void run() {
                    getSharedPreferences().edit().putInt(PREFS_CRASHED_COUNT, 0).apply();
                }
            }, STARTUP_PHASE_DURATION_MS);

        } else if ("Accessibility:Ready".equals(event)) {
            GeckoAccessibility.updateAccessibilitySettings(this);

        } else if ("Accessibility:Event".equals(event)) {
            GeckoAccessibility.sendAccessibilityEvent(mLayerView, message);

        } else if ("Bookmark:Insert".equals(event)) {
            final BrowserDB db = BrowserDB.from(getProfile());
            final boolean bookmarkAdded = db.addBookmark(
                    getContentResolver(), message.getString("title"), message.getString("url"));
            final int resId = bookmarkAdded ? R.string.bookmark_added
                                            : R.string.bookmark_already_added;
            ThreadUtils.postToUiThread(new Runnable() {
                @Override
                public void run() {
                    SnackbarBuilder.builder(GeckoApp.this)
                            .message(resId)
                            .duration(Snackbar.LENGTH_LONG)
                            .buildAndShow();
                }
            });

        } else if ("Contact:Add".equals(event)) {
            final String email = message.getString("email");
            final String phone = message.getString("phone");
            if (email != null) {
                Uri contactUri = Uri.parse(email);
                Intent i = new Intent(ContactsContract.Intents.SHOW_OR_CREATE_CONTACT, contactUri);
                startActivity(i);
            } else if (phone != null) {
                Uri contactUri = Uri.parse(phone);
                Intent i = new Intent(ContactsContract.Intents.SHOW_OR_CREATE_CONTACT, contactUri);
                startActivity(i);
            } else {
                // something went wrong.
                Log.e(LOGTAG, "Received Contact:Add message with no email nor phone number");
            }

        } else if ("DevToolsAuth:Scan".equals(event)) {
            DevToolsAuthHelper.scan(this, callback);

        } else if ("DOMFullScreen:Start".equals(event)) {
            // Local ref to layerView for thread safety
            LayerView layerView = mLayerView;
            if (layerView != null) {
                layerView.setFullScreenState(message.getBoolean("rootElement")
                        ? FullScreenState.ROOT_ELEMENT : FullScreenState.NON_ROOT_ELEMENT);
            }

        } else if ("DOMFullScreen:Stop".equals(event)) {
            // Local ref to layerView for thread safety
            LayerView layerView = mLayerView;
            if (layerView != null) {
                layerView.setFullScreenState(FullScreenState.NONE);
            }

        } else if ("Image:SetAs".equals(event)) {
            String src = message.getString("url");
            setImageAs(src);

        } else if ("Locale:Set".equals(event)) {
            setLocale(message.getString("locale"));

        } else if ("Permissions:Data".equals(event)) {
            final GeckoBundle[] permissions = message.getBundleArray("permissions");
            showSiteSettingsDialog(permissions);

        } else if ("PrivateBrowsing:Data".equals(event)) {
            mPrivateBrowsingSession = message.getString("session");

        } else if ("SystemUI:Visibility".equals(event)) {
            if (message.getBoolean("visible", true)) {
                mMainLayout.setSystemUiVisibility(View.SYSTEM_UI_FLAG_VISIBLE);
            } else {
                mMainLayout.setSystemUiVisibility(View.SYSTEM_UI_FLAG_LOW_PROFILE);
            }

        } else if ("ToggleChrome:Focus".equals(event)) {
            focusChrome();

        } else if ("ToggleChrome:Hide".equals(event)) {
            toggleChrome(false);

        } else if ("ToggleChrome:Show".equals(event)) {
            toggleChrome(true);

        } else if ("Update:Check".equals(event)) {
            UpdateServiceHelper.checkForUpdate(this);

        } else if ("Update:Download".equals(event)) {
            UpdateServiceHelper.downloadUpdate(this);

        } else if ("Update:Install".equals(event)) {
            UpdateServiceHelper.applyUpdate(this);

        } else if ("Mma:reader_available".equals(event)) {
            MmaDelegate.track(READER_AVAILABLE);

        } else if ("Mma:web_save_media".equals(event) || "Mma:web_save_image".equals(event)) {
            MmaDelegate.track(DOWNLOAD_MEDIA_SAVED_IMAGE);

        }

    }

    /**
     * To get a presenter which will response for text-selection. In preMarshmallow Android we want
     * to provide different UI action when user select a text. Text-selection class will uses this
     * presenter to trigger UI updating.
     *
     * @return a presenter which handle showing/hiding of action mode UI. return *null* if this
     * activity doesn't handle any text-selection event.
     */
    protected ActionModePresenter getTextSelectPresenter() {
        return null;
    }

    /**
     * @param permissions
     *        Array of JSON objects to represent site permissions.
     *        Example: { type: "offline-app", setting: "Store Offline Data", value: "Allow" }
     */
    private void showSiteSettingsDialog(final GeckoBundle[] permissions) {
        final AlertDialog.Builder builder = new AlertDialog.Builder(this);
        builder.setTitle(R.string.site_settings_title);

        final ArrayList<HashMap<String, String>> itemList =
                new ArrayList<HashMap<String, String>>();
        for (final GeckoBundle permObj : permissions) {
            final HashMap<String, String> map = new HashMap<String, String>();
            map.put("setting", permObj.getString("setting"));
            map.put("value", permObj.getString("value"));
            itemList.add(map);
        }

        // setMultiChoiceItems doesn't support using an adapter, so we're creating a hack with
        // setSingleChoiceItems and changing the choiceMode below when we create the dialog
        builder.setSingleChoiceItems(new SimpleAdapter(
            GeckoApp.this,
            itemList,
            R.layout.site_setting_item,
            new String[] { "setting", "value" },
            new int[] { R.id.setting, R.id.value }
            ), -1, new DialogInterface.OnClickListener() {
                @Override
                public void onClick(DialogInterface dialog, int id) { }
            });

        builder.setPositiveButton(R.string.site_settings_clear, new DialogInterface.OnClickListener() {
            @Override
            public void onClick(DialogInterface dialog, int id) {
                ListView listView = ((AlertDialog) dialog).getListView();
                SparseBooleanArray checkedItemPositions = listView.getCheckedItemPositions();

                // An array of the indices of the permissions we want to clear.
                final ArrayList<Integer> permissionsToClear = new ArrayList<>();
                for (int i = 0; i < checkedItemPositions.size(); i++) {
                    if (checkedItemPositions.valueAt(i)) {
                        permissionsToClear.add(checkedItemPositions.keyAt(i));
                    }
                }

                final GeckoBundle data = new GeckoBundle(1);
                data.putIntArray("permissions", permissionsToClear);
                EventDispatcher.getInstance().dispatch("Permissions:Clear", data);
            }
        });

        builder.setNegativeButton(R.string.site_settings_cancel, new DialogInterface.OnClickListener() {
            @Override
            public void onClick(DialogInterface dialog, int id) {
                dialog.cancel();
            }
        });

        AlertDialog dialog = builder.create();
        dialog.show();

        final ListView listView = dialog.getListView();
        if (listView != null) {
            listView.setChoiceMode(ListView.CHOICE_MODE_MULTIPLE);
        }

        final Button clearButton = dialog.getButton(DialogInterface.BUTTON_POSITIVE);
        clearButton.setEnabled(false);

        dialog.getListView().setOnItemClickListener(new AdapterView.OnItemClickListener() {
            @Override
            public void onItemClick(AdapterView<?> adapterView, View view, int i, long l) {
                if (listView.getCheckedItemCount() == 0) {
                    clearButton.setEnabled(false);
                } else {
                    clearButton.setEnabled(true);
                }
            }
        });
    }

    private void showSetImageResult(final boolean success, final int message, final String path) {
        ThreadUtils.postToUiThread(new Runnable() {
            @Override
            public void run() {
                if (!success) {
                    SnackbarBuilder.builder(GeckoApp.this)
                            .message(message)
                            .duration(Snackbar.LENGTH_LONG)
                            .buildAndShow();
                    return;
                }

                final Intent intent = new Intent(Intent.ACTION_ATTACH_DATA);
                intent.addCategory(Intent.CATEGORY_DEFAULT);
                intent.setData(Uri.parse(path));

                // Removes the image from storage once the chooser activity ends.
                Intent chooser = Intent.createChooser(intent, getString(message));
                ActivityResultHandler handler = new ActivityResultHandler() {
                    @Override
                    public void onActivityResult (int resultCode, Intent data) {
                        getContentResolver().delete(intent.getData(), null, null);
                    }
                };
                ActivityHandlerHelper.startIntentForActivity(GeckoApp.this, chooser, handler);
            }
        });
    }

    // Checks the necessary permissions before attempting to download and set the image as wallpaper.
    private void setImageAs(final String aSrc) {
        Permissions
                .from(this)
                .onBackgroundThread()
                .withPermissions(Manifest.permission.WRITE_EXTERNAL_STORAGE)
                .andFallback(new Runnable() {
                    @Override
                    public void run() {
                        showSetImageResult(/* success */ false, R.string.set_image_path_fail, null);
                    }
                })
                .run(new Runnable() {
                    @Override
                    public void run() {
                        downloadImageForSetImage(aSrc);
                    }
                });
    }


    /**
     * Downloads the image given by <code>aSrc</code> synchronously and then displays the Chooser
     * activity to set the image as wallpaper.
     *
     * @param aSrc The URI to download the image from.
     */
    private void downloadImageForSetImage(final String aSrc) {
        // Network access from the main thread can cause a StrictMode crash on release builds.
        ThreadUtils.assertOnBackgroundThread();

        boolean isDataURI = aSrc.startsWith("data:");
        Bitmap image = null;
        InputStream is = null;
        ByteArrayOutputStream os = null;
        try {
            if (isDataURI) {
                int dataStart = aSrc.indexOf(",");
                byte[] buf = Base64.decode(aSrc.substring(dataStart + 1), Base64.DEFAULT);
                image = BitmapUtils.decodeByteArray(buf);
            } else {
                int byteRead;
                byte[] buf = new byte[4192];
                os = new ByteArrayOutputStream();
                URL url = new URL(aSrc);
                is = url.openStream();

                // Cannot read from same stream twice. Also, InputStream from
                // URL does not support reset. So converting to byte array.

                while ((byteRead = is.read(buf)) != -1) {
                    os.write(buf, 0, byteRead);
                }
                byte[] imgBuffer = os.toByteArray();
                image = BitmapUtils.decodeByteArray(imgBuffer);
            }
            if (image != null) {
                // Some devices don't have a DCIM folder and the Media.insertImage call will fail.
                File dcimDir = Environment.getExternalStoragePublicDirectory(Environment.DIRECTORY_PICTURES);

                if (!dcimDir.mkdirs() && !dcimDir.isDirectory()) {
                    showSetImageResult(/* success */ false, R.string.set_image_path_fail, null);
                    return;
                }
                String path = Media.insertImage(getContentResolver(), image, null, null);
                if (path == null) {
                    showSetImageResult(/* success */ false, R.string.set_image_path_fail, null);
                    return;
                }
                showSetImageResult(/* success */ true, R.string.set_image_chooser_title, path);
            } else {
                showSetImageResult(/* success */ false, R.string.set_image_fail, null);
            }
        } catch (OutOfMemoryError ome) {
            Log.e(LOGTAG, "Out of Memory when converting to byte array", ome);
        } catch (IOException ioe) {
            Log.e(LOGTAG, "I/O Exception while setting wallpaper", ioe);
        } finally {
            if (is != null) {
                try {
                    is.close();
                } catch (IOException ioe) {
                    Log.w(LOGTAG, "I/O Exception while closing stream", ioe);
                }
            }
            if (os != null) {
                try {
                    os.close();
                } catch (IOException ioe) {
                    Log.w(LOGTAG, "I/O Exception while closing stream", ioe);
                }
            }
        }
    }

    private int getBitmapSampleSize(BitmapFactory.Options options, int idealWidth, int idealHeight) {
        int width = options.outWidth;
        int height = options.outHeight;
        int inSampleSize = 1;
        if (height > idealHeight || width > idealWidth) {
            if (width > height) {
                inSampleSize = Math.round((float)height / idealHeight);
            } else {
                inSampleSize = Math.round((float)width / idealWidth);
            }
        }
        return inSampleSize;
    }

    public void requestRender() {
        mLayerView.requestRender();
    }

    @Override // GeckoView.ContentListener
    public void onTitleChange(final GeckoView view, final String title) {
    }

    @Override // GeckoView.ContentListener
    public void onFullScreen(final GeckoView view, final boolean fullScreen) {
        if (fullScreen) {
            SnackbarBuilder.builder(this)
                    .message(R.string.fullscreen_warning)
                    .duration(Snackbar.LENGTH_LONG).buildAndShow();
        }
        ThreadUtils.assertOnUiThread();
        ActivityUtils.setFullScreen(this, fullScreen);
    }

    protected void setFullScreen(final boolean fullscreen) {
        ThreadUtils.postToUiThread(new Runnable() {
            @Override
            public void run() {
                onFullScreen(mLayerView, fullscreen);
            }
        });
    }

    /**
     * Check and start the Java profiler if MOZ_PROFILER_STARTUP env var is specified.
     **/
    protected static void earlyStartJavaSampler(SafeIntent intent) {
        String env = intent.getStringExtra("env0");
        for (int i = 1; env != null; i++) {
            if (env.startsWith("MOZ_PROFILER_STARTUP=")) {
                if (!env.endsWith("=")) {
                    GeckoJavaSampler.start(10, 1000);
                    Log.d(LOGTAG, "Profiling Java on startup");
                }
                break;
            }
            env = intent.getStringExtra("env" + i);
        }
    }

    /**
     * Called when the activity is first created.
     *
     * Here we initialize all of our profile settings, Firefox Health Report,
     * and other one-shot constructions.
     **/
    @Override
    public void onCreate(Bundle savedInstanceState) {
        // Enable Android Strict Mode for developers' local builds (the "default" channel).
        if ("default".equals(AppConstants.MOZ_UPDATE_CHANNEL)) {
            enableStrictMode();
        }

        // Mozglue should already be loaded by BrowserApp.onCreate() in Fennec, but in
        // custom tabs it may not be.
        GeckoLoader.loadMozGlue(getApplicationContext());

        if (!HardwareUtils.isSupportedSystem() || !GeckoLoader.neonCompatible()) {
            // This build does not support the Android version of the device: Show an error and finish the app.
            mIsAbortingAppLaunch = true;
            super.onCreate(savedInstanceState);
            showSDKVersionError();
            finish();
            return;
        }

        // The clock starts...now. Better hurry!
        mJavaUiStartupTimer = new Telemetry.UptimeTimer("FENNEC_STARTUP_TIME_JAVAUI");
        mGeckoReadyStartupTimer = new Telemetry.UptimeTimer("FENNEC_STARTUP_TIME_GECKOREADY");

        final SafeIntent intent = new SafeIntent(getIntent());

        earlyStartJavaSampler(intent);

        // GeckoLoader wants to dig some environment variables out of the
        // incoming intent, so pass it in here. GeckoLoader will do its
        // business later and dispose of the reference.
        GeckoLoader.setLastIntent(intent);

        // Workaround for <http://code.google.com/p/android/issues/detail?id=20915>.
        try {
            Class.forName("android.os.AsyncTask");
        } catch (ClassNotFoundException e) { }

        GeckoAppShell.setScreenOrientationDelegate(this);

        // Tell Stumbler to register a local broadcast listener to listen for preference intents.
        // We do this via intents since we can't easily access Stumbler directly,
        // as it might be compiled outside of Fennec.
        getApplicationContext().sendBroadcast(
                new Intent(INTENT_REGISTER_STUMBLER_LISTENER)
        );

        // Did the OS locale change while we were backgrounded? If so,
        // we need to die so that Gecko will re-init add-ons that touch
        // the UI.
        // This is using a sledgehammer to crack a nut, but it'll do for
        // now.
        // Our OS locale pref will be detected as invalid after the
        // restart, and will be propagated to Gecko accordingly, so there's
        // no need to touch that here.
        if (BrowserLocaleManager.getInstance().systemLocaleDidChange()) {
            Log.i(LOGTAG, "System locale changed. Restarting.");
            finishAndShutdown(/* restart */ true);
            return;
        }

        if (sAlreadyLoaded) {
            // This happens when the GeckoApp activity is destroyed by Android
            // without killing the entire application (see Bug 769269).
            // Now that we've got multiple GeckoApp-based activities, this can
            // also happen if we're not the first activity to run within a session.
            mIsRestoringActivity = true;
            Telemetry.addToHistogram("FENNEC_RESTORING_ACTIVITY", 1);

        } else {
            final String action = intent.getAction();
            final String args = GeckoApplication.addDefaultGeckoArgs(
                    intent.getStringExtra("args"));
            final int flags = ACTION_DEBUG.equals(action) ? GeckoThread.FLAG_DEBUGGING : 0;

            sAlreadyLoaded = true;
            GeckoThread.initMainProcess(/* profile */ null, args, flags);

            // Speculatively pre-fetch the profile in the background.
            ThreadUtils.postToBackgroundThread(new Runnable() {
                @Override
                public void run() {
                    getProfile();
                }
            });

            final String uri = getURIFromIntent(intent);
            if (!TextUtils.isEmpty(uri)) {
                // Start a speculative connection as soon as Gecko loads.
                GeckoThread.speculativeConnect(uri);
            }
        }

        // To prevent races, register startup events before launching the Gecko thread.
        EventDispatcher.getInstance().registerGeckoThreadListener(this,
            "Accessibility:Ready",
            "Gecko:Ready",
            null);

        EventDispatcher.getInstance().registerUiThreadListener(this,
            "Update:Check",
            "Update:Download",
            "Update:Install",
            null);

        GeckoThread.launch();

        Bundle stateBundle = IntentUtils.getBundleExtraSafe(getIntent(), EXTRA_STATE_BUNDLE);
        if (stateBundle != null) {
            // Use the state bundle if it was given as an intent extra. This is
            // only intended to be used internally via Robocop, so a boolean
            // is read from a private shared pref to prevent other apps from
            // injecting states.
            final SharedPreferences prefs = getSharedPreferences();
            if (prefs.getBoolean(PREFS_ALLOW_STATE_BUNDLE, false)) {
                prefs.edit().remove(PREFS_ALLOW_STATE_BUNDLE).apply();
                savedInstanceState = stateBundle;
            }
        } else if (savedInstanceState != null) {
            // Bug 896992 - This intent has already been handled; reset the intent.
            setIntent(new Intent(Intent.ACTION_MAIN));
        }

        super.onCreate(savedInstanceState);

        GeckoScreenOrientation.getInstance().update(getResources().getConfiguration().orientation);

        setContentView(getLayout());

        // Set up Gecko layout.
        mRootLayout = (RelativeLayout) findViewById(R.id.root_layout);
        mGeckoLayout = (RelativeLayout) findViewById(R.id.gecko_layout);
        mMainLayout = (RelativeLayout) findViewById(R.id.main_layout);
        mLayerView = (GeckoView) findViewById(R.id.layer_view);

        mLayerView.setChromeUri("chrome://browser/content/browser.xul");
        mLayerView.setContentListener(this);

        getAppEventDispatcher().registerGeckoThreadListener(this,
            "Accessibility:Event",
            "Locale:Set",
            null);

        getAppEventDispatcher().registerBackgroundThreadListener(this,
            "Bookmark:Insert",
            "Image:SetAs",
            null);

        getAppEventDispatcher().registerUiThreadListener(this,
            "Contact:Add",
            "DevToolsAuth:Scan",
            "DOMFullScreen:Start",
            "DOMFullScreen:Stop",
            "Mma:reader_available",
            "Mma:web_save_image",
            "Mma:web_save_media",
            "Permissions:Data",
            "PrivateBrowsing:Data",
            "SystemUI:Visibility",
            "ToggleChrome:Focus",
            "ToggleChrome:Hide",
            "ToggleChrome:Show",
            null);

        Tabs.getInstance().attachToContext(this, mLayerView, getAppEventDispatcher());
        Tabs.registerOnTabsChangedListener(this);

        // Use global layout state change to kick off additional initialization
        mMainLayout.getViewTreeObserver().addOnGlobalLayoutListener(this);

        mTextSelection = TextSelection.Factory.create(mLayerView, getTextSelectPresenter());
        mTextSelection.create();

        final Bundle finalSavedInstanceState = savedInstanceState;
        ThreadUtils.postToBackgroundThread(new Runnable() {
            @Override
            public void run() {
                // Determine whether we should restore tabs.
                mLastSessionCrashed = updateCrashedState();
                mShouldRestore = getSessionRestoreState(finalSavedInstanceState);
                if (mShouldRestore && finalSavedInstanceState != null) {
                    boolean wasInBackground =
                            finalSavedInstanceState.getBoolean(SAVED_STATE_IN_BACKGROUND, false);

                    // Don't log OOM-kills if only one activity was destroyed. (For example
                    // from "Don't keep activities" on ICS)
                    if (!wasInBackground && !mIsRestoringActivity) {
                        Telemetry.addToHistogram("FENNEC_WAS_KILLED", 1);
                    }

                    mPrivateBrowsingSession =
                            finalSavedInstanceState.getString(SAVED_STATE_PRIVATE_SESSION);
                }

                // If we are doing a restore, read the session data so we can send it to Gecko later.
                GeckoBundle restoreMessage = null;
                if (!mIsRestoringActivity && mShouldRestore) {
                    final boolean isExternalURL = invokedWithExternalURL(getIntentURI(new SafeIntent(getIntent())));
                    try {
                        // restoreSessionTabs() will create simple tab stubs with the
                        // URL and title for each page, but we also need to restore
                        // session history. restoreSessionTabs() will inject the IDs
                        // of the tab stubs into the JSON data (which holds the session
                        // history). This JSON data is then sent to Gecko so session
                        // history can be restored for each tab.
                        restoreMessage = restoreSessionTabs(isExternalURL, false);
                    } catch (SessionRestoreException e) {
                        // If mShouldRestore was set to false in restoreSessionTabs(), this means
                        // either that we intentionally skipped all tabs read from the session file,
                        // or else that the file was syntactically valid, but didn't contain any
                        // tabs (e.g. because the user cleared history), therefore we don't need
                        // to switch to the backup copy.
                        if (mShouldRestore) {
                            Log.e(LOGTAG, "An error occurred during restore, switching to backup file", e);
                            // To be on the safe side, we will always attempt to restore from the backup
                            // copy if we end up here.
                            // Since we will also hit this situation regularly during first run though,
                            // we'll only report it in telemetry if we failed to restore despite the
                            // file existing, which means it's very probably damaged.
                            if (getProfile().sessionFileExists()) {
                                Telemetry.addToHistogram("FENNEC_SESSIONSTORE_DAMAGED_SESSION_FILE", 1);
                            }
                            try {
                                restoreMessage = restoreSessionTabs(isExternalURL, true);
                                Telemetry.addToHistogram("FENNEC_SESSIONSTORE_RESTORING_FROM_BACKUP", 1);
                            } catch (SessionRestoreException ex) {
                                if (!mShouldRestore) {
                                    // Restoring only "failed" because the backup copy was deliberately empty, too.
                                    Telemetry.addToHistogram("FENNEC_SESSIONSTORE_RESTORING_FROM_BACKUP", 1);
                                } else {
                                    // Restoring the backup failed, too, so do a normal startup.
                                    Log.e(LOGTAG, "An error occurred during restore", ex);
                                    mShouldRestore = false;

                                    if (!getSharedPreferencesForProfile().
                                            getBoolean(PREFS_IS_FIRST_RUN, true)) {
                                        // Except when starting with a fresh profile, we should normally
                                        // always have a session file available, even if it might only
                                        // contain an empty window.
                                        Telemetry.addToHistogram("FENNEC_SESSIONSTORE_ALL_FILES_DAMAGED", 1);
                                    }
                                }
                            }
                        }
                    }
                }

                synchronized (GeckoApp.this) {
                    mSessionRestoreParsingFinished = true;
                    GeckoApp.this.notifyAll();
                }

                // If we are doing a restore, send the parsed session data to Gecko.
                if (!mIsRestoringActivity) {
                    getAppEventDispatcher().dispatch("Session:Restore", restoreMessage);
                }

                // Make sure sessionstore.old is either updated or deleted as necessary.
                getProfile().updateSessionFile(mShouldRestore);
            }
        });

        // Perform background initialization.
        ThreadUtils.postToBackgroundThread(new Runnable() {
            @Override
            public void run() {
                final SharedPreferences prefs = GeckoApp.this.getSharedPreferences();

                // Wait until now to set this, because we'd rather throw an exception than
                // have a caller of BrowserLocaleManager regress startup.
                final LocaleManager localeManager = BrowserLocaleManager.getInstance();
                localeManager.initialize(getApplicationContext());

                SessionInformation previousSession = SessionInformation.fromSharedPrefs(prefs);
                if (previousSession.wasKilled()) {
                    Telemetry.addToHistogram("FENNEC_WAS_KILLED", 1);
                }

                SharedPreferences.Editor editor = prefs.edit();
                editor.putBoolean(GeckoAppShell.PREFS_OOM_EXCEPTION, false);

                // Put a flag to check if we got a normal `onSaveInstanceState`
                // on exit, or if we were suddenly killed (crash or native OOM).
                editor.putBoolean(GeckoApp.PREFS_WAS_STOPPED, false);

                editor.apply();

                // The lifecycle of mHealthRecorder is "shortly after onCreate"
                // through "onDestroy" -- essentially the same as the lifecycle
                // of the activity itself.
                final String profilePath = getProfile().getDir().getAbsolutePath();
                final EventDispatcher dispatcher = getAppEventDispatcher();

                // This is the locale prior to fixing it up.
                final Locale osLocale = Locale.getDefault();

                // Both of these are Java-format locale strings: "en_US", not "en-US".
                final String osLocaleString = osLocale.getLanguage() + "_" + osLocale.getCountry();
                String appLocaleString = localeManager.getAndApplyPersistedLocale(GeckoApp.this);
                Log.d(LOGTAG, "OS locale is " + osLocaleString + ", app locale is " + appLocaleString);

                if (appLocaleString == null) {
                    appLocaleString = osLocaleString;
                }

                mHealthRecorder = GeckoApp.this.createHealthRecorder(GeckoApp.this,
                                                                     profilePath,
                                                                     dispatcher,
                                                                     osLocaleString,
                                                                     appLocaleString,
                                                                     previousSession);

                final String uiLocale = appLocaleString;
                ThreadUtils.postToUiThread(new Runnable() {
                    @Override
                    public void run() {
                        GeckoApp.this.onLocaleReady(uiLocale);
                    }
                });

                // We use per-profile prefs here, because we're tracking against
                // a Gecko pref. The same applies to the locale switcher!
                BrowserLocaleManager.storeAndNotifyOSLocale(getSharedPreferencesForProfile(), osLocale);
            }
        });
    }

    @Override
    public void onStart() {
        super.onStart();
        if (mIsAbortingAppLaunch) {
            return;
        }

        mWasFirstTabShownAfterActivityUnhidden = false; // onStart indicates we were hidden.
    }

    @Override
    protected void onStop() {
        super.onStop();
        // Overriding here is not necessary, but we do this so we don't
        // forget to add the abort if we override this method later.
        if (mIsAbortingAppLaunch) {
            return;
        }
    }


    /**
     * Derived classes may call this if they require something to be done *after* they've
     * done their onStop() handling.
     */
    protected void onAfterStop() {
        final SharedPreferences sharedPrefs = getSharedPreferencesForProfile();
        if (sharedPrefs.getBoolean(PREFS_IS_FIRST_RUN, true)) {
            sharedPrefs.edit().putBoolean(PREFS_IS_FIRST_RUN, false).apply();
        }
    }

    /**
     * At this point, the resource system and the rest of the browser are
     * aware of the locale.
     *
     * Now we can display strings!
     *
     * You can think of this as being something like a second phase of onCreate,
     * where you can do string-related operations. Use this in place of embedding
     * strings in view XML.
     *
     * By contrast, onConfigurationChanged does some locale operations, but is in
     * response to device changes.
     */
    @Override
    public void onLocaleReady(final String locale) {
        if (!ThreadUtils.isOnUiThread()) {
            throw new RuntimeException("onLocaleReady must always be called from the UI thread.");
        }

        final Locale loc = Locales.parseLocaleCode(locale);
        if (loc.equals(mLastLocale)) {
            Log.d(LOGTAG, "New locale same as old; onLocaleReady has nothing to do.");
        }
        BrowserLocaleManager.getInstance().updateConfiguration(GeckoApp.this, loc);
        ViewUtil.setLayoutDirection(getWindow().getDecorView(), loc);
        refreshChrome();

        // The URL bar hint needs to be populated.
        TextView urlBar = (TextView) findViewById(R.id.url_bar_title);
        if (urlBar != null) {
            final String hint = getResources().getString(R.string.url_bar_default_text);
            urlBar.setHint(hint);
        } else {
            Log.d(LOGTAG, "No URL bar in GeckoApp. Not loading localized hint string.");
        }

        mLastLocale = loc;

        // Allow onConfigurationChanged to take care of the rest.
        // We don't call this.onConfigurationChanged, because (a) that does
        // work that's unnecessary after this locale action, and (b) it can
        // cause a loop! See Bug 1011008, Comment 12.
        super.onConfigurationChanged(getResources().getConfiguration());
    }

    protected void initializeChrome() {
        mDoorHangerPopup = new DoorHangerPopup(this, getAppEventDispatcher());
        mDoorHangerPopup.setOnVisibilityChangeListener(this);
        mFormAssistPopup = (FormAssistPopup) findViewById(R.id.form_assist_popup);
        mFormAssistPopup.create(mLayerView);
    }

    @Override
    public void onDoorHangerShow() {
        final View overlay = getDoorhangerOverlay();
        if (overlay != null) {
            final Animator alphaAnimator = ObjectAnimator.ofFloat(overlay, "alpha", 1);
            alphaAnimator.setDuration(250);

            alphaAnimator.start();
        }
    }

    @Override
    public void onDoorHangerHide() {
        final View overlay = getDoorhangerOverlay();
        if (overlay != null) {
            final Animator alphaAnimator = ObjectAnimator.ofFloat(overlay, "alpha", 0);
            alphaAnimator.setDuration(200);

            alphaAnimator.start();
        }
    }

    /**
     * Loads the initial tab at Fennec startup. If we don't restore tabs, this
     * tab will be about:home, or the homepage if the user has set one.
     * If we've temporarily disabled restoring to break out of a crash loop, we'll show
     * the Recent Tabs folder of the Combined History panel, so the user can manually
     * restore tabs as needed.
     * If we restore tabs, we don't need to create a new tab, unless launch intent specify action
     * to be #android.Intent.ACTION_VIEW, which is launched from widget to create a new tab.
     */
    protected void loadStartupTab(final int flags, String action) {
        if (!mShouldRestore || Intent.ACTION_VIEW.equals(action)) {
            if (mLastSessionCrashed) {
                // The Recent Tabs panel no longer exists, but BrowserApp will redirect us
                // to the Recent Tabs folder of the Combined History panel.
                Tabs.getInstance().loadUrl(AboutPages.getURLForBuiltinPanelType(PanelType.DEPRECATED_RECENT_TABS), flags);
            } else {
                Tabs.getInstance().loadUrl(Tabs.getHomepageForStartupTab(this), flags);
            }
        }
    }

    /**
     * Loads the initial tab at Fennec startup. This tab will load with the given
     * external URL. If that URL is invalid, a startup tab will be loaded.
     *
     * @param url    External URL to load.
     * @param intent External intent whose extras modify the request
     * @param flags  Flags used to load the load
     */
    protected void loadStartupTab(final String url, final SafeIntent intent, final int flags) {
        // Invalid url
        if (url == null) {
            loadStartupTab(flags, intent.getAction());
            return;
        }

        Tabs.getInstance().loadUrlWithIntentExtras(url, intent, flags);
    }

    protected String getIntentURI(SafeIntent intent) {
        final String passedUri;
        final String uri = getURIFromIntent(intent);

        if (!TextUtils.isEmpty(uri)) {
            passedUri = uri;
        } else {
            passedUri = null;
        }
        return passedUri;
    }

    private boolean invokedWithExternalURL(String uri) {
        return uri != null && !AboutPages.isAboutHome(uri);
    }

    protected int getNewTabFlags() {
        final boolean isFirstTab = !mWasFirstTabShownAfterActivityUnhidden;

        final SafeIntent intent = new SafeIntent(getIntent());
        final String action = intent.getAction();

        int flags = Tabs.LOADURL_NEW_TAB | Tabs.LOADURL_USER_ENTERED | Tabs.LOADURL_EXTERNAL;
        if (ACTION_HOMESCREEN_SHORTCUT.equals(action)) {
            flags |= Tabs.LOADURL_PINNED;
        }
        if (isFirstTab) {
            flags |= Tabs.LOADURL_FIRST_AFTER_ACTIVITY_UNHIDDEN;
        }

        return flags;
    }

    private void initialize() {
        mInitialized = true;

        mWasFirstTabShownAfterActivityUnhidden = true; // Reset since we'll be loading a tab.

        final SafeIntent intent = new SafeIntent(getIntent());
        final String action = intent.getAction();

        final String passedUri = getIntentURI(intent);

        final boolean isExternalURL = passedUri != null;
        final boolean isAboutHomeURL = isExternalURL && AboutPages.isAboutHome(passedUri);

        // Start migrating as early as possible, can do this in
        // parallel with Gecko load.
        checkMigrateProfile();

        initializeChrome();

        // We need to wait here because mShouldRestore can revert back to
        // false if a parsing error occurs and the startup tab we load
        // depends on whether we restore tabs or not.
        synchronized (this) {
            while (!mSessionRestoreParsingFinished) {
                try {
                    wait();
                } catch (final InterruptedException e) {
                    // Ignore and wait again.
                }
            }
        }

        if (mIsRestoringActivity && hasGeckoTab(intent)) {
            Tabs.getInstance().notifyListeners(null, Tabs.TabEvents.RESTORED);
            handleSelectTabIntent(intent);
        // External URLs and new tab from widget should always be loaded regardless of whether Gecko is
        // already running.
        } else if (isExternalURL) {
            // Restore tabs before opening an external URL so that the new tab
            // is animated properly.
            Tabs.getInstance().notifyListeners(null, Tabs.TabEvents.RESTORED);
            processActionViewIntent(new Runnable() {
                @Override
                public void run() {
                    if (isAboutHomeURL) {
                        // respect the user preferences for about:home from external intent calls
                        loadStartupTab(Tabs.LOADURL_NEW_TAB, action);
                    } else {
                        final int flags = getNewTabFlags();
                        loadStartupTab(passedUri, intent, flags);
                    }
                }
            });
        } else {
            if (!mIsRestoringActivity) {
                loadStartupTab(Tabs.LOADURL_NEW_TAB, action);
            }

            Tabs.getInstance().notifyListeners(null, Tabs.TabEvents.RESTORED);

            processTabQueue();
        }

        recordStartupActionTelemetry(passedUri, action);

        // Check if launched from data reporting notification.
        if (ACTION_LAUNCH_SETTINGS.equals(action)) {
            Intent settingsIntent = new Intent(GeckoApp.this, GeckoPreferences.class);
            // Copy extras.
            settingsIntent.putExtras(intent.getUnsafe());
            startActivity(settingsIntent);
        }

        mPromptService = new PromptService(this, getAppEventDispatcher());

        // Trigger the completion of the telemetry timer that wraps activity startup,
        // then grab the duration to give to FHR.
        mJavaUiStartupTimer.stop();
        final long javaDuration = mJavaUiStartupTimer.getElapsed();

        ThreadUtils.getBackgroundHandler().postDelayed(new Runnable() {
            @Override
            public void run() {
                final HealthRecorder rec = mHealthRecorder;
                if (rec != null) {
                    rec.recordJavaStartupTime(javaDuration);
                }
            }
        }, 50);

        final int updateServiceDelay = 30 * 1000;
        ThreadUtils.getBackgroundHandler().postDelayed(new Runnable() {
            @Override
            public void run() {
                UpdateServiceHelper.registerForUpdates(GeckoAppShell.getApplicationContext());
            }
        }, updateServiceDelay);

        if (mIsRestoringActivity) {
            Tab selectedTab = Tabs.getInstance().getSelectedTab();
            if (selectedTab != null) {
                Tabs.getInstance().notifyListeners(selectedTab, Tabs.TabEvents.SELECTED);
            }

            if (GeckoThread.isRunning()) {
                geckoConnected();
            }
        }
    }

    @TargetApi(Build.VERSION_CODES.JELLY_BEAN)
    @Override
    public void onGlobalLayout() {
        if (Versions.preJB) {
            mMainLayout.getViewTreeObserver().removeGlobalOnLayoutListener(this);
        } else {
            mMainLayout.getViewTreeObserver().removeOnGlobalLayoutListener(this);
        }
        if (!mInitialized) {
            initialize();
        }
    }

    protected void processActionViewIntent(final Runnable openTabsRunnable) {
        // We need to ensure that if we receive a VIEW action and there are tabs queued then the
        // site loaded from the intent is on top (last loaded) and selected with all other tabs
        // being opened behind it. We process the tab queue first and request a callback from the JS - the
        // listener will open the url from the intent as normal when the tab queue has been processed.
        ThreadUtils.postToBackgroundThread(new Runnable() {
            @Override
            public void run() {
                if (TabQueueHelper.TAB_QUEUE_ENABLED && TabQueueHelper.shouldOpenTabQueueUrls(GeckoApp.this)) {

                    getAppEventDispatcher().registerUiThreadListener(new BundleEventListener() {
                        @Override
                        public void handleMessage(String event, GeckoBundle message, EventCallback callback) {
                            if ("Tabs:TabsOpened".equals(event)) {
                                getAppEventDispatcher().unregisterUiThreadListener(this, "Tabs:TabsOpened");
                                openTabsRunnable.run();
                            }
                        }
                    }, "Tabs:TabsOpened");
                    TabQueueHelper.openQueuedUrls(GeckoApp.this, getProfile(), TabQueueHelper.FILE_NAME, true);
                } else {
                    openTabsRunnable.run();
                }
            }
        });
    }

    @WorkerThread
    private GeckoBundle restoreSessionTabs(final boolean isExternalURL, boolean useBackup)
            throws SessionRestoreException {
        String sessionString = getProfile().readSessionFile(useBackup);
        if (sessionString == null) {
            throw new SessionRestoreException("Could not read from session file");
        }

        // If we are doing an OOM restore, parse the session data and
        // stub the restored tabs immediately. This allows the UI to be
        // updated before Gecko has restored.
        final JSONArray tabs = new JSONArray();
        final JSONObject windowObject = new JSONObject();
        final boolean sessionDataValid;

        LastSessionParser parser = new LastSessionParser(tabs, windowObject, isExternalURL);

        if (mPrivateBrowsingSession == null) {
            sessionDataValid = parser.parse(sessionString);
        } else {
            sessionDataValid = parser.parse(sessionString, mPrivateBrowsingSession);
        }

        if (tabs.length() > 0) {
            try {
                // Update all parent tab IDs ...
                parser.updateParentId(tabs);
                windowObject.put("tabs", tabs);
                // ... and for recently closed tabs as well (if we've got any).
                final JSONArray closedTabs = windowObject.optJSONArray("closedTabs");
                parser.updateParentId(closedTabs);
                windowObject.putOpt("closedTabs", closedTabs);

                sessionString = new JSONObject().put(
                        "windows", new JSONArray().put(windowObject)).toString();
            } catch (final JSONException e) {
                throw new SessionRestoreException(e);
            }
        } else {
            if (parser.allTabsSkipped() || sessionDataValid) {
                // If we intentionally skipped all tabs we've read from the session file, we
                // set mShouldRestore back to false at this point already, so the calling code
                // can infer that the exception wasn't due to a damaged session store file.
                // The same applies if the session file was syntactically valid and
                // simply didn't contain any tabs.
                mShouldRestore = false;
            }
            throw new SessionRestoreException("No tabs could be read from session file");
        }

        final GeckoBundle restoreData = new GeckoBundle(1);
        restoreData.putString("sessionString", sessionString);
        return restoreData;
    }

    @RobocopTarget
    public @NonNull EventDispatcher getAppEventDispatcher() {
        if (mLayerView == null) {
            throw new IllegalStateException("Must not call getAppEventDispatcher() until after onCreate()");
        }

        return mLayerView.getEventDispatcher();
    }

    protected static GeckoProfile getProfile() {
        return GeckoThread.getActiveProfile();
    }

    /**
     * Check whether we've crashed during the last browsing session.
     *
     * @return True if the crash reporter ran after the last session.
     */
    protected boolean updateCrashedState() {
        try {
            File crashFlag = new File(GeckoProfileDirectories.getMozillaDirectory(this), "CRASHED");
            if (crashFlag.exists() && crashFlag.delete()) {
                // Set the flag that indicates we were stopped as expected, as
                // the crash reporter has run, so it is not a silent OOM crash.
                getSharedPreferences().edit().putBoolean(PREFS_WAS_STOPPED, true).apply();
                return true;
            }
        } catch (NoMozillaDirectoryException e) {
            // If we can't access the Mozilla directory, we're in trouble anyway.
            Log.e(LOGTAG, "Cannot read crash flag: ", e);
        }
        return false;
    }

    /**
     * Determine whether the session should be restored.
     *
     * @param savedInstanceState Saved instance state given to the activity
     * @return                   Whether to restore
     */
    protected boolean getSessionRestoreState(Bundle savedInstanceState) {
        final SharedPreferences prefs = getSharedPreferences();
        boolean shouldRestore = false;

        final int versionCode = getVersionCode();
        if (getSessionRestoreResumeOnce(prefs)) {
            shouldRestore = true;
        } else if (mLastSessionCrashed) {
            if (incrementCrashCount(prefs) <= getSessionStoreMaxCrashResumes(prefs) &&
                    getSessionRestoreAfterCrashPreference(prefs)) {
                shouldRestore = true;
            } else {
                shouldRestore = false;
            }
        } else if (prefs.getInt(PREFS_VERSION_CODE, 0) != versionCode) {
            // If the version has changed, the user has done an upgrade, so restore
            // previous tabs.
            prefs.edit().putInt(PREFS_VERSION_CODE, versionCode).apply();
            shouldRestore = true;
        } else if (savedInstanceState != null ||
                   getSessionRestorePreference(prefs).equals("always") ||
                   getRestartFromIntent()) {
            // We're coming back from a background kill by the OS, the user
            // has chosen to always restore, or we restarted.
            shouldRestore = true;
        }

        return shouldRestore;
    }

    private boolean getSessionRestoreResumeOnce(SharedPreferences prefs) {
        boolean resumeOnce = prefs.getBoolean(GeckoPreferences.PREFS_RESTORE_SESSION_ONCE, false);
        if (resumeOnce) {
            prefs.edit().putBoolean(GeckoPreferences.PREFS_RESTORE_SESSION_ONCE, false).apply();
        }
        return resumeOnce;
    }

    private int incrementCrashCount(SharedPreferences prefs) {
        final int crashCount = getSuccessiveCrashesCount(prefs) + 1;
        prefs.edit().putInt(PREFS_CRASHED_COUNT, crashCount).apply();
        return crashCount;
    }

    private int getSuccessiveCrashesCount(SharedPreferences prefs) {
        return prefs.getInt(PREFS_CRASHED_COUNT, 0);
    }

    private int getSessionStoreMaxCrashResumes(SharedPreferences prefs) {
        return prefs.getInt(GeckoPreferences.PREFS_RESTORE_SESSION_MAX_CRASH_RESUMES, 1);
    }

    private boolean getSessionRestoreAfterCrashPreference(SharedPreferences prefs) {
        return prefs.getBoolean(GeckoPreferences.PREFS_RESTORE_SESSION_FROM_CRASH, true);
    }

    private String getSessionRestorePreference(SharedPreferences prefs) {
        return prefs.getString(GeckoPreferences.PREFS_RESTORE_SESSION, "always");
    }

    private boolean getRestartFromIntent() {
        return IntentUtils.getBooleanExtraSafe(getIntent(), "didRestart", false);
    }

    /**
     * Enable Android StrictMode checks.
     * http://developer.android.com/reference/android/os/StrictMode.html
     */
    private void enableStrictMode() {
        Log.d(LOGTAG, "Enabling Android StrictMode");

        StrictMode.setThreadPolicy(new StrictMode.ThreadPolicy.Builder()
                                  .detectAll()
                                  .penaltyLog()
                                  // Match Android's default configuration - which we use on
                                  // automation builds, including release - for network access.
                                  .penaltyDeathOnNetwork()
                                  .build());

        StrictMode.setVmPolicy(new StrictMode.VmPolicy.Builder()
                               .detectAll()
                               .penaltyLog()
                               .build());
    }

    @Override
    protected void onNewIntent(Intent externalIntent) {
        final SafeIntent intent = new SafeIntent(externalIntent);
        final String action = intent.getAction();

        if (ACTION_SHUTDOWN.equals(action)) {
            PrefsHelper.getPref(GeckoPreferences.PREFS_SHUTDOWN_INTENT,
                                new PrefsHelper.PrefHandlerBase() {
                @Override public void prefValue(String pref, boolean value) {
                    if (value) {
                        mShutdownOnDestroy = true;
                        GeckoThread.forceQuit();
                    }
                }
            });
            return;
        }

        final boolean isFirstTab = !mWasFirstTabShownAfterActivityUnhidden;
        mWasFirstTabShownAfterActivityUnhidden = true; // Reset since we'll be loading a tab.

        // if we were previously OOM killed, we can end up here when launching
        // from external shortcuts, so set this as the intent for initialization
        if (!mInitialized) {
            setIntent(externalIntent);
            return;
        }

        final String uri = getURIFromIntent(intent);
        final String passedUri;
        if (!TextUtils.isEmpty(uri)) {
            passedUri = uri;
        } else {
            passedUri = null;
        }

        if (hasGeckoTab(intent)) {
            // This also covers ACTION_SWITCH_TAB.
            handleSelectTabIntent(intent);
        } else if (ACTION_LOAD.equals(action)) {
            Tabs.getInstance().loadUrl(intent.getDataString());
        } else if (Intent.ACTION_VIEW.equals(action)) {
            processActionViewIntent(new Runnable() {
                @Override
                public void run() {
                    final String url = intent.getDataString();
                    int flags = Tabs.LOADURL_NEW_TAB | Tabs.LOADURL_USER_ENTERED | Tabs.LOADURL_EXTERNAL;
                    if (isFirstTab) {
                        flags |= Tabs.LOADURL_FIRST_AFTER_ACTIVITY_UNHIDDEN;
                    }
                    Tabs.getInstance().loadUrlWithIntentExtras(url, intent, flags);
                }
            });
        } else if (ACTION_HOMESCREEN_SHORTCUT.equals(action)) {
            mLayerView.loadUri(uri, GeckoView.LOAD_SWITCH_TAB);
        } else if (Intent.ACTION_SEARCH.equals(action)) {
            mLayerView.loadUri(uri, GeckoView.LOAD_NEW_TAB);
        } else if (NotificationHelper.HELPER_BROADCAST_ACTION.equals(action)) {
            NotificationHelper.getInstance(getApplicationContext()).handleNotificationIntent(intent);
        } else if (ACTION_LAUNCH_SETTINGS.equals(action)) {
            // Check if launched from data reporting notification.
            Intent settingsIntent = new Intent(GeckoApp.this, GeckoPreferences.class);
            // Copy extras.
            settingsIntent.putExtras(intent.getUnsafe());
            startActivity(settingsIntent);
        }

        recordStartupActionTelemetry(passedUri, action);
    }

    /**
     * Check whether an intent with tab switch extras refers to a tab that
     * is actually existing at the moment.
     *
     * @param intent The intent to be checked.
     * @return True if the tab specified in the intent is existing in our Tabs list.
     */
    protected boolean hasGeckoTab(SafeIntent intent) {
        final int tabId = intent.getIntExtra(INTENT_EXTRA_TAB_ID, INVALID_TAB_ID);
        final String intentSessionUUID = intent.getStringExtra(INTENT_EXTRA_SESSION_UUID);
        final Tab tabToCheck = Tabs.getInstance().getTab(tabId);

        // We only care about comparing session UUIDs if one was specified in the intent.
        // Otherwise, we just try matching the tab ID with one of our open tabs.
        return tabToCheck != null && (!intent.hasExtra(INTENT_EXTRA_SESSION_UUID) ||
                GeckoApplication.getSessionUUID().equals(intentSessionUUID));
    }

    protected void handleSelectTabIntent(SafeIntent intent) {
        final int tabId = intent.getIntExtra(INTENT_EXTRA_TAB_ID, INVALID_TAB_ID);
        Tabs.getInstance().selectTab(tabId);
    }

    /**
     * Handles getting a URI from an intent in a way that is backwards-
     * compatible with our previous implementations.
     */
    protected String getURIFromIntent(SafeIntent intent) {
        final String action = intent.getAction();
        if (ACTION_ALERT_CALLBACK.equals(action) ||
                NotificationHelper.HELPER_BROADCAST_ACTION.equals(action)) {
            return null;
        }

        return intent.getDataString();
    }

    protected int getOrientation() {
        return GeckoScreenOrientation.getInstance().getAndroidOrientation();
    }

    @WrapForJNI(calledFrom = "gecko")
    public static void launchOrBringToFront() {
        final Activity activity = GeckoActivityMonitor.getInstance().getCurrentActivity();

        // Check that BrowserApp is not the current foreground activity.
        if (activity instanceof BrowserApp && ((GeckoApp) activity).foregrounded) {
            return;
        }

        Intent intent = new Intent(Intent.ACTION_MAIN);
        intent.setFlags(Intent.FLAG_ACTIVITY_NEW_TASK |
                        Intent.FLAG_ACTIVITY_REORDER_TO_FRONT);
        intent.setClassName(AppConstants.ANDROID_PACKAGE_NAME,
                            AppConstants.MOZ_ANDROID_BROWSER_INTENT_CLASS);
        GeckoAppShell.getApplicationContext().startActivity(intent);
    }

    @Override
    public void onResume()
    {
        // After an onPause, the activity is back in the foreground.
        // Undo whatever we did in onPause.
        super.onResume();

        if (mIsAbortingAppLaunch) {
            return;
        }

        foregrounded = true;

        GeckoAppShell.setScreenOrientationDelegate(this);

        int newOrientation = getResources().getConfiguration().orientation;
        if (GeckoScreenOrientation.getInstance().update(newOrientation)) {
            refreshChrome();
        }

        // We use two times: a pseudo-unique wall-clock time to identify the
        // current session across power cycles, and the elapsed realtime to
        // track the duration of the session.
        final long now = System.currentTimeMillis();
        final long realTime = android.os.SystemClock.elapsedRealtime();

        ThreadUtils.postToBackgroundThread(new Runnable() {
            @Override
            public void run() {
                // Now construct the new session on HealthRecorder's behalf. We do this here
                // so it can benefit from a single near-startup prefs commit.
                SessionInformation currentSession = new SessionInformation(now, realTime);

                SharedPreferences prefs = GeckoApp.this.getSharedPreferences();
                SharedPreferences.Editor editor = prefs.edit();
                editor.putBoolean(GeckoApp.PREFS_WAS_STOPPED, false);

                if (!mLastSessionCrashed) {
                    // The last session terminated normally,
                    // so we can reset the count of successive crashes.
                    editor.putInt(GeckoApp.PREFS_CRASHED_COUNT, 0);
                }

                currentSession.recordBegin(editor);
                editor.apply();

                final HealthRecorder rec = mHealthRecorder;
                if (rec != null) {
                    rec.setCurrentSession(currentSession);
                    rec.processDelayed();
                } else {
                    Log.w(LOGTAG, "Can't record session: rec is null.");
                }
            }
        });

        Restrictions.update(this);
    }

    @Override
    public void onWindowFocusChanged(boolean hasFocus) {
        super.onWindowFocusChanged(hasFocus);

        if (!mWindowFocusInitialized && hasFocus) {
            mWindowFocusInitialized = true;
            // XXX our editor tests require the GeckoView to have focus to pass, so we have to
            // manually shift focus to the GeckoView. requestFocus apparently doesn't work at
            // this stage of starting up, so we have to unset and reset the focusability.
            mLayerView.setFocusable(false);
            mLayerView.setFocusable(true);
            mLayerView.setFocusableInTouchMode(true);
            getWindow().setBackgroundDrawable(null);
        }
    }

    @Override
    public void onPause()
    {
        if (mIsAbortingAppLaunch) {
            super.onPause();
            return;
        }

        foregrounded = false;

        final HealthRecorder rec = mHealthRecorder;
        final Context context = this;

        // In some way it's sad that Android will trigger StrictMode warnings
        // here as the whole point is to save to disk while the activity is not
        // interacting with the user.
        ThreadUtils.postToBackgroundThread(new Runnable() {
            @Override
            public void run() {
                SharedPreferences prefs = GeckoApp.this.getSharedPreferences();
                SharedPreferences.Editor editor = prefs.edit();
                editor.putBoolean(GeckoApp.PREFS_WAS_STOPPED, true);
                if (rec != null) {
                    rec.recordSessionEnd("P", editor);
                }

                // onPause might in fact be called even after a crash, but in that case the
                // crash reporter will record this fact for us and we'll pick it up in onCreate.
                mLastSessionCrashed = false;

                // If we haven't done it before, cleanup any old files in our old temp dir
                if (prefs.getBoolean(GeckoApp.PREFS_CLEANUP_TEMP_FILES, true)) {
                    File tempDir = GeckoLoader.getGREDir(GeckoApp.this);
                    FileUtils.delTree(tempDir, new FileUtils.NameAndAgeFilter(null, ONE_DAY_MS), false);

                    editor.putBoolean(GeckoApp.PREFS_CLEANUP_TEMP_FILES, false);
                }

                editor.apply();
            }
        });

        GeckoAppShell.setScreenOrientationDelegate(null);

        super.onPause();
    }

    @Override
    public void onRestart() {
        if (mIsAbortingAppLaunch) {
            super.onRestart();
            return;
        }

        // Faster on main thread with an async apply().
        final StrictMode.ThreadPolicy savedPolicy = StrictMode.allowThreadDiskReads();
        try {
            SharedPreferences.Editor editor = GeckoApp.this.getSharedPreferences().edit();
            editor.putBoolean(GeckoApp.PREFS_WAS_STOPPED, false);
            editor.apply();
        } finally {
            StrictMode.setThreadPolicy(savedPolicy);
        }

        super.onRestart();
    }

    @Override
    public void onDestroy() {
        if (mIsAbortingAppLaunch) {
            // This build does not support the Android version of the device:
            // We did not initialize anything, so skip cleaning up.
            super.onDestroy();
            return;
        }

        if (mFormAssistPopup != null) {
            mFormAssistPopup.destroy();
            mFormAssistPopup = null;
        }

        if (mDoorHangerPopup != null) {
            mDoorHangerPopup.destroy();
            mDoorHangerPopup = null;
        }

        if (mTextSelection != null) {
            mTextSelection.destroy();
            mTextSelection = null;
        }

        EventDispatcher.getInstance().unregisterGeckoThreadListener(this,
            "Accessibility:Ready",
            "Gecko:Ready",
            null);

        EventDispatcher.getInstance().unregisterUiThreadListener(this,
            "Update:Check",
            "Update:Download",
            "Update:Install",
            null);

        getAppEventDispatcher().unregisterGeckoThreadListener(this,
            "Accessibility:Event",
            "Locale:Set",
            null);

        getAppEventDispatcher().unregisterBackgroundThreadListener(this,
            "Bookmark:Insert",
            "Image:SetAs",
            null);

        getAppEventDispatcher().unregisterUiThreadListener(this,
            "Contact:Add",
            "DevToolsAuth:Scan",
            "DOMFullScreen:Start",
            "DOMFullScreen:Stop",
            "Mma:reader_available",
            "Mma:web_save_image",
            "Mma:web_save_media",
            "Permissions:Data",
            "PrivateBrowsing:Data",
            "SystemUI:Visibility",
            "ToggleChrome:Focus",
            "ToggleChrome:Hide",
            "ToggleChrome:Show",
            null);

        if (mPromptService != null) {
            mPromptService.destroy();
            mPromptService = null;
        }

        final HealthRecorder rec = mHealthRecorder;
        mHealthRecorder = null;
        if (rec != null && rec.isEnabled()) {
            // Closing a HealthRecorder could incur a write.
            ThreadUtils.postToBackgroundThread(new Runnable() {
                @Override
                public void run() {
                    rec.close(GeckoApp.this);
                }
            });
        }

        super.onDestroy();

        Tabs.unregisterOnTabsChangedListener(this);
        Tabs.getInstance().detachFromContext();

        if (mShutdownOnDestroy) {
            GeckoApplication.shutdown(!mRestartOnShutdown ? null : new Intent(
                    Intent.ACTION_MAIN, /* uri */ null, getApplicationContext(), getClass()));
        }
    }

    public void showSDKVersionError() {
        final String message = getString(R.string.unsupported_sdk_version,
                HardwareUtils.getRealAbi(), Integer.toString(Build.VERSION.SDK_INT));
        Toast.makeText(this, message, Toast.LENGTH_LONG).show();
    }

    // Get a temporary directory, may return null
    public static File getTempDirectory(@NonNull Context context) {
        return context.getApplicationContext().getExternalFilesDir("temp");
    }

    // Delete any files in our temporary directory
    public static void deleteTempFiles(Context context) {
        File dir = getTempDirectory(context);
        if (dir == null)
            return;
        File[] files = dir.listFiles();
        if (files == null)
            return;
        for (File file : files) {
            file.delete();
        }
    }

    @Override
    public void onConfigurationChanged(Configuration newConfig) {
        Log.d(LOGTAG, "onConfigurationChanged: " + newConfig.locale);

        final LocaleManager localeManager = BrowserLocaleManager.getInstance();
        final Locale changed = localeManager.onSystemConfigurationChanged(this, getResources(), newConfig, mLastLocale);
        if (changed != null) {
            onLocaleChanged(Locales.getLanguageTag(changed));
        }

        // onConfigurationChanged is not called for 180 degree orientation changes,
        // we will miss such rotations and the screen orientation will not be
        // updated.
        if (GeckoScreenOrientation.getInstance().update(newConfig.orientation)) {
            if (mFormAssistPopup != null)
                mFormAssistPopup.hide();
            refreshChrome();
        }
        super.onConfigurationChanged(newConfig);
    }

    public String getContentProcessName() {
        return AppConstants.MOZ_CHILD_PROCESS_NAME;
    }

    public void addEnvToIntent(Intent intent) {
        Map<String, String> envMap = System.getenv();
        Set<Map.Entry<String, String>> envSet = envMap.entrySet();
        Iterator<Map.Entry<String, String>> envIter = envSet.iterator();
        int c = 0;
        while (envIter.hasNext()) {
            Map.Entry<String, String> entry = envIter.next();
            intent.putExtra("env" + c, entry.getKey() + "="
                            + entry.getValue());
            c++;
        }
    }

    @TargetApi(Build.VERSION_CODES.JELLY_BEAN_MR1)
    protected void finishAndShutdown(final boolean restart) {
        ThreadUtils.assertOnUiThread();

        mShutdownOnDestroy = true;
        mRestartOnShutdown = restart;

        // Shut down the activity and then Gecko.
        if (!isFinishing() && (Versions.preJBMR1 || !isDestroyed())) {
            finish();
        }
    }

    private void checkMigrateProfile() {
        final File profileDir = getProfile().getDir();

        if (profileDir != null) {
            ThreadUtils.postToBackgroundThread(new Runnable() {
                @Override
                public void run() {
                    Handler handler = new Handler();
                    handler.postDelayed(new DeferredCleanupTask(), CLEANUP_DEFERRAL_SECONDS * 1000);
                }
            });
        }
    }

    private static class DeferredCleanupTask implements Runnable {
        // The cleanup-version setting is recorded to avoid repeating the same
        // tasks on subsequent startups; CURRENT_CLEANUP_VERSION may be updated
        // if we need to do additional cleanup for future Gecko versions.

        private static final String CLEANUP_VERSION = "cleanup-version";
        private static final int CURRENT_CLEANUP_VERSION = 1;

        @Override
        public void run() {
            final Context context = GeckoAppShell.getApplicationContext();
            long cleanupVersion = GeckoSharedPrefs.forApp(context).getInt(CLEANUP_VERSION, 0);

            if (cleanupVersion < 1) {
                // Reduce device storage footprint by removing .ttf files from
                // the res/fonts directory: we no longer need to copy our
                // bundled fonts out of the APK in order to use them.
                // See https://bugzilla.mozilla.org/show_bug.cgi?id=878674.
                File dir = new File("res/fonts");
                if (dir.exists() && dir.isDirectory()) {
                    for (File file : dir.listFiles()) {
                        if (file.isFile() && file.getName().endsWith(".ttf")) {
                            file.delete();
                        }
                    }
                    if (!dir.delete()) {
                        Log.w(LOGTAG, "unable to delete res/fonts directory (not empty?)");
                    }
                }
            }

            // Additional cleanup needed for future versions would go here

            if (cleanupVersion != CURRENT_CLEANUP_VERSION) {
                SharedPreferences.Editor editor = GeckoSharedPrefs.forApp(context).edit();
                editor.putInt(CLEANUP_VERSION, CURRENT_CLEANUP_VERSION);
                editor.apply();
            }
        }
    }

    protected void onDone() {
        moveTaskToBack(true);
    }

    @Override
    public void onBackPressed() {
        if (getSupportFragmentManager().getBackStackEntryCount() > 0) {
            super.onBackPressed();
            return;
        }

        if (autoHideTabs()) {
            return;
        }

        if (mDoorHangerPopup != null && mDoorHangerPopup.isShowing()) {
            mDoorHangerPopup.dismiss();
            return;
        }

        if (mLayerView != null && mLayerView.isFullScreen()) {
            EventDispatcher.getInstance().dispatch("FullScreen:Exit", null);
            return;
        }

        final Tabs tabs = Tabs.getInstance();
        final Tab tab = tabs.getSelectedTab();
        if (tab == null) {
            onDone();
            return;
        }

        // Give Gecko a chance to handle the back press first, then fallback to the Java UI.
        getAppEventDispatcher().dispatch("Browser:OnBackPressed", null, new EventCallback() {
            @Override
            public void sendSuccess(final Object response) {
                if (!((GeckoBundle) response).getBoolean("handled")) {
                    // Default behavior is Gecko didn't prevent.
                    onDefault();
                }
            }

            @Override
            public void sendError(final Object error) {
                // Default behavior is Gecko didn't prevent, via failure.
                onDefault();
            }

            private void onDefault() {
                ThreadUtils.assertOnUiThread();

                if (tab.doBack()) {
                    return;
                }

                if (tab.isExternal()) {
                    onDone();
                    Tab nextSelectedTab = Tabs.getInstance().getNextTab(tab);
                    // Closing the tab will select the next tab. There's no need to unzombify it
                    // if we're exiting.
                    if (nextSelectedTab != null) {
                        final GeckoBundle data = new GeckoBundle(1);
                        data.putInt("nextSelectedTabId", nextSelectedTab.getId());
                        EventDispatcher.getInstance().dispatch("Tab:KeepZombified", data);
                    }
                    tabs.closeTab(tab);
                    return;
                }

                final int parentId = tab.getParentId();
                final Tab parent = tabs.getTab(parentId);
                if (parent != null) {
                    // The back button should always return to the parent (not a sibling).
                    tabs.closeTab(tab, parent);
                    return;
                }

                onDone();
            }
        });
    }

    @Override
    protected void onActivityResult(int requestCode, int resultCode, Intent data) {
        if (!ActivityHandlerHelper.handleActivityResult(requestCode, resultCode, data)) {
            super.onActivityResult(requestCode, resultCode, data);
        }
    }

    @Override
    public void onRequestPermissionsResult(int requestCode, String[] permissions, int[] grantResults) {
        Permissions.onRequestPermissionsResult(this, permissions, grantResults);
    }

    private void geckoConnected() {
        mLayerView.setOverScrollMode(View.OVER_SCROLL_NEVER);
    }

    public static class MainLayout extends RelativeLayout {
        private TouchEventInterceptor mTouchEventInterceptor;
        private MotionEventInterceptor mMotionEventInterceptor;

        public MainLayout(Context context, AttributeSet attrs) {
            super(context, attrs);
        }

        @Override
        protected void onLayout(boolean changed, int left, int top, int right, int bottom) {
            super.onLayout(changed, left, top, right, bottom);
        }

        public void setTouchEventInterceptor(TouchEventInterceptor interceptor) {
            mTouchEventInterceptor = interceptor;
        }

        public void setMotionEventInterceptor(MotionEventInterceptor interceptor) {
            mMotionEventInterceptor = interceptor;
        }

        @Override
        public boolean onInterceptTouchEvent(MotionEvent event) {
            if (mTouchEventInterceptor != null && mTouchEventInterceptor.onInterceptTouchEvent(this, event)) {
                return true;
            }
            return super.onInterceptTouchEvent(event);
        }

        @Override
        public boolean onTouchEvent(MotionEvent event) {
            if (mTouchEventInterceptor != null && mTouchEventInterceptor.onTouch(this, event)) {
                return true;
            }
            return super.onTouchEvent(event);
        }

        @Override
        public boolean onGenericMotionEvent(MotionEvent event) {
            if (mMotionEventInterceptor != null && mMotionEventInterceptor.onInterceptMotionEvent(this, event)) {
                return true;
            }
            return super.onGenericMotionEvent(event);
        }

        @Override
        public void setDrawingCacheEnabled(boolean enabled) {
            // Instead of setting drawing cache in the view itself, we simply
            // enable drawing caching on its children. This is mainly used in
            // animations (see PropertyAnimator)
            super.setChildrenDrawnWithCacheEnabled(enabled);
        }
    }

    private int getVersionCode() {
        int versionCode = 0;
        try {
            versionCode = getPackageManager().getPackageInfo(getPackageName(), 0).versionCode;
        } catch (NameNotFoundException e) {
            Log.wtf(LOGTAG, getPackageName() + " not found", e);
        }
        return versionCode;
    }

    // FHR reason code for a session end prior to a restart for a
    // locale change.
    private static final String SESSION_END_LOCALE_CHANGED = "L";

    /**
     * This exists so that a locale can be applied in two places: when saved
     * in a nested activity, and then again when we get back up to GeckoApp.
     *
     * GeckoApp needs to do a bunch more stuff than, say, GeckoPreferences.
     */
    protected void onLocaleChanged(final String locale) {
        final boolean startNewSession = true;
        final boolean shouldRestart = false;

        // If the HealthRecorder is not yet initialized (unlikely), the locale change won't
        // trigger a session transition and subsequent events will be recorded in an environment
        // with the wrong locale.
        final HealthRecorder rec = mHealthRecorder;
        if (rec != null) {
            rec.onAppLocaleChanged(locale);
            rec.onEnvironmentChanged(startNewSession, SESSION_END_LOCALE_CHANGED);
        }

        final Runnable runnable = new Runnable() {
            @Override
            public void run() {
                if (!ThreadUtils.isOnUiThread()) {
                    ThreadUtils.postToUiThread(this);
                    return;
                }
                if (!shouldRestart) {
                    GeckoApp.this.onLocaleReady(locale);
                } else {
                    finishAndShutdown(/* restart */ true);
                }
            }
        };

        if (!shouldRestart) {
            ThreadUtils.postToUiThread(runnable);
        } else {
            // Do this in the background so that the health recorder has its
            // time to finish.
            ThreadUtils.postToBackgroundThread(runnable);
        }
    }

    /**
     * Use BrowserLocaleManager to change our persisted and current locales,
     * and poke the system to tell it of our changed state.
     */
    protected void setLocale(final String locale) {
        if (locale == null) {
            return;
        }

        final String resultant = BrowserLocaleManager.getInstance().setSelectedLocale(this, locale);
        if (resultant == null) {
            return;
        }

        onLocaleChanged(resultant);
    }

    protected HealthRecorder createHealthRecorder(final Context context,
                                                  final String profilePath,
                                                  final EventDispatcher dispatcher,
                                                  final String osLocale,
                                                  final String appLocale,
                                                  final SessionInformation previousSession) {
        // GeckoApp does not need to record any health information - return a stub.
        return new StubbedHealthRecorder();
    }

    protected void recordStartupActionTelemetry(final String passedURL, final String action) {
    }

    public GeckoView getGeckoView() {
        return mLayerView;
    }

    @Override
    public boolean setRequestedOrientationForCurrentActivity(int requestedActivityInfoOrientation) {
        // We want to support the Screen Orientation API, and it always makes sense to lock the
        // orientation of a browser Activity, so we support locking.
        if (getRequestedOrientation() == requestedActivityInfoOrientation) {
            return false;
        }
        setRequestedOrientation(requestedActivityInfoOrientation);
        return true;
    }
}

```

## tests/tests/files/webtest/Titler.cpp
```
bool globalVariable = true;

namespace pagetitler {

// Some long comments to make the following class out of the visible area,
// so that the sticky symbol appears when the class is inside the view.
//
//
//
//
//
//
//
//
//
//
//
//
//
//
//
//
//
//
//
//
//
//
//
//
//
//
//
//
//
//
//
//
//
//
//
//
//
//
//
//
//
//
//
//
//
//
//
//
//
//
//
//
//
//
//
//
//
//
//
//
//
//
//
//
//
//
//
//
//
//
//
//
//
//
//
//
//
//
//
//
//
//
//
//
//
//
//
//
//
//
//
//
//
//
//
//
//
//
//
//
//
//
//
//
//
//
//
//
//
//
//
//
//
//
//
//
//
//
//
//
//
//
//
//
//
//
//
//
//
//
//
//
//
//
//
//
//
//
//
//
//
//
//
//
//
//
//
//
//
//
//
//
//
//
//
//
//
//
//
//
//
//
//
//
//
//
//
//
//
//
//
//
//
//
//
//
//
//
//
//
//
//

class PageTitler {};

};  // namespace pagetitler

```

## tests/tests/files/webtest/CopyAsMarkdown.cpp
```
// WARNING: The testcase relies on the line number for each item.

// Comment at the top level.

bool globalVariable = true;

namespace copy_as_markdown {

// Comment inside namespace.

class CopyAsMarkdown {
  // Comment inside class.

  void SomeMethod() {
    // Comment inside method.

    bool LocalVariable = true;
  }
};

};  // namespace copy_as_markdown

```

## tests/tests/files/webtest/Webtest.cpp
```
namespace webtest {

class SimpleSearch {};

class CaseSensitiveness1 {};

class casesensitiveness2 {};

class PathFilter {};

}  // namespace webtest

```

## tests/tests/files/webtest/WebtestPathFilter.cpp
```
namespace webtest_path {

class PathFilter {};

}  // namespace webtest_path

```

## tests/tests/files/ipdl/PTestBasic.ipdl
```
/* This Source Code Form is subject to the terms of the Mozilla Public
 * License, v. 2.0. If a copy of the MPL was not distributed with this
 * file, You can obtain one at http://mozilla.org/MPL/2.0/. */

namespace mozilla {
namespace _ipdltest {

async protocol PTestBasic {
child:
    async Hello();
};

} // namespace _ipdltest
} // namespace mozilla

```

## tests/tests/files/templates6.cpp
```
#include "templates6.h"

template <typename T>
void multiplexer(T t) {
  overloaded(t);
}

int main() {
  multiplexer(1);
  multiplexer('a');
}

```

## tests/tests/files/ipdl/TestBasic.cpp
```
/* -*- Mode: C++; tab-width: 8; indent-tabs-mode: nil; c-basic-offset: 2 -*- */
/* vim: set ts=8 sts=2 et sw=2 tw=80: */
/* This Source Code Form is subject to the terms of the Mozilla Public
 * License, v. 2.0. If a copy of the MPL was not distributed with this file,
 * You can obtain one at http://mozilla.org/MPL/2.0/. */

#include "gtest/gtest.h"

#include "mozilla/_ipdltest/IPDLUnitTest.h"
#include "mozilla/_ipdltest/TestBasicChild.h"
#include "mozilla/_ipdltest/TestBasicParent.h"

using namespace mozilla::ipc;

namespace mozilla::_ipdltest {

IPCResult TestBasicChild::RecvHello() {
  EXPECT_TRUE(CanSend());
  Close();
  EXPECT_FALSE(CanSend());
  return IPC_OK();
}

IPDL_TEST(TestBasic) {
  bool ok = mActor->SendHello();
  ASSERT_TRUE(ok);
}

}  // namespace mozilla::_ipdltest

```

## tests/tests/files/lots_of_calls.cpp
```
#include <stdio.h>

class CallerOne;
class CallerTwo;
class CallerThree;
class CallerFour;

class CallerOne {
 public:
  void one_calls_two_left(CallerTwo* two, CallerThree* three, CallerFour* four);

  void one_calls_two_right(CallerTwo* two, CallerThree* three,
                           CallerFour* four);
};

class CallerTwo {
 public:
  void two_left_calls_three_nexus(CallerThree* three, CallerFour* four);

  void two_right_calls_three_nexus(CallerThree* three, CallerFour* four);
};

class CallerThree {
 public:
  void three_nexus(CallerFour* four);
};

class CallerFour {
 public:
  void four_left() { printf("four_left\n"); };

  void four_right() { printf("four_right\n"); }
};

void CallerOne::one_calls_two_left(CallerTwo* two, CallerThree* three,
                                   CallerFour* four) {
  two->two_left_calls_three_nexus(three, four);
}

void CallerOne::one_calls_two_right(CallerTwo* two, CallerThree* three,
                                    CallerFour* four) {
  two->two_right_calls_three_nexus(three, four);
}

void CallerTwo::two_left_calls_three_nexus(CallerThree* three,
                                           CallerFour* four) {
  three->three_nexus(four);
}

void CallerTwo::two_right_calls_three_nexus(CallerThree* three,
                                            CallerFour* four) {
  three->three_nexus(four);
}

void CallerThree::three_nexus(CallerFour* four) {
  four->four_left();
  four->four_right();
}

int main(void) {
  CallerOne one;
  CallerTwo two;
  CallerThree three;
  CallerFour four;

  one.one_calls_two_left(&two, &three, &four);
  one.one_calls_two_right(&two, &three, &four);

  return 0;
}

```

## tests/tests/files/macro.cpp
```
#ifdef TEST_MACRO1
#  ifdef TEST_MACRO2

int x = 12;

#  endif
#endif

#define EMPTY_MACRO
#define CONST_MACRO 15
#define IDENT_MACRO(Arg) Arg
#define MULTI_LINE_MACRO(Name, Value) \
  static bool Name() { return Value; }
#define NESTED_MACRO CONST_MACRO
#define NESTED_MACRO_WITH_ARG(Arg) IDENT_MACRO(Arg)

EMPTY_MACRO
EMPTY_MACRO int i = CONST_MACRO;
EMPTY_MACRO
EMPTY_MACRO int j = IDENT_MACRO(16);
EMPTY_MACRO
EMPTY_MACRO int k = IDENT_MACRO(IDENT_MACRO(17));
EMPTY_MACRO
EMPTY_MACRO int l = NESTED_MACRO;
EMPTY_MACRO
EMPTY_MACRO int m = NESTED_MACRO_WITH_ARG(18);
EMPTY_MACRO
EMPTY_MACRO int n = NESTED_MACRO_WITH_ARG(CONST_MACRO);
EMPTY_MACRO
EMPTY_MACRO int o =
    NESTED_MACRO_WITH_ARG(IDENT_MACRO(EMPTY_MACRO 19 EMPTY_MACRO)) EMPTY_MACRO;

MULTI_LINE_MACRO(Bool0, true)
MULTI_LINE_MACRO(Bool1, true)
MULTI_LINE_MACRO(Bool2, false)

MULTI_LINE_MACRO(Bool3, false)

#if defined(TARGET_linux64)
#  define PER_TARGET_FUNCTION    \
    bool per_target_function() { \
      int a;                     \
      int b;                     \
      d = 5;                     \
    }                            \
    int f = per_target_function();
#elif defined(TARGET_macosx64)
#  define PER_TARGET_FUNCTION    \
    bool per_target_function() { \
      int b;                     \
      int a;                     \
      b = 2;                     \
    }                            \
    int f = per_target_function();
#elif defined(TARGET_win64)
#  define PER_TARGET_FUNCTION    \
    bool per_target_function() { \
      int c;                     \
      d = 3;                     \
    }                            \
    int f = per_target_function();
#endif

int d;

PER_TARGET_FUNCTION
int g = per_target_function();

#include TEST_MACRO_INCLUDE

```

## tests/tests/files/urlmap/chrome3.mjs
```
export const chrome3 = 13;

```

## tests/tests/files/urlmap/mozsrc1.html
```
<html>
  <head>
    <meta charset="utf-8" />
    <title>mozsrc1</title>
  </head>
  <body>
  </body>
</html>

```

## tests/tests/files/urlmap/chrome2.mjs
```
export const chrome2 = 12;

```

## tests/tests/files/urlmap/resource1.css
```
body {
  color: blue;
}

@import "resource://test/resource2.css";

```

## tests/tests/files/urlmap/root.xhtml
```
<?xml version="1.0" encoding="UTF-8"?>
<!DOCTYPE html>
<html>
  <head>
    <meta charset="utf-8" />
    <title>url map</title>
    <link rel="stylesheet" href="chrome://global/content/test/chrome1.css" />
    <link rel="stylesheet" href="resource://test/resource1.css" />
    <link rel="stylesheet" href="chrome://global/content/test/mozsrc1.css" />
  </head>
  <body>
    <script>
      <![CDATA[
const { chrome1 } = ChromeUtils.importESModule("chrome://global/content/test/chrome1.mjs");
const { resource1 } = ChromeUtils.importESModule("resource://test/resource1.mjs");
const { mozsrc1 } = ChromeUtils.importESModule("moz-src:///urlmap/mozsrc1.mjs");

const lazy = {};
ChromeUtils.defineESModuleGetters(lazy, {
  chrome2: "chrome://global/content/test/chrome2.mjs",
  resource2: "resource://test/resource2.mjs",
  mozsrc2: "moz-src:///urlmap/mozsrc2.mjs",
});

window.open("chrome://global/content/test/chrome1.html");
window.open("resource://test/resource1.html");
window.open("moz-src:///urlmap/mozsrc1.html");
      ]]>
    </script>
    <script type="module">
      <![CDATA[
import { chrome3 } from "chrome://global/content/test/chrome3.mjs";
import { resource3 } from "resource://test/resource3.mjs";
import { mozsrc3 } from "moz-src:///urlmap/mozsrc3.mjs";

const { chrome1 } = ChromeUtils.importESModule("chrome://global/content/test/chrome1.mjs");
const { resource1 } = ChromeUtils.importESModule("resource://test/resource1.mjs");
const { mozsrc1 } = ChromeUtils.importESModule("moz-src:///urlmap/mozsrc1.mjs");

const lazy = {};
ChromeUtils.defineESModuleGetters(lazy, {
  chrome2: "chrome://global/content/test/chrome2.mjs",
  resource2: "resource://test/resource2.mjs",
  mozsrc2: "moz-src:///urlmap/mozsrc2.mjs",
});

window.open("chrome://global/content/test/chrome1.html");
window.open("resource://test/resource1.html");
window.open("moz-src:///urlmap/mozsrc1.html");
      ]]>
    </script>
    <script type="module" src="chrome://global/content/test/chrome4.mjs"></script>
    <script type="module" src="resource://test/resource4.mjs"></script>
    <script type="module" src="moz-src:///urlmap/mozsrc4.mjs"></script>
    <img src="chrome://global/content/test/chrome1.png" />
    <img src="resource://test/resource1.png" />
    <img src="moz-src:///urlmap/mozsrc1.png" />
  </body>
</html>

```

## tests/tests/files/urlmap/chrome1.mjs
```
export const chrome1 = 11;

import { chrome2 } from "./chrome2.mjs";
export { chrome3 } from "./chrome3.mjs";
const { sub } = await import("./subdir/sub.mjs");

const ns2 = await import("./non-existent.mjs");

```

## tests/tests/files/urlmap/subdir/sub.mjs
```
export const sub = 10;

import { chrome2 } from "../chrome2.mjs";

```

## tests/tests/files/urlmap/chrome1.css
```
body {
  color: red;
}

@import "chrome://global/content/test/chrome2.css";

```

## tests/tests/files/urlmap/chrome1b.mjs
```
export const chrome1 = 1011;

```

## tests/tests/files/urlmap/root.js
```
const { chrome1 } = ChromeUtils.importESModule("chrome://global/content/test/chrome1.mjs");
const { resource1 } = ChromeUtils.importESModule("resource://test/resource1.mjs");

const lazy = {};
ChromeUtils.defineESModuleGetters(lazy, {
  chrome2: "chrome://global/content/test/chrome2.mjs",
  resource2: "resource://test/resource2.mjs",
});

window.open("chrome://global/content/test/chrome1.html");
window.open("resource://test/resource1.html");

async function f() {
  const { chrome1 } = await import("chrome://global/content/test/chrome2.mjs");
  const { resource1 } = await import("resource://test/resource2.mjs");

  var ns = await import("chrome://global/content/test/non-existent.html");
  var ns1 = await import("resource://test/non-existent.html");
}

```

## tests/tests/files/urlmap/root.mjs
```
import { chrome3 } from "chrome://global/content/test/chrome3.mjs";
import { resource3 } from "resource://test/resource3.mjs";
import { mozsrc3 } from "moz-src:///urlmap/mozsrc3.mjs";

const { chrome1 } = ChromeUtils.importESModule("chrome://global/content/test/chrome1.mjs");
const { resource1 } = ChromeUtils.importESModule("resource://test/resource1.mjs");
const { mozsrc1 } = ChromeUtils.importESModule("moz-src:///urlmap/mozsrc1.mjs");

const lazy = {};
ChromeUtils.defineESModuleGetters(lazy, {
  chrome2: "chrome://global/content/test/chrome2.mjs",
  resource2: "resource://test/resource2.mjs",
  mozsrc2: "moz-src:///urlmap/mozsrc2.mjs",
});

window.open("chrome://global/content/test/chrome1.html");
window.open("resource://test/resource1.html");
window.open("moz-src:///urlmap/mozsrc1.html");

```

## tests/tests/files/urlmap/chrome1.html
```
<html>
  <head>
    <meta charset="utf-8" />
    <title>chrome1</title>
  </head>
  <body>
  </body>
</html>

```

## tests/tests/files/urlmap/resource3.mjs
```
export const resource3 = 23;

```

## tests/tests/files/urlmap/root.html
```
<!DOCTYPE html>
<html>
  <head>
    <meta charset="utf-8">
    <title>url map</title>
    <link rel="stylesheet" href="chrome://global/content/test/chrome1.css">
    <link rel="stylesheet" href="resource://test/resource1.css">
    <link rel="stylesheet" href="moz-src:///urlmap/mozsrc1.css">
<style>
@import "chrome://global/content/test/chrome1.css";
@import "resource://test/resource1.css";
@import "moz-src:///urlmap/mozsrc1.css";
</style>
  </head>
  <body>
    <script>
const { chrome1 } = ChromeUtils.importESModule("chrome://global/content/test/chrome1.mjs");
const { resource1 } = ChromeUtils.importESModule("resource://test/resource1.mjs");
const { mozsrc1 } = ChromeUtils.importESModule("moz-src:///urlmap/mozsrc1.mjs");

const lazy = {};
ChromeUtils.defineESModuleGetters(lazy, {
  chrome2: "chrome://global/content/test/chrome2.mjs",
  resource2: "resource://test/resource2.mjs",
  mozsrc2: "moz-src:///urlmap/mozsrc2.mjs",
});

window.open("chrome://global/content/test/chrome1.html");
window.open("resource://test/resource1.html");
window.open("moz-src:///urlmap/mozsrc1.html");
    </script>
    <script type="module">
import { chrome3 } from "chrome://global/content/test/chrome3.mjs";
import { resource3 } from "resource://test/resource3.mjs";
import { mozsrc3 } from "moz-src:///urlmap/mozsrc3.mjs";

const { chrome1 } = ChromeUtils.importESModule("chrome://global/content/test/chrome1.mjs");
const { resource1 } = ChromeUtils.importESModule("resource://test/resource1.mjs");
const { mozsrc1 } = ChromeUtils.importESModule("moz-src:///urlmap/mozsrc1.mjs");

const lazy = {};
ChromeUtils.defineESModuleGetters(lazy, {
  chrome2: "chrome://global/content/test/chrome2.mjs",
  resource2: "resource://test/resource2.mjs",
  mozsrc2: "moz-src:///urlmap/mozsrc2.mjs",
});

window.open("chrome://global/content/test/chrome1.html");
window.open("resource://test/resource1.html");
window.open("moz-src:///urlmap/mozsrc1.html");
    </script>
    <script type="module" src="chrome://global/content/test/chrome4.mjs"></script>
    <script type="module" src="resource://test/resource4.mjs"></script>
    <script type="module" src="moz-src:///urlmap/mozsrc4.mjs"></script>
    <img src="chrome://global/content/test/chrome1.png">
    <img src="resource://test/resource1.png">
    <img src="moz-src:///urlmap/mozsrc1.png">
  </body>
</html>

```

## tests/tests/files/urlmap/mozsrc2.mjs
```
export const mozsrc2 = 12;

```

## tests/tests/files/urlmap/resource2.css
```
span {
  color: blue;
}

```

## tests/tests/files/urlmap/mozsrc3.mjs
```
export const mozsrc3 = 13;

```

## tests/tests/files/urlmap/resource2.mjs
```
export const resource2 = 22;

```

## tests/tests/files/urlmap/chrome2.css
```
span {
  color: red;
}

```

## tests/tests/files/urlmap/mozsrc2.css
```
span {
  color: red;
}

```

## tests/tests/files/urlmap/root.cpp
```
/**
 * chrome://global/content/test/chrome1.mjs
 * https://bugzilla.mozilla.org/
 * resource://test/resource1.mjs
 * moz-src:///urlmap/mozsrc1.mjs
 */

// chrome://global/content/test/chrome1.css
// https://bugzilla.mozilla.org/
// resource://test/resource1.png
// moz-src:///urlmap/mozsrc1.mjs

void f() {
  const char* s = "chrome://global/content/test/chrome1.mjs";
  const char* t = "https://bugzilla.mozilla.org/";
  const char* u = "resource://test/resource1.png";
  const char* v = "moz-src:///urlmap/mozsrc1.mjs";
}


```

## tests/tests/files/urlmap/resource1.mjs
```
export const resource1 = 21;

import { resource2 } from "./resource2.mjs";
export { resource3 } from "./resource3.mjs";

```

## tests/tests/files/urlmap/mozsrc1.mjs
```
export const mozsrc1 = 11;

import { mozsrc2 } from "./mozsrc2.mjs";
export { mozsrc3 } from "./mozsrc3.mjs";
const { sub } = await import("./subdir/sub.mjs");

const ns2 = await import("./non-existent.mjs");

```

## tests/tests/files/urlmap/mozsrc1.css
```
body {
  color: red;
}

@import "moz-src:///urlmap/mozsrc2.css";

```

## tests/tests/files/urlmap/mozsrc1b.mjs
```
export const mozsrc1 = 1011;

```

## tests/tests/files/urlmap/resource1.html
```
<html>
  <head>
    <meta charset="utf-8" />
    <title>resource1</title>
  </head>
  <body>
  </body>
</html>

```

## tests/tests/files/template_shapes.cpp
```
// The purpose of this test case is to illustrate the handling of heuristic
// vs. concrete results in templated code.
//
// Specifically, for the draw() call in foo(), we get all three overloads of
// DrawingContext::draw() as heuristic results, but only draw(Circle) as
// a concrete result (because foo is only instantiated with Shape=Circle).
//
// The current behaviour is to use all the results, so we just get all three
// overloads with nothing to distinguish draw(Circle).
//
// An improved behaviour in the future may be to get all three results,
// but have draw(Circle) annotated differently to indicate that we have
// more confidence in this result than the others.

class GenericSurface {};

class Rectangle {};
class Triangle {};
class Circle {};

template <typename Surface>
struct DrawingContext {
  void draw(Rectangle);
  void draw(Triangle);
  void draw(Circle);
};

template <typename Surface, typename Shape>
void foo(DrawingContext<Surface>& d, Shape& s) {
  d.draw(s);
}

int main(void) {
  GenericSurface surface;
  DrawingContext<GenericSurface> context;
  Circle circle;

  foo(context, circle);
}

```

## tests/tests/files/test_custom_element_base.xul
```
<?xml version="1.0"?>
<?xml-stylesheet href="chrome://global/skin" type="text/css"?>
<?xml-stylesheet href="chrome://mochikit/content/tests/SimpleTest/test.css" type="text/css"?>

<window title="Custom Element Base Class Tests"
  onload="runTests();"
  xmlns="http://www.mozilla.org/keymaster/gatekeeper/there.is.only.xul">

  <script src="chrome://mochikit/content/tests/SimpleTest/SimpleTest.js"></script>
  <script src="chrome://mochikit/content/tests/SimpleTest/EventUtils.js"></script>

  <!-- test results are displayed in the html:body -->
  <body xmlns="http://www.w3.org/1999/xhtml" style="height: 300px; overflow: auto;"/>

  <button id="one"/>
  <simpleelement id="two" style="-moz-user-focus: normal;"/>
  <simpleelement id="three" disabled="true" style="-moz-user-focus: normal;"/>
  <button id="four"/>
  <inherited-element-declarative foo="fuagra" empty-string=""></inherited-element-declarative>
  <inherited-element-derived foo="fuagra"></inherited-element-derived>
  <inherited-element-shadowdom-declarative foo="fuagra" empty-string=""></inherited-element-shadowdom-declarative>
  <inherited-element-imperative foo="fuagra" empty-string=""></inherited-element-imperative>
  <inherited-element-beforedomloaded foo="fuagra" empty-string=""></inherited-element-beforedomloaded>

  <!-- test code running before page load goes here -->
  <script type="application/javascript"><![CDATA[
    class InheritAttrsChangeBeforDOMLoaded extends MozXULElement {
      static get inheritedAttributes() {
        return {
          "label": "foo",
        };
      }
      connectedCallback() {
        this.append(MozXULElement.parseXULToFragment(`<label />`));
        this.label = this.querySelector("label");

        this.initializeAttributeInheritance();
        is(this.label.getAttribute("foo"), "fuagra",
           "InheritAttrsChangeBeforDOMLoaded: attribute should be propagated #1");

        this.setAttribute("foo", "chuk&gek");
        is(this.label.getAttribute("foo"), "chuk&gek",
           "InheritAttrsChangeBeforDOMLoaded: attribute should be propagated #2");
      }
    }
    customElements.define("inherited-element-beforedomloaded",
                          InheritAttrsChangeBeforDOMLoaded);
  ]]>
  </script>

  <!-- test code goes here -->
  <script type="application/javascript"><![CDATA[

  SimpleTest.waitForExplicitFinish();

  async function runTests() {
    ok(MozXULElement, "MozXULElement defined on the window");
    testMixin();
    testBaseControl();
    testBaseControlMixin();
    testBaseText();
    testParseXULToFragment();
    testInheritAttributes();
    await testCustomInterface();

    let htmlWin = await new Promise(resolve => {
      let htmlIframe = document.createElement("iframe");
      htmlIframe.src = "file_empty.xhtml";
      htmlIframe.onload = () => resolve(htmlIframe.contentWindow);
      document.documentElement.appendChild(htmlIframe);
    });

    ok(htmlWin.MozXULElement, "MozXULElement defined on a chrome HTML window");
    SimpleTest.finish();
  }

  function testMixin() {
    ok(MozElements.MozElementMixin, "Mixin exists");
    let MixedHTMLElement = MozElements.MozElementMixin(HTMLElement);
    ok(MixedHTMLElement.insertFTLIfNeeded, "Mixed in class contains helper functions");
  }

  function testBaseControl() {
    ok(MozElements.BaseControl, "BaseControl exists");
    ok("disabled" in MozElements.BaseControl.prototype,
      "BaseControl prototype contains base control attributes");
  }

  function testBaseControlMixin() {
    ok(MozElements.BaseControlMixin, "Mixin exists");
    let MixedHTMLSpanElement = MozElements.MozElementMixin(HTMLSpanElement);
    let HTMLSpanBaseControl = MozElements.BaseControlMixin(MixedHTMLSpanElement);
    ok("disabled" in HTMLSpanBaseControl.prototype, "Mixed in class prototype contains base control attributes");
  }

  function testBaseText() {
    ok(MozElements.BaseText, "BaseText exists");
    ok("label" in MozElements.BaseText.prototype,
      "BaseText prototype inherits BaseText attributes");
    ok("disabled" in MozElements.BaseText.prototype,
      "BaseText prototype inherits BaseControl attributes");
  }

  function testParseXULToFragment() {
    ok(MozXULElement.parseXULToFragment, "parseXULToFragment helper exists");

    let frag = MozXULElement.parseXULToFragment(`<deck id='foo' />`);
    ok(frag instanceof DocumentFragment);

    document.documentElement.appendChild(frag);

    let deck = document.documentElement.lastChild;
    ok(deck instanceof MozXULElement, "instance of MozXULElement");
    ok(deck instanceof XULElement, "instance of XULElement");
    is(deck.id, "foo", "attribute set");
    is(deck.selectedIndex, "0", "Custom Element is property attached");
    deck.remove();

    info("Checking that whitespace text is removed but non-whitespace text isn't");
    let boxWithWhitespaceText = MozXULElement.parseXULToFragment(`<box> </box>`).querySelector("box");
    is(boxWithWhitespaceText.textContent, "", "Whitespace removed");
    let boxWithNonWhitespaceText = MozXULElement.parseXULToFragment(`<box>foo</box>`).querySelector("box");
    is(boxWithNonWhitespaceText.textContent, "foo", "Non-whitespace not removed");
  }

  function testInheritAttributes() {
    class InheritsElementDeclarative extends MozXULElement {
      static get inheritedAttributes() {
        return {
          "label": "text=label,foo,empty-string,bardo=bar",
          "unmatched": "foo", // Make sure we don't throw on unmatched selectors
        };
      }

      connectedCallback() {
        this.textContent = "";
        this.append(MozXULElement.parseXULToFragment(`<label />`));
        this.label = this.querySelector("label");
        this.initializeAttributeInheritance();
      }
    }
    customElements.define("inherited-element-declarative", InheritsElementDeclarative);
    let declarativeEl = document.querySelector("inherited-element-declarative");
    ok(declarativeEl, "declarative inheritance element exists");

    class InheritsElementDerived extends InheritsElementDeclarative {
      static get inheritedAttributes() {
        return { label: "renamedfoo=foo" };
      }
    }
    customElements.define("inherited-element-derived", InheritsElementDerived);

    class InheritsElementShadowDOMDeclarative extends MozXULElement {
      constructor() {
        super();
        this.attachShadow({ mode: "open" });
      }
      static get inheritedAttributes() {
        return {
          "label": "text=label,foo,empty-string,bardo=bar",
          "unmatched": "foo", // Make sure we don't throw on unmatched selectors
        };
      }

      connectedCallback() {
        this.shadowRoot.textContent = "";
        this.shadowRoot.append(MozXULElement.parseXULToFragment(`<label />`));
        this.label = this.shadowRoot.querySelector("label");
        this.initializeAttributeInheritance();
      }
    }
    customElements.define("inherited-element-shadowdom-declarative", InheritsElementShadowDOMDeclarative);
    let shadowDOMDeclarativeEl = document.querySelector("inherited-element-shadowdom-declarative");
    ok(shadowDOMDeclarativeEl, "declarative inheritance element with shadow DOM exists");

    class InheritsElementImperative extends MozXULElement {
      static get observedAttributes() {
        return [ "label", "foo", "empty-string", "bar" ];
      }

      attributeChangedCallback(name, oldValue, newValue) {
        if (this.label && oldValue != newValue) {
          this.inherit();
        }
      }

      inherit() {
        let map = {
          "label": [[ "label", "text" ]],
          "foo": [[ "label", "foo" ]],
          "empty-string": [[ "label", "empty-string" ]],
          "bar": [[ "label", "bardo" ]],
        };
        for (let attr of InheritsElementImperative.observedAttributes) {
          this.inheritAttribute(map[attr], attr);
        }
      }

      connectedCallback() {
        // Typically `initializeAttributeInheritance` handles this for us:
        this._inheritedElements = null;

        this.textContent = "";
        this.append(MozXULElement.parseXULToFragment(`<label />`));
        this.label = this.querySelector("label");
        this.inherit();
      }
    }

    customElements.define("inherited-element-imperative", InheritsElementImperative);
    let imperativeEl = document.querySelector("inherited-element-imperative");
    ok(imperativeEl, "imperative inheritance element exists");

    function checkElement(el) {
      is(el.label.getAttribute("foo"), "fuagra", "predefined attribute @foo");
      ok(el.label.hasAttribute("empty-string"), "predefined attribute @empty-string");
      ok(!el.label.hasAttribute("bardo"), "predefined attribute @bardo");
      ok(!el.label.textContent, "predefined attribute @label");

      el.setAttribute("empty-string", "not-empty-anymore");
      is(el.label.getAttribute("empty-string"), "not-empty-anymore",
        "attribute inheritance: empty-string");

      el.setAttribute("label", "label-test");
      is(el.label.textContent, "label-test",
        "attribute inheritance: text=label attribute change");

      el.setAttribute("bar", "bar-test");
      is(el.label.getAttribute("bardo"), "bar-test",
        "attribute inheritance: `=` mapping");

      el.label.setAttribute("bardo", "changed-from-child");
      is(el.label.getAttribute("bardo"), "changed-from-child",
        "attribute inheritance: doesn't apply when host attr hasn't changed and child attr was changed");

      el.label.removeAttribute("bardo");
      ok(!el.label.hasAttribute("bardo"),
        "attribute inheritance: doesn't apply when host attr hasn't changed and child attr was removed");

      el.setAttribute("bar", "changed-from-host");
      is(el.label.getAttribute("bardo"), "changed-from-host",
        "attribute inheritance: does apply when host attr has changed and child attr was changed");

      el.removeAttribute("bar");
      ok(!el.label.hasAttribute("bardo"),
        "attribute inheritance: does apply when host attr has been removed");

      el.setAttribute("bar", "changed-from-host-2");
      is(el.label.getAttribute("bardo"), "changed-from-host-2",
        "attribute inheritance: does apply when host attr has changed after being removed");

      // Restore to the original state so this can be ran again with the same element:
      el.removeAttribute("label");
      el.removeAttribute("bar");
    }

    for (let el of [declarativeEl, shadowDOMDeclarativeEl, imperativeEl]) {
      info(`Running checks for ${el.tagName}`);
      checkElement(el);
      info(`Remove and re-add ${el.tagName} to make sure attribute inheritance still works`);
      el.replaceWith(el);
      checkElement(el);
    }

    let derivedEl = document.querySelector("inherited-element-derived");
    ok(derivedEl, "derived inheritance element exists");
    ok(!derivedEl.label.hasAttribute("foo"),
       "attribute inheritance: base class attribute is not applied in derived class that overrides it");
    ok(derivedEl.label.hasAttribute("renamedfoo"),
       "attribute inheritance: attribute defined in derived class is present");
  }

  async function testCustomInterface() {
    class SimpleElement extends MozXULElement {
      get disabled() {
        return this.getAttribute("disabled") == "true";
      }

      set disabled(val) {
        if (val) this.setAttribute("disabled", "true");
        else this.removeAttribute("disabled");
        return val;
      }

      get tabIndex() {
        return parseInt(this.getAttribute("tabIndex")) || 0;
      }

      set tabIndex(val) {
        if (val) this.setAttribute("tabIndex", val);
        else this.removeAttribute("tabIndex");
        return val;
      }
    }

    MozXULElement.implementCustomInterface(SimpleElement, [Ci.nsIDOMXULControlElement]);
    customElements.define("simpleelement", SimpleElement);

    let twoElement = document.getElementById("two");

    is(document.documentElement.getCustomInterfaceCallback, undefined,
       "No getCustomInterfaceCallback on non-custom element");
    is(typeof twoElement.getCustomInterfaceCallback, "function",
       "getCustomInterfaceCallback available on custom element when set");
    try {
      document.documentElement.QueryInterface(Ci.nsIDOMXULControlElement)
      ok(false, "Non-custom element implements custom interface");
    } catch (ex) {
      ok(true, "Non-custom element implements custom interface");
    }

    // Try various ways to get the custom interface.

    let asControl = twoElement.getCustomInterfaceCallback(Ci.nsIDOMXULControlElement);

    // XXX: Switched to from ok() to todo_is() in Bug 1467712. Follow up in 1500967
    // Not sure if this was suppose to simply check for existence or equality?
    todo_is(asControl, twoElement, "getCustomInterface returns interface implementation ");

    asControl = twoElement.QueryInterface(Ci.nsIDOMXULControlElement);
    ok(asControl, "QueryInterface to nsIDOMXULControlElement");
    ok(asControl instanceof Node, "Control is a Node");

    // Now make sure that the custom element handles focus/tabIndex as needed by shitfing
    // focus around and enabling/disabling the simple elements.

    // Enable Full Keyboard Access emulation on Mac
    await SpecialPowers.pushPrefEnv({"set": [["accessibility.tabfocus", 7]]});

    ok(!twoElement.disabled, "two is enabled");
    ok(document.getElementById("three").disabled, "three is disabled");

    await SimpleTest.promiseFocus();
    ok(document.hasFocus(), "has focus");

    // This should skip the disabled simpleelement.
    synthesizeKey("VK_TAB");
    is(document.activeElement.id, "one", "Tab 1");
    synthesizeKey("VK_TAB");
    is(document.activeElement.id, "two", "Tab 2");
    synthesizeKey("VK_TAB");
    is(document.activeElement.id, "four", "Tab 3");

    twoElement.disabled = true;
    is(twoElement.getAttribute("disabled"), "true", "two disabled after change");

    synthesizeKey("VK_TAB", { shiftKey: true });
    is(document.activeElement.id, "one", "Tab 1");

    info("Checking that interfaces get inherited automatically with implementCustomInterface");
    class ExtendedElement extends SimpleElement { }
    MozXULElement.implementCustomInterface(ExtendedElement, [Ci.nsIDOMXULSelectControlElement]);
    customElements.define("extendedelement", ExtendedElement);
    const extendedInstance = document.createXULElement("extendedelement");
    ok(extendedInstance.QueryInterface(Ci.nsIDOMXULSelectControlElement), "interface applied");
    ok(extendedInstance.QueryInterface(Ci.nsIDOMXULControlElement), "inherited interface applied");
  }
  ]]>
  </script>
</window>

```

## tests/tests/files/AddonUpdateChecker.jsm
```
/* This Source Code Form is subject to the terms of the Mozilla Public
 * License, v. 2.0. If a copy of the MPL was not distributed with this
 * file, You can obtain one at http://mozilla.org/MPL/2.0/. */

/**
 * The AddonUpdateChecker is responsible for retrieving the update information
 * from an add-on's remote update manifest.
 */

"use strict";

const Cc = Components.classes;
const Ci = Components.interfaces;
const Cu = Components.utils;

this.EXPORTED_SYMBOLS = [ "AddonUpdateChecker" ];

const TIMEOUT               = 60 * 1000;
const PREFIX_NS_RDF         = "http://www.w3.org/1999/02/22-rdf-syntax-ns#";
const PREFIX_NS_EM          = "http://www.mozilla.org/2004/em-rdf#";
const PREFIX_ITEM           = "urn:mozilla:item:";
const PREFIX_EXTENSION      = "urn:mozilla:extension:";
const PREFIX_THEME          = "urn:mozilla:theme:";
const TOOLKIT_ID            = "toolkit@mozilla.org"
const XMLURI_PARSE_ERROR    = "http://www.mozilla.org/newlayout/xml/parsererror.xml"

const PREF_UPDATE_REQUIREBUILTINCERTS = "extensions.update.requireBuiltInCerts";

Components.utils.import("resource://gre/modules/Services.jsm");
Components.utils.import("resource://gre/modules/XPCOMUtils.jsm");

XPCOMUtils.defineLazyModuleGetter(this, "AddonManager",
                                  "resource://gre/modules/AddonManager.jsm");
XPCOMUtils.defineLazyModuleGetter(this, "AddonManagerPrivate",
                                  "resource://gre/modules/AddonManager.jsm");
XPCOMUtils.defineLazyModuleGetter(this, "AddonRepository",
                                  "resource://gre/modules/addons/AddonRepository.jsm");
XPCOMUtils.defineLazyModuleGetter(this, "ServiceRequest",
                                  "resource://gre/modules/ServiceRequest.jsm");


// Shared code for suppressing bad cert dialogs.
XPCOMUtils.defineLazyGetter(this, "CertUtils", function() {
  let certUtils = {};
  Components.utils.import("resource://gre/modules/CertUtils.jsm", certUtils);
  return certUtils;
});

var gRDF = Cc["@mozilla.org/rdf/rdf-service;1"].
           getService(Ci.nsIRDFService);

Cu.import("resource://gre/modules/Log.jsm");
const LOGGER_ID = "addons.update-checker";

// Create a new logger for use by the Addons Update Checker
// (Requires AddonManager.jsm)
var logger = Log.repository.getLogger(LOGGER_ID);

/**
 * A serialisation method for RDF data that produces an identical string
 * for matching RDF graphs.
 * The serialisation is not complete, only assertions stemming from a given
 * resource are included, multiple references to the same resource are not
 * permitted, and the RDF prolog and epilog are not included.
 * RDF Blob and Date literals are not supported.
 */
function RDFSerializer() {
  this.cUtils = Cc["@mozilla.org/rdf/container-utils;1"].
                getService(Ci.nsIRDFContainerUtils);
  this.resources = [];
}

RDFSerializer.prototype = {
  INDENT: "  ",      // The indent used for pretty-printing
  resources: null,   // Array of the resources that have been found

  /**
   * Escapes characters from a string that should not appear in XML.
   *
   * @param  aString
   *         The string to be escaped
   * @return a string with all characters invalid in XML character data
   *         converted to entity references.
   */
  escapeEntities(aString) {
    aString = aString.replace(/&/g, "&amp;");
    aString = aString.replace(/</g, "&lt;");
    aString = aString.replace(/>/g, "&gt;");
    return aString.replace(/"/g, "&quot;");
  },

  /**
   * Serializes all the elements of an RDF container.
   *
   * @param  aDs
   *         The RDF datasource
   * @param  aContainer
   *         The RDF container to output the child elements of
   * @param  aIndent
   *         The current level of indent for pretty-printing
   * @return a string containing the serialized elements.
   */
  serializeContainerItems(aDs, aContainer, aIndent) {
    var result = "";
    var items = aContainer.GetElements();
    while (items.hasMoreElements()) {
      var item = items.getNext().QueryInterface(Ci.nsIRDFResource);
      result += aIndent + "<RDF:li>\n"
      result += this.serializeResource(aDs, item, aIndent + this.INDENT);
      result += aIndent + "</RDF:li>\n"
    }
    return result;
  },

  /**
   * Serializes all em:* (see EM_NS) properties of an RDF resource except for
   * the em:signature property. As this serialization is to be compared against
   * the manifest signature it cannot contain the em:signature property itself.
   *
   * @param  aDs
   *         The RDF datasource
   * @param  aResource
   *         The RDF resource that contains the properties to serialize
   * @param  aIndent
   *         The current level of indent for pretty-printing
   * @return a string containing the serialized properties.
   * @throws if the resource contains a property that cannot be serialized
   */
  serializeResourceProperties(aDs, aResource, aIndent) {
    var result = "";
    var items = [];
    var arcs = aDs.ArcLabelsOut(aResource);
    while (arcs.hasMoreElements()) {
      var arc = arcs.getNext().QueryInterface(Ci.nsIRDFResource);
      if (arc.ValueUTF8.substring(0, PREFIX_NS_EM.length) != PREFIX_NS_EM)
        continue;
      var prop = arc.ValueUTF8.substring(PREFIX_NS_EM.length);
      if (prop == "signature")
        continue;

      var targets = aDs.GetTargets(aResource, arc, true);
      while (targets.hasMoreElements()) {
        var target = targets.getNext();
        if (target instanceof Ci.nsIRDFResource) {
          var item = aIndent + "<em:" + prop + ">\n";
          item += this.serializeResource(aDs, target, aIndent + this.INDENT);
          item += aIndent + "</em:" + prop + ">\n";
          items.push(item);
        } else if (target instanceof Ci.nsIRDFLiteral) {
          items.push(aIndent + "<em:" + prop + ">" +
                     this.escapeEntities(target.Value) + "</em:" + prop + ">\n");
        } else if (target instanceof Ci.nsIRDFInt) {
          items.push(aIndent + "<em:" + prop + " NC:parseType=\"Integer\">" +
                     target.Value + "</em:" + prop + ">\n");
        } else {
          throw Components.Exception("Cannot serialize unknown literal type");
        }
      }
    }
    items.sort();
    result += items.join("");
    return result;
  },

  /**
   * Recursively serializes an RDF resource and all resources it links to.
   * This will only output EM_NS properties and will ignore any em:signature
   * property.
   *
   * @param  aDs
   *         The RDF datasource
   * @param  aResource
   *         The RDF resource to serialize
   * @param  aIndent
   *         The current level of indent for pretty-printing. If undefined no
   *         indent will be added
   * @return a string containing the serialized resource.
   * @throws if the RDF data contains multiple references to the same resource.
   */
  serializeResource(aDs, aResource, aIndent) {
    if (this.resources.indexOf(aResource) != -1 ) {
      // We cannot output multiple references to the same resource.
      throw Components.Exception("Cannot serialize multiple references to " + aResource.Value);
    }
    if (aIndent === undefined)
      aIndent = "";

    this.resources.push(aResource);
    var container = null;
    var type = "Description";
    if (this.cUtils.IsSeq(aDs, aResource)) {
      type = "Seq";
      container = this.cUtils.MakeSeq(aDs, aResource);
    } else if (this.cUtils.IsAlt(aDs, aResource)) {
      type = "Alt";
      container = this.cUtils.MakeAlt(aDs, aResource);
    } else if (this.cUtils.IsBag(aDs, aResource)) {
      type = "Bag";
      container = this.cUtils.MakeBag(aDs, aResource);
    }

    var result = aIndent + "<RDF:" + type;
    if (!gRDF.IsAnonymousResource(aResource))
      result += " about=\"" + this.escapeEntities(aResource.ValueUTF8) + "\"";
    result += ">\n";

    if (container)
      result += this.serializeContainerItems(aDs, container, aIndent + this.INDENT);

    result += this.serializeResourceProperties(aDs, aResource, aIndent + this.INDENT);

    result += aIndent + "</RDF:" + type + ">\n";
    return result;
  }
}

/**
 * Sanitizes the update URL in an update item, as returned by
 * parseRDFManifest and parseJSONManifest. Ensures that:
 *
 * - The URL is secure, or secured by a strong enough hash.
 * - The security principal of the update manifest has permission to
 *   load the URL.
 *
 * @param aUpdate
 *        The update item to sanitize.
 * @param aRequest
 *        The XMLHttpRequest used to load the manifest.
 * @param aHashPattern
 *        The regular expression used to validate the update hash.
 * @param aHashString
 *        The human-readable string specifying which hash functions
 *        are accepted.
 */
function sanitizeUpdateURL(aUpdate, aRequest, aHashPattern, aHashString) {
  if (aUpdate.updateURL) {
    let scriptSecurity = Services.scriptSecurityManager;
    let principal = scriptSecurity.getChannelURIPrincipal(aRequest.channel);
    try {
      // This logs an error on failure, so no need to log it a second time
      scriptSecurity.checkLoadURIStrWithPrincipal(principal, aUpdate.updateURL,
                                                  scriptSecurity.DISALLOW_SCRIPT);
    } catch (e) {
      delete aUpdate.updateURL;
      return;
    }

    if (AddonManager.checkUpdateSecurity &&
        !aUpdate.updateURL.startsWith("https:") &&
        !aHashPattern.test(aUpdate.updateHash)) {
      logger.warn(`Update link ${aUpdate.updateURL} is not secure and is not verified ` +
                  `by a strong enough hash (needs to be ${aHashString}).`);
      delete aUpdate.updateURL;
      delete aUpdate.updateHash;
    }
  }
}

/**
 * Parses an RDF style update manifest into an array of update objects.
 *
 * @param  aId
 *         The ID of the add-on being checked for updates
 * @param  aUpdateKey
 *         An optional update key for the add-on
 * @param  aRequest
 *         The XMLHttpRequest that has retrieved the update manifest
 * @param  aManifestData
 *         The pre-parsed manifest, as a bare XML DOM document
 * @return an array of update objects
 * @throws if the update manifest is invalid in any way
 */
function parseRDFManifest(aId, aUpdateKey, aRequest, aManifestData) {
  if (aManifestData.documentElement.namespaceURI != PREFIX_NS_RDF) {
    throw Components.Exception("Update manifest had an unrecognised namespace: " +
                               aManifestData.documentElement.namespaceURI);
  }

  function EM_R(aProp) {
    return gRDF.GetResource(PREFIX_NS_EM + aProp);
  }

  function getValue(aLiteral) {
    if (aLiteral instanceof Ci.nsIRDFLiteral)
      return aLiteral.Value;
    if (aLiteral instanceof Ci.nsIRDFResource)
      return aLiteral.Value;
    if (aLiteral instanceof Ci.nsIRDFInt)
      return aLiteral.Value;
    return null;
  }

  function getProperty(aDs, aSource, aProperty) {
    return getValue(aDs.GetTarget(aSource, EM_R(aProperty), true));
  }

  function getBooleanProperty(aDs, aSource, aProperty) {
    let propValue = aDs.GetTarget(aSource, EM_R(aProperty), true);
    if (!propValue)
      return undefined;
    return getValue(propValue) == "true";
  }

  function getRequiredProperty(aDs, aSource, aProperty) {
    let value = getProperty(aDs, aSource, aProperty);
    if (!value)
      throw Components.Exception("Update manifest is missing a required " + aProperty + " property.");
    return value;
  }

  let rdfParser = Cc["@mozilla.org/rdf/xml-parser;1"].
                  createInstance(Ci.nsIRDFXMLParser);
  let ds = Cc["@mozilla.org/rdf/datasource;1?name=in-memory-datasource"].
           createInstance(Ci.nsIRDFDataSource);
  rdfParser.parseString(ds, aRequest.channel.URI, aRequest.responseText);

  // Differentiating between add-on types is deprecated
  let extensionRes = gRDF.GetResource(PREFIX_EXTENSION + aId);
  let themeRes = gRDF.GetResource(PREFIX_THEME + aId);
  let itemRes = gRDF.GetResource(PREFIX_ITEM + aId);
  let addonRes;
  if (ds.ArcLabelsOut(extensionRes).hasMoreElements())
    addonRes = extensionRes;
  else if (ds.ArcLabelsOut(themeRes).hasMoreElements())
    addonRes = themeRes;
  else
    addonRes = itemRes;

  // If we have an update key then the update manifest must be signed
  if (aUpdateKey) {
    let signature = getProperty(ds, addonRes, "signature");
    if (!signature)
      throw Components.Exception("Update manifest for " + aId + " does not contain a required signature");
    let serializer = new RDFSerializer();
    let updateString = null;

    try {
      updateString = serializer.serializeResource(ds, addonRes);
    } catch (e) {
      throw Components.Exception("Failed to generate signed string for " + aId + ". Serializer threw " + e,
                                 e.result);
    }

    let result = false;

    try {
      let verifier = Cc["@mozilla.org/security/datasignatureverifier;1"].
                     getService(Ci.nsIDataSignatureVerifier);
      result = verifier.verifyData(updateString, signature, aUpdateKey);
    } catch (e) {
      throw Components.Exception("The signature or updateKey for " + aId + " is malformed." +
                                 "Verifier threw " + e, e.result);
    }

    if (!result)
      throw Components.Exception("The signature for " + aId + " was not created by the add-on's updateKey");
  }

  let updates = ds.GetTarget(addonRes, EM_R("updates"), true);

  // A missing updates property doesn't count as a failure, just as no avialable
  // update information
  if (!updates) {
    logger.warn("Update manifest for " + aId + " did not contain an updates property");
    return [];
  }

  if (!(updates instanceof Ci.nsIRDFResource))
    throw Components.Exception("Missing updates property for " + addonRes.Value);

  let cu = Cc["@mozilla.org/rdf/container-utils;1"].
           getService(Ci.nsIRDFContainerUtils);
  if (!cu.IsContainer(ds, updates))
    throw Components.Exception("Updates property was not an RDF container");

  let results = [];
  let ctr = Cc["@mozilla.org/rdf/container;1"].
            createInstance(Ci.nsIRDFContainer);
  ctr.Init(ds, updates);
  let items = ctr.GetElements();
  while (items.hasMoreElements()) {
    let item = items.getNext().QueryInterface(Ci.nsIRDFResource);
    let version = getProperty(ds, item, "version");
    if (!version) {
      logger.warn("Update manifest is missing a required version property.");
      continue;
    }

    logger.debug("Found an update entry for " + aId + " version " + version);

    let targetApps = ds.GetTargets(item, EM_R("targetApplication"), true);
    while (targetApps.hasMoreElements()) {
      let targetApp = targetApps.getNext().QueryInterface(Ci.nsIRDFResource);

      let appEntry = {};
      try {
        appEntry.id = getRequiredProperty(ds, targetApp, "id");
        appEntry.minVersion = getRequiredProperty(ds, targetApp, "minVersion");
        appEntry.maxVersion = getRequiredProperty(ds, targetApp, "maxVersion");
      } catch (e) {
        logger.warn(e);
        continue;
      }

      let result = {
        id: aId,
        version,
        multiprocessCompatible: getBooleanProperty(ds, item, "multiprocessCompatible"),
        updateURL: getProperty(ds, targetApp, "updateLink"),
        updateHash: getProperty(ds, targetApp, "updateHash"),
        updateInfoURL: getProperty(ds, targetApp, "updateInfoURL"),
        strictCompatibility: !!getBooleanProperty(ds, targetApp, "strictCompatibility"),
        targetApplications: [appEntry]
      };

      // The JSON update protocol requires an SHA-2 hash. RDF still
      // supports SHA-1, for compatibility reasons.
      sanitizeUpdateURL(result, aRequest, /^sha/, "sha1 or stronger");

      results.push(result);
    }
  }
  return results;
}

/**
 * Parses an JSON update manifest into an array of update objects.
 *
 * @param  aId
 *         The ID of the add-on being checked for updates
 * @param  aUpdateKey
 *         An optional update key for the add-on
 * @param  aRequest
 *         The XMLHttpRequest that has retrieved the update manifest
 * @param  aManifestData
 *         The pre-parsed manifest, as a JSON object tree
 * @return an array of update objects
 * @throws if the update manifest is invalid in any way
 */
function parseJSONManifest(aId, aUpdateKey, aRequest, aManifestData) {
  if (aUpdateKey)
    throw Components.Exception("Update keys are not supported for JSON update manifests");

  let TYPE_CHECK = {
    "array": val => Array.isArray(val),
    "object": val => val && typeof val == "object" && !Array.isArray(val),
  };

  function getProperty(aObj, aProperty, aType, aDefault = undefined) {
    if (!(aProperty in aObj))
      return aDefault;

    let value = aObj[aProperty];

    let matchesType = aType in TYPE_CHECK ? TYPE_CHECK[aType](value) : typeof value == aType;
    if (!matchesType)
      throw Components.Exception(`Update manifest property '${aProperty}' has incorrect type (expected ${aType})`);

    return value;
  }

  function getRequiredProperty(aObj, aProperty, aType) {
    let value = getProperty(aObj, aProperty, aType);
    if (value === undefined)
      throw Components.Exception(`Update manifest is missing a required ${aProperty} property.`);
    return value;
  }

  let manifest = aManifestData;

  if (!TYPE_CHECK["object"](manifest))
    throw Components.Exception("Root element of update manifest must be a JSON object literal");

  // The set of add-ons this manifest has updates for
  let addons = getRequiredProperty(manifest, "addons", "object");

  // The entry for this particular add-on
  let addon = getProperty(addons, aId, "object");

  // A missing entry doesn't count as a failure, just as no avialable update
  // information
  if (!addon) {
    logger.warn("Update manifest did not contain an entry for " + aId);
    return [];
  }

  // The list of available updates
  let updates = getProperty(addon, "updates", "array", []);

  let results = [];

  for (let update of updates) {
    let version = getRequiredProperty(update, "version", "string");

    logger.debug(`Found an update entry for ${aId} version ${version}`);

    let applications = getProperty(update, "applications", "object",
                                   { gecko: {} });

    // "gecko" is currently the only supported application entry. If
    // it's missing, skip this update.
    if (!("gecko" in applications)) {
      logger.debug("gecko not in application entry, skipping update of ${addon}")
      continue;
    }

    let app = getProperty(applications, "gecko", "object");

    let appEntry = {
      id: TOOLKIT_ID,
      minVersion: getProperty(app, "strict_min_version", "string",
                              AddonManagerPrivate.webExtensionsMinPlatformVersion),
      maxVersion: "*",
    };

    let result = {
      id: aId,
      version,
      multiprocessCompatible: getProperty(update, "multiprocess_compatible", "boolean", true),
      updateURL: getProperty(update, "update_link", "string"),
      updateHash: getProperty(update, "update_hash", "string"),
      updateInfoURL: getProperty(update, "update_info_url", "string"),
      strictCompatibility: false,
      targetApplications: [appEntry],
    };

    if ("strict_max_version" in app) {
      if ("advisory_max_version" in app) {
        logger.warn("Ignoring 'advisory_max_version' update manifest property for " +
                    aId + " property since 'strict_max_version' also present");
      }

      appEntry.maxVersion = getProperty(app, "strict_max_version", "string");
      result.strictCompatibility = appEntry.maxVersion != "*";
    } else if ("advisory_max_version" in app) {
      appEntry.maxVersion = getProperty(app, "advisory_max_version", "string");
    }

    // The JSON update protocol requires an SHA-2 hash. RDF still
    // supports SHA-1, for compatibility reasons.
    sanitizeUpdateURL(result, aRequest, /^sha(256|512):/, "sha256 or sha512");

    results.push(result);
  }
  return results;
}

/**
 * Starts downloading an update manifest and then passes it to an appropriate
 * parser to convert to an array of update objects
 *
 * @param  aId
 *         The ID of the add-on being checked for updates
 * @param  aUpdateKey
 *         An optional update key for the add-on
 * @param  aUrl
 *         The URL of the update manifest
 * @param  aObserver
 *         An observer to pass results to
 */
function UpdateParser(aId, aUpdateKey, aUrl, aObserver) {
  this.id = aId;
  this.updateKey = aUpdateKey;
  this.observer = aObserver;
  this.url = aUrl;

  let requireBuiltIn = true;
  try {
    requireBuiltIn = Services.prefs.getBoolPref(PREF_UPDATE_REQUIREBUILTINCERTS);
  } catch (e) {
  }

  logger.debug("Requesting " + aUrl);
  try {
    this.request = new ServiceRequest();
    this.request.open("GET", this.url, true);
    this.request.channel.notificationCallbacks = new CertUtils.BadCertHandler(!requireBuiltIn);
    this.request.channel.loadFlags |= Ci.nsIRequest.LOAD_BYPASS_CACHE;
    // Prevent the request from writing to cache.
    this.request.channel.loadFlags |= Ci.nsIRequest.INHIBIT_CACHING;
    this.request.overrideMimeType("text/plain");
    this.request.setRequestHeader("Moz-XPI-Update", "1", true);
    this.request.timeout = TIMEOUT;
    this.request.addEventListener("load", () => this.onLoad(), false);
    this.request.addEventListener("error", () => this.onError(), false);
    this.request.addEventListener("timeout", () => this.onTimeout(), false);
    this.request.send(null);
  } catch (e) {
    logger.error("Failed to request update manifest", e);
  }
}

UpdateParser.prototype = {
  id: null,
  updateKey: null,
  observer: null,
  request: null,
  url: null,

  /**
   * Called when the manifest has been successfully loaded.
   */
  onLoad() {
    let request = this.request;
    this.request = null;
    this._doneAt = new Error("place holder");

    let requireBuiltIn = true;
    try {
      requireBuiltIn = Services.prefs.getBoolPref(PREF_UPDATE_REQUIREBUILTINCERTS);
    } catch (e) {
    }

    try {
      CertUtils.checkCert(request.channel, !requireBuiltIn);
    } catch (e) {
      logger.warn("Request failed: " + this.url + " - " + e);
      this.notifyError(AddonUpdateChecker.ERROR_DOWNLOAD_ERROR);
      return;
    }

    if (!Components.isSuccessCode(request.status)) {
      logger.warn("Request failed: " + this.url + " - " + request.status);
      this.notifyError(AddonUpdateChecker.ERROR_DOWNLOAD_ERROR);
      return;
    }

    let channel = request.channel;
    if (channel instanceof Ci.nsIHttpChannel && !channel.requestSucceeded) {
      logger.warn("Request failed: " + this.url + " - " + channel.responseStatus +
           ": " + channel.responseStatusText);
      this.notifyError(AddonUpdateChecker.ERROR_DOWNLOAD_ERROR);
      return;
    }

    // Detect the manifest type by first attempting to parse it as
    // JSON, and falling back to parsing it as XML if that fails.
    let parser;
    try {
      try {
        let json = JSON.parse(request.responseText);

        parser = () => parseJSONManifest(this.id, this.updateKey, request, json);
      } catch (e) {
        if (!(e instanceof SyntaxError))
          throw e;
        let domParser = Cc["@mozilla.org/xmlextras/domparser;1"].createInstance(Ci.nsIDOMParser);
        let xml = domParser.parseFromString(request.responseText, "text/xml");

        if (xml.documentElement.namespaceURI == XMLURI_PARSE_ERROR)
          throw new Error("Update manifest was not valid XML or JSON");

        parser = () => parseRDFManifest(this.id, this.updateKey, request, xml);
      }
    } catch (e) {
      logger.warn("onUpdateCheckComplete failed to determine manifest type");
      this.notifyError(AddonUpdateChecker.ERROR_UNKNOWN_FORMAT);
      return;
    }

    let results;
    try {
      results = parser();
    } catch (e) {
      logger.warn("onUpdateCheckComplete failed to parse update manifest", e);
      this.notifyError(AddonUpdateChecker.ERROR_PARSE_ERROR);
      return;
    }

    if ("onUpdateCheckComplete" in this.observer) {
      try {
        this.observer.onUpdateCheckComplete(results);
      } catch (e) {
        logger.warn("onUpdateCheckComplete notification failed", e);
      }
    } else {
      logger.warn("onUpdateCheckComplete may not properly cancel", new Error("stack marker"));
    }
  },

  /**
   * Called when the request times out
   */
  onTimeout() {
    this.request = null;
    this._doneAt = new Error("Timed out");
    logger.warn("Request for " + this.url + " timed out");
    this.notifyError(AddonUpdateChecker.ERROR_TIMEOUT);
  },

  /**
   * Called when the manifest failed to load.
   */
  onError() {
    if (!Components.isSuccessCode(this.request.status)) {
      logger.warn("Request failed: " + this.url + " - " + this.request.status);
    } else if (this.request.channel instanceof Ci.nsIHttpChannel) {
      try {
        if (this.request.channel.requestSucceeded) {
          logger.warn("Request failed: " + this.url + " - " +
               this.request.channel.responseStatus + ": " +
               this.request.channel.responseStatusText);
        }
      } catch (e) {
        logger.warn("HTTP Request failed for an unknown reason");
      }
    } else {
      logger.warn("Request failed for an unknown reason");
    }

    this.request = null;
    this._doneAt = new Error("UP_onError");

    this.notifyError(AddonUpdateChecker.ERROR_DOWNLOAD_ERROR);
  },

  /**
   * Helper method to notify the observer that an error occured.
   */
  notifyError(aStatus) {
    if ("onUpdateCheckError" in this.observer) {
      try {
        this.observer.onUpdateCheckError(aStatus);
      } catch (e) {
        logger.warn("onUpdateCheckError notification failed", e);
      }
    }
  },

  /**
   * Called to cancel an in-progress update check.
   */
  cancel() {
    if (!this.request) {
      logger.error("Trying to cancel already-complete request", this._doneAt);
      return;
    }
    this.request.abort();
    this.request = null;
    this._doneAt = new Error("UP_cancel");
    this.notifyError(AddonUpdateChecker.ERROR_CANCELLED);
  }
};

/**
 * Tests if an update matches a version of the application or platform
 *
 * @param  aUpdate
 *         The available update
 * @param  aAppVersion
 *         The application version to use
 * @param  aPlatformVersion
 *         The platform version to use
 * @param  aIgnoreMaxVersion
 *         Ignore maxVersion when testing if an update matches. Optional.
 * @param  aIgnoreStrictCompat
 *         Ignore strictCompatibility when testing if an update matches. Optional.
 * @param  aCompatOverrides
 *         AddonCompatibilityOverride objects to match against. Optional.
 * @return true if the update is compatible with the application/platform
 */
function matchesVersions(aUpdate, aAppVersion, aPlatformVersion,
                         aIgnoreMaxVersion, aIgnoreStrictCompat,
                         aCompatOverrides) {
  if (aCompatOverrides) {
    let override = AddonRepository.findMatchingCompatOverride(aUpdate.version,
                                                              aCompatOverrides,
                                                              aAppVersion,
                                                              aPlatformVersion);
    if (override && override.type == "incompatible")
      return false;
  }

  if (aUpdate.strictCompatibility && !aIgnoreStrictCompat)
    aIgnoreMaxVersion = false;

  let result = false;
  for (let app of aUpdate.targetApplications) {
    if (app.id == Services.appinfo.ID) {
      return (Services.vc.compare(aAppVersion, app.minVersion) >= 0) &&
             (aIgnoreMaxVersion || (Services.vc.compare(aAppVersion, app.maxVersion) <= 0));
    }
    if (app.id == TOOLKIT_ID) {
      result = (Services.vc.compare(aPlatformVersion, app.minVersion) >= 0) &&
               (aIgnoreMaxVersion || (Services.vc.compare(aPlatformVersion, app.maxVersion) <= 0));
    }
  }
  return result;
}

this.AddonUpdateChecker = {
  // These must be kept in sync with AddonManager
  // The update check timed out
  ERROR_TIMEOUT: -1,
  // There was an error while downloading the update information.
  ERROR_DOWNLOAD_ERROR: -2,
  // The update information was malformed in some way.
  ERROR_PARSE_ERROR: -3,
  // The update information was not in any known format.
  ERROR_UNKNOWN_FORMAT: -4,
  // The update information was not correctly signed or there was an SSL error.
  ERROR_SECURITY_ERROR: -5,
  // The update was cancelled
  ERROR_CANCELLED: -6,

  /**
   * Retrieves the best matching compatibility update for the application from
   * a list of available update objects.
   *
   * @param  aUpdates
   *         An array of update objects
   * @param  aVersion
   *         The version of the add-on to get new compatibility information for
   * @param  aIgnoreCompatibility
   *         An optional parameter to get the first compatibility update that
   *         is compatible with any version of the application or toolkit
   * @param  aAppVersion
   *         The version of the application or null to use the current version
   * @param  aPlatformVersion
   *         The version of the platform or null to use the current version
   * @param  aIgnoreMaxVersion
   *         Ignore maxVersion when testing if an update matches. Optional.
   * @param  aIgnoreStrictCompat
   *         Ignore strictCompatibility when testing if an update matches. Optional.
   * @return an update object if one matches or null if not
   */
  getCompatibilityUpdate(aUpdates, aVersion, aIgnoreCompatibility,
                                   aAppVersion, aPlatformVersion,
                                   aIgnoreMaxVersion, aIgnoreStrictCompat) {
    if (!aAppVersion)
      aAppVersion = Services.appinfo.version;
    if (!aPlatformVersion)
      aPlatformVersion = Services.appinfo.platformVersion;

    for (let update of aUpdates) {
      if (Services.vc.compare(update.version, aVersion) == 0) {
        if (aIgnoreCompatibility) {
          for (let targetApp of update.targetApplications) {
            let id = targetApp.id;
            if (id == Services.appinfo.ID || id == TOOLKIT_ID)
              return update;
          }
        } else if (matchesVersions(update, aAppVersion, aPlatformVersion,
                                 aIgnoreMaxVersion, aIgnoreStrictCompat)) {
          return update;
        }
      }
    }
    return null;
  },

  /**
   * Returns the newest available update from a list of update objects.
   *
   * @param  aUpdates
   *         An array of update objects
   * @param  aAppVersion
   *         The version of the application or null to use the current version
   * @param  aPlatformVersion
   *         The version of the platform or null to use the current version
   * @param  aIgnoreMaxVersion
   *         When determining compatible updates, ignore maxVersion. Optional.
   * @param  aIgnoreStrictCompat
   *         When determining compatible updates, ignore strictCompatibility. Optional.
   * @param  aCompatOverrides
   *         Array of AddonCompatibilityOverride to take into account. Optional.
   * @return an update object if one matches or null if not
   */
  getNewestCompatibleUpdate(aUpdates, aAppVersion, aPlatformVersion,
                                      aIgnoreMaxVersion, aIgnoreStrictCompat,
                                      aCompatOverrides) {
    if (!aAppVersion)
      aAppVersion = Services.appinfo.version;
    if (!aPlatformVersion)
      aPlatformVersion = Services.appinfo.platformVersion;

    let blocklist = Cc["@mozilla.org/extensions/blocklist;1"].
                    getService(Ci.nsIBlocklistService);

    let newest = null;
    for (let update of aUpdates) {
      if (!update.updateURL)
        continue;
      let state = blocklist.getAddonBlocklistState(update, aAppVersion, aPlatformVersion);
      if (state != Ci.nsIBlocklistService.STATE_NOT_BLOCKED)
        continue;
      if ((newest == null || (Services.vc.compare(newest.version, update.version) < 0)) &&
          matchesVersions(update, aAppVersion, aPlatformVersion,
                          aIgnoreMaxVersion, aIgnoreStrictCompat,
                          aCompatOverrides)) {
        newest = update;
      }
    }
    return newest;
  },

  /**
   * Starts an update check.
   *
   * @param  aId
   *         The ID of the add-on being checked for updates
   * @param  aUpdateKey
   *         An optional update key for the add-on
   * @param  aUrl
   *         The URL of the add-on's update manifest
   * @param  aObserver
   *         An observer to notify of results
   * @return UpdateParser so that the caller can use UpdateParser.cancel() to shut
   *         down in-progress update requests
   */
  checkForUpdates(aId, aUpdateKey, aUrl, aObserver) {
    return new UpdateParser(aId, aUpdateKey, aUrl, aObserver);
  }
};

```

## tests/tests/files/templates5.h
```
template <typename T>
class nsTArray {
 public:
  nsTArray(const T& aT) : myT(aT) {}

  template <typename Foo = T>
  bool Contains(T aT, Foo foo) {
    return myT == aT || myT == foo;
  }

 private:
  T myT;
};

```

## tests/tests/files/templates6.h
```
inline void overloaded(int i) {}
inline void overloaded(char c) {}

```

## tests/tests/files/using.cpp
```
#include <stdint.h>

template <typename T>
using MyType = T;

template <class T>
class TemplatedClass {
  using SomeType = MyType<T>;

  int foo() {
    SomeType x;
    return x;
  }
};

MyType<int32_t> xyz;
TemplatedClass<int> abc;

```

## tests/tests/files/templates7.cpp
```
// Testcase proposed by Botond on
// https://bugzilla.mozilla.org/show_bug.cgi?id=1833695

struct Theme {
  void CreateWebRenderCommandsForWidget() {
    OutOfLineTemplateShouldntHaveContextSym(0);
    OutOfLineShouldntHaveContextSym(0);
    InlineTemplateShouldHaveContextSym(0);
    InlineShouldHaveContextSym(0);
  }

  template <typename T>
  void OutOfLineTemplateShouldntHaveContextSym(T);

  void OutOfLineShouldntHaveContextSym(int);

  template <typename T>
  inline void InlineTemplateShouldHaveContextSym(T) {}

  inline void InlineShouldHaveContextSym(int) {}
};

template <typename T>
void Theme::OutOfLineTemplateShouldntHaveContextSym(T) {}

void Theme::OutOfLineShouldntHaveContextSym(int) {}

```

## tests/tests/files/bug1435345.cpp
```
/* Searchfox should analyze uses of operator== */

struct Foo {
  Foo(int aX) : mX(aX) {}
  bool operator==(const Foo& aOther) { return mX == aOther.mX; }

  int mX;
};

int main(int argc, char** argv) {
  Foo a(0);
  Foo b(1);
  return a == b;
}

```

## tests/tests/files/simple.rs
```
extern crate test_rust_dependency;

use std::path::{Path, PathBuf};
#[allow(unused_imports)]
use test_rust_dependency::my_mod::MyOtherType;
use test_rust_dependency::{MyTrait, MyType};

/* A grab-bug of rust code to exercise the searchfox indexer.
   Note how this comment ends up in the file description, too!
*/

mod build_time_generated {
    include!(concat!(env!("OUT_DIR"), "/generated.rs"));
}

#[derive(Clone)]
#[allow(dead_code)]
pub struct Loader {
    #[allow(unused)]
    whatever: build_time_generated::GeneratedType,
    deps_dir: PathBuf,
    my_type: MyType,
}

impl MyTrait for Loader {
    fn do_bar() -> i32 {
        10000
    }

    fn do_foo() -> MyType {
        MyType::new().do_foo()
    }
}

#[allow(dead_code, non_snake_case)]
extern "C" fn WithoutNoMangle() {}

#[no_mangle]
extern "C" fn WithNoMangle() {}

#[allow(dead_code)]
impl Loader {
    pub fn new(deps_dir: PathBuf) -> Self {
        Self {
            whatever: build_time_generated::GeneratedType { some_num: 1 },
            deps_dir,
            my_type: MyType::new(),
        }
    }

    fn needs_hard_reload(&self, _: &Path) -> bool {
        unsafe {
            test_rust_dependency::ExternFunctionImplementedInCpp();
        }
        true
    }

    fn set_path_prefix(&mut self, _: &Path) {
        MyType::new().do_foo();
    }

    fn abs_path_prefix(&self) -> Option<PathBuf> {
        None
    }
    fn search_directories(&self) -> Vec<PathBuf> {
        vec![self.deps_dir.clone()]
    }
}

#[allow(dead_code)]
enum AnEnum {
    Variant1,
    Variant2,
}

#[allow(dead_code)]
fn simple_fn() {
    let my_enum = AnEnum::Variant1;
    match my_enum {
        AnEnum::Variant1 => println!("Yay"),
        _ => println!("Boo"),
    }
}

#[allow(dead_code)]
struct MultiParams<'a, T> {
    myvar: T,
    another_var: &'a u32,
}

#[allow(dead_code, clippy::needless_lifetimes)]
impl<'a, T> MultiParams<'a, T> {
    fn fn_with_params_in_signature(&self) {}
}

mod rust;

```

## tests/tests/files/xpidl/xpctest_cenums.idl
```
/* -*- Mode: C; tab-width: 8; indent-tabs-mode: nil; c-basic-offset: 4 -*-
 *
 * This Source Code Form is subject to the terms of the Mozilla Public
 * License, v. 2.0. If a copy of the MPL was not distributed with this
 * file, You can obtain one at http://mozilla.org/MPL/2.0/. */

#include "nsISupports.idl"
/*
 *	This defines the interface for a test object.
 *
 */

[scriptable, uuid(6a2f918e-cda2-11e8-bc9a-a34c716d1f2a)]
interface nsIXPCTestCEnums : nsISupports {
  const long testConst = 1;

  cenum testFlagsExplicit: 8 {
    shouldBe1Explicit = 1,
    shouldBe2Explicit = 2,
    shouldBe4Explicit = 4,
    shouldBe8Explicit = 8,
    shouldBe12Explicit = shouldBe4Explicit | shouldBe8Explicit,
  };

  cenum testFlagsImplicit: 8 {
    shouldBe0Implicit,
    shouldBe1Implicit,
    shouldBe2Implicit,
    shouldBe3Implicit,
    shouldBe5Implicit = 5,
    shouldBe6Implicit,
    shouldBe2AgainImplicit = 2,
    shouldBe3AgainImplicit,
  };

  void testCEnumInput(in nsIXPCTestCEnums_testFlagsExplicit abc);

  nsIXPCTestCEnums_testFlagsExplicit testCEnumOutput();
};

```

## tests/tests/files/xpidl/xpctest_params.idl
```
/* This Source Code Form is subject to the terms of the Mozilla Public
 * License, v. 2.0. If a copy of the MPL was not distributed with this
 * file, You can obtain one at http://mozilla.org/MPL/2.0/. */

/**
 * Test pararameter passing and argument conversion.
 *
 * Each test method returns the value in 'b', and copies 'a' into 'b'. This lets
 * us test return values, in params, and inout params (out params should be
 * covered by the intersection of return values and inout).
 */

#include "nsISupports.idl"

interface nsIURI;
interface nsIXPCTestInterfaceA;
interface nsIXPCTestInterfaceB;

[scriptable, uuid(812145c7-9fcc-425e-a878-36ad1b7730b7)]
interface nsIXPCTestParams : nsISupports {

  // These types correspond to the ones in typelib.py
  boolean               testBoolean(in boolean a, inout boolean b);
  octet                 testOctet(in octet a, inout octet b);
  short                 testShort(in short a, inout short b);
  long                  testLong(in long a, inout long b);
  long long             testLongLong(in long long a, inout long long b);
  unsigned short        testUnsignedShort(in unsigned short a, inout unsigned short b);
  unsigned long         testUnsignedLong(in unsigned long a, inout unsigned long b);
  unsigned long long    testUnsignedLongLong(in unsigned long long a, inout unsigned long long b);
  float                 testFloat(in float a, inout float b);
  double                testDouble(in double a, inout float b);
  char                  testChar(in char a, inout char b);
  string                testString(in string a, inout string b);
  wchar                 testWchar(in wchar a, inout wchar b);
  wstring               testWstring(in wstring a, inout wstring b);
  AString               testAString(in AString a, inout AString b);
  AUTF8String           testAUTF8String(in AUTF8String a, inout AUTF8String b);
  ACString              testACString(in ACString a, inout ACString b);
  jsval                 testJsval(in jsval a, inout jsval b);

  // Test various forms of the Array<T> type.
  Array<short>          testShortSequence(in Array<short> a, inout Array<short> b);
  Array<double>         testDoubleSequence(in Array<double> a, inout Array<double> b);
  Array<nsIXPCTestInterfaceA> testInterfaceSequence(in Array<nsIXPCTestInterfaceA> a, inout Array<nsIXPCTestInterfaceA> b);
  Array<AString>        testAStringSequence(in Array<AString> a, inout Array<AString> b);
  Array<ACString>       testACStringSequence(in Array<ACString> a, inout Array<ACString> b);
  Array<jsval>          testJsvalSequence(in Array<jsval> a, inout Array<jsval> b);
  Array<Array<short> >  testSequenceSequence(in Array<Array<short> > a, inout Array<Array<short> > b);

  void                  testInterfaceIsSequence(in nsIIDPtr aIID, [iid_is(aIID)] in Array<nsQIResult> a,
                                                inout nsIIDPtr bIID, [iid_is(bIID)] inout Array<nsQIResult> b,
                                                out nsIIDPtr rvIID, [retval, iid_is(rvIID)] out Array<nsQIResult> rv);

  // Returns whatever was passed in.
  Array<uint8_t>        testOptionalSequence([optional] in Array<uint8_t> arr);

  //
  // Dependent parameters use the same types as above, but are handled much differently.
  //

  // Test arrays.
  void                  testShortArray(in unsigned long aLength, [array, size_is(aLength)] in short a,
                                       inout unsigned long bLength, [array, size_is(bLength)] inout short b,
                                       out unsigned long rvLength, [retval, array, size_is(rvLength)] out short rv);
  void                  testDoubleArray(in unsigned long aLength, [array, size_is(aLength)] in double a,
                                        inout unsigned long bLength, [array, size_is(bLength)] inout double b,
                                        out unsigned long rvLength, [retval, array, size_is(rvLength)] out double rv);
  void                  testStringArray(in unsigned long aLength, [array, size_is(aLength)] in string a,
                                        inout unsigned long bLength, [array, size_is(bLength)] inout string b,
                                        out unsigned long rvLength, [retval, array, size_is(rvLength)] out string rv);
  void                  testWstringArray(in unsigned long aLength, [array, size_is(aLength)] in wstring a,
                                         inout unsigned long bLength, [array, size_is(bLength)] inout wstring b,
                                         out unsigned long rvLength, [retval, array, size_is(rvLength)] out wstring rv);
  void                  testInterfaceArray(in unsigned long aLength, [array, size_is(aLength)] in nsIXPCTestInterfaceA a,
                                           inout unsigned long bLength, [array, size_is(bLength)] inout nsIXPCTestInterfaceA b,
                                           out unsigned long rvLength, [retval, array, size_is(rvLength)] out nsIXPCTestInterfaceA rv);

  // uint8 array with optional length. Returns array length.
  unsigned long         testByteArrayOptionalLength([array, size_is(aLength)] in uint8_t a, [optional] in unsigned long aLength);

  // Test sized strings.
  void                  testSizedString(in unsigned long aLength, [size_is(aLength)] in string a,
                                        inout unsigned long bLength, [size_is(bLength)] inout string b,
                                        out unsigned long rvLength, [retval, size_is(rvLength)] out string rv);
  void                  testSizedWstring(in unsigned long aLength, [size_is(aLength)] in wstring a,
                                         inout unsigned long bLength, [size_is(bLength)] inout wstring b,
                                         out unsigned long rvLength, [retval, size_is(rvLength)] out wstring rv);

  // Test iid_is.
  void                  testInterfaceIs(in nsIIDPtr aIID, [iid_is(aIID)] in nsQIResult a,
                                        inout nsIIDPtr bIID, [iid_is(bIID)] inout nsQIResult b,
                                        out nsIIDPtr rvIID, [retval, iid_is(rvIID)] out nsQIResult rv);

  // Test arrays of iid_is. According to khuey we don't use it for anything
  // in mozilla-central, but calendar stuff depends on it.
  void                  testInterfaceIsArray(in unsigned long aLength, in nsIIDPtr aIID,
                                             [array, size_is(aLength), iid_is(aIID)] in nsQIResult a,
                                             inout unsigned long bLength, inout nsIIDPtr bIID,
                                             [array, size_is(bLength), iid_is(bIID)] inout nsQIResult b,
                                             out unsigned long rvLength, out nsIIDPtr rvIID,
                                             [retval, array, size_is(rvLength), iid_is(rvIID)] out nsQIResult rv);

  // Test arrays of jsvals
  void                  testJsvalArray(in unsigned long aLength, [array, size_is(aLength)] in jsval a,
                                       inout unsigned long bLength, [array, size_is(bLength)] inout jsval b,
                                       out unsigned long rvLength, [retval, array, size_is(rvLength)] out jsval rv);


  // Test for out dipper parameters
  void                 testOutAString(out AString o);

  // Test for optional array size_is.
  ACString             testStringArrayOptionalSize([array, size_is(aLength)] in string a, [optional] in unsigned long aLength);

  // Test for omitted optional out parameter.
  void                 testOmittedOptionalOut([optional] out nsIURI aOut);
};

```

## tests/tests/files/xpidl/nsrootidl.idl
```
/* -*- Mode: IDL; tab-width: 4; indent-tabs-mode: nil; c-basic-offset: 4 -*- */
/* This Source Code Form is subject to the terms of the Mozilla Public
 * License, v. 2.0. If a copy of the MPL was not distributed with this
 * file, You can obtain one at http://mozilla.org/MPL/2.0/. */

/**
 * Root idl declarations to be used by all.
 */

%{C++

#include "nscore.h"
typedef int64_t PRTime;

/*
 * Forward declarations for new string types
 */
#include "nsStringFwd.h"

struct JSContext;

/*
 * Forward declaration of mozilla::dom::Promise
 */
namespace mozilla {
namespace dom {
class Promise;
} // namespace dom
} // namespace mozilla

/*
 * Start commenting out the C++ versions of the below in the output header
 */
#if 0
%}

typedef boolean             bool   ;
typedef octet               uint8_t  ;
typedef unsigned short      uint16_t ;
typedef unsigned short      char16_t;
typedef unsigned long       uint32_t ;
typedef unsigned long long  uint64_t ;
typedef long long           PRTime   ;
typedef short               int16_t  ;
typedef long                int32_t  ;
typedef long long           int64_t  ;

typedef unsigned long       nsrefcnt ;
typedef unsigned long       nsresult ;

// XXX need this built into xpidl compiler so that it's really size_t or size_t
// and it's scriptable:
typedef unsigned long       size_t;

[ptr]         native voidPtr(void);
[ptr]         native charPtr(char);
[ptr]         native unicharPtr(char16_t);

[ref, nsid]   native nsIDRef(nsID);
[ref, nsid]   native nsIIDRef(nsIID);
[ref, nsid]   native nsCIDRef(nsCID);

[ptr, nsid]   native nsIDPtr(nsID);
[ptr, nsid]   native nsIIDPtr(nsIID);
[ptr, nsid]   native nsCIDPtr(nsCID);

// NOTE: Be careful in using the following 3 types. The *Ref and *Ptr variants
// are more commonly used (and better supported). Those variants require
// nsMemory alloc'd copies when used as 'out' params while these types do not.
// However, currently these types can not be used for 'in' params. And, methods
// that use them as 'out' params *must* be declared [notxpcom] (with an explicit
// return type of nsresult). This makes such methods implicitly not scriptable.
// Use of these types in methods without a [notxpcom] declaration will cause
// the xpidl compiler to raise an error.
// See: http://bugzilla.mozilla.org/show_bug.cgi?id=93792

[nsid]        native nsIID(nsIID);
[nsid]        native nsID(nsID);
[nsid]        native nsCID(nsCID);

[ptr]         native nsQIResult(void);

[ref, utf8string] native AUTF8String(ignored);
[ref, utf8string] native AUTF8StringRef(ignored);
[ptr, utf8string] native AUTF8StringPtr(ignored);

[ref, cstring] native ACString(ignored);
[ref, cstring] native ACStringRef(ignored);
[ptr, cstring] native ACStringPtr(ignored);

[ref, astring] native AString(ignored);
[ref, astring] native AStringRef(ignored);
[ptr, astring] native AStringPtr(ignored);

[ref, jsval]  native jsval(jsval);
              native jsid(jsid);

[ptr, promise] native Promise(ignored);

%{C++
/*
 * End commenting out the C++ versions of the above in the output header
 */
#endif
%}

```

## tests/tests/files/xpidl/nsISupports.idl
```
/* -*- Mode: IDL; tab-width: 4; indent-tabs-mode: nil; c-basic-offset: 4 -*- */
/* This Source Code Form is subject to the terms of the Mozilla Public
 * License, v. 2.0. If a copy of the MPL was not distributed with this
 * file, You can obtain one at http://mozilla.org/MPL/2.0/. */

/**
 * The mother of all xpcom interfaces.
 */

/* In order to get both the right typelib and the right header we force
*  the 'real' output from xpidl to be commented out in the generated header
*  and includes a copy of the original nsISupports.h. This is all just to deal
*  with the Mac specific ": public __comobject" thing.
*/

#include "nsrootidl.idl"

%{C++
/*
 * Start commenting out the C++ versions of the below in the output header
 */
#if 0
%}

[scriptable, uuid(00000000-0000-0000-c000-000000000046)]
interface nsISupports {
  void QueryInterface(in nsIIDRef uuid,
                      [iid_is(uuid),retval] out nsQIResult result);
  [noscript, notxpcom] nsrefcnt AddRef();
  [noscript, notxpcom] nsrefcnt Release();
};

%{C++
/*
 * End commenting out the C++ versions of the above in the output header
 */
#endif
%}


%{C++
#include "nsISupportsBase.h"
#include "nsISupportsUtils.h"
%}

```

## tests/tests/files/xpidl/xpctest_attributes.idl
```
/* -*- Mode: C; tab-width: 8; indent-tabs-mode: nil; c-basic-offset: 4 -*-
 *
 * This Source Code Form is subject to the terms of the Mozilla Public
 * License, v. 2.0. If a copy of the MPL was not distributed with this
 * file, You can obtain one at http://mozilla.org/MPL/2.0/. */

#include "nsISupports.idl"
/*
 *	This defines the interface for a test object.
 *
 */

[scriptable, uuid(42fbd9f6-b12d-47ef-b7a1-02d73c11fe53)]
interface nsIXPCTestObjectReadOnly : nsISupports {
	readonly attribute string  strReadOnly;
	readonly attribute boolean boolReadOnly;
	readonly attribute short   shortReadOnly;
	readonly attribute long    longReadOnly;
	readonly attribute float   floatReadOnly;
	readonly attribute char    charReadOnly;
	readonly attribute PRTime  timeReadOnly;
};

[scriptable, uuid(f07529b0-a479-4954-aba5-ab3142c6b1cb)]
interface nsIXPCTestObjectReadWrite : nsISupports {
	attribute string  stringProperty;
	attribute boolean booleanProperty;
	attribute short   shortProperty;
	attribute long    longProperty;
	attribute float   floatProperty;
	attribute char    charProperty;
	attribute PRTime  timeProperty;
};

```

## tests/tests/files/xpidl/xpctest_bug809674.idl
```
/* -*- Mode: C; tab-width: 8; indent-tabs-mode: nil; c-basic-offset: 4 -*-
 *
 * This Source Code Form is subject to the terms of the Mozilla Public
 * License, v. 2.0. If a copy of the MPL was not distributed with this
 * file, You can obtain one at http://mozilla.org/MPL/2.0/. */

#include "nsISupports.idl"
/*
 * Test interface for https://bugzilla.mozilla.org/show_bug.cgi?id=809674 .
 *
 * This test makes sure that accessing JS-implemented attributes or methods
 * marked with [implicit_jscontext] works as expected.
 *
 * It also makes sure [optional_argc] is not supported on JS-implemented
 * methods.
 */

[scriptable, uuid(2df46559-da21-49bf-b863-0d7b7bbcbc73)]
interface nsIXPCTestBug809674 : nsISupports {
    // Various interesting [implicit_jscontext] cases.
    [implicit_jscontext] unsigned long addArgs(in unsigned long x, in unsigned long y);
    [implicit_jscontext] unsigned long addSubMulArgs(in unsigned long x, in unsigned long y,
                                                     out unsigned long subOut,
                                                     out unsigned long mulOut);
    [implicit_jscontext] jsval addVals(in jsval x, in jsval y);

    [implicit_jscontext] unsigned long methodNoArgs();
    [implicit_jscontext] void methodNoArgsNoRetVal();

    // When there are many arguments, the context is passed on the stack on
    // most platforms.
    [implicit_jscontext] unsigned long addMany(in unsigned long x1,
                                               in unsigned long x2,
                                               in unsigned long x3,
                                               in unsigned long x4,
                                               in unsigned long x5,
                                               in unsigned long x6,
                                               in unsigned long x7,
                                               in unsigned long x8);

    // Attributes can use [implicit_jscontext], too.
    [implicit_jscontext] attribute jsval valProperty;
    [implicit_jscontext] attribute unsigned long uintProperty;

    // [optional_argc] is not supported.
    [optional_argc] void methodWithOptionalArgc();
};

```

## tests/tests/files/xpidl/xpctest_interfaces.idl
```
/* This Source Code Form is subject to the terms of the Mozilla Public
 * License, v. 2.0. If a copy of the MPL was not distributed with this
 * file, You can obtain one at http://mozilla.org/MPL/2.0/. */

/**
 * Very simple test interfaces.
 *
 * This is used by the other test functionality when it needs to play around with
 * interface pointers.
 */

#include "nsISupports.idl"

[scriptable, uuid(3c8fd2f5-970c-42c6-b5dd-cda1c16dcfd8)]
interface nsIXPCTestInterfaceA : nsISupports {
  attribute string name;
};

[scriptable, uuid(ff528c3a-2410-46de-acaa-449aa6403a33)]
interface nsIXPCTestInterfaceB : nsISupports {
  attribute string name;
};

[scriptable, uuid(401cf1b4-355b-4cee-b7b3-c7973aee49bd)]
interface nsIXPCTestInterfaceC : nsISupports {
  attribute long someInteger;
};

```

## tests/tests/files/xpidl/xpctest_returncode.idl
```
/* -*- Mode: C; tab-width: 8; indent-tabs-mode: nil; c-basic-offset: 4 -*-
 *
 * This Source Code Form is subject to the terms of the Mozilla Public
 * License, v. 2.0. If a copy of the MPL was not distributed with this
 * file, You can obtain one at http://mozilla.org/MPL/2.0/. */

/**
 * Test the use of Components.returnCode
 *
 * This ("parent") interface defines a method that in-turn calls another
 * ("child") interface implemented in JS, and returns the nsresult from that
 * child interface.  The child interface manages the return code by way of
 * Components.returnCode.
 */

#include "nsISupports.idl"


[scriptable, uuid(479e4532-95cf-48b8-a99b-8a5881e47138)]
interface nsIXPCTestReturnCodeParent : nsISupports {
  // Calls the "child" interface with the specified behavior flag.  Returns
  // the NSRESULT from the child interface.
  nsresult        callChild(in long childBehavior);
};

[scriptable, uuid(672cfd34-1fd1-455d-9901-d879fa6fdb95)]
interface nsIXPCTestReturnCodeChild : nsISupports {
  void doIt(in long behavior);

  // Flags to control that the child does.
  // child will throw a JS exception
  const long CHILD_SHOULD_THROW = 0;

  // child will just return normally
  const long CHILD_SHOULD_RETURN_SUCCESS = 1;

  // child will return after setting Components.returnCode to NS_ERROR_FAILURE
  const long CHILD_SHOULD_RETURN_RESULTCODE = 2;

  // child will set Components.returnCode to NS_ERROR_UNEXPECTED, then create
  // a new component that sets Components.returnCode to NS_ERROR_FAILURE.
  // Our caller should see the NS_ERROR_UNEXPECTED we set rather than the
  // value set later by the "inner" child.
  const long CHILD_SHOULD_NEST_RESULTCODES = 3;
};

```

## tests/tests/files/xpidl/xpctest_utils.idl
```
/* This Source Code Form is subject to the terms of the Mozilla Public
 * License, v. 2.0. If a copy of the MPL was not distributed with this
 * file, You can obtain one at http://mozilla.org/MPL/2.0/. */

/**
 * Utility interfaces for testing.
 */

#include "nsISupports.idl"

[scriptable, function, uuid(d58a82ab-d8f7-4ca9-9273-b3290d42a0cf)]
interface nsIXPCTestFunctionInterface : nsISupports {
  string echo(in string arg);
};

[scriptable, uuid(1e9cddeb-510d-449a-b152-3c1b5b31d41d)]
interface nsIXPCTestUtils : nsISupports {
  nsIXPCTestFunctionInterface doubleWrapFunction(in nsIXPCTestFunctionInterface f);
};

```

## tests/tests/files/python/contextual-keyword.py
```
def print():
    pass

def exec():
    pass

print()
exec()

```

## tests/tests/files/staticprefs/StaticPrefsConsumer.cpp
```
#include <stdint.h>

#include "bindings/StaticPrefList_test.h"
#include "bindings/StaticPrefList_test2.h"

int32_t StaticPrefsConsumer() {
  int32_t v1 = mozilla::StaticPrefs::test_int();
  uint32_t v2 = mozilla::StaticPrefs::test2_uint();
  bool v3 = mozilla::StaticPrefs::test_bool();

  return int32_t(v1) + int32_t(v2) + int32_t(v3);
}

```

## tests/tests/files/staticprefs/StaticPrefList.yaml
```
- name: test.int
  type: int32_t
  value: 10
  mirror: always

- name: test2.uint
  type: uint32_t
  value: 20
  mirror: always

- name: test.bool
  type: bool
  value: false
  mirror: always

# String won't get binding.
- name: test.str
  type: String
  value: "Hello"
  mirror: never

```

## tests/tests/files/staticprefs/bindings/StaticPrefList_test.h
```
#include <stdint.h>

namespace mozilla {
namespace StaticPrefs {

inline int32_t test_int() { return 10; }

inline bool test_bool() { return false; }

}  // namespace StaticPrefs
}  // namespace mozilla

```

## tests/tests/files/staticprefs/bindings/StaticPrefList_test2.h
```
#include <stdint.h>

namespace mozilla {
namespace StaticPrefs {

inline uint32_t test2_uint() { return 20; }

}  // namespace StaticPrefs
}  // namespace mozilla

```

## tests/tests/files/spaces  are bad.txt
```
The spaces in this file's name cause trouble!
Trouble right here in test directory!

```

## tests/tests/files/runnables.cpp
```
#include <stdio.h>
#include <stdlib.h>

class Dispatcher;

class nsIRunnable {
 public:
  virtual void Run(Dispatcher* aDispatcher) = 0;
};

void common_shoe_related_method(void) { printf("shoes do things\n"); }

class Dispatcher {
 public:
  void Dispatch(nsIRunnable* aRunnable) {
    // eh, we don't do anything right now.
  }
};

class ShoelaceRunnable : public nsIRunnable {
 public:
  ShoelaceRunnable() = default;

  void Run(Dispatcher* aDispatcher) override {
    printf("I am a shoelace!\n");
    common_shoe_related_method();
  }
};

class ShoeRunnable : public nsIRunnable {
 public:
  ShoeRunnable() = default;

  void Run(Dispatcher* aDispatcher) override {
    printf("I am a shoe that runs!\n");
    ShoelaceRunnable shoelace;
    aDispatcher->Dispatch(&shoelace);
  }
};

class SandalRunnable : public ShoeRunnable {
 public:
  SandalRunnable() = default;

  void Run(Dispatcher* aDispatcher) override {
    printf("I am a shoe that is a sandal that runs!\n");
    common_shoe_related_method();
  }
};

int main(void) {
  Dispatcher d;

  ShoeRunnable shoe;
  SandalRunnable sandal;
  d.Dispatch(&shoe);
  d.Dispatch(&sandal);
}

```

## tests/tests/files/test_DOMWindowCreated_chromeonly.html
```
<!DOCTYPE html>
<head>
  <title>DOMWindowCreated not visible in content</title>
  <script type="application/javascript" src="/tests/SimpleTest/SimpleTest.js"></script>
  <link rel="stylesheet" type="text/css" href="/tests/SimpleTest/test.css"/>
<body onload="ok(true, 'Test finished'); SimpleTest.finish();">
  <p id="display"></p>

  <script type="application/javascript">
  SimpleTest.waitForExplicitFinish();
  window.addEventListener("DOMWindowCreated", function() { ok(false, "DOMWindowCreated should not have fired"); }, false);
  window.addEventListener("DOMDocElementInserted", function() { ok(false, "DOMDocElementInserted should not have fired"); }, false);

  <iframe src="data:text/plain,Hi"></iframe>

```

## tests/tests/files/big_cpp.cpp
```
/**
 * This test file attempts to create the following situations:
 * - Multiple levels of lexical scoping.
 * - Those lexical blocks are sufficiently large that it's common for the
 *   block open to be off of the screen so that a "position:sticky" style
 *   display would be appropriate.
 * - Call structures potentially look interesting if you graph them.
 *
 * This is accomplished by:
 * - Many silly comment blocks.
 * - Tons of copy and pasting and search and replace.
 * - Panicking when trying to trying to come up with subject matter and deciding
 *   that cats and dogs work.
 * - Not using templates.
 */

#include <memory>
#include <stdio.h>
#include <stdlib.h>

#include "big_header.h"
#include "subdir/header@with,many^strange~chars.h"
#include "atom_magic.h"

class GlobalContext {
 public:
  static bool decideBooleanTrait() {
    int rval = rand();

    // boop boop
    //
    // boop

    int midpointValue = RAND_MAX / 2;

    if (rval > midpointValue) {
      // beep
      //
      // beep beep beep
      //
      // beep
      //
      //      beep
      //
      //           beep
      return true;
    }

    // BEEP BEEP BEEP BEEP
    // BEEP           BEEP
    // BEEP           BEEP
    // BEEP           BEEP
    // BEEP           BEEP
    // BEEP           BEEP
    // BEEP           BEEP
    // BEEP           BEEP
    // BEEP           BEEP
    // BEEP BEEP BEEP BEEP

    return false;
  }

  static bool decideEnigmaticAnimalBooleanTrait() {
    return decideBooleanTrait();
  }

  static bool decideCatBooleanTrait() {
    return decideEnigmaticAnimalBooleanTrait();
  }

  static bool decideBestFriendBooleanTrait() { return decideBooleanTrait(); }

  class LessGlobalContext {
   public:
    // BARK
    //     BARK
    //         BARK
    //             BARK
    //                 BARK
    //                     BARK
    //                         BARK
    //                             BARK
    //                                 BARK
    static bool decideWhetherToDecide() {
      // BEEP BEEP BEEP BEEP
      // BEEP           BEEP
      // BEEP           BEEP
      // BEEP           BEEP
      // BEEP           BEEP
      // BEEP           BEEP
      // BEEP           BEEP
      // BEEP           BEEP
      // BEEP           BEEP
      // BEEP BEEP BEEP BEEP
      return true;
    }

    // BARK
    //     BARK
    //         BARK
    //             BARK
    //                 BARK
    //                     BARK
    //                         BARK
    //                             BARK
    //                                 BARK
  };

  // BARK
  //     BARK
  //         BARK
  //             BARK
  //                 BARK
  //                     BARK
  //                         BARK
  //                             BARK
  //                                 BARK

  static bool decideDogBooleanTrait() {
    if (LessGlobalContext::decideWhetherToDecide()) {
      return decideBestFriendBooleanTrait();
    }

    return false;
  };
};

namespace outerNS {

#define HUMAN_HP 100

class CycleCollectingMagic {
  virtual void unlink(void* raw) {}
};

class Thing {
 protected:
  // Finally, an example class that could evolve into a MUD!
  //
  // Health
  // Points
  int mHP;

  // Existence
  // Points
  bool mDefunct;

 public:
  Thing(int baseHP) : mHP(baseHP), mDefunct(false) {
    // bop.
  }

  void ignore();

  virtual void takeDamage(int damage) {
    mHP -= damage;

    if (damage <= 0) {
      mDefunct = true;
      damage = 0;
    }
  }
};

void Thing::ignore() {
  // ignore
  // i g n o r e
  // i  g  n  o  r  e
  // i   g   n   o   r   e
  // i    g    n    o    r    e
  // i   g   n   o   r   e
  // i    g    n    o    r    e
  // i   g   n   o   r   e
  // i  g  n  o  r  e
  // i g n o r e
}

class Human : public Thing {
 public:
  Human() : Thing(HUMAN_HP) {}
};

class Superhero : public Human {
 public:
  Superhero() : Human() {}

  void takeDamage(int damage) override {
    // nope
    //   nope
    //     nope!
    //   ...
    // Superheroes don't take damage.
  }
};

class Couch : public Thing {
 public:
  Couch(int couchHP = 20) : Thing(couchHP) {
    Superhero superBob;
    WhatsYourVector<Superhero> victor(&superBob);

    Human bob;
    WhatsYourVector<Human> goodReferenceRight(&bob);

    victor.forwardDeclaredTemplateThingInlinedBelow(this);
    goodReferenceRight.forwardDeclaredTemplateThingInlinedBelow(this);
  }
};

class OuterCat : Thing {
 private:
  // Cat secrets!
  // The secrets of cats!
  // These cannot be known to humans.
  // Or anyone.
  // Except perhaps other cats.
  // Or perhaps not.
  // Or perhaps...
  //           ...
  //           ...
  //           ...
  //           ...not!

  bool mIsFriendly;
  bool mIsSecretlyUnfriendly;

  std::shared_ptr<Human> mOwner;
  Couch* mFavoriteCouch;

 public:
  // These things can be known.
  // But they are methods.
  // So they're not really things you know.
  // Although there are getters.
  // Shoot, maybe those should be protected.
  // Okay, now they're protected.
  // Although you haven't read that far down yet.
  // The comments don't get better.

  OuterCat(bool beFriendly,
           // what gets position:sticky'd here do you suppose
           // and how long does it last?
           // ...
           // ...
           // hm.
           bool beSecretlyUnfriendly)
      // more hm.
      // yes, very hm.
      // hm hm hm.
      : Thing(9 * HUMAN_HP),
        mIsFriendly(beFriendly)
        // Unknown.
        // ...
        // Okay, we can probably implement things now.
        ,
        mIsSecretlyUnfriendly(beSecretlyUnfriendly) {
    // Meow
    //  Meow
    //   Meow
    //    Meow
    //     Meow
    //      Meow
    //       Meow
    //     Meow
    //    Meow
    //   Meow
    //    Meow
    //     Meow
    //      Meow
    //       Meow
    //        Meow
    //         Meow
    //          Meow
    //           Meow
    //            Meow
    //             Meow
    //              Meow Meow Meow Meow Meow Meow Meow
    //                                            Meow
    //                                             Meow
    //                                              Meow
    //                                             Meow
    //                                            Meow
    //                                           Meow
    //                                          Meow
    //                                         Meow
    //                                        Meow
    //                                       Meow
    //                                      Meow
    //                                     Meow
  }

 protected:
  // Sorta secret things.
  // Like, other cats know these things.
  // But still, not for humans.
  // Not now.
  // Not ever.
  // Unless some type of special cat x-ray is developed.
  // Boy, wouldn't that be a thing.
  // Humanity is come so far, and yet we don't have a specialized cat x-ray...
  // one capable of seeing into the true nature of a cat.

  bool isFriendlyCat() { return mIsFriendly; }

  bool isSecretlyUnfriendly() {
    //                                             Meow
    //                                              Meow
    //                                             Meow
    //                                            Meow
    //                                           Meow
    //                                          Meow
    //                                         Meow
    //                                        Meow
    //                                       Meow
    //                                      Meow
    //                                     Meow
    return mIsSecretlyUnfriendly;
  }

  bool isFriendlyIfNotCurrentlyVisible() {
    if (isSecretlyUnfriendly()) {
      return true;
    }

    return isFriendlyCat();
  }

 public:
  class CycleCollectingCat : CycleCollectingMagic {
    void unlink(void* raw) override {
      OuterCat* cat = (OuterCat*)raw;
      cat->mOwner = nullptr;
      cat->mFavoriteCouch = nullptr;
    }
  };

  friend class CycleCollectingCat;

  void meet(Human& human) { human.ignore(); }

  /**
   * Something there is that doesn't love a couch.
   *
   * A cat.
   *
   * A cat doesn't love a couch.
   */
  void meet(Couch& couch) {
    shred(couch);

    if (!isFriendlyCat()) {
      // D
      //  E
      //   S
      //    T
      //     R
      //      O
      //       Y
      destroy(couch);
    } else if (isFriendlyIfNotCurrentlyVisible()) {
      // NO
      //
      // D
      //  E
      //   S
      //    T
      //     R
      //      O
      //       Y

      // do nothing
    } else {
      // D
      //  E
      //   S
      //    T
      //     R
      //      O
      //       Y
      destroy(couch);
    }
  }

  /**
   * Standard cat destruction.
   */
  void shred(Thing& thing) { thing.takeDamage(1); }

  /**
   * More thorough cat destruction.
   */
  void destroy(Thing& thing) {
    // s
    shred(thing);

    //  h
    shred(thing);

    //   r
    shred(thing);

    //    e
    shred(thing);

    //     d
    shred(thing);
  }
};

#define ART_HP 100

class AbstractArt : public Thing {
 public:
  AbstractArt() : Thing(ART_HP) {}

  // This pure virtual method needs to be treated like a definition for our
  // structured record emission purposes.
  virtual void beArt() = 0;
};

class PracticalArt : public AbstractArt {
 public:
  PracticalArt() : AbstractArt() {}

  // This should properly see the beArt as something it's overriding.
  void beArt() override {
    // Apprecaite in value!
    mHP++;
  }
};

class StackArtHolder {
  PracticalArt& mHeldArt;

 public:
  StackArtHolder(PracticalArt& aArt) : mHeldArt(aArt) {}
};

namespace innerNS {

class InnerCat {};

namespace {

class AnonCat {};

#ifdef DEBUG

class DebugAnonCat {};

#else  // not ifdef DEBUG

class NondebugAnonCat {};

#endif

};  // end anonymous namespace

}  // end namespace innerNS

}  // end namespace outerNS

void i_was_declared_in_the_header() {
  // Perhaps there was a bug where the declaration might have been treated as a
  // definition and then, adding insult to injury, the range from this file was
  // exposed in the header.
}

```

## tests/tests/files/atom_magic.h
```
/* This file tries to approximate nsGkAtoms.h by operating in conjunction with
   `atom_list.h` while also providing an example of a header file that's
   included multiple times.  (We didn't really have coverage for this before,
   and this is important for our merging logic.) */

#ifndef atom_magic_h___
#define atom_magic_h___

struct YoAtoms {
#define YO_ATOM(name_, value_) const char16_t name_##_string[sizeof(value_)];
#include "atom_list.h"
#undef YO_ATOM

  enum class Atoms {
#define YO_ATOM(name_, value_) name_,
#include "atom_list.h"
#undef YO_ATOM
    AtomsCount
  };
};

#endif /* atom_magic_h__ */

```

## tests/tests/files/css/embed_css.html
```
<!DOCTYPE html>
<html>
  <head>
    <meta charset="utf-8">
    <title>css</title>
  </head>
  <body>
    <style>
:root {
  --in-content-text-color: #0c0c0d;
  --text-color: var(--in-content-text-color);
  --base-unit: 4px;
  --secondary-text-color: var(--grey-50);
  --è‰²: red;
}

body {
  background: url("chrome://global/content/test/chrome1.png");
}

a {
  background-image: url("chrome://global/content/test/chrome1.png");
}
    </style>
    <div style="color: var(--text-color);">A</div>
    <div style="background: url('chrome://global/content/test/chrome1.png');">B</div>
  </body>
</html>

```

## tests/tests/files/css/test.css
```
:root {
  --in-content-text-color: #0c0c0d;
  --text-color: var(--in-content-text-color);
  --base-unit: 4px;
  --secondary-text-color: var(--grey-50);
  --è‰²: red;
}

body {
  background: url("chrome://global/content/test/chrome1.png");
}

a {
  background-image: url("chrome://global/content/test/chrome1.png");
}

div {
  background-image: url(chrome://global/content/test/chrome1.png);
}

@property --at-prop {
}

div {
  --at-prop: 10;
  transition: --at-prop 10s;

  @property --at-prop-2 {
  }
  --at-prop-2: 10;
  transition: --at-prop-2 10s;
}

```

## tests/tests/files/third_party/a_file
```
This is a file in the magical third_party folder, that shows up in a separate section in the search results.

```

## tests/tests/files/invalid-files/unterminated-rust-str.rs
```
br##

```

## tests/tests/files/jni.cpp
```
struct JNIEnv {};
struct jobject {};
struct jclass {};

static void Java_sample_Jni_autoNativeStaticMethod(JNIEnv*, jclass) {
  // Real code
}

static void Java_sample_Jni_autoNativeMethod(JNIEnv*, jobject) {
  // Real code
}

class __attribute__((
    annotate("binding_to", "jvm", "class", "S_jvm_sample/Jni#"))) Jni {
  __attribute__((annotate("binding_to", "jvm", "method",
                          "S_jvm_sample/Jni#javaStaticMethod()."))) static void
  javaStaticMethod() {
    // Wrapper
  }
  __attribute__((annotate("binding_to", "jvm", "method",
                          "S_jvm_sample/Jni#javaMethod()."))) void
  javaMethod() {
    // Wrapper
  }

  __attribute__((annotate("binding_to", "jvm", "getter",
                          "S_jvm_sample/Jni#javaField."))) int
  javaField() {
    // Wrapper
    return 0;
  }
  __attribute__((annotate("binding_to", "jvm", "setter",
                          "S_jvm_sample/Jni#javaField."))) void
  javaField(int) {
    // Wrapper
  }
  __attribute__((annotate(
      "binding_to", "jvm", "const",
      "S_jvm_sample/Jni#javaConst."))) static constexpr int javaConst = 5;

  void user() {
    javaStaticMethod();
    javaMethod();
    javaField();
    javaField(javaConst);
  }
};

class __attribute__((
    annotate("bound_as", "jvm", "class", "S_jvm_sample/Jni#"))) Nji {
  __attribute__((
      annotate("bound_as", "jvm", "method",
               "S_jvm_sample/Jni#nativeStaticMethod()."))) static void
  nativeStaticMethod() {
    // Real code
  }
  __attribute__((annotate("bound_as", "jvm", "method",
                          "S_jvm_sample/Jni#nativeMethod()."))) void
  nativeMethod() {
    // Real code
  }

  void user() {
    nativeStaticMethod();
    nativeMethod();
  }
};

```

## tests/tests/files/templates5.cpp
```
#include "templates5.h"

bool func() {
  nsTArray<int> intarray(3);
  nsTArray<float> floatarray(3.5);
  return intarray.Contains(5, 3) || floatarray.Contains(3.5, 5);
}

```

## tests/tests/files/simple.cpp
```
#include <stdio.h>

#include "atom_magic.h"

extern "C" void WithNoMangle();

extern "C" void ExternFunctionImplementedInCpp() {}

namespace NS {

struct R;

enum { TAG3 };

typedef struct {
  int f;

  bool operator()(int);
} Abc;

struct R {
  enum XYZ { TAG1, TAG2 };

  virtual void v() = 0;
};

struct S : public R {
  S();
  ~S();
  void m();
  void m(int);
  virtual void v();
};

namespace {
int xyz;
};

struct S2 {
  virtual void v() = 0;
};

struct T : public S, public S2 {
  virtual void v();
  void m();
  void m(int);

  int field;
};

template <typename T>
struct OtherObj {
  OtherObj(char c) {}
};

template <typename T>
struct StackObj {
  StackObj(int x) : mOther('x') {}

  OtherObj<T> mOther;
};

void f() {}
void g();

int cxx14DigitSeparators() { return 0b1100'1111; }

typedef R OtherR;

template <typename T>
class X {
 public:
  X() {}

  void f();

  int field;
};

template <typename T>
void X<T>::f() {}

template <>
void X<int>::f() {}

template <typename T>
void templateFunc(const T& arg);

template <>
void templateFunc(const char& arg);

struct Dummy {
#define DECL_SOMETHING(Env, Name) \
  static bool Name() { return Env; }

  DECL_SOMETHING(true, Hello);
  DECL_SOMETHING(false, Goodbye);
};
}  // namespace NS

#define HELLO s.m

class Q {
  typedef int (Q::*AddressReader)(const char*) const;
};

extern int GLOBAL;

int main() {
  GLOBAL = NS::TAG3;

  NS::OtherR* otherr;

  NS::f();
  NS::g();
  NS::S s;
  s.m();
  HELLO(4);

#ifdef HELLO
  int abc;
#endif

#if defined(HELLO)
  int abc1;
#endif

#undef HELLO

  void (*fp)();
  fp = &NS::f;
  fp();

  NS::S* sp = new NS::S();

  NS::X<char> xx;
  xx.f();
  xx.field = 12;

  NS::X<int> xy;
  xy.f();

  NS::templateFunc(47);

  NS::templateFunc('c');

  NS::Dummy::Hello();

  NS::StackObj<int> stackobj(10);

  WithNoMangle();

  return 0;
}

```

## tests/tests/files/templates1.cpp
```
class Foo {
 public:
  void Method();
};

class Bar {
 public:
  template <class T>
  void Function(T* t);
};

template <class T>
inline void Bar::Function(T* t) {
  t->Method();
}

int main() {
  Foo* f;
  Bar* b;
  b->Function(f);
  return 0;
}

```

## tests/tests/files/multiline.cpp
```
#include "atom_magic.h"

/**
 * Some documentation.
 */
void f(int a, int b, int c) { return; }

struct Forward;

class B {};
class C {};

/* Class is is some
 * thing that does stuff.
 */
class A : public B, private C {
  /*
   * This is field.
   */
  int field;

  // other field;
  int other;

  friend class B;
};

```

## tests/tests/mc-generated/ipc/ipdl/_ipdlheaders/mozilla/_ipdltest/PTestBasic.h
```
//
// Automatically generated by ipdlc.
// Edit at your own risk
//

#ifndef PTestBasic_h
#define PTestBasic_h

#include "mozilla/Attributes.h"
#include "IPCMessageStart.h"
#include "mozilla/RefPtr.h"
#include "nsString.h"
#include "nsTArray.h"
#include "nsTHashtable.h"
#include "mozilla/MozPromise.h"
#include "mozilla/OperatorNewExtensions.h"
#include "mozilla/UniquePtr.h"
#include "mozilla/ipc/ByteBuf.h"
#include "mozilla/ipc/FileDescriptor.h"
#include "mozilla/ipc/ProtocolUtilsFwd.h"
#include "mozilla/ipc/Shmem.h"

namespace mozilla {
namespace _ipdltest {
class PTestBasicParent;
} // namespace _ipdltest
} // namespace mozilla
namespace mozilla {
namespace _ipdltest {
class PTestBasicChild;
} // namespace _ipdltest
} // namespace mozilla

//-----------------------------------------------------------------------------
// Code common to PTestBasicChild and PTestBasicParent
//
namespace mozilla {
namespace _ipdltest {
namespace PTestBasic {

nsresult
CreateEndpoints(
        base::ProcessId aParentDestPid,
        base::ProcessId aChildDestPid,
        mozilla::ipc::Endpoint<mozilla::_ipdltest::PTestBasicParent>* aParent,
        mozilla::ipc::Endpoint<mozilla::_ipdltest::PTestBasicChild>* aChild);

nsresult
CreateEndpoints(
        mozilla::ipc::Endpoint<mozilla::_ipdltest::PTestBasicParent>* aParent,
        mozilla::ipc::Endpoint<mozilla::_ipdltest::PTestBasicChild>* aChild);

enum MessageType {
    PTestBasicStart = PTestBasicMsgStart << 16,
    Msg_Hello__ID,
    PTestBasicEnd
};

mozilla::UniquePtr<IPC::Message>
Msg_Hello(int32_t routingId);



} // namespace PTestBasic
} // namespace _ipdltest
} // namespace mozilla

#endif // ifndef PTestBasic_h

```

## tests/tests/mc-generated/ipc/ipdl/_ipdlheaders/mozilla/_ipdltest/PTestBasicChild.h
```
//
// Automatically generated by ipdlc.
// Edit at your own risk
//

#ifndef PTestBasicChild_h
#define PTestBasicChild_h

#include "mozilla/_ipdltest/PTestBasic.h"
#ifdef DEBUG
#include "prenv.h"
#endif  // DEBUG

#include "mozilla/Tainting.h"
#include "mozilla/ipc/MessageChannel.h"
#include "mozilla/ipc/ProtocolUtils.h"
namespace mozilla {
namespace _ipdltest {


class PTestBasicChild :
    public mozilla::ipc::IToplevelProtocol
{
private:
protected:
    typedef mozilla::ipc::ActorHandle ActorHandle;
    typedef mozilla::ipc::ByteBuf ByteBuf;
    template<class FooSide> using Endpoint = mozilla::ipc::Endpoint<FooSide>;
    typedef mozilla::ipc::FileDescriptor FileDescriptor;
    template<class FooSide> using ManagedEndpoint = mozilla::ipc::ManagedEndpoint<FooSide>;
    typedef base::ProcessId ProcessId;
    typedef mozilla::ipc::ProtocolId ProtocolId;
    typedef mozilla::ipc::ResponseRejectReason ResponseRejectReason;
    typedef mozilla::ipc::Shmem Shmem;
    template<class T> using UniquePtr = mozilla::UniquePtr<T>;

protected:
    void
    ProcessingError(
            Result aCode,
            const char* aReason) override;
    bool
    ShouldContinueFromReplyTimeout() override;

public:
    typedef mozilla::ipc::IProtocol IProtocol;
    typedef IPC::Message Message;
    typedef base::ProcessHandle ProcessHandle;
    typedef mozilla::ipc::MessageChannel MessageChannel;
    typedef mozilla::ipc::SharedMemory SharedMemory;

public:
    MOZ_IMPLICIT PTestBasicChild();

    virtual ~PTestBasicChild();

    NS_INLINE_DECL_PURE_VIRTUAL_REFCOUNTING
protected:
    void ActorAlloc() final { AddRef(); }
    void ActorDealloc() final { Release(); }
public:
    void
    AllManagedActors(nsTArray<RefPtr<mozilla::ipc::ActorLifecycleProxy>>& arr__) const override;

    void
    RemoveManagee(
            int32_t aProtocolId,
            IProtocol* aListener) override;
    void
    DeallocManagee(
            int32_t aProtocolId,
            IProtocol* aListener) override;

    Result
    OnMessageReceived(const Message& msg__) override;

    Result
    OnMessageReceived(
            const Message& msg__,
            UniquePtr<Message>& reply__) override;

    Result
    OnCallReceived(
            const Message& msg__,
            UniquePtr<Message>& reply__) override;

    void
    OnChannelClose() override;

    void
    OnChannelError() override;

private:
    void
    ClearSubtree();

};


} // namespace _ipdltest
} // namespace mozilla
namespace IPC {
template<>
struct ParamTraits <mozilla::_ipdltest::PTestBasicChild*>
{
    typedef mozilla::_ipdltest::PTestBasicChild* paramType;
    static void
    Write(
            IPC::MessageWriter* aWriter,
            const paramType& aVar);
    static bool
    Read(
            IPC::MessageReader* aReader,
            paramType* aVar);
};
} // namespace IPC

#endif // ifndef PTestBasicChild_h

```

## tests/tests/mc-generated/ipc/ipdl/_ipdlheaders/mozilla/_ipdltest/PTestBasicParent.h
```
//
// Automatically generated by ipdlc.
// Edit at your own risk
//

#ifndef PTestBasicParent_h
#define PTestBasicParent_h

#include "mozilla/_ipdltest/PTestBasic.h"
#ifdef DEBUG
#include "prenv.h"
#endif  // DEBUG

#include "mozilla/Tainting.h"
#include "mozilla/ipc/MessageChannel.h"
#include "mozilla/ipc/ProtocolUtils.h"
class nsIFile;

namespace mozilla {
namespace _ipdltest {


class PTestBasicParent :
    public mozilla::ipc::IToplevelProtocol
{
private:
protected:
    typedef mozilla::ipc::ActorHandle ActorHandle;
    typedef mozilla::ipc::ByteBuf ByteBuf;
    template<class FooSide> using Endpoint = mozilla::ipc::Endpoint<FooSide>;
    typedef mozilla::ipc::FileDescriptor FileDescriptor;
    template<class FooSide> using ManagedEndpoint = mozilla::ipc::ManagedEndpoint<FooSide>;
    typedef base::ProcessId ProcessId;
    typedef mozilla::ipc::ProtocolId ProtocolId;
    typedef mozilla::ipc::ResponseRejectReason ResponseRejectReason;
    typedef mozilla::ipc::Shmem Shmem;
    template<class T> using UniquePtr = mozilla::UniquePtr<T>;

protected:
    void
    ProcessingError(
            Result aCode,
            const char* aReason) override;
    bool
    ShouldContinueFromReplyTimeout() override;

public:
    typedef mozilla::ipc::IProtocol IProtocol;
    typedef IPC::Message Message;
    typedef base::ProcessHandle ProcessHandle;
    typedef mozilla::ipc::MessageChannel MessageChannel;
    typedef mozilla::ipc::SharedMemory SharedMemory;

public:
    MOZ_IMPLICIT PTestBasicParent();

    virtual ~PTestBasicParent();

    NS_INLINE_DECL_PURE_VIRTUAL_REFCOUNTING
protected:
    void ActorAlloc() final { AddRef(); }
    void ActorDealloc() final { Release(); }
public:
    void
    AllManagedActors(nsTArray<RefPtr<mozilla::ipc::ActorLifecycleProxy>>& arr__) const override;

    [[nodiscard]] bool
    SendHello();

    void
    RemoveManagee(
            int32_t aProtocolId,
            IProtocol* aListener) override;
    void
    DeallocManagee(
            int32_t aProtocolId,
            IProtocol* aListener) override;

    Result
    OnMessageReceived(const Message& msg__) override;

    Result
    OnMessageReceived(
            const Message& msg__,
            UniquePtr<Message>& reply__) override;

    Result
    OnCallReceived(
            const Message& msg__,
            UniquePtr<Message>& reply__) override;

    void
    OnChannelClose() override;

    void
    OnChannelError() override;

private:
    void
    ClearSubtree();

};


} // namespace _ipdltest
} // namespace mozilla
namespace IPC {
template<>
struct ParamTraits <mozilla::_ipdltest::PTestBasicParent*>
{
    typedef mozilla::_ipdltest::PTestBasicParent* paramType;
    static void
    Write(
            IPC::MessageWriter* aWriter,
            const paramType& aVar);
    static bool
    Read(
            IPC::MessageReader* aReader,
            paramType* aVar);
};
} // namespace IPC

#endif // ifndef PTestBasicParent_h

```

## tests/tests/mc-generated/ipdl/PTestBasic.cpp
```
//
// Automatically generated by ipdlc.
// Edit at your own risk
//


#include "mozilla/_ipdltest/PTestBasic.h"
#include "mozilla/_ipdltest/PTestBasicParent.h"
#include "mozilla/_ipdltest/PTestBasicChild.h"

#include "ipc/IPCMessageUtils.h"
#include "ipc/IPCMessageUtilsSpecializations.h"
#include "nsIFile.h"
#include "mozilla/ipc/Endpoint.h"
#include "mozilla/ipc/ProtocolMessageUtils.h"
#include "mozilla/ipc/ProtocolUtils.h"
#include "mozilla/ipc/ShmemMessageUtils.h"
#include "mozilla/ipc/TaintingIPCUtils.h"

namespace mozilla {
namespace _ipdltest {
namespace PTestBasic {

nsresult
CreateEndpoints(
        base::ProcessId aParentDestPid,
        base::ProcessId aChildDestPid,
        mozilla::ipc::Endpoint<mozilla::_ipdltest::PTestBasicParent>* aParent,
        mozilla::ipc::Endpoint<mozilla::_ipdltest::PTestBasicChild>* aChild)
{
    return mozilla::ipc::CreateEndpoints(
        mozilla::ipc::PrivateIPDLInterface(),
        aParentDestPid, aChildDestPid, aParent, aChild);
}
nsresult
CreateEndpoints(
        mozilla::ipc::Endpoint<mozilla::_ipdltest::PTestBasicParent>* aParent,
        mozilla::ipc::Endpoint<mozilla::_ipdltest::PTestBasicChild>* aChild)
{
    return mozilla::ipc::CreateEndpoints(
        mozilla::ipc::PrivateIPDLInterface(),
        aParent, aChild);
}
mozilla::UniquePtr<IPC::Message>
Msg_Hello(int32_t routingId)
{
    return IPC::Message::IPDLMessage(routingId, Msg_Hello__ID, 0, IPC::Message::HeaderFlags(IPC::Message::NOT_NESTED, IPC::Message::NORMAL_PRIORITY, IPC::Message::COMPRESSION_NONE, IPC::Message::NOT_CONSTRUCTOR, IPC::Message::ASYNC, IPC::Message::NOT_REPLY));
}

} // namespace PTestBasic
} // namespace _ipdltest
} // namespace mozilla

```

## tests/tests/mc-generated/ipdl/PTestBasicChild.cpp
```
//
// Automatically generated by ipdlc.
// Edit at your own risk
//


#include "mozilla/_ipdltest/PTestBasicChild.h"
#include "mozilla/ProfilerLabels.h"
#include "mozilla/_ipdltest/TestBasicChild.h"


#include "ipc/IPCMessageUtils.h"
#include "ipc/IPCMessageUtilsSpecializations.h"
#include "nsIFile.h"
#include "mozilla/ipc/Endpoint.h"
#include "mozilla/ipc/ProtocolMessageUtils.h"
#include "mozilla/ipc/ProtocolUtils.h"
#include "mozilla/ipc/ShmemMessageUtils.h"
#include "mozilla/ipc/TaintingIPCUtils.h"

namespace mozilla {
namespace _ipdltest {


auto PTestBasicChild::ProcessingError(
        Result aCode,
        const char* aReason) -> void
{
}

auto PTestBasicChild::ShouldContinueFromReplyTimeout() -> bool
{
    return true;
}

MOZ_IMPLICIT PTestBasicChild::PTestBasicChild() :
    mozilla::ipc::IToplevelProtocol("PTestBasicChild", PTestBasicMsgStart, mozilla::ipc::ChildSide)
{
    MOZ_COUNT_CTOR(PTestBasicChild);
}

PTestBasicChild::~PTestBasicChild()
{
    MOZ_COUNT_DTOR(PTestBasicChild);
}

auto PTestBasicChild::AllManagedActors(nsTArray<RefPtr<mozilla::ipc::ActorLifecycleProxy>>& arr__) const -> void
{
    uint32_t total = 0;
    arr__.SetCapacity(total);

}

auto PTestBasicChild::RemoveManagee(
        int32_t aProtocolId,
        IProtocol* aListener) -> void
{
    FatalError("unreached");
    return;
}

auto PTestBasicChild::DeallocManagee(
        int32_t aProtocolId,
        IProtocol* aListener) -> void
{
    FatalError("unreached");
    return;
}

auto PTestBasicChild::OnMessageReceived(const Message& msg__) -> PTestBasicChild::Result
{
    switch (msg__.type()) {
    case PTestBasic::Msg_Hello__ID:
        {
            if (mozilla::ipc::LoggingEnabledFor("PTestBasicChild")) {
                mozilla::ipc::LogMessageForProtocol(
                    "PTestBasicChild",
                    this->ToplevelProtocol()->OtherPidMaybeInvalid(),
                    "Received ",
                    (&(msg__))->type(),
                    mozilla::ipc::MessageDirection::eReceiving);
            }
            AUTO_PROFILER_LABEL("PTestBasic::Msg_Hello", OTHER);

            mozilla::ipc::IPCResult __ok = (static_cast<TestBasicChild*>(this))->RecvHello();
            if ((!(__ok))) {
                mozilla::ipc::ProtocolErrorBreakpoint("Handler returned error code!");
                // Error handled in mozilla::ipc::IPCResult
                return MsgProcessingError;
            }

            return MsgProcessed;
        }
    default:
        return MsgNotKnown;
    case SHMEM_CREATED_MESSAGE_TYPE:
        {
            FatalError("this protocol tree does not use shmem");
            return MsgNotKnown;
        }
    case SHMEM_DESTROYED_MESSAGE_TYPE:
        {
            FatalError("this protocol tree does not use shmem");
            return MsgNotKnown;
        }
    }
}

auto PTestBasicChild::OnMessageReceived(
        const Message& msg__,
        UniquePtr<Message>& reply__) -> PTestBasicChild::Result
{
    MOZ_ASSERT_UNREACHABLE("message protocol not supported");
    return MsgNotKnown;
}

auto PTestBasicChild::OnCallReceived(
        const Message& msg__,
        UniquePtr<Message>& reply__) -> PTestBasicChild::Result
{
    MOZ_ASSERT_UNREACHABLE("message protocol not supported");
    return MsgNotKnown;
}

auto PTestBasicChild::OnChannelClose() -> void
{
    DestroySubtree(NormalShutdown);
    ClearSubtree();
    DeallocShmems();
    if (GetLifecycleProxy()) {
        GetLifecycleProxy()->Release();
    }
}

auto PTestBasicChild::OnChannelError() -> void
{
    DestroySubtree(AbnormalShutdown);
    ClearSubtree();
    DeallocShmems();
    if (GetLifecycleProxy()) {
        GetLifecycleProxy()->Release();
    }
}

auto PTestBasicChild::ClearSubtree() -> void
{
}



} // namespace _ipdltest
} // namespace mozilla
namespace IPC {
auto ParamTraits<mozilla::_ipdltest::PTestBasicChild*>::Write(
        IPC::MessageWriter* aWriter,
        const paramType& aVar) -> void
{
    MOZ_RELEASE_ASSERT(
        aWriter->GetActor(),
        "Cannot serialize managed actors without an actor");

    int32_t id;
    if (!aVar) {
        id = 0;  // kNullActorId
    } else {
        id = aVar->Id();
        if (id == 1) {  // kFreedActorId
            aVar->FatalError("Actor has been |delete|d");
        }
        MOZ_RELEASE_ASSERT(
            aWriter->GetActor()->GetIPCChannel() == aVar->GetIPCChannel(),
            "Actor must be from the same channel as the"
            " actor it's being sent over");
        MOZ_RELEASE_ASSERT(
            aVar->CanSend(),
            "Actor must still be open when sending");
    }

    IPC::WriteParam(aWriter, id);
}

auto ParamTraits<mozilla::_ipdltest::PTestBasicChild*>::Read(
        IPC::MessageReader* aReader,
        paramType* aVar) -> bool
{
    MOZ_RELEASE_ASSERT(
        aReader->GetActor(),
        "Cannot deserialize managed actors without an actor");

    mozilla::Maybe<mozilla::ipc::IProtocol*> actor =
        aReader->GetActor()->ReadActor(aReader, true, "PTestBasic", PTestBasicMsgStart);
    if (actor.isNothing()) {
        return false;
    }

    *aVar = static_cast<mozilla::_ipdltest::PTestBasicChild*>(actor.value());
    return true;
}

} // namespace IPC

```

## tests/tests/mc-generated/ipdl/PTestBasicParent.cpp
```
//
// Automatically generated by ipdlc.
// Edit at your own risk
//


#include "mozilla/_ipdltest/PTestBasicParent.h"
#include "mozilla/ProfilerLabels.h"
#include "mozilla/_ipdltest/TestBasicParent.h"


#include "ipc/IPCMessageUtils.h"
#include "ipc/IPCMessageUtilsSpecializations.h"
#include "nsIFile.h"
#include "mozilla/ipc/Endpoint.h"
#include "mozilla/ipc/ProtocolMessageUtils.h"
#include "mozilla/ipc/ProtocolUtils.h"
#include "mozilla/ipc/ShmemMessageUtils.h"
#include "mozilla/ipc/TaintingIPCUtils.h"

namespace mozilla {
namespace _ipdltest {


auto PTestBasicParent::ProcessingError(
        Result aCode,
        const char* aReason) -> void
{
}

auto PTestBasicParent::ShouldContinueFromReplyTimeout() -> bool
{
    return true;
}

MOZ_IMPLICIT PTestBasicParent::PTestBasicParent() :
    mozilla::ipc::IToplevelProtocol("PTestBasicParent", PTestBasicMsgStart, mozilla::ipc::ParentSide)
{
    MOZ_COUNT_CTOR(PTestBasicParent);
}

PTestBasicParent::~PTestBasicParent()
{
    MOZ_COUNT_DTOR(PTestBasicParent);
}

auto PTestBasicParent::AllManagedActors(nsTArray<RefPtr<mozilla::ipc::ActorLifecycleProxy>>& arr__) const -> void
{
    uint32_t total = 0;
    arr__.SetCapacity(total);

}

auto PTestBasicParent::SendHello() -> bool
{
    UniquePtr<IPC::Message> msg__ = PTestBasic::Msg_Hello(MSG_ROUTING_CONTROL);
    IPC::MessageWriter writer__{
            (*(msg__)),
            this};





    if (mozilla::ipc::LoggingEnabledFor("PTestBasicParent")) {
        mozilla::ipc::LogMessageForProtocol(
            "PTestBasicParent",
            this->ToplevelProtocol()->OtherPidMaybeInvalid(),
            "Sending ",
            msg__->type(),
            mozilla::ipc::MessageDirection::eSending);
    }
    AUTO_PROFILER_LABEL("PTestBasic::Msg_Hello", OTHER);

    bool sendok__ = ChannelSend(std::move(msg__));
    return sendok__;
}

auto PTestBasicParent::RemoveManagee(
        int32_t aProtocolId,
        IProtocol* aListener) -> void
{
    FatalError("unreached");
    return;
}

auto PTestBasicParent::DeallocManagee(
        int32_t aProtocolId,
        IProtocol* aListener) -> void
{
    FatalError("unreached");
    return;
}

auto PTestBasicParent::OnMessageReceived(const Message& msg__) -> PTestBasicParent::Result
{
    switch (msg__.type()) {
    default:
        return MsgNotKnown;
    case SHMEM_CREATED_MESSAGE_TYPE:
        {
            FatalError("this protocol tree does not use shmem");
            return MsgNotKnown;
        }
    case SHMEM_DESTROYED_MESSAGE_TYPE:
        {
            FatalError("this protocol tree does not use shmem");
            return MsgNotKnown;
        }
    }
}

auto PTestBasicParent::OnMessageReceived(
        const Message& msg__,
        UniquePtr<Message>& reply__) -> PTestBasicParent::Result
{
    MOZ_ASSERT_UNREACHABLE("message protocol not supported");
    return MsgNotKnown;
}

auto PTestBasicParent::OnCallReceived(
        const Message& msg__,
        UniquePtr<Message>& reply__) -> PTestBasicParent::Result
{
    MOZ_ASSERT_UNREACHABLE("message protocol not supported");
    return MsgNotKnown;
}

auto PTestBasicParent::OnChannelClose() -> void
{
    DestroySubtree(NormalShutdown);
    ClearSubtree();
    DeallocShmems();
    if (GetLifecycleProxy()) {
        GetLifecycleProxy()->Release();
    }
}

auto PTestBasicParent::OnChannelError() -> void
{
    DestroySubtree(AbnormalShutdown);
    ClearSubtree();
    DeallocShmems();
    if (GetLifecycleProxy()) {
        GetLifecycleProxy()->Release();
    }
}

auto PTestBasicParent::ClearSubtree() -> void
{
}



} // namespace _ipdltest
} // namespace mozilla
namespace IPC {
auto ParamTraits<mozilla::_ipdltest::PTestBasicParent*>::Write(
        IPC::MessageWriter* aWriter,
        const paramType& aVar) -> void
{
    MOZ_RELEASE_ASSERT(
        aWriter->GetActor(),
        "Cannot serialize managed actors without an actor");

    int32_t id;
    if (!aVar) {
        id = 0;  // kNullActorId
    } else {
        id = aVar->Id();
        if (id == 1) {  // kFreedActorId
            aVar->FatalError("Actor has been |delete|d");
        }
        MOZ_RELEASE_ASSERT(
            aWriter->GetActor()->GetIPCChannel() == aVar->GetIPCChannel(),
            "Actor must be from the same channel as the"
            " actor it's being sent over");
        MOZ_RELEASE_ASSERT(
            aVar->CanSend(),
            "Actor must still be open when sending");
    }

    IPC::WriteParam(aWriter, id);
}

auto ParamTraits<mozilla::_ipdltest::PTestBasicParent*>::Read(
        IPC::MessageReader* aReader,
        paramType* aVar) -> bool
{
    MOZ_RELEASE_ASSERT(
        aReader->GetActor(),
        "Cannot deserialize managed actors without an actor");

    mozilla::Maybe<mozilla::ipc::IProtocol*> actor =
        aReader->GetActor()->ReadActor(aReader, true, "PTestBasic", PTestBasicMsgStart);
    if (actor.isNothing()) {
        return false;
    }

    *aVar = static_cast<mozilla::_ipdltest::PTestBasicParent*>(actor.value());
    return true;
}

} // namespace IPC

```

## tests/tests/mc-analysis/mozilla/_ipdltest/TestBasicParent.h
```
{"loc":"00001:0","target":1,"kind":"def","pretty":"ipc/ipdl/test/gtest/TestBasicParent.h","sym":"FILE_ipc/ipdl/test/gtest/TestBasicParent@2Eh"}
{"loc":"00008:8-43","target":1,"kind":"def","pretty":"mozilla__ipdltest_TestBasicParent_h","sym":"M_993728439989ebea"}
{"loc":"00010:9-47","target":1,"kind":"use","pretty":"__GENERATED__/ipc/ipdl/_ipdlheaders/mozilla/_ipdltest/PTestBasicParent.h","sym":"FILE_android-armv7@__GENERATED__/ipc/ipdl/_ipdlheaders/mozilla/_ipdltest/PTestBasicParent@2Eh"}
{"loc":"00012:10-17","target":1,"kind":"def","pretty":"mozilla","sym":"NS_mozilla"}
{"loc":"00012:19-28","target":1,"kind":"def","pretty":"mozilla::_ipdltest","sym":"NS_mozilla::_ipdltest"}
{"loc":"00014:31-47","target":1,"kind":"use","pretty":"mozilla::_ipdltest::PTestBasicParent","sym":"T_mozilla::_ipdltest::PTestBasicParent","context":"mozilla::_ipdltest::TestBasicParent","contextsym":"T_mozilla::_ipdltest::TestBasicParent"}
{"loc":"00014:6-21","target":1,"kind":"def","pretty":"mozilla::_ipdltest::TestBasicParent","sym":"T_mozilla::_ipdltest::TestBasicParent","peekRange":"14-14"}
{"loc":"00015:2-39","target":1,"kind":"def","pretty":"mozilla::_ipdltest::TestBasicParent::AddRef","sym":"_ZN7mozilla9_ipdltest15TestBasicParent6AddRefEv"}
{"loc":"00015:2-39","target":1,"kind":"def","pretty":"mozilla::_ipdltest::TestBasicParent::HasThreadSafeRefCnt","sym":"T_mozilla::_ipdltest::TestBasicParent::HasThreadSafeRefCnt"}
{"loc":"00015:2-39","target":1,"kind":"def","pretty":"mozilla::_ipdltest::TestBasicParent::Release","sym":"_ZN7mozilla9_ipdltest15TestBasicParent7ReleaseEv"}
{"loc":"00015:2-39","target":1,"kind":"def","pretty":"mozilla::_ipdltest::TestBasicParent::mRefCnt","sym":"F_<T_mozilla::_ipdltest::TestBasicParent>_mRefCnt"}
{"loc":"00015:2-39","target":1,"kind":"use","pretty":"NS_INLINE_DECL_THREADSAFE_REFCOUNTING","sym":"M_85e68e64b289c7f3"}
{"loc":"00015:40-55","target":1,"kind":"use","pretty":"mozilla::_ipdltest::TestBasicParent","sym":"T_mozilla::_ipdltest::TestBasicParent","context":"mozilla::_ipdltest::TestBasicParent::AddRef","contextsym":"_ZN7mozilla9_ipdltest15TestBasicParent6AddRefEv"}
{"loc":"00018:3-18","target":1,"kind":"def","pretty":"mozilla::_ipdltest::TestBasicParent::~TestBasicParent","sym":"_ZN7mozilla9_ipdltest15TestBasicParentD1Ev","context":"mozilla::_ipdltest::TestBasicParent","contextsym":"T_mozilla::_ipdltest::TestBasicParent"}
{"loc":"00018:3-18","target":1,"kind":"use","pretty":"mozilla::_ipdltest::TestBasicParent","sym":"T_mozilla::_ipdltest::TestBasicParent","context":"mozilla::_ipdltest::TestBasicParent::~TestBasicParent","contextsym":"_ZN7mozilla9_ipdltest15TestBasicParentD1Ev"}
{"loc":"00010:9-47","target":1,"kind":"use","pretty":"__GENERATED__/ipc/ipdl/_ipdlheaders/mozilla/_ipdltest/PTestBasicParent.h","sym":"FILE_linux@__GENERATED__/ipc/ipdl/_ipdlheaders/mozilla/_ipdltest/PTestBasicParent@2Eh"}
{"loc":"00010:9-47","target":1,"kind":"use","pretty":"__GENERATED__/ipc/ipdl/_ipdlheaders/mozilla/_ipdltest/PTestBasicParent.h","sym":"FILE_macosx@__GENERATED__/ipc/ipdl/_ipdlheaders/mozilla/_ipdltest/PTestBasicParent@2Eh"}
{"loc":"00010:9-47","target":1,"kind":"use","pretty":"__GENERATED__/ipc/ipdl/_ipdlheaders/mozilla/_ipdltest/PTestBasicParent.h","sym":"FILE_windows@__GENERATED__/ipc/ipdl/_ipdlheaders/mozilla/_ipdltest/PTestBasicParent@2Eh"}
{"loc":"00001:0","source":1,"syntax":"def,file","pretty":"file ipc/ipdl/test/gtest/TestBasicParent.h","sym":"FILE_ipc/ipdl/test/gtest/TestBasicParent@2Eh"}
{"loc":"00008:8-43","source":1,"syntax":"def,macro","pretty":"macro mozilla__ipdltest_TestBasicParent_h","sym":"M_993728439989ebea"}
{"loc":"00010:9-47","source":1,"syntax":"file,use","pretty":"file __GENERATED__/ipc/ipdl/_ipdlheaders/mozilla/_ipdltest/PTestBasicParent.h","sym":"FILE_windows@__GENERATED__/ipc/ipdl/_ipdlheaders/mozilla/_ipdltest/PTestBasicParent@2Eh,FILE_macosx@__GENERATED__/ipc/ipdl/_ipdlheaders/mozilla/_ipdltest/PTestBasicParent@2Eh,FILE_linux@__GENERATED__/ipc/ipdl/_ipdlheaders/mozilla/_ipdltest/PTestBasicParent@2Eh,FILE_android-armv7@__GENERATED__/ipc/ipdl/_ipdlheaders/mozilla/_ipdltest/PTestBasicParent@2Eh"}
{"loc":"00012:10-17","source":1,"syntax":"def,namespace","pretty":"namespace mozilla","sym":"NS_mozilla"}
{"loc":"00012:19-28","source":1,"syntax":"def,namespace","pretty":"namespace mozilla::_ipdltest","sym":"NS_mozilla::_ipdltest","nestingRange":"12:30-21:0"}
{"loc":"00014:6-21","source":1,"syntax":"def,type","pretty":"type mozilla::_ipdltest::TestBasicParent","sym":"T_mozilla::_ipdltest::TestBasicParent","nestingRange":"14:48-19:0"}
{"loc":"00014:31-47","source":1,"syntax":"type,use","pretty":"type mozilla::_ipdltest::PTestBasicParent","sym":"T_mozilla::_ipdltest::PTestBasicParent","type":"class mozilla::_ipdltest::PTestBasicParent","typesym":"T_mozilla::_ipdltest::PTestBasicParent"}
{"loc":"00015:2-39","source":1,"syntax":"def,field","pretty":"field mozilla::_ipdltest::TestBasicParent::mRefCnt","sym":"F_<T_mozilla::_ipdltest::TestBasicParent>_mRefCnt","type":"::mozilla::ThreadSafeAutoRefCnt","typesym":"T_mozilla::ThreadSafeAutoRefCnt"}
{"loc":"00015:2-39","source":1,"syntax":"def,function","pretty":"function mozilla::_ipdltest::TestBasicParent::AddRef","sym":"_ZN7mozilla9_ipdltest15TestBasicParent6AddRefEv","type":"MozExternalRefCountType (void)"}
{"loc":"00015:2-39","source":1,"syntax":"def,function","pretty":"function mozilla::_ipdltest::TestBasicParent::Release","sym":"_ZN7mozilla9_ipdltest15TestBasicParent7ReleaseEv","type":"MozExternalRefCountType (void)"}
{"loc":"00015:2-39","source":1,"syntax":"macro,use","pretty":"macro NS_INLINE_DECL_THREADSAFE_REFCOUNTING","sym":"M_85e68e64b289c7f3"}
{"loc":"00015:2-39","source":1,"syntax":"def,type","pretty":"type mozilla::_ipdltest::TestBasicParent::HasThreadSafeRefCnt","sym":"T_mozilla::_ipdltest::TestBasicParent::HasThreadSafeRefCnt"}
{"loc":"00015:2-39","source":1,"syntax":"","pretty":"variable count","sym":"V_5e5b2_e685d3f013","no_crossref":1,"type":"nsrefcnt"}
{"loc":"00015:40-55","source":1,"syntax":"type,use","pretty":"type mozilla::_ipdltest::TestBasicParent","sym":"T_mozilla::_ipdltest::TestBasicParent","type":"class mozilla::_ipdltest::TestBasicParent","typesym":"T_mozilla::_ipdltest::TestBasicParent"}
{"loc":"00018:3-18","source":1,"syntax":"def,destructor","pretty":"destructor mozilla::_ipdltest::TestBasicParent::~TestBasicParent","sym":"_ZN7mozilla9_ipdltest15TestBasicParentD1Ev","type":"void (void) noexcept"}
{"loc":"00018:3-18","source":1,"syntax":"type,use","pretty":"type mozilla::_ipdltest::TestBasicParent","sym":"T_mozilla::_ipdltest::TestBasicParent","type":"class mozilla::_ipdltest::TestBasicParent","typesym":"T_mozilla::_ipdltest::TestBasicParent"}
{"loc":"00015:2-39","structured":1,"pretty":"mozilla::_ipdltest::TestBasicParent::mRefCnt","sym":"F_<T_mozilla::_ipdltest::TestBasicParent>_mRefCnt","kind":"field","parentsym":"T_mozilla::_ipdltest::TestBasicParent","implKind":"","sizeBytes":null,"supers":[],"methods":[],"fields":[],"overrides":[],"props":[]}
{"loc":"00014:6-21","structured":1,"pretty":"mozilla::_ipdltest::TestBasicParent","sym":"T_mozilla::_ipdltest::TestBasicParent","kind":"class","implKind":"","sizeBytes":376,"supers":[{"pretty":"mozilla::_ipdltest::PTestBasicParent","sym":"T_mozilla::_ipdltest::PTestBasicParent","props":[]}],"methods":[{"pretty":"mozilla::_ipdltest::TestBasicParent::AddRef","sym":"_ZN7mozilla9_ipdltest15TestBasicParent6AddRefEv","props":["instance","virtual","user"]},{"pretty":"mozilla::_ipdltest::TestBasicParent::Release","sym":"_ZN7mozilla9_ipdltest15TestBasicParent7ReleaseEv","props":["instance","virtual","user"]},{"pretty":"mozilla::_ipdltest::TestBasicParent::~TestBasicParent","sym":"_ZN7mozilla9_ipdltest15TestBasicParentD1Ev","props":["instance","virtual","defaulted"]},{"pretty":"mozilla::_ipdltest::TestBasicParent::TestBasicParent","sym":"_ZN7mozilla9_ipdltest15TestBasicParentC1ERKS1_","props":["instance","defaulted","deleted"]},{"pretty":"mozilla::_ipdltest::TestBasicParent::operator=","sym":"_ZN7mozilla9_ipdltest15TestBasicParentaSERKS1_","props":["instance","defaulted","deleted"]}],"fields":[{"pretty":"mozilla::_ipdltest::TestBasicParent::mRefCnt","sym":"F_<T_mozilla::_ipdltest::TestBasicParent>_mRefCnt","type":"::mozilla::ThreadSafeAutoRefCnt","typesym":"T_mozilla::ThreadSafeAutoRefCnt","offsetBytes":368,"bitPositions":null,"sizeBytes":8}],"overrides":[],"props":[],"platforms":["win64"],"variants":[{"structured":1,"pretty":"mozilla::_ipdltest::TestBasicParent","sym":"T_mozilla::_ipdltest::TestBasicParent","kind":"class","implKind":"","sizeBytes":384,"supers":[{"pretty":"mozilla::_ipdltest::PTestBasicParent","sym":"T_mozilla::_ipdltest::PTestBasicParent","props":[]}],"methods":[{"pretty":"mozilla::_ipdltest::TestBasicParent::AddRef","sym":"_ZN7mozilla9_ipdltest15TestBasicParent6AddRefEv","props":["instance","virtual","user"]},{"pretty":"mozilla::_ipdltest::TestBasicParent::Release","sym":"_ZN7mozilla9_ipdltest15TestBasicParent7ReleaseEv","props":["instance","virtual","user"]},{"pretty":"mozilla::_ipdltest::TestBasicParent::~TestBasicParent","sym":"_ZN7mozilla9_ipdltest15TestBasicParentD1Ev","props":["instance","virtual","defaulted"]},{"pretty":"mozilla::_ipdltest::TestBasicParent::TestBasicParent","sym":"_ZN7mozilla9_ipdltest15TestBasicParentC1ERKS1_","props":["instance","defaulted","deleted"]},{"pretty":"mozilla::_ipdltest::TestBasicParent::operator=","sym":"_ZN7mozilla9_ipdltest15TestBasicParentaSERKS1_","props":["instance","defaulted","deleted"]}],"fields":[{"pretty":"mozilla::_ipdltest::TestBasicParent::mRefCnt","sym":"F_<T_mozilla::_ipdltest::TestBasicParent>_mRefCnt","type":"::mozilla::ThreadSafeAutoRefCnt","typesym":"T_mozilla::ThreadSafeAutoRefCnt","offsetBytes":376,"bitPositions":null,"sizeBytes":8}],"overrides":[],"props":[],"platforms":["linux64"]},{"structured":1,"pretty":"mozilla::_ipdltest::TestBasicParent","sym":"T_mozilla::_ipdltest::TestBasicParent","kind":"class","implKind":"","sizeBytes":376,"supers":[{"pretty":"mozilla::_ipdltest::PTestBasicParent","sym":"T_mozilla::_ipdltest::PTestBasicParent","props":[]}],"methods":[{"pretty":"mozilla::_ipdltest::TestBasicParent::AddRef","sym":"_ZN7mozilla9_ipdltest15TestBasicParent6AddRefEv","props":["instance","virtual","user"]},{"pretty":"mozilla::_ipdltest::TestBasicParent::Release","sym":"_ZN7mozilla9_ipdltest15TestBasicParent7ReleaseEv","props":["instance","virtual","user"]},{"pretty":"mozilla::_ipdltest::TestBasicParent::~TestBasicParent","sym":"_ZN7mozilla9_ipdltest15TestBasicParentD1Ev","props":["instance","virtual","defaulted"]},{"pretty":"mozilla::_ipdltest::TestBasicParent::TestBasicParent","sym":"_ZN7mozilla9_ipdltest15TestBasicParentC1ERKS1_","props":["instance","defaulted","deleted"]},{"pretty":"mozilla::_ipdltest::TestBasicParent::operator=","sym":"_ZN7mozilla9_ipdltest15TestBasicParentaSERKS1_","props":["instance","defaulted","deleted"]},{"pretty":"mozilla::_ipdltest::TestBasicParent::TestBasicParent","sym":"_ZN7mozilla9_ipdltest15TestBasicParentC1Ev","props":["instance","defaulted"]}],"fields":[{"pretty":"mozilla::_ipdltest::TestBasicParent::mRefCnt","sym":"F_<T_mozilla::_ipdltest::TestBasicParent>_mRefCnt","type":"::mozilla::ThreadSafeAutoRefCnt","typesym":"T_mozilla::ThreadSafeAutoRefCnt","offsetBytes":368,"bitPositions":null,"sizeBytes":8}],"overrides":[],"props":[],"platforms":["win64"]},{"structured":1,"pretty":"mozilla::_ipdltest::TestBasicParent","sym":"T_mozilla::_ipdltest::TestBasicParent","kind":"class","implKind":"","sizeBytes":360,"supers":[{"pretty":"mozilla::_ipdltest::PTestBasicParent","sym":"T_mozilla::_ipdltest::PTestBasicParent","props":[]}],"methods":[{"pretty":"mozilla::_ipdltest::TestBasicParent::AddRef","sym":"_ZN7mozilla9_ipdltest15TestBasicParent6AddRefEv","props":["instance","virtual","user"]},{"pretty":"mozilla::_ipdltest::TestBasicParent::Release","sym":"_ZN7mozilla9_ipdltest15TestBasicParent7ReleaseEv","props":["instance","virtual","user"]},{"pretty":"mozilla::_ipdltest::TestBasicParent::~TestBasicParent","sym":"_ZN7mozilla9_ipdltest15TestBasicParentD1Ev","props":["instance","virtual","defaulted"]},{"pretty":"mozilla::_ipdltest::TestBasicParent::TestBasicParent","sym":"_ZN7mozilla9_ipdltest15TestBasicParentC1ERKS1_","props":["instance","defaulted","deleted"]},{"pretty":"mozilla::_ipdltest::TestBasicParent::operator=","sym":"_ZN7mozilla9_ipdltest15TestBasicParentaSERKS1_","props":["instance","defaulted","deleted"]},{"pretty":"mozilla::_ipdltest::TestBasicParent::TestBasicParent","sym":"_ZN7mozilla9_ipdltest15TestBasicParentC1Ev","props":["instance","defaulted"]}],"fields":[{"pretty":"mozilla::_ipdltest::TestBasicParent::mRefCnt","sym":"F_<T_mozilla::_ipdltest::TestBasicParent>_mRefCnt","type":"::mozilla::ThreadSafeAutoRefCnt","typesym":"T_mozilla::ThreadSafeAutoRefCnt","offsetBytes":352,"bitPositions":null,"sizeBytes":8}],"overrides":[],"props":[],"platforms":["macosx64"]},{"structured":1,"pretty":"mozilla::_ipdltest::TestBasicParent","sym":"T_mozilla::_ipdltest::TestBasicParent","kind":"class","implKind":"","sizeBytes":228,"supers":[{"pretty":"mozilla::_ipdltest::PTestBasicParent","sym":"T_mozilla::_ipdltest::PTestBasicParent","props":[]}],"methods":[{"pretty":"mozilla::_ipdltest::TestBasicParent::AddRef","sym":"_ZN7mozilla9_ipdltest15TestBasicParent6AddRefEv","props":["instance","virtual","user"]},{"pretty":"mozilla::_ipdltest::TestBasicParent::Release","sym":"_ZN7mozilla9_ipdltest15TestBasicParent7ReleaseEv","props":["instance","virtual","user"]},{"pretty":"mozilla::_ipdltest::TestBasicParent::~TestBasicParent","sym":"_ZN7mozilla9_ipdltest15TestBasicParentD1Ev","props":["instance","virtual","defaulted"]},{"pretty":"mozilla::_ipdltest::TestBasicParent::TestBasicParent","sym":"_ZN7mozilla9_ipdltest15TestBasicParentC1ERKS1_","props":["instance","defaulted","deleted"]},{"pretty":"mozilla::_ipdltest::TestBasicParent::operator=","sym":"_ZN7mozilla9_ipdltest15TestBasicParentaSERKS1_","props":["instance","defaulted","deleted"]},{"pretty":"mozilla::_ipdltest::TestBasicParent::TestBasicParent","sym":"_ZN7mozilla9_ipdltest15TestBasicParentC1Ev","props":["instance","defaulted"]}],"fields":[{"pretty":"mozilla::_ipdltest::TestBasicParent::mRefCnt","sym":"F_<T_mozilla::_ipdltest::TestBasicParent>_mRefCnt","type":"::mozilla::ThreadSafeAutoRefCnt","typesym":"T_mozilla::ThreadSafeAutoRefCnt","offsetBytes":224,"bitPositions":null,"sizeBytes":4}],"overrides":[],"props":[],"platforms":["android-armv7"]},{"structured":1,"pretty":"mozilla::_ipdltest::TestBasicParent","sym":"T_mozilla::_ipdltest::TestBasicParent","kind":"class","implKind":"","sizeBytes":384,"supers":[{"pretty":"mozilla::_ipdltest::PTestBasicParent","sym":"T_mozilla::_ipdltest::PTestBasicParent","props":[]}],"methods":[{"pretty":"mozilla::_ipdltest::TestBasicParent::AddRef","sym":"_ZN7mozilla9_ipdltest15TestBasicParent6AddRefEv","props":["instance","virtual","user"]},{"pretty":"mozilla::_ipdltest::TestBasicParent::Release","sym":"_ZN7mozilla9_ipdltest15TestBasicParent7ReleaseEv","props":["instance","virtual","user"]},{"pretty":"mozilla::_ipdltest::TestBasicParent::~TestBasicParent","sym":"_ZN7mozilla9_ipdltest15TestBasicParentD1Ev","props":["instance","virtual","defaulted"]},{"pretty":"mozilla::_ipdltest::TestBasicParent::TestBasicParent","sym":"_ZN7mozilla9_ipdltest15TestBasicParentC1ERKS1_","props":["instance","defaulted","deleted"]},{"pretty":"mozilla::_ipdltest::TestBasicParent::operator=","sym":"_ZN7mozilla9_ipdltest15TestBasicParentaSERKS1_","props":["instance","defaulted","deleted"]},{"pretty":"mozilla::_ipdltest::TestBasicParent::TestBasicParent","sym":"_ZN7mozilla9_ipdltest15TestBasicParentC1Ev","props":["instance","defaulted"]}],"fields":[{"pretty":"mozilla::_ipdltest::TestBasicParent::mRefCnt","sym":"F_<T_mozilla::_ipdltest::TestBasicParent>_mRefCnt","type":"::mozilla::ThreadSafeAutoRefCnt","typesym":"T_mozilla::ThreadSafeAutoRefCnt","offsetBytes":376,"bitPositions":null,"sizeBytes":8}],"overrides":[],"props":[],"platforms":["linux64"]},{"structured":1,"pretty":"mozilla::_ipdltest::TestBasicParent","sym":"T_mozilla::_ipdltest::TestBasicParent","kind":"class","implKind":"","sizeBytes":360,"supers":[{"pretty":"mozilla::_ipdltest::PTestBasicParent","sym":"T_mozilla::_ipdltest::PTestBasicParent","props":[]}],"methods":[{"pretty":"mozilla::_ipdltest::TestBasicParent::AddRef","sym":"_ZN7mozilla9_ipdltest15TestBasicParent6AddRefEv","props":["instance","virtual","user"]},{"pretty":"mozilla::_ipdltest::TestBasicParent::Release","sym":"_ZN7mozilla9_ipdltest15TestBasicParent7ReleaseEv","props":["instance","virtual","user"]},{"pretty":"mozilla::_ipdltest::TestBasicParent::~TestBasicParent","sym":"_ZN7mozilla9_ipdltest15TestBasicParentD1Ev","props":["instance","virtual","defaulted"]},{"pretty":"mozilla::_ipdltest::TestBasicParent::TestBasicParent","sym":"_ZN7mozilla9_ipdltest15TestBasicParentC1ERKS1_","props":["instance","defaulted","deleted"]},{"pretty":"mozilla::_ipdltest::TestBasicParent::operator=","sym":"_ZN7mozilla9_ipdltest15TestBasicParentaSERKS1_","props":["instance","defaulted","deleted"]}],"fields":[{"pretty":"mozilla::_ipdltest::TestBasicParent::mRefCnt","sym":"F_<T_mozilla::_ipdltest::TestBasicParent>_mRefCnt","type":"::mozilla::ThreadSafeAutoRefCnt","typesym":"T_mozilla::ThreadSafeAutoRefCnt","offsetBytes":352,"bitPositions":null,"sizeBytes":8}],"overrides":[],"props":[],"platforms":["macosx64"]},{"structured":1,"pretty":"mozilla::_ipdltest::TestBasicParent","sym":"T_mozilla::_ipdltest::TestBasicParent","kind":"class","implKind":"","sizeBytes":228,"supers":[{"pretty":"mozilla::_ipdltest::PTestBasicParent","sym":"T_mozilla::_ipdltest::PTestBasicParent","props":[]}],"methods":[{"pretty":"mozilla::_ipdltest::TestBasicParent::AddRef","sym":"_ZN7mozilla9_ipdltest15TestBasicParent6AddRefEv","props":["instance","virtual","user"]},{"pretty":"mozilla::_ipdltest::TestBasicParent::Release","sym":"_ZN7mozilla9_ipdltest15TestBasicParent7ReleaseEv","props":["instance","virtual","user"]},{"pretty":"mozilla::_ipdltest::TestBasicParent::~TestBasicParent","sym":"_ZN7mozilla9_ipdltest15TestBasicParentD1Ev","props":["instance","virtual","defaulted"]},{"pretty":"mozilla::_ipdltest::TestBasicParent::TestBasicParent","sym":"_ZN7mozilla9_ipdltest15TestBasicParentC1ERKS1_","props":["instance","defaulted","deleted"]},{"pretty":"mozilla::_ipdltest::TestBasicParent::operator=","sym":"_ZN7mozilla9_ipdltest15TestBasicParentaSERKS1_","props":["instance","defaulted","deleted"]}],"fields":[{"pretty":"mozilla::_ipdltest::TestBasicParent::mRefCnt","sym":"F_<T_mozilla::_ipdltest::TestBasicParent>_mRefCnt","type":"::mozilla::ThreadSafeAutoRefCnt","typesym":"T_mozilla::ThreadSafeAutoRefCnt","offsetBytes":224,"bitPositions":null,"sizeBytes":4}],"overrides":[],"props":[],"platforms":["android-armv7"]}]}
{"loc":"00015:2-39","structured":1,"pretty":"mozilla::_ipdltest::TestBasicParent::AddRef","sym":"_ZN7mozilla9_ipdltest15TestBasicParent6AddRefEv","kind":"method","parentsym":"T_mozilla::_ipdltest::TestBasicParent","implKind":"","sizeBytes":null,"supers":[],"methods":[],"fields":[],"overrides":[{"pretty":"mozilla::_ipdltest::PTestBasicParent::AddRef","sym":"_ZN7mozilla9_ipdltest16PTestBasicParent6AddRefEv"}],"props":["instance","virtual","user"]}
{"loc":"00015:2-39","structured":1,"pretty":"mozilla::_ipdltest::TestBasicParent::Release","sym":"_ZN7mozilla9_ipdltest15TestBasicParent7ReleaseEv","kind":"method","parentsym":"T_mozilla::_ipdltest::TestBasicParent","implKind":"","sizeBytes":null,"supers":[],"methods":[],"fields":[],"overrides":[{"pretty":"mozilla::_ipdltest::PTestBasicParent::Release","sym":"_ZN7mozilla9_ipdltest16PTestBasicParent7ReleaseEv"}],"props":["instance","virtual","user"]}
{"loc":"00018:3-18","structured":1,"pretty":"mozilla::_ipdltest::TestBasicParent::~TestBasicParent","sym":"_ZN7mozilla9_ipdltest15TestBasicParentD1Ev","kind":"method","parentsym":"T_mozilla::_ipdltest::TestBasicParent","implKind":"","sizeBytes":null,"supers":[],"methods":[],"fields":[],"overrides":[{"pretty":"mozilla::_ipdltest::PTestBasicParent::~PTestBasicParent","sym":"_ZN7mozilla9_ipdltest16PTestBasicParentD1Ev"}],"props":["instance","virtual","defaulted"]}

```

## tests/tests/mc-analysis/mozilla/_ipdltest/TestBasicChild.h
```
{"loc":"00001:0","target":1,"kind":"def","pretty":"ipc/ipdl/test/gtest/TestBasicChild.h","sym":"FILE_ipc/ipdl/test/gtest/TestBasicChild@2Eh"}
{"loc":"00008:8-42","target":1,"kind":"def","pretty":"mozilla__ipdltest_TestBasicChild_h","sym":"M_3bd24926bd640d6f"}
{"loc":"00010:9-46","target":1,"kind":"use","pretty":"__GENERATED__/ipc/ipdl/_ipdlheaders/mozilla/_ipdltest/PTestBasicChild.h","sym":"FILE_android-armv7@__GENERATED__/ipc/ipdl/_ipdlheaders/mozilla/_ipdltest/PTestBasicChild@2Eh"}
{"loc":"00012:10-17","target":1,"kind":"def","pretty":"mozilla","sym":"NS_mozilla"}
{"loc":"00012:19-28","target":1,"kind":"def","pretty":"mozilla::_ipdltest","sym":"NS_mozilla::_ipdltest"}
{"loc":"00014:30-45","target":1,"kind":"use","pretty":"mozilla::_ipdltest::PTestBasicChild","sym":"T_mozilla::_ipdltest::PTestBasicChild","context":"mozilla::_ipdltest::TestBasicChild","contextsym":"T_mozilla::_ipdltest::TestBasicChild"}
{"loc":"00014:6-20","target":1,"kind":"def","pretty":"mozilla::_ipdltest::TestBasicChild","sym":"T_mozilla::_ipdltest::TestBasicChild","peekRange":"14-14"}
{"loc":"00015:2-39","target":1,"kind":"def","pretty":"mozilla::_ipdltest::TestBasicChild::AddRef","sym":"_ZN7mozilla9_ipdltest14TestBasicChild6AddRefEv"}
{"loc":"00015:2-39","target":1,"kind":"def","pretty":"mozilla::_ipdltest::TestBasicChild::HasThreadSafeRefCnt","sym":"T_mozilla::_ipdltest::TestBasicChild::HasThreadSafeRefCnt"}
{"loc":"00015:2-39","target":1,"kind":"def","pretty":"mozilla::_ipdltest::TestBasicChild::Release","sym":"_ZN7mozilla9_ipdltest14TestBasicChild7ReleaseEv"}
{"loc":"00015:2-39","target":1,"kind":"def","pretty":"mozilla::_ipdltest::TestBasicChild::mRefCnt","sym":"F_<T_mozilla::_ipdltest::TestBasicChild>_mRefCnt"}
{"loc":"00015:2-39","target":1,"kind":"use","pretty":"NS_INLINE_DECL_THREADSAFE_REFCOUNTING","sym":"M_85e68e64b289c7f3"}
{"loc":"00015:40-54","target":1,"kind":"use","pretty":"mozilla::_ipdltest::TestBasicChild","sym":"T_mozilla::_ipdltest::TestBasicChild","context":"mozilla::_ipdltest::TestBasicChild::AddRef","contextsym":"_ZN7mozilla9_ipdltest14TestBasicChild6AddRefEv"}
{"loc":"00018:16-25","target":1,"kind":"use","pretty":"mozilla::ipc::IPCResult","sym":"T_mozilla::ipc::IPCResult","context":"mozilla::_ipdltest::TestBasicChild::RecvHello","contextsym":"_ZN7mozilla9_ipdltest14TestBasicChild9RecvHelloEv"}
{"loc":"00018:26-35","target":1,"kind":"decl","pretty":"mozilla::_ipdltest::TestBasicChild::RecvHello","sym":"_ZN7mozilla9_ipdltest14TestBasicChild9RecvHelloEv","context":"mozilla::_ipdltest::TestBasicChild","contextsym":"T_mozilla::_ipdltest::TestBasicChild","peekRange":"18-18"}
{"loc":"00021:3-17","target":1,"kind":"def","pretty":"mozilla::_ipdltest::TestBasicChild::~TestBasicChild","sym":"_ZN7mozilla9_ipdltest14TestBasicChildD1Ev","context":"mozilla::_ipdltest::TestBasicChild","contextsym":"T_mozilla::_ipdltest::TestBasicChild"}
{"loc":"00021:3-17","target":1,"kind":"use","pretty":"mozilla::_ipdltest::TestBasicChild","sym":"T_mozilla::_ipdltest::TestBasicChild","context":"mozilla::_ipdltest::TestBasicChild::~TestBasicChild","contextsym":"_ZN7mozilla9_ipdltest14TestBasicChildD1Ev"}
{"loc":"00010:9-46","target":1,"kind":"use","pretty":"__GENERATED__/ipc/ipdl/_ipdlheaders/mozilla/_ipdltest/PTestBasicChild.h","sym":"FILE_linux@__GENERATED__/ipc/ipdl/_ipdlheaders/mozilla/_ipdltest/PTestBasicChild@2Eh"}
{"loc":"00010:9-46","target":1,"kind":"use","pretty":"__GENERATED__/ipc/ipdl/_ipdlheaders/mozilla/_ipdltest/PTestBasicChild.h","sym":"FILE_macosx@__GENERATED__/ipc/ipdl/_ipdlheaders/mozilla/_ipdltest/PTestBasicChild@2Eh"}
{"loc":"00010:9-46","target":1,"kind":"use","pretty":"__GENERATED__/ipc/ipdl/_ipdlheaders/mozilla/_ipdltest/PTestBasicChild.h","sym":"FILE_windows@__GENERATED__/ipc/ipdl/_ipdlheaders/mozilla/_ipdltest/PTestBasicChild@2Eh"}
{"loc":"00001:0","source":1,"syntax":"def,file","pretty":"file ipc/ipdl/test/gtest/TestBasicChild.h","sym":"FILE_ipc/ipdl/test/gtest/TestBasicChild@2Eh"}
{"loc":"00008:8-42","source":1,"syntax":"def,macro","pretty":"macro mozilla__ipdltest_TestBasicChild_h","sym":"M_3bd24926bd640d6f"}
{"loc":"00010:9-46","source":1,"syntax":"file,use","pretty":"file __GENERATED__/ipc/ipdl/_ipdlheaders/mozilla/_ipdltest/PTestBasicChild.h","sym":"FILE_windows@__GENERATED__/ipc/ipdl/_ipdlheaders/mozilla/_ipdltest/PTestBasicChild@2Eh,FILE_macosx@__GENERATED__/ipc/ipdl/_ipdlheaders/mozilla/_ipdltest/PTestBasicChild@2Eh,FILE_linux@__GENERATED__/ipc/ipdl/_ipdlheaders/mozilla/_ipdltest/PTestBasicChild@2Eh,FILE_android-armv7@__GENERATED__/ipc/ipdl/_ipdlheaders/mozilla/_ipdltest/PTestBasicChild@2Eh"}
{"loc":"00012:10-17","source":1,"syntax":"def,namespace","pretty":"namespace mozilla","sym":"NS_mozilla"}
{"loc":"00012:19-28","source":1,"syntax":"def,namespace","pretty":"namespace mozilla::_ipdltest","sym":"NS_mozilla::_ipdltest","nestingRange":"12:30-24:0"}
{"loc":"00014:6-20","source":1,"syntax":"def,type","pretty":"type mozilla::_ipdltest::TestBasicChild","sym":"T_mozilla::_ipdltest::TestBasicChild","nestingRange":"14:46-22:0"}
{"loc":"00014:30-45","source":1,"syntax":"type,use","pretty":"type mozilla::_ipdltest::PTestBasicChild","sym":"T_mozilla::_ipdltest::PTestBasicChild","type":"class mozilla::_ipdltest::PTestBasicChild","typesym":"T_mozilla::_ipdltest::PTestBasicChild"}
{"loc":"00015:2-39","source":1,"syntax":"def,field","pretty":"field mozilla::_ipdltest::TestBasicChild::mRefCnt","sym":"F_<T_mozilla::_ipdltest::TestBasicChild>_mRefCnt","type":"::mozilla::ThreadSafeAutoRefCnt","typesym":"T_mozilla::ThreadSafeAutoRefCnt"}
{"loc":"00015:2-39","source":1,"syntax":"def,function","pretty":"function mozilla::_ipdltest::TestBasicChild::AddRef","sym":"_ZN7mozilla9_ipdltest14TestBasicChild6AddRefEv","type":"MozExternalRefCountType (void)"}
{"loc":"00015:2-39","source":1,"syntax":"def,function","pretty":"function mozilla::_ipdltest::TestBasicChild::Release","sym":"_ZN7mozilla9_ipdltest14TestBasicChild7ReleaseEv","type":"MozExternalRefCountType (void)"}
{"loc":"00015:2-39","source":1,"syntax":"macro,use","pretty":"macro NS_INLINE_DECL_THREADSAFE_REFCOUNTING","sym":"M_85e68e64b289c7f3"}
{"loc":"00015:2-39","source":1,"syntax":"def,type","pretty":"type mozilla::_ipdltest::TestBasicChild::HasThreadSafeRefCnt","sym":"T_mozilla::_ipdltest::TestBasicChild::HasThreadSafeRefCnt"}
{"loc":"00015:2-39","source":1,"syntax":"","pretty":"variable count","sym":"V_5e5b2_e685d3f013","no_crossref":1,"type":"nsrefcnt"}
{"loc":"00015:40-54","source":1,"syntax":"type,use","pretty":"type mozilla::_ipdltest::TestBasicChild","sym":"T_mozilla::_ipdltest::TestBasicChild","type":"class mozilla::_ipdltest::TestBasicChild","typesym":"T_mozilla::_ipdltest::TestBasicChild"}
{"loc":"00018:16-25","source":1,"syntax":"type,use","pretty":"type mozilla::ipc::IPCResult","sym":"T_mozilla::ipc::IPCResult","type":"class mozilla::ipc::IPCResult","typesym":"T_mozilla::ipc::IPCResult"}
{"loc":"00018:26-35","source":1,"syntax":"decl,function","pretty":"function mozilla::_ipdltest::TestBasicChild::RecvHello","sym":"_ZN7mozilla9_ipdltest14TestBasicChild9RecvHelloEv","type":"mozilla::ipc::IPCResult (void)"}
{"loc":"00021:3-17","source":1,"syntax":"def,destructor","pretty":"destructor mozilla::_ipdltest::TestBasicChild::~TestBasicChild","sym":"_ZN7mozilla9_ipdltest14TestBasicChildD1Ev","type":"void (void) noexcept"}
{"loc":"00021:3-17","source":1,"syntax":"type,use","pretty":"type mozilla::_ipdltest::TestBasicChild","sym":"T_mozilla::_ipdltest::TestBasicChild","type":"class mozilla::_ipdltest::TestBasicChild","typesym":"T_mozilla::_ipdltest::TestBasicChild"}
{"loc":"00015:2-39","structured":1,"pretty":"mozilla::_ipdltest::TestBasicChild::mRefCnt","sym":"F_<T_mozilla::_ipdltest::TestBasicChild>_mRefCnt","kind":"field","parentsym":"T_mozilla::_ipdltest::TestBasicChild","implKind":"","sizeBytes":null,"supers":[],"methods":[],"fields":[],"overrides":[],"props":[]}
{"loc":"00014:6-20","structured":1,"pretty":"mozilla::_ipdltest::TestBasicChild","sym":"T_mozilla::_ipdltest::TestBasicChild","kind":"class","implKind":"","sizeBytes":376,"supers":[{"pretty":"mozilla::_ipdltest::PTestBasicChild","sym":"T_mozilla::_ipdltest::PTestBasicChild","props":[]}],"methods":[{"pretty":"mozilla::_ipdltest::TestBasicChild::AddRef","sym":"_ZN7mozilla9_ipdltest14TestBasicChild6AddRefEv","props":["instance","virtual","user"]},{"pretty":"mozilla::_ipdltest::TestBasicChild::Release","sym":"_ZN7mozilla9_ipdltest14TestBasicChild7ReleaseEv","props":["instance","virtual","user"]},{"pretty":"mozilla::_ipdltest::TestBasicChild::RecvHello","sym":"_ZN7mozilla9_ipdltest14TestBasicChild9RecvHelloEv","props":["instance","user"]},{"pretty":"mozilla::_ipdltest::TestBasicChild::~TestBasicChild","sym":"_ZN7mozilla9_ipdltest14TestBasicChildD1Ev","props":["instance","virtual","defaulted"]},{"pretty":"mozilla::_ipdltest::TestBasicChild::TestBasicChild","sym":"_ZN7mozilla9_ipdltest14TestBasicChildC1ERKS1_","props":["instance","defaulted","deleted"]},{"pretty":"mozilla::_ipdltest::TestBasicChild::operator=","sym":"_ZN7mozilla9_ipdltest14TestBasicChildaSERKS1_","props":["instance","defaulted","deleted"]}],"fields":[{"pretty":"mozilla::_ipdltest::TestBasicChild::mRefCnt","sym":"F_<T_mozilla::_ipdltest::TestBasicChild>_mRefCnt","type":"::mozilla::ThreadSafeAutoRefCnt","typesym":"T_mozilla::ThreadSafeAutoRefCnt","offsetBytes":368,"bitPositions":null,"sizeBytes":8}],"overrides":[],"props":[],"platforms":["win64"],"variants":[{"structured":1,"pretty":"mozilla::_ipdltest::TestBasicChild","sym":"T_mozilla::_ipdltest::TestBasicChild","kind":"class","implKind":"","sizeBytes":384,"supers":[{"pretty":"mozilla::_ipdltest::PTestBasicChild","sym":"T_mozilla::_ipdltest::PTestBasicChild","props":[]}],"methods":[{"pretty":"mozilla::_ipdltest::TestBasicChild::AddRef","sym":"_ZN7mozilla9_ipdltest14TestBasicChild6AddRefEv","props":["instance","virtual","user"]},{"pretty":"mozilla::_ipdltest::TestBasicChild::Release","sym":"_ZN7mozilla9_ipdltest14TestBasicChild7ReleaseEv","props":["instance","virtual","user"]},{"pretty":"mozilla::_ipdltest::TestBasicChild::RecvHello","sym":"_ZN7mozilla9_ipdltest14TestBasicChild9RecvHelloEv","props":["instance","user"]},{"pretty":"mozilla::_ipdltest::TestBasicChild::~TestBasicChild","sym":"_ZN7mozilla9_ipdltest14TestBasicChildD1Ev","props":["instance","virtual","defaulted"]},{"pretty":"mozilla::_ipdltest::TestBasicChild::TestBasicChild","sym":"_ZN7mozilla9_ipdltest14TestBasicChildC1ERKS1_","props":["instance","defaulted","deleted"]},{"pretty":"mozilla::_ipdltest::TestBasicChild::operator=","sym":"_ZN7mozilla9_ipdltest14TestBasicChildaSERKS1_","props":["instance","defaulted","deleted"]},{"pretty":"mozilla::_ipdltest::TestBasicChild::TestBasicChild","sym":"_ZN7mozilla9_ipdltest14TestBasicChildC1Ev","props":["instance","defaulted"]}],"fields":[{"pretty":"mozilla::_ipdltest::TestBasicChild::mRefCnt","sym":"F_<T_mozilla::_ipdltest::TestBasicChild>_mRefCnt","type":"::mozilla::ThreadSafeAutoRefCnt","typesym":"T_mozilla::ThreadSafeAutoRefCnt","offsetBytes":376,"bitPositions":null,"sizeBytes":8}],"overrides":[],"props":[],"platforms":["linux64"]},{"structured":1,"pretty":"mozilla::_ipdltest::TestBasicChild","sym":"T_mozilla::_ipdltest::TestBasicChild","kind":"class","implKind":"","sizeBytes":384,"supers":[{"pretty":"mozilla::_ipdltest::PTestBasicChild","sym":"T_mozilla::_ipdltest::PTestBasicChild","props":[]}],"methods":[{"pretty":"mozilla::_ipdltest::TestBasicChild::AddRef","sym":"_ZN7mozilla9_ipdltest14TestBasicChild6AddRefEv","props":["instance","virtual","user"]},{"pretty":"mozilla::_ipdltest::TestBasicChild::Release","sym":"_ZN7mozilla9_ipdltest14TestBasicChild7ReleaseEv","props":["instance","virtual","user"]},{"pretty":"mozilla::_ipdltest::TestBasicChild::RecvHello","sym":"_ZN7mozilla9_ipdltest14TestBasicChild9RecvHelloEv","props":["instance","user"]},{"pretty":"mozilla::_ipdltest::TestBasicChild::~TestBasicChild","sym":"_ZN7mozilla9_ipdltest14TestBasicChildD1Ev","props":["instance","virtual","defaulted"]},{"pretty":"mozilla::_ipdltest::TestBasicChild::TestBasicChild","sym":"_ZN7mozilla9_ipdltest14TestBasicChildC1ERKS1_","props":["instance","defaulted","deleted"]},{"pretty":"mozilla::_ipdltest::TestBasicChild::operator=","sym":"_ZN7mozilla9_ipdltest14TestBasicChildaSERKS1_","props":["instance","defaulted","deleted"]}],"fields":[{"pretty":"mozilla::_ipdltest::TestBasicChild::mRefCnt","sym":"F_<T_mozilla::_ipdltest::TestBasicChild>_mRefCnt","type":"::mozilla::ThreadSafeAutoRefCnt","typesym":"T_mozilla::ThreadSafeAutoRefCnt","offsetBytes":376,"bitPositions":null,"sizeBytes":8}],"overrides":[],"props":[],"platforms":["linux64"]},{"structured":1,"pretty":"mozilla::_ipdltest::TestBasicChild","sym":"T_mozilla::_ipdltest::TestBasicChild","kind":"class","implKind":"","sizeBytes":360,"supers":[{"pretty":"mozilla::_ipdltest::PTestBasicChild","sym":"T_mozilla::_ipdltest::PTestBasicChild","props":[]}],"methods":[{"pretty":"mozilla::_ipdltest::TestBasicChild::AddRef","sym":"_ZN7mozilla9_ipdltest14TestBasicChild6AddRefEv","props":["instance","virtual","user"]},{"pretty":"mozilla::_ipdltest::TestBasicChild::Release","sym":"_ZN7mozilla9_ipdltest14TestBasicChild7ReleaseEv","props":["instance","virtual","user"]},{"pretty":"mozilla::_ipdltest::TestBasicChild::RecvHello","sym":"_ZN7mozilla9_ipdltest14TestBasicChild9RecvHelloEv","props":["instance","user"]},{"pretty":"mozilla::_ipdltest::TestBasicChild::~TestBasicChild","sym":"_ZN7mozilla9_ipdltest14TestBasicChildD1Ev","props":["instance","virtual","defaulted"]},{"pretty":"mozilla::_ipdltest::TestBasicChild::TestBasicChild","sym":"_ZN7mozilla9_ipdltest14TestBasicChildC1ERKS1_","props":["instance","defaulted","deleted"]},{"pretty":"mozilla::_ipdltest::TestBasicChild::operator=","sym":"_ZN7mozilla9_ipdltest14TestBasicChildaSERKS1_","props":["instance","defaulted","deleted"]}],"fields":[{"pretty":"mozilla::_ipdltest::TestBasicChild::mRefCnt","sym":"F_<T_mozilla::_ipdltest::TestBasicChild>_mRefCnt","type":"::mozilla::ThreadSafeAutoRefCnt","typesym":"T_mozilla::ThreadSafeAutoRefCnt","offsetBytes":352,"bitPositions":null,"sizeBytes":8}],"overrides":[],"props":[],"platforms":["macosx64"]},{"structured":1,"pretty":"mozilla::_ipdltest::TestBasicChild","sym":"T_mozilla::_ipdltest::TestBasicChild","kind":"class","implKind":"","sizeBytes":228,"supers":[{"pretty":"mozilla::_ipdltest::PTestBasicChild","sym":"T_mozilla::_ipdltest::PTestBasicChild","props":[]}],"methods":[{"pretty":"mozilla::_ipdltest::TestBasicChild::AddRef","sym":"_ZN7mozilla9_ipdltest14TestBasicChild6AddRefEv","props":["instance","virtual","user"]},{"pretty":"mozilla::_ipdltest::TestBasicChild::Release","sym":"_ZN7mozilla9_ipdltest14TestBasicChild7ReleaseEv","props":["instance","virtual","user"]},{"pretty":"mozilla::_ipdltest::TestBasicChild::RecvHello","sym":"_ZN7mozilla9_ipdltest14TestBasicChild9RecvHelloEv","props":["instance","user"]},{"pretty":"mozilla::_ipdltest::TestBasicChild::~TestBasicChild","sym":"_ZN7mozilla9_ipdltest14TestBasicChildD1Ev","props":["instance","virtual","defaulted"]},{"pretty":"mozilla::_ipdltest::TestBasicChild::TestBasicChild","sym":"_ZN7mozilla9_ipdltest14TestBasicChildC1ERKS1_","props":["instance","defaulted","deleted"]},{"pretty":"mozilla::_ipdltest::TestBasicChild::operator=","sym":"_ZN7mozilla9_ipdltest14TestBasicChildaSERKS1_","props":["instance","defaulted","deleted"]}],"fields":[{"pretty":"mozilla::_ipdltest::TestBasicChild::mRefCnt","sym":"F_<T_mozilla::_ipdltest::TestBasicChild>_mRefCnt","type":"::mozilla::ThreadSafeAutoRefCnt","typesym":"T_mozilla::ThreadSafeAutoRefCnt","offsetBytes":224,"bitPositions":null,"sizeBytes":4}],"overrides":[],"props":[],"platforms":["android-armv7"]},{"structured":1,"pretty":"mozilla::_ipdltest::TestBasicChild","sym":"T_mozilla::_ipdltest::TestBasicChild","kind":"class","implKind":"","sizeBytes":376,"supers":[{"pretty":"mozilla::_ipdltest::PTestBasicChild","sym":"T_mozilla::_ipdltest::PTestBasicChild","props":[]}],"methods":[{"pretty":"mozilla::_ipdltest::TestBasicChild::AddRef","sym":"_ZN7mozilla9_ipdltest14TestBasicChild6AddRefEv","props":["instance","virtual","user"]},{"pretty":"mozilla::_ipdltest::TestBasicChild::Release","sym":"_ZN7mozilla9_ipdltest14TestBasicChild7ReleaseEv","props":["instance","virtual","user"]},{"pretty":"mozilla::_ipdltest::TestBasicChild::RecvHello","sym":"_ZN7mozilla9_ipdltest14TestBasicChild9RecvHelloEv","props":["instance","user"]},{"pretty":"mozilla::_ipdltest::TestBasicChild::~TestBasicChild","sym":"_ZN7mozilla9_ipdltest14TestBasicChildD1Ev","props":["instance","virtual","defaulted"]},{"pretty":"mozilla::_ipdltest::TestBasicChild::TestBasicChild","sym":"_ZN7mozilla9_ipdltest14TestBasicChildC1ERKS1_","props":["instance","defaulted","deleted"]},{"pretty":"mozilla::_ipdltest::TestBasicChild::operator=","sym":"_ZN7mozilla9_ipdltest14TestBasicChildaSERKS1_","props":["instance","defaulted","deleted"]},{"pretty":"mozilla::_ipdltest::TestBasicChild::TestBasicChild","sym":"_ZN7mozilla9_ipdltest14TestBasicChildC1Ev","props":["instance","defaulted"]}],"fields":[{"pretty":"mozilla::_ipdltest::TestBasicChild::mRefCnt","sym":"F_<T_mozilla::_ipdltest::TestBasicChild>_mRefCnt","type":"::mozilla::ThreadSafeAutoRefCnt","typesym":"T_mozilla::ThreadSafeAutoRefCnt","offsetBytes":368,"bitPositions":null,"sizeBytes":8}],"overrides":[],"props":[],"platforms":["win64"]},{"structured":1,"pretty":"mozilla::_ipdltest::TestBasicChild","sym":"T_mozilla::_ipdltest::TestBasicChild","kind":"class","implKind":"","sizeBytes":360,"supers":[{"pretty":"mozilla::_ipdltest::PTestBasicChild","sym":"T_mozilla::_ipdltest::PTestBasicChild","props":[]}],"methods":[{"pretty":"mozilla::_ipdltest::TestBasicChild::AddRef","sym":"_ZN7mozilla9_ipdltest14TestBasicChild6AddRefEv","props":["instance","virtual","user"]},{"pretty":"mozilla::_ipdltest::TestBasicChild::Release","sym":"_ZN7mozilla9_ipdltest14TestBasicChild7ReleaseEv","props":["instance","virtual","user"]},{"pretty":"mozilla::_ipdltest::TestBasicChild::RecvHello","sym":"_ZN7mozilla9_ipdltest14TestBasicChild9RecvHelloEv","props":["instance","user"]},{"pretty":"mozilla::_ipdltest::TestBasicChild::~TestBasicChild","sym":"_ZN7mozilla9_ipdltest14TestBasicChildD1Ev","props":["instance","virtual","defaulted"]},{"pretty":"mozilla::_ipdltest::TestBasicChild::TestBasicChild","sym":"_ZN7mozilla9_ipdltest14TestBasicChildC1ERKS1_","props":["instance","defaulted","deleted"]},{"pretty":"mozilla::_ipdltest::TestBasicChild::operator=","sym":"_ZN7mozilla9_ipdltest14TestBasicChildaSERKS1_","props":["instance","defaulted","deleted"]},{"pretty":"mozilla::_ipdltest::TestBasicChild::TestBasicChild","sym":"_ZN7mozilla9_ipdltest14TestBasicChildC1Ev","props":["instance","defaulted"]}],"fields":[{"pretty":"mozilla::_ipdltest::TestBasicChild::mRefCnt","sym":"F_<T_mozilla::_ipdltest::TestBasicChild>_mRefCnt","type":"::mozilla::ThreadSafeAutoRefCnt","typesym":"T_mozilla::ThreadSafeAutoRefCnt","offsetBytes":352,"bitPositions":null,"sizeBytes":8}],"overrides":[],"props":[],"platforms":["macosx64"]},{"structured":1,"pretty":"mozilla::_ipdltest::TestBasicChild","sym":"T_mozilla::_ipdltest::TestBasicChild","kind":"class","implKind":"","sizeBytes":228,"supers":[{"pretty":"mozilla::_ipdltest::PTestBasicChild","sym":"T_mozilla::_ipdltest::PTestBasicChild","props":[]}],"methods":[{"pretty":"mozilla::_ipdltest::TestBasicChild::AddRef","sym":"_ZN7mozilla9_ipdltest14TestBasicChild6AddRefEv","props":["instance","virtual","user"]},{"pretty":"mozilla::_ipdltest::TestBasicChild::Release","sym":"_ZN7mozilla9_ipdltest14TestBasicChild7ReleaseEv","props":["instance","virtual","user"]},{"pretty":"mozilla::_ipdltest::TestBasicChild::RecvHello","sym":"_ZN7mozilla9_ipdltest14TestBasicChild9RecvHelloEv","props":["instance","user"]},{"pretty":"mozilla::_ipdltest::TestBasicChild::~TestBasicChild","sym":"_ZN7mozilla9_ipdltest14TestBasicChildD1Ev","props":["instance","virtual","defaulted"]},{"pretty":"mozilla::_ipdltest::TestBasicChild::TestBasicChild","sym":"_ZN7mozilla9_ipdltest14TestBasicChildC1ERKS1_","props":["instance","defaulted","deleted"]},{"pretty":"mozilla::_ipdltest::TestBasicChild::operator=","sym":"_ZN7mozilla9_ipdltest14TestBasicChildaSERKS1_","props":["instance","defaulted","deleted"]},{"pretty":"mozilla::_ipdltest::TestBasicChild::TestBasicChild","sym":"_ZN7mozilla9_ipdltest14TestBasicChildC1Ev","props":["instance","defaulted"]}],"fields":[{"pretty":"mozilla::_ipdltest::TestBasicChild::mRefCnt","sym":"F_<T_mozilla::_ipdltest::TestBasicChild>_mRefCnt","type":"::mozilla::ThreadSafeAutoRefCnt","typesym":"T_mozilla::ThreadSafeAutoRefCnt","offsetBytes":224,"bitPositions":null,"sizeBytes":4}],"overrides":[],"props":[],"platforms":["android-armv7"]}]}
{"loc":"00015:2-39","structured":1,"pretty":"mozilla::_ipdltest::TestBasicChild::AddRef","sym":"_ZN7mozilla9_ipdltest14TestBasicChild6AddRefEv","kind":"method","parentsym":"T_mozilla::_ipdltest::TestBasicChild","implKind":"","sizeBytes":null,"supers":[],"methods":[],"fields":[],"overrides":[{"pretty":"mozilla::_ipdltest::PTestBasicChild::AddRef","sym":"_ZN7mozilla9_ipdltest15PTestBasicChild6AddRefEv"}],"props":["instance","virtual","user"]}
{"loc":"00015:2-39","structured":1,"pretty":"mozilla::_ipdltest::TestBasicChild::Release","sym":"_ZN7mozilla9_ipdltest14TestBasicChild7ReleaseEv","kind":"method","parentsym":"T_mozilla::_ipdltest::TestBasicChild","implKind":"","sizeBytes":null,"supers":[],"methods":[],"fields":[],"overrides":[{"pretty":"mozilla::_ipdltest::PTestBasicChild::Release","sym":"_ZN7mozilla9_ipdltest15PTestBasicChild7ReleaseEv"}],"props":["instance","virtual","user"]}
{"loc":"00021:3-17","structured":1,"pretty":"mozilla::_ipdltest::TestBasicChild::~TestBasicChild","sym":"_ZN7mozilla9_ipdltest14TestBasicChildD1Ev","kind":"method","parentsym":"T_mozilla::_ipdltest::TestBasicChild","implKind":"","sizeBytes":null,"supers":[],"methods":[],"fields":[],"overrides":[{"pretty":"mozilla::_ipdltest::PTestBasicChild::~PTestBasicChild","sym":"_ZN7mozilla9_ipdltest15PTestBasicChildD1Ev"}],"props":["instance","virtual","defaulted"]}

```

## tests/tests/mc-analysis/ipdl/TestBasic.cpp
```
{"loc":"00001:0","target":1,"kind":"def","pretty":"ipc/ipdl/test/gtest/TestBasic.cpp","sym":"FILE_ipc/ipdl/test/gtest/TestBasic@2Ecpp"}
{"loc":"00007:9-24","target":1,"kind":"use","pretty":"third_party/googletest/googletest/include/gtest/gtest.h","sym":"FILE_third_party/googletest/googletest/include/gtest/gtest@2Eh"}
{"loc":"00009:9-43","target":1,"kind":"use","pretty":"ipc/ipdl/test/gtest/IPDLUnitTest.h","sym":"FILE_ipc/ipdl/test/gtest/IPDLUnitTest@2Eh"}
{"loc":"00010:9-45","target":1,"kind":"use","pretty":"ipc/ipdl/test/gtest/TestBasicChild.h","sym":"FILE_ipc/ipdl/test/gtest/TestBasicChild@2Eh"}
{"loc":"00011:9-46","target":1,"kind":"use","pretty":"ipc/ipdl/test/gtest/TestBasicParent.h","sym":"FILE_ipc/ipdl/test/gtest/TestBasicParent@2Eh"}
{"loc":"00015:10-17","target":1,"kind":"def","pretty":"mozilla","sym":"NS_mozilla"}
{"loc":"00015:19-28","target":1,"kind":"def","pretty":"mozilla::_ipdltest","sym":"NS_mozilla::_ipdltest"}
{"loc":"00017:0-9","target":1,"kind":"use","pretty":"mozilla::ipc::IPCResult","sym":"T_mozilla::ipc::IPCResult","context":"mozilla::_ipdltest::TestBasicChild::RecvHello","contextsym":"_ZN7mozilla9_ipdltest14TestBasicChild9RecvHelloEv"}
{"loc":"00017:10-24","target":1,"kind":"use","pretty":"mozilla::_ipdltest::TestBasicChild","sym":"T_mozilla::_ipdltest::TestBasicChild","context":"mozilla::_ipdltest::TestBasicChild::RecvHello","contextsym":"_ZN7mozilla9_ipdltest14TestBasicChild9RecvHelloEv"}
{"loc":"00017:26-35","target":1,"kind":"def","pretty":"mozilla::_ipdltest::TestBasicChild::RecvHello","sym":"_ZN7mozilla9_ipdltest14TestBasicChild9RecvHelloEv","peekRange":"17-17"}
{"loc":"00018:14-21","target":1,"kind":"use","pretty":"mozilla::ipc::IProtocol::CanSend","sym":"_ZNK7mozilla3ipc9IProtocol7CanSendEv","context":"mozilla::_ipdltest::TestBasicChild::RecvHello","contextsym":"_ZN7mozilla9_ipdltest14TestBasicChild9RecvHelloEv"}
{"loc":"00018:2-13","target":1,"kind":"use","pretty":"EXPECT_TRUE","sym":"M_e6015e2edc4cf2c3"}
{"loc":"00019:2-7","target":1,"kind":"use","pretty":"mozilla::ipc::IToplevelProtocol::Close","sym":"_ZN7mozilla3ipc17IToplevelProtocol5CloseEv","context":"mozilla::_ipdltest::TestBasicChild::RecvHello","contextsym":"_ZN7mozilla9_ipdltest14TestBasicChild9RecvHelloEv"}
{"loc":"00020:15-22","target":1,"kind":"use","pretty":"mozilla::ipc::IProtocol::CanSend","sym":"_ZNK7mozilla3ipc9IProtocol7CanSendEv","context":"mozilla::_ipdltest::TestBasicChild::RecvHello","contextsym":"_ZN7mozilla9_ipdltest14TestBasicChild9RecvHelloEv"}
{"loc":"00020:2-14","target":1,"kind":"use","pretty":"EXPECT_FALSE","sym":"M_27125e2edc4cf2c3"}
{"loc":"00021:9-15","target":1,"kind":"use","pretty":"IPC_OK","sym":"M_53444620e89184de"}
{"loc":"00024:0-9","target":1,"kind":"decl","pretty":"mozilla::_ipdltest::IPDLTest_CrossProcess_TestBasic_Test::TestBody","sym":"_ZN7mozilla9_ipdltest36IPDLTest_CrossProcess_TestBasic_Test8TestBodyEv"}
{"loc":"00024:0-9","target":1,"kind":"decl","pretty":"mozilla::_ipdltest::IPDLTest_CrossProcess_TestBasic_Test::test_info_","sym":"_ZN7mozilla9_ipdltest36IPDLTest_CrossProcess_TestBasic_Test10test_info_E"}
{"loc":"00024:0-9","target":1,"kind":"decl","pretty":"mozilla::_ipdltest::IPDLTest_CrossThread_TestBasic_Test::TestBody","sym":"_ZN7mozilla9_ipdltest35IPDLTest_CrossThread_TestBasic_Test8TestBodyEv"}
{"loc":"00024:0-9","target":1,"kind":"decl","pretty":"mozilla::_ipdltest::IPDLTest_CrossThread_TestBasic_Test::test_info_","sym":"_ZN7mozilla9_ipdltest35IPDLTest_CrossThread_TestBasic_Test10test_info_E"}
{"loc":"00024:0-9","target":1,"kind":"decl","pretty":"mozilla::_ipdltest::IPDL_TEST_TestBasic::TestBody","sym":"_ZN7mozilla9_ipdltest19IPDL_TEST_TestBasic8TestBodyEv"}
{"loc":"00024:0-9","target":1,"kind":"decl","pretty":"mozilla::_ipdltest::IPDL_TEST_TestBasic::sName","sym":"_ZN7mozilla9_ipdltest19IPDL_TEST_TestBasic5sNameE"}
{"loc":"00024:0-9","target":1,"kind":"def","pretty":"mozilla::_ipdltest::IPDLTest_CrossProcess_TestBasic_Test","sym":"T_mozilla::_ipdltest::IPDLTest_CrossProcess_TestBasic_Test"}
{"loc":"00024:0-9","target":1,"kind":"def","pretty":"mozilla::_ipdltest::IPDLTest_CrossProcess_TestBasic_Test::IPDLTest_CrossProcess_TestBasic_Test","sym":"_ZN7mozilla9_ipdltest36IPDLTest_CrossProcess_TestBasic_TestC1Ev"}
{"loc":"00024:0-9","target":1,"kind":"def","pretty":"mozilla::_ipdltest::IPDLTest_CrossProcess_TestBasic_Test::TestBody","sym":"_ZN7mozilla9_ipdltest36IPDLTest_CrossProcess_TestBasic_Test8TestBodyEv"}
{"loc":"00024:0-9","target":1,"kind":"def","pretty":"mozilla::_ipdltest::IPDLTest_CrossProcess_TestBasic_Test::operator=","sym":"_ZN7mozilla9_ipdltest36IPDLTest_CrossProcess_TestBasic_TestaSEOS1_"}
{"loc":"00024:0-9","target":1,"kind":"def","pretty":"mozilla::_ipdltest::IPDLTest_CrossProcess_TestBasic_Test::operator=","sym":"_ZN7mozilla9_ipdltest36IPDLTest_CrossProcess_TestBasic_TestaSERKS1_"}
{"loc":"00024:0-9","target":1,"kind":"def","pretty":"mozilla::_ipdltest::IPDLTest_CrossProcess_TestBasic_Test::test_info_","sym":"_ZN7mozilla9_ipdltest36IPDLTest_CrossProcess_TestBasic_Test10test_info_E"}
{"loc":"00024:0-9","target":1,"kind":"def","pretty":"mozilla::_ipdltest::IPDLTest_CrossProcess_TestBasic_Test::~IPDLTest_CrossProcess_TestBasic_Test","sym":"_ZN7mozilla9_ipdltest36IPDLTest_CrossProcess_TestBasic_TestD1Ev"}
{"loc":"00024:0-9","target":1,"kind":"def","pretty":"mozilla::_ipdltest::IPDLTest_CrossThread_TestBasic_Test","sym":"T_mozilla::_ipdltest::IPDLTest_CrossThread_TestBasic_Test"}
{"loc":"00024:0-9","target":1,"kind":"def","pretty":"mozilla::_ipdltest::IPDLTest_CrossThread_TestBasic_Test::IPDLTest_CrossThread_TestBasic_Test","sym":"_ZN7mozilla9_ipdltest35IPDLTest_CrossThread_TestBasic_TestC1Ev"}
{"loc":"00024:0-9","target":1,"kind":"def","pretty":"mozilla::_ipdltest::IPDLTest_CrossThread_TestBasic_Test::TestBody","sym":"_ZN7mozilla9_ipdltest35IPDLTest_CrossThread_TestBasic_Test8TestBodyEv"}
{"loc":"00024:0-9","target":1,"kind":"def","pretty":"mozilla::_ipdltest::IPDLTest_CrossThread_TestBasic_Test::operator=","sym":"_ZN7mozilla9_ipdltest35IPDLTest_CrossThread_TestBasic_TestaSEOS1_"}
{"loc":"00024:0-9","target":1,"kind":"def","pretty":"mozilla::_ipdltest::IPDLTest_CrossThread_TestBasic_Test::operator=","sym":"_ZN7mozilla9_ipdltest35IPDLTest_CrossThread_TestBasic_TestaSERKS1_"}
{"loc":"00024:0-9","target":1,"kind":"def","pretty":"mozilla::_ipdltest::IPDLTest_CrossThread_TestBasic_Test::test_info_","sym":"_ZN7mozilla9_ipdltest35IPDLTest_CrossThread_TestBasic_Test10test_info_E"}
{"loc":"00024:0-9","target":1,"kind":"def","pretty":"mozilla::_ipdltest::IPDLTest_CrossThread_TestBasic_Test::~IPDLTest_CrossThread_TestBasic_Test","sym":"_ZN7mozilla9_ipdltest35IPDLTest_CrossThread_TestBasic_TestD1Ev"}
{"loc":"00024:0-9","target":1,"kind":"def","pretty":"mozilla::_ipdltest::IPDL_TEST_TestBasic","sym":"T_mozilla::_ipdltest::IPDL_TEST_TestBasic"}
{"loc":"00024:0-9","target":1,"kind":"def","pretty":"mozilla::_ipdltest::IPDL_TEST_TestBasic::GetActor","sym":"_ZN7mozilla9_ipdltest19IPDL_TEST_TestBasic8GetActorEv"}
{"loc":"00024:0-9","target":1,"kind":"def","pretty":"mozilla::_ipdltest::IPDL_TEST_TestBasic::GetName","sym":"_ZN7mozilla9_ipdltest19IPDL_TEST_TestBasic7GetNameEv"}
{"loc":"00024:0-9","target":1,"kind":"def","pretty":"mozilla::_ipdltest::IPDL_TEST_TestBasic::IPDL_TEST_TestBasic","sym":"_ZN7mozilla9_ipdltest19IPDL_TEST_TestBasicC1Ev"}
{"loc":"00024:0-9","target":1,"kind":"def","pretty":"mozilla::_ipdltest::IPDL_TEST_TestBasic::TestBody","sym":"_ZN7mozilla9_ipdltest19IPDL_TEST_TestBasic8TestBodyEv"}
{"loc":"00024:0-9","target":1,"kind":"def","pretty":"mozilla::_ipdltest::IPDL_TEST_TestBasic::mActor","sym":"F_<T_mozilla::_ipdltest::IPDL_TEST_TestBasic>_mActor"}
{"loc":"00024:0-9","target":1,"kind":"def","pretty":"mozilla::_ipdltest::IPDL_TEST_TestBasic::sName","sym":"_ZN7mozilla9_ipdltest19IPDL_TEST_TestBasic5sNameE"}
{"loc":"00024:0-9","target":1,"kind":"use","pretty":"IPDL_TEST","sym":"M_6befec05994d4d33"}
{"loc":"00025:12-18","target":1,"kind":"use","pretty":"mozilla::_ipdltest::IPDL_TEST_TestBasic::mActor","sym":"F_<T_mozilla::_ipdltest::IPDL_TEST_TestBasic>_mActor","context":"mozilla::_ipdltest::IPDL_TEST_TestBasic::TestBody","contextsym":"_ZN7mozilla9_ipdltest19IPDL_TEST_TestBasic8TestBodyEv"}
{"loc":"00025:20-29","target":1,"kind":"use","pretty":"mozilla::_ipdltest::PTestBasicParent::SendHello","sym":"_ZN7mozilla9_ipdltest16PTestBasicParent9SendHelloEv","context":"mozilla::_ipdltest::IPDL_TEST_TestBasic::TestBody","contextsym":"_ZN7mozilla9_ipdltest19IPDL_TEST_TestBasic8TestBodyEv"}
{"loc":"00026:2-13","target":1,"kind":"use","pretty":"ASSERT_TRUE","sym":"M_67235e2edc4cf2c3"}
{"loc":"00001:0","source":1,"syntax":"def,file","pretty":"file ipc/ipdl/test/gtest/TestBasic.cpp","sym":"FILE_ipc/ipdl/test/gtest/TestBasic@2Ecpp"}
{"loc":"00007:9-24","source":1,"syntax":"file,use","pretty":"file third_party/googletest/googletest/include/gtest/gtest.h","sym":"FILE_third_party/googletest/googletest/include/gtest/gtest@2Eh"}
{"loc":"00009:9-43","source":1,"syntax":"file,use","pretty":"file ipc/ipdl/test/gtest/IPDLUnitTest.h","sym":"FILE_ipc/ipdl/test/gtest/IPDLUnitTest@2Eh"}
{"loc":"00010:9-45","source":1,"syntax":"file,use","pretty":"file ipc/ipdl/test/gtest/TestBasicChild.h","sym":"FILE_ipc/ipdl/test/gtest/TestBasicChild@2Eh"}
{"loc":"00011:9-46","source":1,"syntax":"file,use","pretty":"file ipc/ipdl/test/gtest/TestBasicParent.h","sym":"FILE_ipc/ipdl/test/gtest/TestBasicParent@2Eh"}
{"loc":"00015:10-17","source":1,"syntax":"def,namespace","pretty":"namespace mozilla","sym":"NS_mozilla"}
{"loc":"00015:19-28","source":1,"syntax":"def,namespace","pretty":"namespace mozilla::_ipdltest","sym":"NS_mozilla::_ipdltest","nestingRange":"15:30-29:0"}
{"loc":"00017:0-9","source":1,"syntax":"type,use","pretty":"type mozilla::ipc::IPCResult","sym":"T_mozilla::ipc::IPCResult","type":"class mozilla::ipc::IPCResult","typesym":"T_mozilla::ipc::IPCResult"}
{"loc":"00017:10-24","source":1,"syntax":"type,use","pretty":"type mozilla::_ipdltest::TestBasicChild","sym":"T_mozilla::_ipdltest::TestBasicChild","type":"class mozilla::_ipdltest::TestBasicChild","typesym":"T_mozilla::_ipdltest::TestBasicChild"}
{"loc":"00017:26-35","source":1,"syntax":"def,function","pretty":"function mozilla::_ipdltest::TestBasicChild::RecvHello","sym":"_ZN7mozilla9_ipdltest14TestBasicChild9RecvHelloEv","nestingRange":"17:38-22:0","type":"class mozilla::ipc::IPCResult (void)"}
{"loc":"00018:2-13","source":1,"syntax":"macro,use","pretty":"macro EXPECT_TRUE","sym":"M_e6015e2edc4cf2c3"}
{"loc":"00018:2-13","source":1,"syntax":"","pretty":"variable gtest_ar_","sym":"V_5e5b2_d9e866151c9c773","no_crossref":1,"type":"const ::testing::AssertionResult","typesym":"T_testing::AssertionResult"}
{"loc":"00018:14-21","source":1,"syntax":"function,use","pretty":"function mozilla::ipc::IProtocol::CanSend","sym":"_ZNK7mozilla3ipc9IProtocol7CanSendEv","type":"_Bool"}
{"loc":"00019:2-7","source":1,"syntax":"function,use","pretty":"function mozilla::ipc::IToplevelProtocol::Close","sym":"_ZN7mozilla3ipc17IToplevelProtocol5CloseEv","type":"void"}
{"loc":"00020:2-14","source":1,"syntax":"macro,use","pretty":"macro EXPECT_FALSE","sym":"M_27125e2edc4cf2c3"}
{"loc":"00020:2-14","source":1,"syntax":"","pretty":"variable gtest_ar_","sym":"V_5e5b2_d9e866151c9c773","no_crossref":1,"type":"const ::testing::AssertionResult","typesym":"T_testing::AssertionResult"}
{"loc":"00020:15-22","source":1,"syntax":"function,use","pretty":"function mozilla::ipc::IProtocol::CanSend","sym":"_ZNK7mozilla3ipc9IProtocol7CanSendEv","type":"_Bool"}
{"loc":"00021:9-15","source":1,"syntax":"macro,use","pretty":"macro IPC_OK","sym":"M_53444620e89184de"}
{"loc":"00024:0-9","source":1,"syntax":"def,destructor","pretty":"destructor mozilla::_ipdltest::IPDLTest_CrossProcess_TestBasic_Test::~IPDLTest_CrossProcess_TestBasic_Test","sym":"_ZN7mozilla9_ipdltest36IPDLTest_CrossProcess_TestBasic_TestD1Ev","type":"void (void) noexcept"}
{"loc":"00024:0-9","source":1,"syntax":"def,destructor","pretty":"destructor mozilla::_ipdltest::IPDLTest_CrossThread_TestBasic_Test::~IPDLTest_CrossThread_TestBasic_Test","sym":"_ZN7mozilla9_ipdltest35IPDLTest_CrossThread_TestBasic_TestD1Ev","type":"void (void) noexcept"}
{"loc":"00024:0-9","source":1,"syntax":"def,field","pretty":"field mozilla::_ipdltest::IPDL_TEST_TestBasic::mActor","sym":"F_<T_mozilla::_ipdltest::IPDL_TEST_TestBasic>_mActor","type":"class mozilla::_ipdltest::TestBasicParent *"}
{"loc":"00024:0-9","source":1,"syntax":"def,function","pretty":"function mozilla::_ipdltest::IPDLTest_CrossProcess_TestBasic_Test::IPDLTest_CrossProcess_TestBasic_Test","sym":"_ZN7mozilla9_ipdltest36IPDLTest_CrossProcess_TestBasic_TestC1Ev","type":"void (void) noexcept(false)"}
{"loc":"00024:0-9","source":1,"syntax":"decl,def,function","pretty":"function mozilla::_ipdltest::IPDLTest_CrossProcess_TestBasic_Test::TestBody","sym":"_ZN7mozilla9_ipdltest36IPDLTest_CrossProcess_TestBasic_Test8TestBodyEv","type":"void (void)"}
{"loc":"00024:0-9","source":1,"syntax":"def,function","pretty":"function mozilla::_ipdltest::IPDLTest_CrossProcess_TestBasic_Test::operator=","sym":"_ZN7mozilla9_ipdltest36IPDLTest_CrossProcess_TestBasic_TestaSERKS1_,_ZN7mozilla9_ipdltest36IPDLTest_CrossProcess_TestBasic_TestaSEOS1_","type":"class mozilla::_ipdltest::IPDLTest_CrossProcess_TestBasic_Test &(class mozilla::_ipdltest::IPDLTest_CrossProcess_TestBasic_Test &&) noexcept"}
{"loc":"00024:0-9","source":1,"syntax":"def,function","pretty":"function mozilla::_ipdltest::IPDLTest_CrossThread_TestBasic_Test::IPDLTest_CrossThread_TestBasic_Test","sym":"_ZN7mozilla9_ipdltest35IPDLTest_CrossThread_TestBasic_TestC1Ev","type":"void (void) noexcept(false)"}
{"loc":"00024:0-9","source":1,"syntax":"decl,def,function","pretty":"function mozilla::_ipdltest::IPDLTest_CrossThread_TestBasic_Test::TestBody","sym":"_ZN7mozilla9_ipdltest35IPDLTest_CrossThread_TestBasic_Test8TestBodyEv","type":"void (void)"}
{"loc":"00024:0-9","source":1,"syntax":"def,function","pretty":"function mozilla::_ipdltest::IPDLTest_CrossThread_TestBasic_Test::operator=","sym":"_ZN7mozilla9_ipdltest35IPDLTest_CrossThread_TestBasic_TestaSERKS1_,_ZN7mozilla9_ipdltest35IPDLTest_CrossThread_TestBasic_TestaSEOS1_","type":"class mozilla::_ipdltest::IPDLTest_CrossThread_TestBasic_Test &(class mozilla::_ipdltest::IPDLTest_CrossThread_TestBasic_Test &&) noexcept"}
{"loc":"00024:0-9","source":1,"syntax":"def,function","pretty":"function mozilla::_ipdltest::IPDL_TEST_TestBasic::GetActor","sym":"_ZN7mozilla9_ipdltest19IPDL_TEST_TestBasic8GetActorEv","type":"class mozilla::_ipdltest::TestBasicParent *(void)"}
{"loc":"00024:0-9","source":1,"syntax":"def,function","pretty":"function mozilla::_ipdltest::IPDL_TEST_TestBasic::GetName","sym":"_ZN7mozilla9_ipdltest19IPDL_TEST_TestBasic7GetNameEv","type":"const char *(void)"}
{"loc":"00024:0-9","source":1,"syntax":"def,function","pretty":"function mozilla::_ipdltest::IPDL_TEST_TestBasic::IPDL_TEST_TestBasic","sym":"_ZN7mozilla9_ipdltest19IPDL_TEST_TestBasicC1Ev","type":"void (void)"}
{"loc":"00024:0-9","source":1,"syntax":"decl,def,function","pretty":"function mozilla::_ipdltest::IPDL_TEST_TestBasic::TestBody","sym":"_ZN7mozilla9_ipdltest19IPDL_TEST_TestBasic8TestBodyEv","nestingRange":"24:21-27:0","type":"void (void)"}
{"loc":"00024:0-9","source":1,"syntax":"macro,use","pretty":"macro IPDL_TEST","sym":"M_6befec05994d4d33"}
{"loc":"00024:0-9","source":1,"syntax":"def,type","pretty":"type mozilla::_ipdltest::IPDLTest_CrossProcess_TestBasic_Test","sym":"T_mozilla::_ipdltest::IPDLTest_CrossProcess_TestBasic_Test"}
{"loc":"00024:0-9","source":1,"syntax":"def,type","pretty":"type mozilla::_ipdltest::IPDLTest_CrossThread_TestBasic_Test","sym":"T_mozilla::_ipdltest::IPDLTest_CrossThread_TestBasic_Test"}
{"loc":"00024:0-9","source":1,"syntax":"def,type","pretty":"type mozilla::_ipdltest::IPDL_TEST_TestBasic","sym":"T_mozilla::_ipdltest::IPDL_TEST_TestBasic"}
{"loc":"00024:0-9","source":1,"syntax":"decl,def,variable","pretty":"variable mozilla::_ipdltest::IPDLTest_CrossProcess_TestBasic_Test::test_info_","sym":"_ZN7mozilla9_ipdltest36IPDLTest_CrossProcess_TestBasic_Test10test_info_E","type":"::testing::TestInfo *const"}
{"loc":"00024:0-9","source":1,"syntax":"decl,def,variable","pretty":"variable mozilla::_ipdltest::IPDLTest_CrossThread_TestBasic_Test::test_info_","sym":"_ZN7mozilla9_ipdltest35IPDLTest_CrossThread_TestBasic_Test10test_info_E","type":"::testing::TestInfo *const"}
{"loc":"00024:0-9","source":1,"syntax":"decl,def,variable","pretty":"variable mozilla::_ipdltest::IPDL_TEST_TestBasic::sName","sym":"_ZN7mozilla9_ipdltest19IPDL_TEST_TestBasic5sNameE","type":"const char *"}
{"loc":"00024:0-9","source":1,"syntax":"","pretty":"variable test","sym":"V_5e5b2_5686e9c71","no_crossref":1,"type":"class mozilla::_ipdltest::IPDL_TEST_TestBasic","typesym":"T_mozilla::_ipdltest::IPDL_TEST_TestBasic"}
{"loc":"00025:7-9","source":1,"syntax":"","pretty":"variable ok","sym":"V_80bf280ab3005b39_ff8795","no_crossref":1,"type":"_Bool"}
{"loc":"00025:12-18","source":1,"syntax":"field,use","pretty":"field mozilla::_ipdltest::IPDL_TEST_TestBasic::mActor","sym":"F_<T_mozilla::_ipdltest::IPDL_TEST_TestBasic>_mActor","type":"class mozilla::_ipdltest::TestBasicParent *"}
{"loc":"00025:20-29","source":1,"syntax":"function,use","pretty":"function mozilla::_ipdltest::PTestBasicParent::SendHello","sym":"_ZN7mozilla9_ipdltest16PTestBasicParent9SendHelloEv","type":"_Bool"}
{"loc":"00026:2-13","source":1,"syntax":"macro,use","pretty":"macro ASSERT_TRUE","sym":"M_67235e2edc4cf2c3"}
{"loc":"00026:2-13","source":1,"syntax":"","pretty":"variable gtest_ar_","sym":"V_5e5b2_d9e866151c9c773","no_crossref":1,"type":"const ::testing::AssertionResult","typesym":"T_testing::AssertionResult"}
{"loc":"00026:14-16","source":1,"syntax":"","pretty":"variable ok","sym":"V_80bf280ab3005b39_ff8795","no_crossref":1,"type":"_Bool"}
{"loc":"00024:0-9","structured":1,"pretty":"mozilla::_ipdltest::IPDL_TEST_TestBasic::mActor","sym":"F_<T_mozilla::_ipdltest::IPDL_TEST_TestBasic>_mActor","kind":"field","parentsym":"T_mozilla::_ipdltest::IPDL_TEST_TestBasic","implKind":"","sizeBytes":null,"supers":[],"methods":[],"fields":[],"overrides":[],"props":[]}
{"loc":"00024:0-9","structured":1,"pretty":"mozilla::_ipdltest::IPDLTest_CrossProcess_TestBasic_Test","sym":"T_mozilla::_ipdltest::IPDLTest_CrossProcess_TestBasic_Test","kind":"class","implKind":"","sizeBytes":16,"supers":[{"pretty":"testing::Test","sym":"T_testing::Test","props":[]}],"methods":[{"pretty":"mozilla::_ipdltest::IPDLTest_CrossProcess_TestBasic_Test::IPDLTest_CrossProcess_TestBasic_Test","sym":"_ZN7mozilla9_ipdltest36IPDLTest_CrossProcess_TestBasic_TestC1Ev","props":["instance","defaulted"]},{"pretty":"mozilla::_ipdltest::IPDLTest_CrossProcess_TestBasic_Test::~IPDLTest_CrossProcess_TestBasic_Test","sym":"_ZN7mozilla9_ipdltest36IPDLTest_CrossProcess_TestBasic_TestD1Ev","props":["instance","virtual","defaulted"]},{"pretty":"mozilla::_ipdltest::IPDLTest_CrossProcess_TestBasic_Test::IPDLTest_CrossProcess_TestBasic_Test","sym":"_ZN7mozilla9_ipdltest36IPDLTest_CrossProcess_TestBasic_TestC1ERKS1_","props":["instance","deleted"]},{"pretty":"mozilla::_ipdltest::IPDLTest_CrossProcess_TestBasic_Test::operator=","sym":"_ZN7mozilla9_ipdltest36IPDLTest_CrossProcess_TestBasic_TestaSERKS1_","props":["instance","deleted"]},{"pretty":"mozilla::_ipdltest::IPDLTest_CrossProcess_TestBasic_Test::IPDLTest_CrossProcess_TestBasic_Test","sym":"_ZN7mozilla9_ipdltest36IPDLTest_CrossProcess_TestBasic_TestC1EOS1_","props":["instance","deleted"]},{"pretty":"mozilla::_ipdltest::IPDLTest_CrossProcess_TestBasic_Test::operator=","sym":"_ZN7mozilla9_ipdltest36IPDLTest_CrossProcess_TestBasic_TestaSEOS1_","props":["instance","deleted"]},{"pretty":"mozilla::_ipdltest::IPDLTest_CrossProcess_TestBasic_Test::TestBody","sym":"_ZN7mozilla9_ipdltest36IPDLTest_CrossProcess_TestBasic_Test8TestBodyEv","props":["instance","virtual","user"]}],"fields":[],"overrides":[],"props":[],"platforms":["linux64","macosx64","win64"],"variants":[{"structured":1,"pretty":"mozilla::_ipdltest::IPDLTest_CrossProcess_TestBasic_Test","sym":"T_mozilla::_ipdltest::IPDLTest_CrossProcess_TestBasic_Test","kind":"class","implKind":"","sizeBytes":8,"supers":[{"pretty":"testing::Test","sym":"T_testing::Test","props":[]}],"methods":[{"pretty":"mozilla::_ipdltest::IPDLTest_CrossProcess_TestBasic_Test::IPDLTest_CrossProcess_TestBasic_Test","sym":"_ZN7mozilla9_ipdltest36IPDLTest_CrossProcess_TestBasic_TestC1Ev","props":["instance","defaulted"]},{"pretty":"mozilla::_ipdltest::IPDLTest_CrossProcess_TestBasic_Test::~IPDLTest_CrossProcess_TestBasic_Test","sym":"_ZN7mozilla9_ipdltest36IPDLTest_CrossProcess_TestBasic_TestD1Ev","props":["instance","virtual","defaulted"]},{"pretty":"mozilla::_ipdltest::IPDLTest_CrossProcess_TestBasic_Test::IPDLTest_CrossProcess_TestBasic_Test","sym":"_ZN7mozilla9_ipdltest36IPDLTest_CrossProcess_TestBasic_TestC1ERKS1_","props":["instance","deleted"]},{"pretty":"mozilla::_ipdltest::IPDLTest_CrossProcess_TestBasic_Test::operator=","sym":"_ZN7mozilla9_ipdltest36IPDLTest_CrossProcess_TestBasic_TestaSERKS1_","props":["instance","deleted"]},{"pretty":"mozilla::_ipdltest::IPDLTest_CrossProcess_TestBasic_Test::IPDLTest_CrossProcess_TestBasic_Test","sym":"_ZN7mozilla9_ipdltest36IPDLTest_CrossProcess_TestBasic_TestC1EOS1_","props":["instance","deleted"]},{"pretty":"mozilla::_ipdltest::IPDLTest_CrossProcess_TestBasic_Test::operator=","sym":"_ZN7mozilla9_ipdltest36IPDLTest_CrossProcess_TestBasic_TestaSEOS1_","props":["instance","deleted"]},{"pretty":"mozilla::_ipdltest::IPDLTest_CrossProcess_TestBasic_Test::TestBody","sym":"_ZN7mozilla9_ipdltest36IPDLTest_CrossProcess_TestBasic_Test8TestBodyEv","props":["instance","virtual","user"]}],"fields":[],"overrides":[],"props":[],"platforms":["android-armv7"]}]}
{"loc":"00024:0-9","structured":1,"pretty":"mozilla::_ipdltest::IPDLTest_CrossThread_TestBasic_Test","sym":"T_mozilla::_ipdltest::IPDLTest_CrossThread_TestBasic_Test","kind":"class","implKind":"","sizeBytes":16,"supers":[{"pretty":"testing::Test","sym":"T_testing::Test","props":[]}],"methods":[{"pretty":"mozilla::_ipdltest::IPDLTest_CrossThread_TestBasic_Test::IPDLTest_CrossThread_TestBasic_Test","sym":"_ZN7mozilla9_ipdltest35IPDLTest_CrossThread_TestBasic_TestC1Ev","props":["instance","defaulted"]},{"pretty":"mozilla::_ipdltest::IPDLTest_CrossThread_TestBasic_Test::~IPDLTest_CrossThread_TestBasic_Test","sym":"_ZN7mozilla9_ipdltest35IPDLTest_CrossThread_TestBasic_TestD1Ev","props":["instance","virtual","defaulted"]},{"pretty":"mozilla::_ipdltest::IPDLTest_CrossThread_TestBasic_Test::IPDLTest_CrossThread_TestBasic_Test","sym":"_ZN7mozilla9_ipdltest35IPDLTest_CrossThread_TestBasic_TestC1ERKS1_","props":["instance","deleted"]},{"pretty":"mozilla::_ipdltest::IPDLTest_CrossThread_TestBasic_Test::operator=","sym":"_ZN7mozilla9_ipdltest35IPDLTest_CrossThread_TestBasic_TestaSERKS1_","props":["instance","deleted"]},{"pretty":"mozilla::_ipdltest::IPDLTest_CrossThread_TestBasic_Test::IPDLTest_CrossThread_TestBasic_Test","sym":"_ZN7mozilla9_ipdltest35IPDLTest_CrossThread_TestBasic_TestC1EOS1_","props":["instance","deleted"]},{"pretty":"mozilla::_ipdltest::IPDLTest_CrossThread_TestBasic_Test::operator=","sym":"_ZN7mozilla9_ipdltest35IPDLTest_CrossThread_TestBasic_TestaSEOS1_","props":["instance","deleted"]},{"pretty":"mozilla::_ipdltest::IPDLTest_CrossThread_TestBasic_Test::TestBody","sym":"_ZN7mozilla9_ipdltest35IPDLTest_CrossThread_TestBasic_Test8TestBodyEv","props":["instance","virtual","user"]}],"fields":[],"overrides":[],"props":[],"platforms":["linux64","macosx64","win64"],"variants":[{"structured":1,"pretty":"mozilla::_ipdltest::IPDLTest_CrossThread_TestBasic_Test","sym":"T_mozilla::_ipdltest::IPDLTest_CrossThread_TestBasic_Test","kind":"class","implKind":"","sizeBytes":8,"supers":[{"pretty":"testing::Test","sym":"T_testing::Test","props":[]}],"methods":[{"pretty":"mozilla::_ipdltest::IPDLTest_CrossThread_TestBasic_Test::IPDLTest_CrossThread_TestBasic_Test","sym":"_ZN7mozilla9_ipdltest35IPDLTest_CrossThread_TestBasic_TestC1Ev","props":["instance","defaulted"]},{"pretty":"mozilla::_ipdltest::IPDLTest_CrossThread_TestBasic_Test::~IPDLTest_CrossThread_TestBasic_Test","sym":"_ZN7mozilla9_ipdltest35IPDLTest_CrossThread_TestBasic_TestD1Ev","props":["instance","virtual","defaulted"]},{"pretty":"mozilla::_ipdltest::IPDLTest_CrossThread_TestBasic_Test::IPDLTest_CrossThread_TestBasic_Test","sym":"_ZN7mozilla9_ipdltest35IPDLTest_CrossThread_TestBasic_TestC1ERKS1_","props":["instance","deleted"]},{"pretty":"mozilla::_ipdltest::IPDLTest_CrossThread_TestBasic_Test::operator=","sym":"_ZN7mozilla9_ipdltest35IPDLTest_CrossThread_TestBasic_TestaSERKS1_","props":["instance","deleted"]},{"pretty":"mozilla::_ipdltest::IPDLTest_CrossThread_TestBasic_Test::IPDLTest_CrossThread_TestBasic_Test","sym":"_ZN7mozilla9_ipdltest35IPDLTest_CrossThread_TestBasic_TestC1EOS1_","props":["instance","deleted"]},{"pretty":"mozilla::_ipdltest::IPDLTest_CrossThread_TestBasic_Test::operator=","sym":"_ZN7mozilla9_ipdltest35IPDLTest_CrossThread_TestBasic_TestaSEOS1_","props":["instance","deleted"]},{"pretty":"mozilla::_ipdltest::IPDLTest_CrossThread_TestBasic_Test::TestBody","sym":"_ZN7mozilla9_ipdltest35IPDLTest_CrossThread_TestBasic_Test8TestBodyEv","props":["instance","virtual","user"]}],"fields":[],"overrides":[],"props":[],"platforms":["android-armv7"]}]}
{"loc":"00024:0-9","structured":1,"pretty":"mozilla::_ipdltest::IPDL_TEST_TestBasic","sym":"T_mozilla::_ipdltest::IPDL_TEST_TestBasic","kind":"class","implKind":"","sizeBytes":16,"supers":[{"pretty":"mozilla::_ipdltest::IPDLTestHelper","sym":"T_mozilla::_ipdltest::IPDLTestHelper","props":[]}],"methods":[{"pretty":"mozilla::_ipdltest::IPDL_TEST_TestBasic::IPDL_TEST_TestBasic","sym":"_ZN7mozilla9_ipdltest19IPDL_TEST_TestBasicC1Ev","props":["instance","user"]},{"pretty":"mozilla::_ipdltest::IPDL_TEST_TestBasic::TestBody","sym":"_ZN7mozilla9_ipdltest19IPDL_TEST_TestBasic8TestBodyEv","props":["instance","virtual","user"]},{"pretty":"mozilla::_ipdltest::IPDL_TEST_TestBasic::GetName","sym":"_ZN7mozilla9_ipdltest19IPDL_TEST_TestBasic7GetNameEv","props":["instance","virtual","user"]},{"pretty":"mozilla::_ipdltest::IPDL_TEST_TestBasic::GetActor","sym":"_ZN7mozilla9_ipdltest19IPDL_TEST_TestBasic8GetActorEv","props":["instance","virtual","user"]},{"pretty":"mozilla::_ipdltest::IPDL_TEST_TestBasic::operator=","sym":"_ZN7mozilla9_ipdltest19IPDL_TEST_TestBasicaSERKS1_","props":["instance","defaulted"]},{"pretty":"mozilla::_ipdltest::IPDL_TEST_TestBasic::operator=","sym":"_ZN7mozilla9_ipdltest19IPDL_TEST_TestBasicaSEOS1_","props":["instance","defaulted"]},{"pretty":"mozilla::_ipdltest::IPDL_TEST_TestBasic::~IPDL_TEST_TestBasic","sym":"_ZN7mozilla9_ipdltest19IPDL_TEST_TestBasicD1Ev","props":["instance","defaulted"]},{"pretty":"mozilla::_ipdltest::IPDL_TEST_TestBasic::IPDL_TEST_TestBasic","sym":"_ZN7mozilla9_ipdltest19IPDL_TEST_TestBasicC1ERKS1_","props":["instance","defaulted","constexpr"]},{"pretty":"mozilla::_ipdltest::IPDL_TEST_TestBasic::IPDL_TEST_TestBasic","sym":"_ZN7mozilla9_ipdltest19IPDL_TEST_TestBasicC1EOS1_","props":["instance","defaulted","constexpr"]}],"fields":[{"pretty":"mozilla::_ipdltest::IPDL_TEST_TestBasic::mActor","sym":"F_<T_mozilla::_ipdltest::IPDL_TEST_TestBasic>_mActor","type":"class mozilla::_ipdltest::TestBasicParent *","typesym":"","offsetBytes":8,"bitPositions":null,"sizeBytes":8}],"overrides":[],"props":[],"platforms":["linux64","macosx64","win64"],"variants":[{"structured":1,"pretty":"mozilla::_ipdltest::IPDL_TEST_TestBasic","sym":"T_mozilla::_ipdltest::IPDL_TEST_TestBasic","kind":"class","implKind":"","sizeBytes":8,"supers":[{"pretty":"mozilla::_ipdltest::IPDLTestHelper","sym":"T_mozilla::_ipdltest::IPDLTestHelper","props":[]}],"methods":[{"pretty":"mozilla::_ipdltest::IPDL_TEST_TestBasic::IPDL_TEST_TestBasic","sym":"_ZN7mozilla9_ipdltest19IPDL_TEST_TestBasicC1Ev","props":["instance","user"]},{"pretty":"mozilla::_ipdltest::IPDL_TEST_TestBasic::TestBody","sym":"_ZN7mozilla9_ipdltest19IPDL_TEST_TestBasic8TestBodyEv","props":["instance","virtual","user"]},{"pretty":"mozilla::_ipdltest::IPDL_TEST_TestBasic::GetName","sym":"_ZN7mozilla9_ipdltest19IPDL_TEST_TestBasic7GetNameEv","props":["instance","virtual","user"]},{"pretty":"mozilla::_ipdltest::IPDL_TEST_TestBasic::GetActor","sym":"_ZN7mozilla9_ipdltest19IPDL_TEST_TestBasic8GetActorEv","props":["instance","virtual","user"]},{"pretty":"mozilla::_ipdltest::IPDL_TEST_TestBasic::operator=","sym":"_ZN7mozilla9_ipdltest19IPDL_TEST_TestBasicaSERKS1_","props":["instance","defaulted"]},{"pretty":"mozilla::_ipdltest::IPDL_TEST_TestBasic::operator=","sym":"_ZN7mozilla9_ipdltest19IPDL_TEST_TestBasicaSEOS1_","props":["instance","defaulted"]},{"pretty":"mozilla::_ipdltest::IPDL_TEST_TestBasic::~IPDL_TEST_TestBasic","sym":"_ZN7mozilla9_ipdltest19IPDL_TEST_TestBasicD1Ev","props":["instance","defaulted"]},{"pretty":"mozilla::_ipdltest::IPDL_TEST_TestBasic::IPDL_TEST_TestBasic","sym":"_ZN7mozilla9_ipdltest19IPDL_TEST_TestBasicC1ERKS1_","props":["instance","defaulted","constexpr"]},{"pretty":"mozilla::_ipdltest::IPDL_TEST_TestBasic::IPDL_TEST_TestBasic","sym":"_ZN7mozilla9_ipdltest19IPDL_TEST_TestBasicC1EOS1_","props":["instance","defaulted","constexpr"]}],"fields":[{"pretty":"mozilla::_ipdltest::IPDL_TEST_TestBasic::mActor","sym":"F_<T_mozilla::_ipdltest::IPDL_TEST_TestBasic>_mActor","type":"class mozilla::_ipdltest::TestBasicParent *","typesym":"","offsetBytes":4,"bitPositions":null,"sizeBytes":4}],"overrides":[],"props":[],"platforms":["android-armv7"]}]}
{"loc":"00017:26-35","structured":1,"pretty":"mozilla::_ipdltest::TestBasicChild::RecvHello","sym":"_ZN7mozilla9_ipdltest14TestBasicChild9RecvHelloEv","kind":"method","parentsym":"T_mozilla::_ipdltest::TestBasicChild","implKind":"","sizeBytes":null,"supers":[],"methods":[],"fields":[],"overrides":[],"props":["instance","user"]}
{"loc":"00024:0-9","structured":1,"pretty":"mozilla::_ipdltest::IPDL_TEST_TestBasic::GetName","sym":"_ZN7mozilla9_ipdltest19IPDL_TEST_TestBasic7GetNameEv","kind":"method","parentsym":"T_mozilla::_ipdltest::IPDL_TEST_TestBasic","implKind":"","sizeBytes":null,"supers":[],"methods":[],"fields":[],"overrides":[{"pretty":"mozilla::_ipdltest::IPDLTestHelper::GetName","sym":"_ZN7mozilla9_ipdltest14IPDLTestHelper7GetNameEv"}],"props":["instance","virtual","user"]}
{"loc":"00024:0-9","structured":1,"pretty":"mozilla::_ipdltest::IPDL_TEST_TestBasic::GetActor","sym":"_ZN7mozilla9_ipdltest19IPDL_TEST_TestBasic8GetActorEv","kind":"method","parentsym":"T_mozilla::_ipdltest::IPDL_TEST_TestBasic","implKind":"","sizeBytes":null,"supers":[],"methods":[],"fields":[],"overrides":[{"pretty":"mozilla::_ipdltest::IPDLTestHelper::GetActor","sym":"_ZN7mozilla9_ipdltest14IPDLTestHelper8GetActorEv"}],"props":["instance","virtual","user"]}
{"loc":"00024:0-9","structured":1,"pretty":"mozilla::_ipdltest::IPDL_TEST_TestBasic::TestBody","sym":"_ZN7mozilla9_ipdltest19IPDL_TEST_TestBasic8TestBodyEv","kind":"method","parentsym":"T_mozilla::_ipdltest::IPDL_TEST_TestBasic","implKind":"","sizeBytes":null,"supers":[],"methods":[],"fields":[],"overrides":[{"pretty":"mozilla::_ipdltest::IPDLTestHelper::TestBody","sym":"_ZN7mozilla9_ipdltest14IPDLTestHelper8TestBodyEv"}],"props":["instance","virtual","user"]}
{"loc":"00024:0-9","structured":1,"pretty":"mozilla::_ipdltest::IPDL_TEST_TestBasic::IPDL_TEST_TestBasic","sym":"_ZN7mozilla9_ipdltest19IPDL_TEST_TestBasicC1Ev","kind":"method","parentsym":"T_mozilla::_ipdltest::IPDL_TEST_TestBasic","implKind":"","sizeBytes":null,"supers":[],"methods":[],"fields":[],"overrides":[],"props":["instance","user"]}
{"loc":"00024:0-9","structured":1,"pretty":"mozilla::_ipdltest::IPDLTest_CrossThread_TestBasic_Test::TestBody","sym":"_ZN7mozilla9_ipdltest35IPDLTest_CrossThread_TestBasic_Test8TestBodyEv","kind":"method","parentsym":"T_mozilla::_ipdltest::IPDLTest_CrossThread_TestBasic_Test","implKind":"","sizeBytes":null,"supers":[],"methods":[],"fields":[],"overrides":[{"pretty":"testing::Test::TestBody","sym":"_ZN7testing4Test8TestBodyEv"}],"props":["instance","virtual","user"]}
{"loc":"00024:0-9","structured":1,"pretty":"mozilla::_ipdltest::IPDLTest_CrossThread_TestBasic_Test::IPDLTest_CrossThread_TestBasic_Test","sym":"_ZN7mozilla9_ipdltest35IPDLTest_CrossThread_TestBasic_TestC1Ev","kind":"method","parentsym":"T_mozilla::_ipdltest::IPDLTest_CrossThread_TestBasic_Test","implKind":"","sizeBytes":null,"supers":[],"methods":[],"fields":[],"overrides":[],"props":["instance","defaulted"]}
{"loc":"00024:0-9","structured":1,"pretty":"mozilla::_ipdltest::IPDLTest_CrossThread_TestBasic_Test::~IPDLTest_CrossThread_TestBasic_Test","sym":"_ZN7mozilla9_ipdltest35IPDLTest_CrossThread_TestBasic_TestD1Ev","kind":"method","parentsym":"T_mozilla::_ipdltest::IPDLTest_CrossThread_TestBasic_Test","implKind":"","sizeBytes":null,"supers":[],"methods":[],"fields":[],"overrides":[{"pretty":"testing::Test::~Test","sym":"_ZN7testing4TestD1Ev"}],"props":["instance","virtual","defaulted"]}
{"loc":"00024:0-9","structured":1,"pretty":"mozilla::_ipdltest::IPDLTest_CrossThread_TestBasic_Test::operator=","sym":"_ZN7mozilla9_ipdltest35IPDLTest_CrossThread_TestBasic_TestaSEOS1_","kind":"method","parentsym":"T_mozilla::_ipdltest::IPDLTest_CrossThread_TestBasic_Test","implKind":"","sizeBytes":null,"supers":[],"methods":[],"fields":[],"overrides":[],"props":["instance","deleted"]}
{"loc":"00024:0-9","structured":1,"pretty":"mozilla::_ipdltest::IPDLTest_CrossThread_TestBasic_Test::operator=","sym":"_ZN7mozilla9_ipdltest35IPDLTest_CrossThread_TestBasic_TestaSERKS1_","kind":"method","parentsym":"T_mozilla::_ipdltest::IPDLTest_CrossThread_TestBasic_Test","implKind":"","sizeBytes":null,"supers":[],"methods":[],"fields":[],"overrides":[],"props":["instance","deleted"]}
{"loc":"00024:0-9","structured":1,"pretty":"mozilla::_ipdltest::IPDLTest_CrossProcess_TestBasic_Test::TestBody","sym":"_ZN7mozilla9_ipdltest36IPDLTest_CrossProcess_TestBasic_Test8TestBodyEv","kind":"method","parentsym":"T_mozilla::_ipdltest::IPDLTest_CrossProcess_TestBasic_Test","implKind":"","sizeBytes":null,"supers":[],"methods":[],"fields":[],"overrides":[{"pretty":"testing::Test::TestBody","sym":"_ZN7testing4Test8TestBodyEv"}],"props":["instance","virtual","user"]}
{"loc":"00024:0-9","structured":1,"pretty":"mozilla::_ipdltest::IPDLTest_CrossProcess_TestBasic_Test::IPDLTest_CrossProcess_TestBasic_Test","sym":"_ZN7mozilla9_ipdltest36IPDLTest_CrossProcess_TestBasic_TestC1Ev","kind":"method","parentsym":"T_mozilla::_ipdltest::IPDLTest_CrossProcess_TestBasic_Test","implKind":"","sizeBytes":null,"supers":[],"methods":[],"fields":[],"overrides":[],"props":["instance","defaulted"]}
{"loc":"00024:0-9","structured":1,"pretty":"mozilla::_ipdltest::IPDLTest_CrossProcess_TestBasic_Test::~IPDLTest_CrossProcess_TestBasic_Test","sym":"_ZN7mozilla9_ipdltest36IPDLTest_CrossProcess_TestBasic_TestD1Ev","kind":"method","parentsym":"T_mozilla::_ipdltest::IPDLTest_CrossProcess_TestBasic_Test","implKind":"","sizeBytes":null,"supers":[],"methods":[],"fields":[],"overrides":[{"pretty":"testing::Test::~Test","sym":"_ZN7testing4TestD1Ev"}],"props":["instance","virtual","defaulted"]}
{"loc":"00024:0-9","structured":1,"pretty":"mozilla::_ipdltest::IPDLTest_CrossProcess_TestBasic_Test::operator=","sym":"_ZN7mozilla9_ipdltest36IPDLTest_CrossProcess_TestBasic_TestaSEOS1_","kind":"method","parentsym":"T_mozilla::_ipdltest::IPDLTest_CrossProcess_TestBasic_Test","implKind":"","sizeBytes":null,"supers":[],"methods":[],"fields":[],"overrides":[],"props":["instance","deleted"]}
{"loc":"00024:0-9","structured":1,"pretty":"mozilla::_ipdltest::IPDLTest_CrossProcess_TestBasic_Test::operator=","sym":"_ZN7mozilla9_ipdltest36IPDLTest_CrossProcess_TestBasic_TestaSERKS1_","kind":"method","parentsym":"T_mozilla::_ipdltest::IPDLTest_CrossProcess_TestBasic_Test","implKind":"","sizeBytes":null,"supers":[],"methods":[],"fields":[],"overrides":[],"props":["instance","deleted"]}

```

## tests/tests/mc-analysis/__GENERATED__/ipc/ipdl/_ipdlheaders/mozilla/_ipdltest/PTestBasic.h
```
{"loc":"00001:0","target":1,"kind":"def","pretty":"__GENERATED__/ipc/ipdl/_ipdlheaders/mozilla/_ipdltest/PTestBasic.h","sym":"FILE_android-armv7@__GENERATED__/ipc/ipdl/_ipdlheaders/mozilla/_ipdltest/PTestBasic@2Eh"}
{"loc":"00007:8-20","target":1,"kind":"def","pretty":"PTestBasic_h","sym":"M_fde80d1f1072afec"}
{"loc":"00009:9-31","target":1,"kind":"use","pretty":"mfbt/Attributes.h","sym":"FILE_mfbt/Attributes@2Eh"}
{"loc":"00010:9-28","target":1,"kind":"use","pretty":"__GENERATED__/ipc/ipdl/_ipdlheaders/IPCMessageStart.h","sym":"FILE_android-armv7@__GENERATED__/ipc/ipdl/_ipdlheaders/IPCMessageStart@2Eh"}
{"loc":"00011:9-27","target":1,"kind":"use","pretty":"mfbt/RefPtr.h","sym":"FILE_mfbt/RefPtr@2Eh"}
{"loc":"00012:9-21","target":1,"kind":"use","pretty":"xpcom/string/nsString.h","sym":"FILE_xpcom/string/nsString@2Eh"}
{"loc":"00013:9-21","target":1,"kind":"use","pretty":"xpcom/ds/nsTArray.h","sym":"FILE_xpcom/ds/nsTArray@2Eh"}
{"loc":"00014:9-25","target":1,"kind":"use","pretty":"xpcom/ds/nsTHashtable.h","sym":"FILE_xpcom/ds/nsTHashtable@2Eh"}
{"loc":"00015:9-31","target":1,"kind":"use","pretty":"xpcom/threads/MozPromise.h","sym":"FILE_xpcom/threads/MozPromise@2Eh"}
{"loc":"00016:9-42","target":1,"kind":"use","pretty":"mfbt/OperatorNewExtensions.h","sym":"FILE_mfbt/OperatorNewExtensions@2Eh"}
{"loc":"00017:9-30","target":1,"kind":"use","pretty":"mfbt/UniquePtr.h","sym":"FILE_mfbt/UniquePtr@2Eh"}
{"loc":"00018:9-32","target":1,"kind":"use","pretty":"ipc/glue/ByteBuf.h","sym":"FILE_ipc/glue/ByteBuf@2Eh"}
{"loc":"00019:9-39","target":1,"kind":"use","pretty":"ipc/glue/FileDescriptor.h","sym":"FILE_ipc/glue/FileDescriptor@2Eh"}
{"loc":"00020:9-41","target":1,"kind":"use","pretty":"ipc/glue/ProtocolUtilsFwd.h","sym":"FILE_ipc/glue/ProtocolUtilsFwd@2Eh"}
{"loc":"00021:9-30","target":1,"kind":"use","pretty":"ipc/glue/Shmem.h","sym":"FILE_ipc/glue/Shmem@2Eh"}
{"loc":"00023:10-17","target":1,"kind":"def","pretty":"mozilla","sym":"NS_mozilla"}
{"loc":"00024:10-19","target":1,"kind":"def","pretty":"mozilla::_ipdltest","sym":"NS_mozilla::_ipdltest"}
{"loc":"00025:6-22","target":1,"kind":"forward","pretty":"mozilla::_ipdltest::PTestBasicParent","sym":"T_mozilla::_ipdltest::PTestBasicParent"}
{"loc":"00028:10-17","target":1,"kind":"def","pretty":"mozilla","sym":"NS_mozilla"}
{"loc":"00029:10-19","target":1,"kind":"def","pretty":"mozilla::_ipdltest","sym":"NS_mozilla::_ipdltest"}
{"loc":"00030:6-21","target":1,"kind":"forward","pretty":"mozilla::_ipdltest::PTestBasicChild","sym":"T_mozilla::_ipdltest::PTestBasicChild"}
{"loc":"00037:10-17","target":1,"kind":"def","pretty":"mozilla","sym":"NS_mozilla"}
{"loc":"00038:10-19","target":1,"kind":"def","pretty":"mozilla::_ipdltest","sym":"NS_mozilla::_ipdltest"}
{"loc":"00039:10-20","target":1,"kind":"def","pretty":"mozilla::_ipdltest::PTestBasic","sym":"NS_mozilla::_ipdltest::PTestBasic"}
{"loc":"00041:0-8","target":1,"kind":"use","pretty":"nsresult","sym":"T_nsresult","context":"mozilla::_ipdltest::PTestBasic::CreateEndpoints","contextsym":"_ZN7mozilla9_ipdltest10PTestBasic15CreateEndpointsEiiPNS_3ipc8EndpointINS0_16PTestBasicParentEEEPNS3_INS0_15PTestBasicChildEEE"}
{"loc":"00042:0-15","target":1,"kind":"decl","pretty":"mozilla::_ipdltest::PTestBasic::CreateEndpoints","sym":"_ZN7mozilla9_ipdltest10PTestBasic15CreateEndpointsEiiPNS_3ipc8EndpointINS0_16PTestBasicParentEEEPNS3_INS0_15PTestBasicChildEEE","peekRange":"41-46"}
{"loc":"00043:14-23","target":1,"kind":"use","pretty":"base::ProcessId","sym":"T_base::ProcessId","context":"mozilla::_ipdltest::PTestBasic::CreateEndpoints","contextsym":"_ZN7mozilla9_ipdltest10PTestBasic15CreateEndpointsEiiPNS_3ipc8EndpointINS0_16PTestBasicParentEEEPNS3_INS0_15PTestBasicChildEEE"}
{"loc":"00044:14-23","target":1,"kind":"use","pretty":"base::ProcessId","sym":"T_base::ProcessId","context":"mozilla::_ipdltest::PTestBasic::CreateEndpoints","contextsym":"_ZN7mozilla9_ipdltest10PTestBasic15CreateEndpointsEiiPNS_3ipc8EndpointINS0_16PTestBasicParentEEEPNS3_INS0_15PTestBasicChildEEE"}
{"loc":"00045:22-30","target":1,"kind":"use","pretty":"mozilla::ipc::Endpoint","sym":"T_mozilla::ipc::Endpoint","context":"mozilla::_ipdltest::PTestBasic::CreateEndpoints","contextsym":"_ZN7mozilla9_ipdltest10PTestBasic15CreateEndpointsEiiPNS_3ipc8EndpointINS0_16PTestBasicParentEEEPNS3_INS0_15PTestBasicChildEEE"}
{"loc":"00045:51-67","target":1,"kind":"use","pretty":"mozilla::_ipdltest::PTestBasicParent","sym":"T_mozilla::_ipdltest::PTestBasicParent","context":"mozilla::_ipdltest::PTestBasic::CreateEndpoints","contextsym":"_ZN7mozilla9_ipdltest10PTestBasic15CreateEndpointsEiiPNS_3ipc8EndpointINS0_16PTestBasicParentEEEPNS3_INS0_15PTestBasicChildEEE"}
{"loc":"00046:22-30","target":1,"kind":"use","pretty":"mozilla::ipc::Endpoint","sym":"T_mozilla::ipc::Endpoint","context":"mozilla::_ipdltest::PTestBasic::CreateEndpoints","contextsym":"_ZN7mozilla9_ipdltest10PTestBasic15CreateEndpointsEiiPNS_3ipc8EndpointINS0_16PTestBasicParentEEEPNS3_INS0_15PTestBasicChildEEE"}
{"loc":"00046:51-66","target":1,"kind":"use","pretty":"mozilla::_ipdltest::PTestBasicChild","sym":"T_mozilla::_ipdltest::PTestBasicChild","context":"mozilla::_ipdltest::PTestBasic::CreateEndpoints","contextsym":"_ZN7mozilla9_ipdltest10PTestBasic15CreateEndpointsEiiPNS_3ipc8EndpointINS0_16PTestBasicParentEEEPNS3_INS0_15PTestBasicChildEEE"}
{"loc":"00048:0-8","target":1,"kind":"use","pretty":"nsresult","sym":"T_nsresult","context":"mozilla::_ipdltest::PTestBasic::CreateEndpoints","contextsym":"_ZN7mozilla9_ipdltest10PTestBasic15CreateEndpointsEPNS_3ipc8EndpointINS0_16PTestBasicParentEEEPNS3_INS0_15PTestBasicChildEEE"}
{"loc":"00049:0-15","target":1,"kind":"decl","pretty":"mozilla::_ipdltest::PTestBasic::CreateEndpoints","sym":"_ZN7mozilla9_ipdltest10PTestBasic15CreateEndpointsEPNS_3ipc8EndpointINS0_16PTestBasicParentEEEPNS3_INS0_15PTestBasicChildEEE","peekRange":"48-51"}
{"loc":"00050:22-30","target":1,"kind":"use","pretty":"mozilla::ipc::Endpoint","sym":"T_mozilla::ipc::Endpoint","context":"mozilla::_ipdltest::PTestBasic::CreateEndpoints","contextsym":"_ZN7mozilla9_ipdltest10PTestBasic15CreateEndpointsEPNS_3ipc8EndpointINS0_16PTestBasicParentEEEPNS3_INS0_15PTestBasicChildEEE"}
{"loc":"00050:51-67","target":1,"kind":"use","pretty":"mozilla::_ipdltest::PTestBasicParent","sym":"T_mozilla::_ipdltest::PTestBasicParent","context":"mozilla::_ipdltest::PTestBasic::CreateEndpoints","contextsym":"_ZN7mozilla9_ipdltest10PTestBasic15CreateEndpointsEPNS_3ipc8EndpointINS0_16PTestBasicParentEEEPNS3_INS0_15PTestBasicChildEEE"}
{"loc":"00051:22-30","target":1,"kind":"use","pretty":"mozilla::ipc::Endpoint","sym":"T_mozilla::ipc::Endpoint","context":"mozilla::_ipdltest::PTestBasic::CreateEndpoints","contextsym":"_ZN7mozilla9_ipdltest10PTestBasic15CreateEndpointsEPNS_3ipc8EndpointINS0_16PTestBasicParentEEEPNS3_INS0_15PTestBasicChildEEE"}
{"loc":"00051:51-66","target":1,"kind":"use","pretty":"mozilla::_ipdltest::PTestBasicChild","sym":"T_mozilla::_ipdltest::PTestBasicChild","context":"mozilla::_ipdltest::PTestBasic::CreateEndpoints","contextsym":"_ZN7mozilla9_ipdltest10PTestBasic15CreateEndpointsEPNS_3ipc8EndpointINS0_16PTestBasicParentEEEPNS3_INS0_15PTestBasicChildEEE"}
{"loc":"00053:5-16","target":1,"kind":"def","pretty":"mozilla::_ipdltest::PTestBasic::MessageType","sym":"T_mozilla::_ipdltest::PTestBasic::MessageType","peekRange":"53-53"}
{"loc":"00054:22-40","target":1,"kind":"use","pretty":"IPCMessageStart::PTestBasicMsgStart","sym":"E_<T_IPCMessageStart>_PTestBasicMsgStart","context":"mozilla::_ipdltest::PTestBasic::MessageType","contextsym":"T_mozilla::_ipdltest::PTestBasic::MessageType"}
{"loc":"00054:4-19","target":1,"kind":"def","pretty":"mozilla::_ipdltest::PTestBasic::MessageType::PTestBasicStart","sym":"E_<T_mozilla::_ipdltest::PTestBasic::MessageType>_PTestBasicStart","context":"mozilla::_ipdltest::PTestBasic::MessageType","contextsym":"T_mozilla::_ipdltest::PTestBasic::MessageType","peekRange":"54-54"}
{"loc":"00055:4-17","target":1,"kind":"def","pretty":"mozilla::_ipdltest::PTestBasic::MessageType::Msg_Hello__ID","sym":"E_<T_mozilla::_ipdltest::PTestBasic::MessageType>_Msg_Hello__ID","context":"mozilla::_ipdltest::PTestBasic::MessageType","contextsym":"T_mozilla::_ipdltest::PTestBasic::MessageType"}
{"loc":"00056:4-17","target":1,"kind":"def","pretty":"mozilla::_ipdltest::PTestBasic::MessageType::PTestBasicEnd","sym":"E_<T_mozilla::_ipdltest::PTestBasic::MessageType>_PTestBasicEnd","context":"mozilla::_ipdltest::PTestBasic::MessageType","contextsym":"T_mozilla::_ipdltest::PTestBasic::MessageType"}
{"loc":"00059:24-31","target":1,"kind":"use","pretty":"IPC::Message","sym":"T_IPC::Message","context":"mozilla::_ipdltest::PTestBasic::Msg_Hello","contextsym":"_ZN7mozilla9_ipdltest10PTestBasic9Msg_HelloEi"}
{"loc":"00059:9-18","target":1,"kind":"use","pretty":"mozilla::UniquePtr","sym":"T_mozilla::UniquePtr","context":"mozilla::_ipdltest::PTestBasic::Msg_Hello","contextsym":"_ZN7mozilla9_ipdltest10PTestBasic9Msg_HelloEi"}
{"loc":"00060:0-9","target":1,"kind":"decl","pretty":"mozilla::_ipdltest::PTestBasic::Msg_Hello","sym":"_ZN7mozilla9_ipdltest10PTestBasic9Msg_HelloEi","peekRange":"59-60"}
{"loc":"00060:10-17","target":1,"kind":"use","pretty":"int32_t","sym":"T_int32_t","context":"mozilla::_ipdltest::PTestBasic::Msg_Hello","contextsym":"_ZN7mozilla9_ipdltest10PTestBasic9Msg_HelloEi"}
{"loc":"00001:0","target":1,"kind":"def","pretty":"__GENERATED__/ipc/ipdl/_ipdlheaders/mozilla/_ipdltest/PTestBasic.h","sym":"FILE_linux@__GENERATED__/ipc/ipdl/_ipdlheaders/mozilla/_ipdltest/PTestBasic@2Eh"}
{"loc":"00007:8-20","target":1,"kind":"def","pretty":"PTestBasic_h","sym":"M_43a1d9252c3248e9"}
{"loc":"00010:9-28","target":1,"kind":"use","pretty":"__GENERATED__/ipc/ipdl/_ipdlheaders/IPCMessageStart.h","sym":"FILE_linux@__GENERATED__/ipc/ipdl/_ipdlheaders/IPCMessageStart@2Eh"}
{"loc":"00001:0","target":1,"kind":"def","pretty":"__GENERATED__/ipc/ipdl/_ipdlheaders/mozilla/_ipdltest/PTestBasic.h","sym":"FILE_macosx@__GENERATED__/ipc/ipdl/_ipdlheaders/mozilla/_ipdltest/PTestBasic@2Eh"}
{"loc":"00007:8-20","target":1,"kind":"def","pretty":"PTestBasic_h","sym":"M_fa0851c6a0ebef37"}
{"loc":"00010:9-28","target":1,"kind":"use","pretty":"__GENERATED__/ipc/ipdl/_ipdlheaders/IPCMessageStart.h","sym":"FILE_macosx@__GENERATED__/ipc/ipdl/_ipdlheaders/IPCMessageStart@2Eh"}
{"loc":"00001:0","target":1,"kind":"def","pretty":"__GENERATED__/ipc/ipdl/_ipdlheaders/mozilla/_ipdltest/PTestBasic.h","sym":"FILE_windows@__GENERATED__/ipc/ipdl/_ipdlheaders/mozilla/_ipdltest/PTestBasic@2Eh"}
{"loc":"00007:8-20","target":1,"kind":"def","pretty":"PTestBasic_h","sym":"M_fe7733305eb29afd"}
{"loc":"00010:9-28","target":1,"kind":"use","pretty":"__GENERATED__/ipc/ipdl/_ipdlheaders/IPCMessageStart.h","sym":"FILE_windows@__GENERATED__/ipc/ipdl/_ipdlheaders/IPCMessageStart@2Eh"}
{"loc":"00041:0-8","target":1,"kind":"use","pretty":"nsresult","sym":"T_nsresult","context":"mozilla::_ipdltest::PTestBasic::CreateEndpoints","contextsym":"_ZN7mozilla9_ipdltest10PTestBasic15CreateEndpointsEmmPNS_3ipc8EndpointINS0_16PTestBasicParentEEEPNS3_INS0_15PTestBasicChildEEE"}
{"loc":"00042:0-15","target":1,"kind":"decl","pretty":"mozilla::_ipdltest::PTestBasic::CreateEndpoints","sym":"_ZN7mozilla9_ipdltest10PTestBasic15CreateEndpointsEmmPNS_3ipc8EndpointINS0_16PTestBasicParentEEEPNS3_INS0_15PTestBasicChildEEE","peekRange":"41-46"}
{"loc":"00043:14-23","target":1,"kind":"use","pretty":"base::ProcessId","sym":"T_base::ProcessId","context":"mozilla::_ipdltest::PTestBasic::CreateEndpoints","contextsym":"_ZN7mozilla9_ipdltest10PTestBasic15CreateEndpointsEmmPNS_3ipc8EndpointINS0_16PTestBasicParentEEEPNS3_INS0_15PTestBasicChildEEE"}
{"loc":"00044:14-23","target":1,"kind":"use","pretty":"base::ProcessId","sym":"T_base::ProcessId","context":"mozilla::_ipdltest::PTestBasic::CreateEndpoints","contextsym":"_ZN7mozilla9_ipdltest10PTestBasic15CreateEndpointsEmmPNS_3ipc8EndpointINS0_16PTestBasicParentEEEPNS3_INS0_15PTestBasicChildEEE"}
{"loc":"00045:22-30","target":1,"kind":"use","pretty":"mozilla::ipc::Endpoint","sym":"T_mozilla::ipc::Endpoint","context":"mozilla::_ipdltest::PTestBasic::CreateEndpoints","contextsym":"_ZN7mozilla9_ipdltest10PTestBasic15CreateEndpointsEmmPNS_3ipc8EndpointINS0_16PTestBasicParentEEEPNS3_INS0_15PTestBasicChildEEE"}
{"loc":"00045:51-67","target":1,"kind":"use","pretty":"mozilla::_ipdltest::PTestBasicParent","sym":"T_mozilla::_ipdltest::PTestBasicParent","context":"mozilla::_ipdltest::PTestBasic::CreateEndpoints","contextsym":"_ZN7mozilla9_ipdltest10PTestBasic15CreateEndpointsEmmPNS_3ipc8EndpointINS0_16PTestBasicParentEEEPNS3_INS0_15PTestBasicChildEEE"}
{"loc":"00046:22-30","target":1,"kind":"use","pretty":"mozilla::ipc::Endpoint","sym":"T_mozilla::ipc::Endpoint","context":"mozilla::_ipdltest::PTestBasic::CreateEndpoints","contextsym":"_ZN7mozilla9_ipdltest10PTestBasic15CreateEndpointsEmmPNS_3ipc8EndpointINS0_16PTestBasicParentEEEPNS3_INS0_15PTestBasicChildEEE"}
{"loc":"00046:51-66","target":1,"kind":"use","pretty":"mozilla::_ipdltest::PTestBasicChild","sym":"T_mozilla::_ipdltest::PTestBasicChild","context":"mozilla::_ipdltest::PTestBasic::CreateEndpoints","contextsym":"_ZN7mozilla9_ipdltest10PTestBasic15CreateEndpointsEmmPNS_3ipc8EndpointINS0_16PTestBasicParentEEEPNS3_INS0_15PTestBasicChildEEE"}
{"loc":"00001:0","source":1,"syntax":"def,file","pretty":"file __GENERATED__/ipc/ipdl/_ipdlheaders/mozilla/_ipdltest/PTestBasic.h","sym":"FILE_windows@__GENERATED__/ipc/ipdl/_ipdlheaders/mozilla/_ipdltest/PTestBasic@2Eh,FILE_macosx@__GENERATED__/ipc/ipdl/_ipdlheaders/mozilla/_ipdltest/PTestBasic@2Eh,FILE_linux@__GENERATED__/ipc/ipdl/_ipdlheaders/mozilla/_ipdltest/PTestBasic@2Eh,FILE_android-armv7@__GENERATED__/ipc/ipdl/_ipdlheaders/mozilla/_ipdltest/PTestBasic@2Eh"}
{"loc":"00007:8-20","source":1,"syntax":"def,macro","pretty":"macro PTestBasic_h","sym":"M_fe7733305eb29afd,M_fa0851c6a0ebef37,M_43a1d9252c3248e9,M_fde80d1f1072afec"}
{"loc":"00009:9-31","source":1,"syntax":"file,use","pretty":"file mfbt/Attributes.h","sym":"FILE_mfbt/Attributes@2Eh"}
{"loc":"00010:9-28","source":1,"syntax":"file,use","pretty":"file __GENERATED__/ipc/ipdl/_ipdlheaders/IPCMessageStart.h","sym":"FILE_windows@__GENERATED__/ipc/ipdl/_ipdlheaders/IPCMessageStart@2Eh,FILE_macosx@__GENERATED__/ipc/ipdl/_ipdlheaders/IPCMessageStart@2Eh,FILE_linux@__GENERATED__/ipc/ipdl/_ipdlheaders/IPCMessageStart@2Eh,FILE_android-armv7@__GENERATED__/ipc/ipdl/_ipdlheaders/IPCMessageStart@2Eh"}
{"loc":"00011:9-27","source":1,"syntax":"file,use","pretty":"file mfbt/RefPtr.h","sym":"FILE_mfbt/RefPtr@2Eh"}
{"loc":"00012:9-21","source":1,"syntax":"file,use","pretty":"file xpcom/string/nsString.h","sym":"FILE_xpcom/string/nsString@2Eh"}
{"loc":"00013:9-21","source":1,"syntax":"file,use","pretty":"file xpcom/ds/nsTArray.h","sym":"FILE_xpcom/ds/nsTArray@2Eh"}
{"loc":"00014:9-25","source":1,"syntax":"file,use","pretty":"file xpcom/ds/nsTHashtable.h","sym":"FILE_xpcom/ds/nsTHashtable@2Eh"}
{"loc":"00015:9-31","source":1,"syntax":"file,use","pretty":"file xpcom/threads/MozPromise.h","sym":"FILE_xpcom/threads/MozPromise@2Eh"}
{"loc":"00016:9-42","source":1,"syntax":"file,use","pretty":"file mfbt/OperatorNewExtensions.h","sym":"FILE_mfbt/OperatorNewExtensions@2Eh"}
{"loc":"00017:9-30","source":1,"syntax":"file,use","pretty":"file mfbt/UniquePtr.h","sym":"FILE_mfbt/UniquePtr@2Eh"}
{"loc":"00018:9-32","source":1,"syntax":"file,use","pretty":"file ipc/glue/ByteBuf.h","sym":"FILE_ipc/glue/ByteBuf@2Eh"}
{"loc":"00019:9-39","source":1,"syntax":"file,use","pretty":"file ipc/glue/FileDescriptor.h","sym":"FILE_ipc/glue/FileDescriptor@2Eh"}
{"loc":"00020:9-41","source":1,"syntax":"file,use","pretty":"file ipc/glue/ProtocolUtilsFwd.h","sym":"FILE_ipc/glue/ProtocolUtilsFwd@2Eh"}
{"loc":"00021:9-30","source":1,"syntax":"file,use","pretty":"file ipc/glue/Shmem.h","sym":"FILE_ipc/glue/Shmem@2Eh"}
{"loc":"00023:10-17","source":1,"syntax":"def,namespace","pretty":"namespace mozilla","sym":"NS_mozilla","nestingRange":"23:19-27:0"}
{"loc":"00024:10-19","source":1,"syntax":"def,namespace","pretty":"namespace mozilla::_ipdltest","sym":"NS_mozilla::_ipdltest","nestingRange":"24:21-26:0"}
{"loc":"00025:6-22","source":1,"syntax":"forward,type","pretty":"type mozilla::_ipdltest::PTestBasicParent","sym":"T_mozilla::_ipdltest::PTestBasicParent"}
{"loc":"00028:10-17","source":1,"syntax":"def,namespace","pretty":"namespace mozilla","sym":"NS_mozilla","nestingRange":"28:19-32:0"}
{"loc":"00029:10-19","source":1,"syntax":"def,namespace","pretty":"namespace mozilla::_ipdltest","sym":"NS_mozilla::_ipdltest","nestingRange":"29:21-31:0"}
{"loc":"00030:6-21","source":1,"syntax":"forward,type","pretty":"type mozilla::_ipdltest::PTestBasicChild","sym":"T_mozilla::_ipdltest::PTestBasicChild"}
{"loc":"00037:10-17","source":1,"syntax":"def,namespace","pretty":"namespace mozilla","sym":"NS_mozilla","nestingRange":"37:19-66:0"}
{"loc":"00038:10-19","source":1,"syntax":"def,namespace","pretty":"namespace mozilla::_ipdltest","sym":"NS_mozilla::_ipdltest","nestingRange":"38:21-65:0"}
{"loc":"00039:10-20","source":1,"syntax":"def,namespace","pretty":"namespace mozilla::_ipdltest::PTestBasic","sym":"NS_mozilla::_ipdltest::PTestBasic","nestingRange":"39:22-64:0"}
{"loc":"00041:0-8","source":1,"syntax":"type,use","pretty":"type nsresult","sym":"T_nsresult","type":"enum nsresult","typesym":"T_nsresult"}
{"loc":"00042:0-15","source":1,"syntax":"decl,function","pretty":"function mozilla::_ipdltest::PTestBasic::CreateEndpoints","sym":"_ZN7mozilla9_ipdltest10PTestBasic15CreateEndpointsEmmPNS_3ipc8EndpointINS0_16PTestBasicParentEEEPNS3_INS0_15PTestBasicChildEEE,_ZN7mozilla9_ipdltest10PTestBasic15CreateEndpointsEiiPNS_3ipc8EndpointINS0_16PTestBasicParentEEEPNS3_INS0_15PTestBasicChildEEE","type":"enum nsresult (base::ProcessId, base::ProcessId, mozilla::ipc::Endpoint<mozilla::_ipdltest::PTestBasicParent> *, mozilla::ipc::Endpoint<mozilla::_ipdltest::PTestBasicChild> *)"}
{"loc":"00043:14-23","source":1,"syntax":"type,use","pretty":"type base::ProcessId","sym":"T_base::ProcessId","type":"base::ProcessId"}
{"loc":"00043:24-38","source":1,"syntax":"","pretty":"variable aParentDestPid","sym":"V_db5a8e96588aec4d_d1259d65da7e98a3,V_d76cb0fe75f76d3f_d1259d65da7e98a3,V_2a09686ac0c980f6_d1259d65da7e98a3,V_daa982c20470f3ea_d1259d65da7e98a3","no_crossref":1,"type":"base::ProcessId"}
{"loc":"00044:14-23","source":1,"syntax":"type,use","pretty":"type base::ProcessId","sym":"T_base::ProcessId","type":"base::ProcessId"}
{"loc":"00044:24-37","source":1,"syntax":"","pretty":"variable aChildDestPid","sym":"V_e1239e96588aec4d_797885af5cc6dac7,V_ed25c0fe75f76d3f_797885af5cc6dac7,V_30d1786ac0c980f6_797885af5cc6dac7,V_e07292c20470f3ea_797885af5cc6dac7","no_crossref":1,"type":"base::ProcessId"}
{"loc":"00045:22-30","source":1,"syntax":"type,use","pretty":"type mozilla::ipc::Endpoint","sym":"T_mozilla::ipc::Endpoint"}
{"loc":"00045:51-67","source":1,"syntax":"type,use","pretty":"type mozilla::_ipdltest::PTestBasicParent","sym":"T_mozilla::_ipdltest::PTestBasicParent","type":"class mozilla::_ipdltest::PTestBasicParent","typesym":"T_mozilla::_ipdltest::PTestBasicParent"}
{"loc":"00045:70-77","source":1,"syntax":"","pretty":"variable aParent","sym":"V_02fb9e96588aec4d_03dd840f0b0d,V_0efdc0fe75f76d3f_03dd840f0b0d,V_50aa786ac0c980f6_03dd840f0b0d,V_014b92c20470f3ea_03dd840f0b0d","no_crossref":1,"type":"mozilla::ipc::Endpoint<mozilla::_ipdltest::PTestBasicParent> *"}
{"loc":"00046:22-30","source":1,"syntax":"type,use","pretty":"type mozilla::ipc::Endpoint","sym":"T_mozilla::ipc::Endpoint"}
{"loc":"00046:51-66","source":1,"syntax":"type,use","pretty":"type mozilla::_ipdltest::PTestBasicChild","sym":"T_mozilla::_ipdltest::PTestBasicChild","type":"class mozilla::_ipdltest::PTestBasicChild","typesym":"T_mozilla::_ipdltest::PTestBasicChild"}
{"loc":"00046:69-75","source":1,"syntax":"","pretty":"variable aChild","sym":"V_96b4ae96588aec4d_ae7aa1fe256,V_92c6d0fe75f76d3f_ae7aa1fe256,V_e463886ac0c980f6_ae7aa1fe256,V_9504a2c20470f3ea_ae7aa1fe256","no_crossref":1,"type":"mozilla::ipc::Endpoint<mozilla::_ipdltest::PTestBasicChild> *"}
{"loc":"00048:0-8","source":1,"syntax":"type,use","pretty":"type nsresult","sym":"T_nsresult","type":"enum nsresult","typesym":"T_nsresult"}
{"loc":"00049:0-15","source":1,"syntax":"decl,function","pretty":"function mozilla::_ipdltest::PTestBasic::CreateEndpoints","sym":"_ZN7mozilla9_ipdltest10PTestBasic15CreateEndpointsEPNS_3ipc8EndpointINS0_16PTestBasicParentEEEPNS3_INS0_15PTestBasicChildEEE","type":"enum nsresult (mozilla::ipc::Endpoint<mozilla::_ipdltest::PTestBasicParent> *, mozilla::ipc::Endpoint<mozilla::_ipdltest::PTestBasicChild> *)"}
{"loc":"00050:22-30","source":1,"syntax":"type,use","pretty":"type mozilla::ipc::Endpoint","sym":"T_mozilla::ipc::Endpoint"}
{"loc":"00050:51-67","source":1,"syntax":"type,use","pretty":"type mozilla::_ipdltest::PTestBasicParent","sym":"T_mozilla::_ipdltest::PTestBasicParent","type":"class mozilla::_ipdltest::PTestBasicParent","typesym":"T_mozilla::_ipdltest::PTestBasicParent"}
{"loc":"00050:70-77","source":1,"syntax":"","pretty":"variable aParent","sym":"V_cb919f96588aec4d_03dd840f0b0d,V_c7a3c1fe75f76d3f_03dd840f0b0d,V_1a40796ac0c980f6_03dd840f0b0d,V_cae093c20470f3ea_03dd840f0b0d","no_crossref":1,"type":"mozilla::ipc::Endpoint<mozilla::_ipdltest::PTestBasicParent> *"}
{"loc":"00051:22-30","source":1,"syntax":"type,use","pretty":"type mozilla::ipc::Endpoint","sym":"T_mozilla::ipc::Endpoint"}
{"loc":"00051:51-66","source":1,"syntax":"type,use","pretty":"type mozilla::_ipdltest::PTestBasicChild","sym":"T_mozilla::_ipdltest::PTestBasicChild","type":"class mozilla::_ipdltest::PTestBasicChild","typesym":"T_mozilla::_ipdltest::PTestBasicChild"}
{"loc":"00051:69-75","source":1,"syntax":"","pretty":"variable aChild","sym":"V_506a9f96588aec4d_ae7aa1fe256,V_5c6cc1fe75f76d3f_ae7aa1fe256,V_ae09796ac0c980f6_ae7aa1fe256,V_5fa993c20470f3ea_ae7aa1fe256","no_crossref":1,"type":"mozilla::ipc::Endpoint<mozilla::_ipdltest::PTestBasicChild> *"}
{"loc":"00053:5-16","source":1,"syntax":"def,type","pretty":"type mozilla::_ipdltest::PTestBasic::MessageType","sym":"T_mozilla::_ipdltest::PTestBasic::MessageType","nestingRange":"53:17-57:0"}
{"loc":"00054:4-19","source":1,"syntax":"def,enum constant","pretty":"enum constant mozilla::_ipdltest::PTestBasic::MessageType::PTestBasicStart","sym":"E_<T_mozilla::_ipdltest::PTestBasic::MessageType>_PTestBasicStart","type":"enum mozilla::_ipdltest::PTestBasic::MessageType","typesym":"T_mozilla::_ipdltest::PTestBasic::MessageType"}
{"loc":"00054:22-40","source":1,"syntax":"enum,use","pretty":"enum IPCMessageStart::PTestBasicMsgStart","sym":"E_<T_IPCMessageStart>_PTestBasicMsgStart","type":"enum IPCMessageStart","typesym":"T_IPCMessageStart"}
{"loc":"00055:4-17","source":1,"syntax":"def,enum constant","pretty":"enum constant mozilla::_ipdltest::PTestBasic::MessageType::Msg_Hello__ID","sym":"E_<T_mozilla::_ipdltest::PTestBasic::MessageType>_Msg_Hello__ID","type":"enum mozilla::_ipdltest::PTestBasic::MessageType","typesym":"T_mozilla::_ipdltest::PTestBasic::MessageType"}
{"loc":"00056:4-17","source":1,"syntax":"def,enum constant","pretty":"enum constant mozilla::_ipdltest::PTestBasic::MessageType::PTestBasicEnd","sym":"E_<T_mozilla::_ipdltest::PTestBasic::MessageType>_PTestBasicEnd","type":"enum mozilla::_ipdltest::PTestBasic::MessageType","typesym":"T_mozilla::_ipdltest::PTestBasic::MessageType"}
{"loc":"00059:9-18","source":1,"syntax":"type,use","pretty":"type mozilla::UniquePtr","sym":"T_mozilla::UniquePtr"}
{"loc":"00059:24-31","source":1,"syntax":"type,use","pretty":"type IPC::Message","sym":"T_IPC::Message","type":"class IPC::Message","typesym":"T_IPC::Message"}
{"loc":"00060:0-9","source":1,"syntax":"decl,function","pretty":"function mozilla::_ipdltest::PTestBasic::Msg_Hello","sym":"_ZN7mozilla9_ipdltest10PTestBasic9Msg_HelloEi","type":"mozilla::UniquePtr<IPC::Message> (int32_t)"}
{"loc":"00060:10-17","source":1,"syntax":"type,use","pretty":"type int32_t","sym":"T_int32_t","type":"int32_t"}
{"loc":"00060:18-27","source":1,"syntax":"","pretty":"variable routingId","sym":"V_f713b0a6588aec4d_a5670b686a7d773,V_f325e2fe75f76d3f_a5670b686a7d773,V_46c19a6ac0c980f6_a5670b686a7d773,V_f662b4c20470f3ea_a5670b686a7d773","no_crossref":1,"type":"int32_t"}

```

## tests/tests/mc-analysis/__GENERATED__/ipc/ipdl/_ipdlheaders/mozilla/_ipdltest/PTestBasicChild.h
```
{"loc":"00001:0","target":1,"kind":"def","pretty":"__GENERATED__/ipc/ipdl/_ipdlheaders/mozilla/_ipdltest/PTestBasicChild.h","sym":"FILE_android-armv7@__GENERATED__/ipc/ipdl/_ipdlheaders/mozilla/_ipdltest/PTestBasicChild@2Eh"}
{"loc":"00007:8-25","target":1,"kind":"def","pretty":"PTestBasicChild_h","sym":"M_3063ff3329fd125d"}
{"loc":"00009:9-41","target":1,"kind":"use","pretty":"__GENERATED__/ipc/ipdl/_ipdlheaders/mozilla/_ipdltest/PTestBasic.h","sym":"FILE_android-armv7@__GENERATED__/ipc/ipdl/_ipdlheaders/mozilla/_ipdltest/PTestBasic@2Eh"}
{"loc":"00010:7-12","target":1,"kind":"use","pretty":"DEBUG","sym":"M_DEBUG"}
{"loc":"00011:9-18","target":1,"kind":"use","pretty":"__GENERATED__/dist/system_wrappers/prenv.h","sym":"FILE_android-armv7@__GENERATED__/dist/system_wrappers/prenv@2Eh"}
{"loc":"00014:9-29","target":1,"kind":"use","pretty":"mfbt/Tainting.h","sym":"FILE_mfbt/Tainting@2Eh"}
{"loc":"00015:9-39","target":1,"kind":"use","pretty":"ipc/glue/MessageChannel.h","sym":"FILE_ipc/glue/MessageChannel@2Eh"}
{"loc":"00016:9-38","target":1,"kind":"use","pretty":"ipc/glue/ProtocolUtils.h","sym":"FILE_ipc/glue/ProtocolUtils@2Eh"}
{"loc":"00017:10-17","target":1,"kind":"def","pretty":"mozilla","sym":"NS_mozilla"}
{"loc":"00018:10-19","target":1,"kind":"def","pretty":"mozilla::_ipdltest","sym":"NS_mozilla::_ipdltest"}
{"loc":"00021:6-21","target":1,"kind":"def","pretty":"mozilla::_ipdltest::PTestBasicChild","sym":"T_mozilla::_ipdltest::PTestBasicChild","peekRange":"21-22"}
{"loc":"00022:25-42","target":1,"kind":"use","pretty":"mozilla::ipc::IToplevelProtocol","sym":"T_mozilla::ipc::IToplevelProtocol","context":"mozilla::_ipdltest::PTestBasicChild","contextsym":"T_mozilla::_ipdltest::PTestBasicChild"}
{"loc":"00026:26-37","target":1,"kind":"use","pretty":"mozilla::ipc::ActorHandle","sym":"T_mozilla::ipc::ActorHandle","context":"mozilla::_ipdltest::PTestBasicChild","contextsym":"T_mozilla::_ipdltest::PTestBasicChild"}
{"loc":"00026:38-49","target":1,"kind":"def","pretty":"mozilla::_ipdltest::PTestBasicChild::ActorHandle","sym":"T_mozilla::_ipdltest::PTestBasicChild::ActorHandle","context":"mozilla::_ipdltest::PTestBasicChild","contextsym":"T_mozilla::_ipdltest::PTestBasicChild"}
{"loc":"00027:26-33","target":1,"kind":"use","pretty":"mozilla::ipc::ByteBuf","sym":"T_mozilla::ipc::ByteBuf","context":"mozilla::_ipdltest::PTestBasicChild","contextsym":"T_mozilla::_ipdltest::PTestBasicChild"}
{"loc":"00027:34-41","target":1,"kind":"def","pretty":"mozilla::_ipdltest::PTestBasicChild::ByteBuf","sym":"T_mozilla::_ipdltest::PTestBasicChild::ByteBuf","context":"mozilla::_ipdltest::PTestBasicChild","contextsym":"T_mozilla::_ipdltest::PTestBasicChild"}
{"loc":"00028:34-42","target":1,"kind":"def","pretty":"mozilla::_ipdltest::PTestBasicChild::Endpoint","sym":"T_mozilla::_ipdltest::PTestBasicChild::Endpoint","context":"mozilla::_ipdltest::PTestBasicChild","contextsym":"T_mozilla::_ipdltest::PTestBasicChild"}
{"loc":"00028:59-67","target":1,"kind":"use","pretty":"mozilla::ipc::Endpoint","sym":"T_mozilla::ipc::Endpoint","context":"mozilla::_ipdltest::PTestBasicChild","contextsym":"T_mozilla::_ipdltest::PTestBasicChild"}
{"loc":"00029:26-40","target":1,"kind":"use","pretty":"mozilla::ipc::FileDescriptor","sym":"T_mozilla::ipc::FileDescriptor","context":"mozilla::_ipdltest::PTestBasicChild","contextsym":"T_mozilla::_ipdltest::PTestBasicChild"}
{"loc":"00029:41-55","target":1,"kind":"def","pretty":"mozilla::_ipdltest::PTestBasicChild::FileDescriptor","sym":"T_mozilla::_ipdltest::PTestBasicChild::FileDescriptor","context":"mozilla::_ipdltest::PTestBasicChild","contextsym":"T_mozilla::_ipdltest::PTestBasicChild"}
{"loc":"00030:34-49","target":1,"kind":"def","pretty":"mozilla::_ipdltest::PTestBasicChild::ManagedEndpoint","sym":"T_mozilla::_ipdltest::PTestBasicChild::ManagedEndpoint","context":"mozilla::_ipdltest::PTestBasicChild","contextsym":"T_mozilla::_ipdltest::PTestBasicChild"}
{"loc":"00030:66-81","target":1,"kind":"use","pretty":"mozilla::ipc::ManagedEndpoint","sym":"T_mozilla::ipc::ManagedEndpoint","context":"mozilla::_ipdltest::PTestBasicChild","contextsym":"T_mozilla::_ipdltest::PTestBasicChild"}
{"loc":"00031:18-27","target":1,"kind":"use","pretty":"base::ProcessId","sym":"T_base::ProcessId","context":"mozilla::_ipdltest::PTestBasicChild","contextsym":"T_mozilla::_ipdltest::PTestBasicChild"}
{"loc":"00031:28-37","target":1,"kind":"def","pretty":"mozilla::_ipdltest::PTestBasicChild::ProcessId","sym":"T_mozilla::_ipdltest::PTestBasicChild::ProcessId","context":"mozilla::_ipdltest::PTestBasicChild","contextsym":"T_mozilla::_ipdltest::PTestBasicChild"}
{"loc":"00032:26-36","target":1,"kind":"use","pretty":"mozilla::ipc::ProtocolId","sym":"T_mozilla::ipc::ProtocolId","context":"mozilla::_ipdltest::PTestBasicChild","contextsym":"T_mozilla::_ipdltest::PTestBasicChild"}
{"loc":"00032:37-47","target":1,"kind":"def","pretty":"mozilla::_ipdltest::PTestBasicChild::ProtocolId","sym":"T_mozilla::_ipdltest::PTestBasicChild::ProtocolId","context":"mozilla::_ipdltest::PTestBasicChild","contextsym":"T_mozilla::_ipdltest::PTestBasicChild"}
{"loc":"00033:26-46","target":1,"kind":"use","pretty":"mozilla::ipc::ResponseRejectReason","sym":"T_mozilla::ipc::ResponseRejectReason","context":"mozilla::_ipdltest::PTestBasicChild","contextsym":"T_mozilla::_ipdltest::PTestBasicChild"}
{"loc":"00033:47-67","target":1,"kind":"def","pretty":"mozilla::_ipdltest::PTestBasicChild::ResponseRejectReason","sym":"T_mozilla::_ipdltest::PTestBasicChild::ResponseRejectReason","context":"mozilla::_ipdltest::PTestBasicChild","contextsym":"T_mozilla::_ipdltest::PTestBasicChild"}
{"loc":"00034:26-31","target":1,"kind":"use","pretty":"mozilla::ipc::Shmem","sym":"T_mozilla::ipc::Shmem","context":"mozilla::_ipdltest::PTestBasicChild","contextsym":"T_mozilla::_ipdltest::PTestBasicChild"}
{"loc":"00034:32-37","target":1,"kind":"def","pretty":"mozilla::_ipdltest::PTestBasicChild::Shmem","sym":"T_mozilla::_ipdltest::PTestBasicChild::Shmem","context":"mozilla::_ipdltest::PTestBasicChild","contextsym":"T_mozilla::_ipdltest::PTestBasicChild"}
{"loc":"00035:28-37","target":1,"kind":"def","pretty":"mozilla::_ipdltest::PTestBasicChild::UniquePtr","sym":"T_mozilla::_ipdltest::PTestBasicChild::UniquePtr","context":"mozilla::_ipdltest::PTestBasicChild","contextsym":"T_mozilla::_ipdltest::PTestBasicChild"}
{"loc":"00035:49-58","target":1,"kind":"use","pretty":"mozilla::UniquePtr","sym":"T_mozilla::UniquePtr","context":"mozilla::_ipdltest::PTestBasicChild","contextsym":"T_mozilla::_ipdltest::PTestBasicChild"}
{"loc":"00039:4-19","target":1,"kind":"decl","pretty":"mozilla::_ipdltest::PTestBasicChild::ProcessingError","sym":"_ZN7mozilla9_ipdltest15PTestBasicChild15ProcessingErrorENS_3ipc14HasResultCodes6ResultEPKc","context":"mozilla::_ipdltest::PTestBasicChild","contextsym":"T_mozilla::_ipdltest::PTestBasicChild","peekRange":"38-41"}
{"loc":"00040:12-18","target":1,"kind":"use","pretty":"mozilla::ipc::HasResultCodes::Result","sym":"T_mozilla::ipc::HasResultCodes::Result","context":"mozilla::_ipdltest::PTestBasicChild::ProcessingError","contextsym":"_ZN7mozilla9_ipdltest15PTestBasicChild15ProcessingErrorENS_3ipc14HasResultCodes6ResultEPKc"}
{"loc":"00043:4-34","target":1,"kind":"decl","pretty":"mozilla::_ipdltest::PTestBasicChild::ShouldContinueFromReplyTimeout","sym":"_ZN7mozilla9_ipdltest15PTestBasicChild30ShouldContinueFromReplyTimeoutEv","context":"mozilla::_ipdltest::PTestBasicChild","contextsym":"T_mozilla::_ipdltest::PTestBasicChild","peekRange":"42-43"}
{"loc":"00046:26-35","target":1,"kind":"use","pretty":"mozilla::ipc::IProtocol","sym":"T_mozilla::ipc::IProtocol","context":"mozilla::_ipdltest::PTestBasicChild","contextsym":"T_mozilla::_ipdltest::PTestBasicChild"}
{"loc":"00046:36-45","target":1,"kind":"def","pretty":"mozilla::_ipdltest::PTestBasicChild::IProtocol","sym":"T_mozilla::_ipdltest::PTestBasicChild::IProtocol","context":"mozilla::_ipdltest::PTestBasicChild","contextsym":"T_mozilla::_ipdltest::PTestBasicChild"}
{"loc":"00047:17-24","target":1,"kind":"use","pretty":"IPC::Message","sym":"T_IPC::Message","context":"mozilla::_ipdltest::PTestBasicChild","contextsym":"T_mozilla::_ipdltest::PTestBasicChild"}
{"loc":"00047:25-32","target":1,"kind":"def","pretty":"mozilla::_ipdltest::PTestBasicChild::Message","sym":"T_mozilla::_ipdltest::PTestBasicChild::Message","context":"mozilla::_ipdltest::PTestBasicChild","contextsym":"T_mozilla::_ipdltest::PTestBasicChild"}
{"loc":"00048:18-31","target":1,"kind":"use","pretty":"base::ProcessHandle","sym":"T_base::ProcessHandle","context":"mozilla::_ipdltest::PTestBasicChild","contextsym":"T_mozilla::_ipdltest::PTestBasicChild"}
{"loc":"00048:32-45","target":1,"kind":"def","pretty":"mozilla::_ipdltest::PTestBasicChild::ProcessHandle","sym":"T_mozilla::_ipdltest::PTestBasicChild::ProcessHandle","context":"mozilla::_ipdltest::PTestBasicChild","contextsym":"T_mozilla::_ipdltest::PTestBasicChild"}
{"loc":"00049:26-40","target":1,"kind":"use","pretty":"mozilla::ipc::MessageChannel","sym":"T_mozilla::ipc::MessageChannel","context":"mozilla::_ipdltest::PTestBasicChild","contextsym":"T_mozilla::_ipdltest::PTestBasicChild"}
{"loc":"00049:41-55","target":1,"kind":"def","pretty":"mozilla::_ipdltest::PTestBasicChild::MessageChannel","sym":"T_mozilla::_ipdltest::PTestBasicChild::MessageChannel","context":"mozilla::_ipdltest::PTestBasicChild","contextsym":"T_mozilla::_ipdltest::PTestBasicChild"}
{"loc":"00050:26-38","target":1,"kind":"use","pretty":"mozilla::ipc::SharedMemory","sym":"T_mozilla::ipc::SharedMemory","context":"mozilla::_ipdltest::PTestBasicChild","contextsym":"T_mozilla::_ipdltest::PTestBasicChild"}
{"loc":"00050:39-51","target":1,"kind":"def","pretty":"mozilla::_ipdltest::PTestBasicChild::SharedMemory","sym":"T_mozilla::_ipdltest::PTestBasicChild::SharedMemory","context":"mozilla::_ipdltest::PTestBasicChild","contextsym":"T_mozilla::_ipdltest::PTestBasicChild"}
{"loc":"00053:17-32","target":1,"kind":"decl","pretty":"mozilla::_ipdltest::PTestBasicChild::PTestBasicChild","sym":"_ZN7mozilla9_ipdltest15PTestBasicChildC1Ev","context":"mozilla::_ipdltest::PTestBasicChild","contextsym":"T_mozilla::_ipdltest::PTestBasicChild"}
{"loc":"00053:4-16","target":1,"kind":"use","pretty":"MOZ_IMPLICIT","sym":"M_5f65895d5a5258f7"}
{"loc":"00055:13-28","target":1,"kind":"decl","pretty":"mozilla::_ipdltest::PTestBasicChild::~PTestBasicChild","sym":"_ZN7mozilla9_ipdltest15PTestBasicChildD1Ev","context":"mozilla::_ipdltest::PTestBasicChild","contextsym":"T_mozilla::_ipdltest::PTestBasicChild","peekRange":"55-55"}
{"loc":"00055:13-28","target":1,"kind":"use","pretty":"mozilla::_ipdltest::PTestBasicChild","sym":"T_mozilla::_ipdltest::PTestBasicChild","context":"mozilla::_ipdltest::PTestBasicChild::~PTestBasicChild","contextsym":"_ZN7mozilla9_ipdltest15PTestBasicChildD1Ev"}
{"loc":"00057:4-43","target":1,"kind":"def","pretty":"mozilla::_ipdltest::PTestBasicChild::AddRef","sym":"_ZN7mozilla9_ipdltest15PTestBasicChild6AddRefEv"}
{"loc":"00057:4-43","target":1,"kind":"def","pretty":"mozilla::_ipdltest::PTestBasicChild::Release","sym":"_ZN7mozilla9_ipdltest15PTestBasicChild7ReleaseEv"}
{"loc":"00057:4-43","target":1,"kind":"use","pretty":"NS_INLINE_DECL_PURE_VIRTUAL_REFCOUNTING","sym":"M_19459f64b289c7f3"}
{"loc":"00059:30-36","target":1,"kind":"use","pretty":"mozilla::_ipdltest::PTestBasicChild::AddRef","sym":"_ZN7mozilla9_ipdltest15PTestBasicChild6AddRefEv","context":"mozilla::_ipdltest::PTestBasicChild::ActorAlloc","contextsym":"_ZN7mozilla9_ipdltest15PTestBasicChild10ActorAllocEv"}
{"loc":"00059:9-19","target":1,"kind":"def","pretty":"mozilla::_ipdltest::PTestBasicChild::ActorAlloc","sym":"_ZN7mozilla9_ipdltest15PTestBasicChild10ActorAllocEv","context":"mozilla::_ipdltest::PTestBasicChild","contextsym":"T_mozilla::_ipdltest::PTestBasicChild","peekRange":"59-59"}
{"loc":"00060:32-39","target":1,"kind":"use","pretty":"mozilla::_ipdltest::PTestBasicChild::Release","sym":"_ZN7mozilla9_ipdltest15PTestBasicChild7ReleaseEv","context":"mozilla::_ipdltest::PTestBasicChild::ActorDealloc","contextsym":"_ZN7mozilla9_ipdltest15PTestBasicChild12ActorDeallocEv"}
{"loc":"00060:9-21","target":1,"kind":"def","pretty":"mozilla::_ipdltest::PTestBasicChild::ActorDealloc","sym":"_ZN7mozilla9_ipdltest15PTestBasicChild12ActorDeallocEv","context":"mozilla::_ipdltest::PTestBasicChild","contextsym":"T_mozilla::_ipdltest::PTestBasicChild","peekRange":"60-60"}
{"loc":"00063:21-29","target":1,"kind":"use","pretty":"nsTArray","sym":"T_nsTArray","context":"mozilla::_ipdltest::PTestBasicChild::AllManagedActors","contextsym":"_ZNK7mozilla9_ipdltest15PTestBasicChild16AllManagedActorsER8nsTArrayI6RefPtrINS_3ipc19ActorLifecycleProxyEEE"}
{"loc":"00063:30-36","target":1,"kind":"use","pretty":"RefPtr","sym":"T_RefPtr","context":"mozilla::_ipdltest::PTestBasicChild::AllManagedActors","contextsym":"_ZNK7mozilla9_ipdltest15PTestBasicChild16AllManagedActorsER8nsTArrayI6RefPtrINS_3ipc19ActorLifecycleProxyEEE"}
{"loc":"00063:4-20","target":1,"kind":"decl","pretty":"mozilla::_ipdltest::PTestBasicChild::AllManagedActors","sym":"_ZNK7mozilla9_ipdltest15PTestBasicChild16AllManagedActorsER8nsTArrayI6RefPtrINS_3ipc19ActorLifecycleProxyEEE","context":"mozilla::_ipdltest::PTestBasicChild","contextsym":"T_mozilla::_ipdltest::PTestBasicChild","peekRange":"62-63"}
{"loc":"00063:51-70","target":1,"kind":"use","pretty":"mozilla::ipc::ActorLifecycleProxy","sym":"T_mozilla::ipc::ActorLifecycleProxy","context":"mozilla::_ipdltest::PTestBasicChild::AllManagedActors","contextsym":"_ZNK7mozilla9_ipdltest15PTestBasicChild16AllManagedActorsER8nsTArrayI6RefPtrINS_3ipc19ActorLifecycleProxyEEE"}
{"loc":"00066:4-17","target":1,"kind":"decl","pretty":"mozilla::_ipdltest::PTestBasicChild::RemoveManagee","sym":"_ZN7mozilla9_ipdltest15PTestBasicChild13RemoveManageeEiPNS_3ipc9IProtocolE","context":"mozilla::_ipdltest::PTestBasicChild","contextsym":"T_mozilla::_ipdltest::PTestBasicChild","peekRange":"65-68"}
{"loc":"00067:12-19","target":1,"kind":"use","pretty":"int32_t","sym":"T_int32_t","context":"mozilla::_ipdltest::PTestBasicChild::RemoveManagee","contextsym":"_ZN7mozilla9_ipdltest15PTestBasicChild13RemoveManageeEiPNS_3ipc9IProtocolE"}
{"loc":"00068:12-21","target":1,"kind":"use","pretty":"mozilla::_ipdltest::PTestBasicChild::IProtocol","sym":"T_mozilla::_ipdltest::PTestBasicChild::IProtocol","context":"mozilla::_ipdltest::PTestBasicChild::RemoveManagee","contextsym":"_ZN7mozilla9_ipdltest15PTestBasicChild13RemoveManageeEiPNS_3ipc9IProtocolE"}
{"loc":"00070:4-18","target":1,"kind":"decl","pretty":"mozilla::_ipdltest::PTestBasicChild::DeallocManagee","sym":"_ZN7mozilla9_ipdltest15PTestBasicChild14DeallocManageeEiPNS_3ipc9IProtocolE","context":"mozilla::_ipdltest::PTestBasicChild","contextsym":"T_mozilla::_ipdltest::PTestBasicChild","peekRange":"69-72"}
{"loc":"00071:12-19","target":1,"kind":"use","pretty":"int32_t","sym":"T_int32_t","context":"mozilla::_ipdltest::PTestBasicChild::DeallocManagee","contextsym":"_ZN7mozilla9_ipdltest15PTestBasicChild14DeallocManageeEiPNS_3ipc9IProtocolE"}
{"loc":"00072:12-21","target":1,"kind":"use","pretty":"mozilla::_ipdltest::PTestBasicChild::IProtocol","sym":"T_mozilla::_ipdltest::PTestBasicChild::IProtocol","context":"mozilla::_ipdltest::PTestBasicChild::DeallocManagee","contextsym":"_ZN7mozilla9_ipdltest15PTestBasicChild14DeallocManageeEiPNS_3ipc9IProtocolE"}
{"loc":"00074:4-10","target":1,"kind":"use","pretty":"mozilla::ipc::HasResultCodes::Result","sym":"T_mozilla::ipc::HasResultCodes::Result","context":"mozilla::_ipdltest::PTestBasicChild::OnMessageReceived","contextsym":"_ZN7mozilla9_ipdltest15PTestBasicChild17OnMessageReceivedERKN3IPC7MessageE"}
{"loc":"00075:28-35","target":1,"kind":"use","pretty":"mozilla::_ipdltest::PTestBasicChild::Message","sym":"T_mozilla::_ipdltest::PTestBasicChild::Message","context":"mozilla::_ipdltest::PTestBasicChild::OnMessageReceived","contextsym":"_ZN7mozilla9_ipdltest15PTestBasicChild17OnMessageReceivedERKN3IPC7MessageE"}
{"loc":"00075:4-21","target":1,"kind":"decl","pretty":"mozilla::_ipdltest::PTestBasicChild::OnMessageReceived","sym":"_ZN7mozilla9_ipdltest15PTestBasicChild17OnMessageReceivedERKN3IPC7MessageE","context":"mozilla::_ipdltest::PTestBasicChild","contextsym":"T_mozilla::_ipdltest::PTestBasicChild","peekRange":"74-75"}
{"loc":"00077:4-10","target":1,"kind":"use","pretty":"mozilla::ipc::HasResultCodes::Result","sym":"T_mozilla::ipc::HasResultCodes::Result","context":"mozilla::_ipdltest::PTestBasicChild::OnMessageReceived","contextsym":"_ZN7mozilla9_ipdltest15PTestBasicChild17OnMessageReceivedERKN3IPC7MessageERNS_9UniquePtrIS3_NS_13DefaultDeleteIS3_EEEE"}
{"loc":"00078:4-21","target":1,"kind":"decl","pretty":"mozilla::_ipdltest::PTestBasicChild::OnMessageReceived","sym":"_ZN7mozilla9_ipdltest15PTestBasicChild17OnMessageReceivedERKN3IPC7MessageERNS_9UniquePtrIS3_NS_13DefaultDeleteIS3_EEEE","context":"mozilla::_ipdltest::PTestBasicChild","contextsym":"T_mozilla::_ipdltest::PTestBasicChild","peekRange":"77-80"}
{"loc":"00079:18-25","target":1,"kind":"use","pretty":"mozilla::_ipdltest::PTestBasicChild::Message","sym":"T_mozilla::_ipdltest::PTestBasicChild::Message","context":"mozilla::_ipdltest::PTestBasicChild::OnMessageReceived","contextsym":"_ZN7mozilla9_ipdltest15PTestBasicChild17OnMessageReceivedERKN3IPC7MessageERNS_9UniquePtrIS3_NS_13DefaultDeleteIS3_EEEE"}
{"loc":"00080:12-21","target":1,"kind":"use","pretty":"mozilla::_ipdltest::PTestBasicChild::UniquePtr","sym":"T_mozilla::_ipdltest::PTestBasicChild::UniquePtr","context":"mozilla::_ipdltest::PTestBasicChild::OnMessageReceived","contextsym":"_ZN7mozilla9_ipdltest15PTestBasicChild17OnMessageReceivedERKN3IPC7MessageERNS_9UniquePtrIS3_NS_13DefaultDeleteIS3_EEEE"}
{"loc":"00080:22-29","target":1,"kind":"use","pretty":"mozilla::_ipdltest::PTestBasicChild::Message","sym":"T_mozilla::_ipdltest::PTestBasicChild::Message","context":"mozilla::_ipdltest::PTestBasicChild::OnMessageReceived","contextsym":"_ZN7mozilla9_ipdltest15PTestBasicChild17OnMessageReceivedERKN3IPC7MessageERNS_9UniquePtrIS3_NS_13DefaultDeleteIS3_EEEE"}
{"loc":"00082:4-10","target":1,"kind":"use","pretty":"mozilla::ipc::HasResultCodes::Result","sym":"T_mozilla::ipc::HasResultCodes::Result","context":"mozilla::_ipdltest::PTestBasicChild::OnCallReceived","contextsym":"_ZN7mozilla9_ipdltest15PTestBasicChild14OnCallReceivedERKN3IPC7MessageERNS_9UniquePtrIS3_NS_13DefaultDeleteIS3_EEEE"}
{"loc":"00083:4-18","target":1,"kind":"decl","pretty":"mozilla::_ipdltest::PTestBasicChild::OnCallReceived","sym":"_ZN7mozilla9_ipdltest15PTestBasicChild14OnCallReceivedERKN3IPC7MessageERNS_9UniquePtrIS3_NS_13DefaultDeleteIS3_EEEE","context":"mozilla::_ipdltest::PTestBasicChild","contextsym":"T_mozilla::_ipdltest::PTestBasicChild","peekRange":"82-85"}
{"loc":"00084:18-25","target":1,"kind":"use","pretty":"mozilla::_ipdltest::PTestBasicChild::Message","sym":"T_mozilla::_ipdltest::PTestBasicChild::Message","context":"mozilla::_ipdltest::PTestBasicChild::OnCallReceived","contextsym":"_ZN7mozilla9_ipdltest15PTestBasicChild14OnCallReceivedERKN3IPC7MessageERNS_9UniquePtrIS3_NS_13DefaultDeleteIS3_EEEE"}
{"loc":"00085:12-21","target":1,"kind":"use","pretty":"mozilla::_ipdltest::PTestBasicChild::UniquePtr","sym":"T_mozilla::_ipdltest::PTestBasicChild::UniquePtr","context":"mozilla::_ipdltest::PTestBasicChild::OnCallReceived","contextsym":"_ZN7mozilla9_ipdltest15PTestBasicChild14OnCallReceivedERKN3IPC7MessageERNS_9UniquePtrIS3_NS_13DefaultDeleteIS3_EEEE"}
{"loc":"00085:22-29","target":1,"kind":"use","pretty":"mozilla::_ipdltest::PTestBasicChild::Message","sym":"T_mozilla::_ipdltest::PTestBasicChild::Message","context":"mozilla::_ipdltest::PTestBasicChild::OnCallReceived","contextsym":"_ZN7mozilla9_ipdltest15PTestBasicChild14OnCallReceivedERKN3IPC7MessageERNS_9UniquePtrIS3_NS_13DefaultDeleteIS3_EEEE"}
{"loc":"00088:4-18","target":1,"kind":"decl","pretty":"mozilla::_ipdltest::PTestBasicChild::OnChannelClose","sym":"_ZN7mozilla9_ipdltest15PTestBasicChild14OnChannelCloseEv","context":"mozilla::_ipdltest::PTestBasicChild","contextsym":"T_mozilla::_ipdltest::PTestBasicChild","peekRange":"87-88"}
{"loc":"00091:4-18","target":1,"kind":"decl","pretty":"mozilla::_ipdltest::PTestBasicChild::OnChannelError","sym":"_ZN7mozilla9_ipdltest15PTestBasicChild14OnChannelErrorEv","context":"mozilla::_ipdltest::PTestBasicChild","contextsym":"T_mozilla::_ipdltest::PTestBasicChild","peekRange":"90-91"}
{"loc":"00095:4-16","target":1,"kind":"decl","pretty":"mozilla::_ipdltest::PTestBasicChild::ClearSubtree","sym":"_ZN7mozilla9_ipdltest15PTestBasicChild12ClearSubtreeEv","context":"mozilla::_ipdltest::PTestBasicChild","contextsym":"T_mozilla::_ipdltest::PTestBasicChild","peekRange":"94-95"}
{"loc":"00102:10-13","target":1,"kind":"def","pretty":"IPC","sym":"NS_IPC"}
{"loc":"00104:40-55","target":1,"kind":"use","pretty":"mozilla::_ipdltest::PTestBasicChild","sym":"T_mozilla::_ipdltest::PTestBasicChild"}
{"loc":"00104:7-18","target":1,"kind":"def","pretty":"IPC::ParamTraits","sym":"T_IPC::ParamTraits","peekRange":"104-104"}
{"loc":"00104:7-18","target":1,"kind":"use","pretty":"IPC::ParamTraits","sym":"T_IPC::ParamTraits"}
{"loc":"00106:32-47","target":1,"kind":"use","pretty":"mozilla::_ipdltest::PTestBasicChild","sym":"T_mozilla::_ipdltest::PTestBasicChild"}
{"loc":"00106:49-58","target":1,"kind":"def","pretty":"IPC::ParamTraits<mozilla::_ipdltest::PTestBasicChild *>::paramType","sym":"T_IPC::ParamTraits<mozilla::_ipdltest::PTestBasicChild_*>::paramType"}
{"loc":"00108:4-9","target":1,"kind":"decl","pretty":"IPC::ParamTraits<mozilla::_ipdltest::PTestBasicChild *>::Write","sym":"_ZN3IPC11ParamTraitsIPN7mozilla9_ipdltest15PTestBasicChildEE5WriteEPNS_13MessageWriterERKS4_","peekRange":"107-110"}
{"loc":"00109:17-30","target":1,"kind":"use","pretty":"IPC::MessageWriter","sym":"T_IPC::MessageWriter","context":"IPC::ParamTraits<mozilla::_ipdltest::PTestBasicChild *>::Write","contextsym":"_ZN3IPC11ParamTraitsIPN7mozilla9_ipdltest15PTestBasicChildEE5WriteEPNS_13MessageWriterERKS4_"}
{"loc":"00110:18-27","target":1,"kind":"use","pretty":"IPC::ParamTraits<mozilla::_ipdltest::PTestBasicChild *>::paramType","sym":"T_IPC::ParamTraits<mozilla::_ipdltest::PTestBasicChild_*>::paramType","context":"IPC::ParamTraits<mozilla::_ipdltest::PTestBasicChild *>::Write","contextsym":"_ZN3IPC11ParamTraitsIPN7mozilla9_ipdltest15PTestBasicChildEE5WriteEPNS_13MessageWriterERKS4_"}
{"loc":"00112:4-8","target":1,"kind":"decl","pretty":"IPC::ParamTraits<mozilla::_ipdltest::PTestBasicChild *>::Read","sym":"_ZN3IPC11ParamTraitsIPN7mozilla9_ipdltest15PTestBasicChildEE4ReadEPNS_13MessageReaderEPS4_","peekRange":"111-114"}
{"loc":"00113:17-30","target":1,"kind":"use","pretty":"IPC::MessageReader","sym":"T_IPC::MessageReader","context":"IPC::ParamTraits<mozilla::_ipdltest::PTestBasicChild *>::Read","contextsym":"_ZN3IPC11ParamTraitsIPN7mozilla9_ipdltest15PTestBasicChildEE4ReadEPNS_13MessageReaderEPS4_"}
{"loc":"00114:12-21","target":1,"kind":"use","pretty":"IPC::ParamTraits<mozilla::_ipdltest::PTestBasicChild *>::paramType","sym":"T_IPC::ParamTraits<mozilla::_ipdltest::PTestBasicChild_*>::paramType","context":"IPC::ParamTraits<mozilla::_ipdltest::PTestBasicChild *>::Read","contextsym":"_ZN3IPC11ParamTraitsIPN7mozilla9_ipdltest15PTestBasicChildEE4ReadEPNS_13MessageReaderEPS4_"}
{"loc":"00001:0","target":1,"kind":"def","pretty":"__GENERATED__/ipc/ipdl/_ipdlheaders/mozilla/_ipdltest/PTestBasicChild.h","sym":"FILE_linux@__GENERATED__/ipc/ipdl/_ipdlheaders/mozilla/_ipdltest/PTestBasicChild@2Eh"}
{"loc":"00007:8-25","target":1,"kind":"def","pretty":"PTestBasicChild_h","sym":"M_87e11a7ffa0a9075"}
{"loc":"00009:9-41","target":1,"kind":"use","pretty":"__GENERATED__/ipc/ipdl/_ipdlheaders/mozilla/_ipdltest/PTestBasic.h","sym":"FILE_linux@__GENERATED__/ipc/ipdl/_ipdlheaders/mozilla/_ipdltest/PTestBasic@2Eh"}
{"loc":"00011:9-18","target":1,"kind":"use","pretty":"__GENERATED__/dist/system_wrappers/prenv.h","sym":"FILE_linux@__GENERATED__/dist/system_wrappers/prenv@2Eh"}
{"loc":"00001:0","target":1,"kind":"def","pretty":"__GENERATED__/ipc/ipdl/_ipdlheaders/mozilla/_ipdltest/PTestBasicChild.h","sym":"FILE_macosx@__GENERATED__/ipc/ipdl/_ipdlheaders/mozilla/_ipdltest/PTestBasicChild@2Eh"}
{"loc":"00007:8-25","target":1,"kind":"def","pretty":"PTestBasicChild_h","sym":"M_3d9c339c9c1536b8"}
{"loc":"00009:9-41","target":1,"kind":"use","pretty":"__GENERATED__/ipc/ipdl/_ipdlheaders/mozilla/_ipdltest/PTestBasic.h","sym":"FILE_macosx@__GENERATED__/ipc/ipdl/_ipdlheaders/mozilla/_ipdltest/PTestBasic@2Eh"}
{"loc":"00011:9-18","target":1,"kind":"use","pretty":"nsprpub/pr/include/prenv.h","sym":"FILE_nsprpub/pr/include/prenv@2Eh"}
{"loc":"00001:0","target":1,"kind":"def","pretty":"__GENERATED__/ipc/ipdl/_ipdlheaders/mozilla/_ipdltest/PTestBasicChild.h","sym":"FILE_windows@__GENERATED__/ipc/ipdl/_ipdlheaders/mozilla/_ipdltest/PTestBasicChild@2Eh"}
{"loc":"00007:8-25","target":1,"kind":"def","pretty":"PTestBasicChild_h","sym":"M_3194ee911c8fe711"}
{"loc":"00009:9-41","target":1,"kind":"use","pretty":"__GENERATED__/ipc/ipdl/_ipdlheaders/mozilla/_ipdltest/PTestBasic.h","sym":"FILE_windows@__GENERATED__/ipc/ipdl/_ipdlheaders/mozilla/_ipdltest/PTestBasic@2Eh"}
{"loc":"00001:0","source":1,"syntax":"def,file","pretty":"file __GENERATED__/ipc/ipdl/_ipdlheaders/mozilla/_ipdltest/PTestBasicChild.h","sym":"FILE_windows@__GENERATED__/ipc/ipdl/_ipdlheaders/mozilla/_ipdltest/PTestBasicChild@2Eh,FILE_macosx@__GENERATED__/ipc/ipdl/_ipdlheaders/mozilla/_ipdltest/PTestBasicChild@2Eh,FILE_linux@__GENERATED__/ipc/ipdl/_ipdlheaders/mozilla/_ipdltest/PTestBasicChild@2Eh,FILE_android-armv7@__GENERATED__/ipc/ipdl/_ipdlheaders/mozilla/_ipdltest/PTestBasicChild@2Eh"}
{"loc":"00007:8-25","source":1,"syntax":"def,macro","pretty":"macro PTestBasicChild_h","sym":"M_3194ee911c8fe711,M_3d9c339c9c1536b8,M_87e11a7ffa0a9075,M_3063ff3329fd125d"}
{"loc":"00009:9-41","source":1,"syntax":"file,use","pretty":"file __GENERATED__/ipc/ipdl/_ipdlheaders/mozilla/_ipdltest/PTestBasic.h","sym":"FILE_windows@__GENERATED__/ipc/ipdl/_ipdlheaders/mozilla/_ipdltest/PTestBasic@2Eh,FILE_macosx@__GENERATED__/ipc/ipdl/_ipdlheaders/mozilla/_ipdltest/PTestBasic@2Eh,FILE_linux@__GENERATED__/ipc/ipdl/_ipdlheaders/mozilla/_ipdltest/PTestBasic@2Eh,FILE_android-armv7@__GENERATED__/ipc/ipdl/_ipdlheaders/mozilla/_ipdltest/PTestBasic@2Eh"}
{"loc":"00010:7-12","source":1,"syntax":"macro,use","pretty":"macro DEBUG","sym":"M_DEBUG"}
{"loc":"00011:9-18","source":1,"syntax":"file,use","pretty":"file __GENERATED__/dist/system_wrappers/prenv.h","sym":"FILE_linux@__GENERATED__/dist/system_wrappers/prenv@2Eh,FILE_android-armv7@__GENERATED__/dist/system_wrappers/prenv@2Eh"}
{"loc":"00011:9-18","source":1,"syntax":"file,use","pretty":"file nsprpub/pr/include/prenv.h","sym":"FILE_nsprpub/pr/include/prenv@2Eh"}
{"loc":"00014:9-29","source":1,"syntax":"file,use","pretty":"file mfbt/Tainting.h","sym":"FILE_mfbt/Tainting@2Eh"}
{"loc":"00015:9-39","source":1,"syntax":"file,use","pretty":"file ipc/glue/MessageChannel.h","sym":"FILE_ipc/glue/MessageChannel@2Eh"}
{"loc":"00016:9-38","source":1,"syntax":"file,use","pretty":"file ipc/glue/ProtocolUtils.h","sym":"FILE_ipc/glue/ProtocolUtils@2Eh"}
{"loc":"00017:10-17","source":1,"syntax":"def,namespace","pretty":"namespace mozilla","sym":"NS_mozilla","nestingRange":"17:19-101:0"}
{"loc":"00018:10-19","source":1,"syntax":"def,namespace","pretty":"namespace mozilla::_ipdltest","sym":"NS_mozilla::_ipdltest","nestingRange":"18:21-100:0"}
{"loc":"00021:6-21","source":1,"syntax":"def,type","pretty":"type mozilla::_ipdltest::PTestBasicChild","sym":"T_mozilla::_ipdltest::PTestBasicChild","nestingRange":"23:0-97:0"}
{"loc":"00022:25-42","source":1,"syntax":"type,use","pretty":"type mozilla::ipc::IToplevelProtocol","sym":"T_mozilla::ipc::IToplevelProtocol","type":"class mozilla::ipc::IToplevelProtocol","typesym":"T_mozilla::ipc::IToplevelProtocol"}
{"loc":"00026:26-37","source":1,"syntax":"type,use","pretty":"type mozilla::ipc::ActorHandle","sym":"T_mozilla::ipc::ActorHandle","type":"struct mozilla::ipc::ActorHandle","typesym":"T_mozilla::ipc::ActorHandle"}
{"loc":"00026:38-49","source":1,"syntax":"def,type","pretty":"type mozilla::_ipdltest::PTestBasicChild::ActorHandle","sym":"T_mozilla::_ipdltest::PTestBasicChild::ActorHandle"}
{"loc":"00027:26-33","source":1,"syntax":"type,use","pretty":"type mozilla::ipc::ByteBuf","sym":"T_mozilla::ipc::ByteBuf","type":"class mozilla::ipc::ByteBuf","typesym":"T_mozilla::ipc::ByteBuf"}
{"loc":"00027:34-41","source":1,"syntax":"def,type","pretty":"type mozilla::_ipdltest::PTestBasicChild::ByteBuf","sym":"T_mozilla::_ipdltest::PTestBasicChild::ByteBuf"}
{"loc":"00028:34-42","source":1,"syntax":"def,type","pretty":"type mozilla::_ipdltest::PTestBasicChild::Endpoint","sym":"T_mozilla::_ipdltest::PTestBasicChild::Endpoint"}
{"loc":"00028:59-67","source":1,"syntax":"type,use","pretty":"type mozilla::ipc::Endpoint","sym":"T_mozilla::ipc::Endpoint"}
{"loc":"00029:26-40","source":1,"syntax":"type,use","pretty":"type mozilla::ipc::FileDescriptor","sym":"T_mozilla::ipc::FileDescriptor","type":"class mozilla::ipc::FileDescriptor","typesym":"T_mozilla::ipc::FileDescriptor"}
{"loc":"00029:41-55","source":1,"syntax":"def,type","pretty":"type mozilla::_ipdltest::PTestBasicChild::FileDescriptor","sym":"T_mozilla::_ipdltest::PTestBasicChild::FileDescriptor"}
{"loc":"00030:34-49","source":1,"syntax":"def,type","pretty":"type mozilla::_ipdltest::PTestBasicChild::ManagedEndpoint","sym":"T_mozilla::_ipdltest::PTestBasicChild::ManagedEndpoint"}
{"loc":"00030:66-81","source":1,"syntax":"type,use","pretty":"type mozilla::ipc::ManagedEndpoint","sym":"T_mozilla::ipc::ManagedEndpoint"}
{"loc":"00031:18-27","source":1,"syntax":"type,use","pretty":"type base::ProcessId","sym":"T_base::ProcessId","type":"base::ProcessId"}
{"loc":"00031:28-37","source":1,"syntax":"def,type","pretty":"type mozilla::_ipdltest::PTestBasicChild::ProcessId","sym":"T_mozilla::_ipdltest::PTestBasicChild::ProcessId"}
{"loc":"00032:26-36","source":1,"syntax":"type,use","pretty":"type mozilla::ipc::ProtocolId","sym":"T_mozilla::ipc::ProtocolId","type":"mozilla::ipc::ProtocolId","typesym":"T_IPCMessageStart"}
{"loc":"00032:37-47","source":1,"syntax":"def,type","pretty":"type mozilla::_ipdltest::PTestBasicChild::ProtocolId","sym":"T_mozilla::_ipdltest::PTestBasicChild::ProtocolId"}
{"loc":"00033:26-46","source":1,"syntax":"type,use","pretty":"type mozilla::ipc::ResponseRejectReason","sym":"T_mozilla::ipc::ResponseRejectReason","type":"enum mozilla::ipc::ResponseRejectReason","typesym":"T_mozilla::ipc::ResponseRejectReason"}
{"loc":"00033:47-67","source":1,"syntax":"def,type","pretty":"type mozilla::_ipdltest::PTestBasicChild::ResponseRejectReason","sym":"T_mozilla::_ipdltest::PTestBasicChild::ResponseRejectReason"}
{"loc":"00034:26-31","source":1,"syntax":"type,use","pretty":"type mozilla::ipc::Shmem","sym":"T_mozilla::ipc::Shmem","type":"class mozilla::ipc::Shmem","typesym":"T_mozilla::ipc::Shmem"}
{"loc":"00034:32-37","source":1,"syntax":"def,type","pretty":"type mozilla::_ipdltest::PTestBasicChild::Shmem","sym":"T_mozilla::_ipdltest::PTestBasicChild::Shmem"}
{"loc":"00035:28-37","source":1,"syntax":"def,type","pretty":"type mozilla::_ipdltest::PTestBasicChild::UniquePtr","sym":"T_mozilla::_ipdltest::PTestBasicChild::UniquePtr"}
{"loc":"00035:49-58","source":1,"syntax":"type,use","pretty":"type mozilla::UniquePtr","sym":"T_mozilla::UniquePtr"}
{"loc":"00039:4-19","source":1,"syntax":"decl,function","pretty":"function mozilla::_ipdltest::PTestBasicChild::ProcessingError","sym":"_ZN7mozilla9_ipdltest15PTestBasicChild15ProcessingErrorENS_3ipc14HasResultCodes6ResultEPKc","type":"void (enum mozilla::ipc::HasResultCodes::Result, const char *)"}
{"loc":"00040:12-18","source":1,"syntax":"type,use","pretty":"type mozilla::ipc::HasResultCodes::Result","sym":"T_mozilla::ipc::HasResultCodes::Result","type":"enum mozilla::ipc::HasResultCodes::Result","typesym":"T_mozilla::ipc::HasResultCodes::Result"}
{"loc":"00040:19-24","source":1,"syntax":"","pretty":"variable aCode","sym":"V_226fbf754e01e514_1ebe00f013,V_2ee81ffe20b8dc7f_1ebe00f013,V_728790ceea6bd383_1ebe00f013,V_2118a24b8d1dd597_1ebe00f013","no_crossref":1,"type":"enum mozilla::ipc::HasResultCodes::Result","typesym":"T_mozilla::ipc::HasResultCodes::Result"}
{"loc":"00041:24-31","source":1,"syntax":"","pretty":"variable aReason","sym":"V_f928cf754e01e514_ea97235f0b0d,V_f5b12ffe20b8dc7f_ea97235f0b0d,V_4a40a0ceea6bd383_ea97235f0b0d,V_f8d0b24b8d1dd597_ea97235f0b0d","no_crossref":1,"type":"const char *"}
{"loc":"00043:4-34","source":1,"syntax":"decl,function","pretty":"function mozilla::_ipdltest::PTestBasicChild::ShouldContinueFromReplyTimeout","sym":"_ZN7mozilla9_ipdltest15PTestBasicChild30ShouldContinueFromReplyTimeoutEv","type":"_Bool (void)"}
{"loc":"00046:26-35","source":1,"syntax":"type,use","pretty":"type mozilla::ipc::IProtocol","sym":"T_mozilla::ipc::IProtocol","type":"class mozilla::ipc::IProtocol","typesym":"T_mozilla::ipc::IProtocol"}
{"loc":"00046:36-45","source":1,"syntax":"def,type","pretty":"type mozilla::_ipdltest::PTestBasicChild::IProtocol","sym":"T_mozilla::_ipdltest::PTestBasicChild::IProtocol"}
{"loc":"00047:17-24","source":1,"syntax":"type,use","pretty":"type IPC::Message","sym":"T_IPC::Message","type":"class IPC::Message","typesym":"T_IPC::Message"}
{"loc":"00047:25-32","source":1,"syntax":"def,type","pretty":"type mozilla::_ipdltest::PTestBasicChild::Message","sym":"T_mozilla::_ipdltest::PTestBasicChild::Message"}
{"loc":"00048:18-31","source":1,"syntax":"type,use","pretty":"type base::ProcessHandle","sym":"T_base::ProcessHandle","type":"base::ProcessHandle"}
{"loc":"00048:32-45","source":1,"syntax":"def,type","pretty":"type mozilla::_ipdltest::PTestBasicChild::ProcessHandle","sym":"T_mozilla::_ipdltest::PTestBasicChild::ProcessHandle"}
{"loc":"00049:26-40","source":1,"syntax":"type,use","pretty":"type mozilla::ipc::MessageChannel","sym":"T_mozilla::ipc::MessageChannel","type":"class mozilla::ipc::MessageChannel","typesym":"T_mozilla::ipc::MessageChannel"}
{"loc":"00049:41-55","source":1,"syntax":"def,type","pretty":"type mozilla::_ipdltest::PTestBasicChild::MessageChannel","sym":"T_mozilla::_ipdltest::PTestBasicChild::MessageChannel"}
{"loc":"00050:26-38","source":1,"syntax":"type,use","pretty":"type mozilla::ipc::SharedMemory","sym":"T_mozilla::ipc::SharedMemory","type":"class mozilla::ipc::SharedMemory","typesym":"T_mozilla::ipc::SharedMemory"}
{"loc":"00050:39-51","source":1,"syntax":"def,type","pretty":"type mozilla::_ipdltest::PTestBasicChild::SharedMemory","sym":"T_mozilla::_ipdltest::PTestBasicChild::SharedMemory"}
{"loc":"00053:4-16","source":1,"syntax":"macro,use","pretty":"macro MOZ_IMPLICIT","sym":"M_5f65895d5a5258f7"}
{"loc":"00053:17-32","source":1,"syntax":"decl,function","pretty":"function mozilla::_ipdltest::PTestBasicChild::PTestBasicChild","sym":"_ZN7mozilla9_ipdltest15PTestBasicChildC1Ev","type":"void (void)"}
{"loc":"00055:13-28","source":1,"syntax":"decl,destructor","pretty":"destructor mozilla::_ipdltest::PTestBasicChild::~PTestBasicChild","sym":"_ZN7mozilla9_ipdltest15PTestBasicChildD1Ev","type":"void (void) noexcept"}
{"loc":"00055:13-28","source":1,"syntax":"type,use","pretty":"type mozilla::_ipdltest::PTestBasicChild","sym":"T_mozilla::_ipdltest::PTestBasicChild","type":"class mozilla::_ipdltest::PTestBasicChild","typesym":"T_mozilla::_ipdltest::PTestBasicChild"}
{"loc":"00057:4-43","source":1,"syntax":"def,function","pretty":"function mozilla::_ipdltest::PTestBasicChild::AddRef","sym":"_ZN7mozilla9_ipdltest15PTestBasicChild6AddRefEv","type":"MozExternalRefCountType (void)"}
{"loc":"00057:4-43","source":1,"syntax":"def,function","pretty":"function mozilla::_ipdltest::PTestBasicChild::Release","sym":"_ZN7mozilla9_ipdltest15PTestBasicChild7ReleaseEv","type":"MozExternalRefCountType (void)"}
{"loc":"00057:4-43","source":1,"syntax":"macro,use","pretty":"macro NS_INLINE_DECL_PURE_VIRTUAL_REFCOUNTING","sym":"M_19459f64b289c7f3"}
{"loc":"00059:9-19","source":1,"syntax":"def,function","pretty":"function mozilla::_ipdltest::PTestBasicChild::ActorAlloc","sym":"_ZN7mozilla9_ipdltest15PTestBasicChild10ActorAllocEv","nestingRange":"59:28-59:40","type":"void (void)"}
{"loc":"00059:30-36","source":1,"syntax":"function,use","pretty":"function mozilla::_ipdltest::PTestBasicChild::AddRef","sym":"_ZN7mozilla9_ipdltest15PTestBasicChild6AddRefEv","type":"MozExternalRefCountType"}
{"loc":"00060:9-21","source":1,"syntax":"def,function","pretty":"function mozilla::_ipdltest::PTestBasicChild::ActorDealloc","sym":"_ZN7mozilla9_ipdltest15PTestBasicChild12ActorDeallocEv","nestingRange":"60:30-60:43","type":"void (void)"}
{"loc":"00060:32-39","source":1,"syntax":"function,use","pretty":"function mozilla::_ipdltest::PTestBasicChild::Release","sym":"_ZN7mozilla9_ipdltest15PTestBasicChild7ReleaseEv","type":"MozExternalRefCountType"}
{"loc":"00063:4-20","source":1,"syntax":"decl,function","pretty":"function mozilla::_ipdltest::PTestBasicChild::AllManagedActors","sym":"_ZNK7mozilla9_ipdltest15PTestBasicChild16AllManagedActorsER8nsTArrayI6RefPtrINS_3ipc19ActorLifecycleProxyEEE","type":"void (nsTArray<RefPtr<mozilla::ipc::ActorLifecycleProxy> > &) const"}
{"loc":"00063:21-29","source":1,"syntax":"type,use","pretty":"type nsTArray","sym":"T_nsTArray"}
{"loc":"00063:30-36","source":1,"syntax":"type,use","pretty":"type RefPtr","sym":"T_RefPtr"}
{"loc":"00063:51-70","source":1,"syntax":"type,use","pretty":"type mozilla::ipc::ActorLifecycleProxy","sym":"T_mozilla::ipc::ActorLifecycleProxy","type":"class mozilla::ipc::ActorLifecycleProxy","typesym":"T_mozilla::ipc::ActorLifecycleProxy"}
{"loc":"00063:74-79","source":1,"syntax":"","pretty":"variable arr__","sym":"V_80dc12854e01e514_8cdba1f013,V_8c56710f20b8dc7f_8cdba1f013,V_d0f4f2ceea6bd383_8cdba1f013,V_8f75054b8d1dd597_8cdba1f013","no_crossref":1,"type":"nsTArray<RefPtr<mozilla::ipc::ActorLifecycleProxy> > &"}
{"loc":"00066:4-17","source":1,"syntax":"decl,function","pretty":"function mozilla::_ipdltest::PTestBasicChild::RemoveManagee","sym":"_ZN7mozilla9_ipdltest15PTestBasicChild13RemoveManageeEiPNS_3ipc9IProtocolE","type":"void (int32_t, mozilla::_ipdltest::PTestBasicChild::IProtocol *)"}
{"loc":"00067:12-19","source":1,"syntax":"type,use","pretty":"type int32_t","sym":"T_int32_t","type":"int32_t"}
{"loc":"00067:20-31","source":1,"syntax":"","pretty":"variable aProtocolId","sym":"V_3edf32854e01e514_542cb52f47aba60c,V_3a69910f20b8dc7f_542cb52f47aba60c,V_8ef713ceea6bd383_542cb52f47aba60c,V_3d88254b8d1dd597_542cb52f47aba60c","no_crossref":1,"type":"int32_t"}
{"loc":"00068:12-21","source":1,"syntax":"type,use","pretty":"type mozilla::_ipdltest::PTestBasicChild::IProtocol","sym":"T_mozilla::_ipdltest::PTestBasicChild::IProtocol","type":"mozilla::_ipdltest::PTestBasicChild::IProtocol","typesym":"T_mozilla::ipc::IProtocol"}
{"loc":"00068:23-32","source":1,"syntax":"","pretty":"variable aListener","sym":"V_74a842854e01e514_c681f10e880c773,V_7032a10f20b8dc7f_c681f10e880c773,V_c4c023ceea6bd383_c681f10e880c773,V_7351354b8d1dd597_c681f10e880c773","no_crossref":1,"type":"mozilla::_ipdltest::PTestBasicChild::IProtocol *"}
{"loc":"00070:4-18","source":1,"syntax":"decl,function","pretty":"function mozilla::_ipdltest::PTestBasicChild::DeallocManagee","sym":"_ZN7mozilla9_ipdltest15PTestBasicChild14DeallocManageeEiPNS_3ipc9IProtocolE","type":"void (int32_t, mozilla::_ipdltest::PTestBasicChild::IProtocol *)"}
{"loc":"00071:12-19","source":1,"syntax":"type,use","pretty":"type int32_t","sym":"T_int32_t","type":"int32_t"}
{"loc":"00071:20-31","source":1,"syntax":"","pretty":"variable aProtocolId","sym":"V_e1cc23854e01e514_542cb52f47aba60c,V_ed46820f20b8dc7f_542cb52f47aba60c,V_32e404ceea6bd383_542cb52f47aba60c,V_e075164b8d1dd597_542cb52f47aba60c","no_crossref":1,"type":"int32_t"}
{"loc":"00072:12-21","source":1,"syntax":"type,use","pretty":"type mozilla::_ipdltest::PTestBasicChild::IProtocol","sym":"T_mozilla::_ipdltest::PTestBasicChild::IProtocol","type":"mozilla::_ipdltest::PTestBasicChild::IProtocol","typesym":"T_mozilla::ipc::IProtocol"}
{"loc":"00072:23-32","source":1,"syntax":"","pretty":"variable aListener","sym":"V_288533854e01e514_c681f10e880c773,V_241f820f20b8dc7f_c681f10e880c773,V_78ad04ceea6bd383_c681f10e880c773,V_273e164b8d1dd597_c681f10e880c773","no_crossref":1,"type":"mozilla::_ipdltest::PTestBasicChild::IProtocol *"}
{"loc":"00074:4-10","source":1,"syntax":"type,use","pretty":"type mozilla::ipc::HasResultCodes::Result","sym":"T_mozilla::ipc::HasResultCodes::Result","type":"enum mozilla::ipc::HasResultCodes::Result","typesym":"T_mozilla::ipc::HasResultCodes::Result"}
{"loc":"00075:4-21","source":1,"syntax":"decl,function","pretty":"function mozilla::_ipdltest::PTestBasicChild::OnMessageReceived","sym":"_ZN7mozilla9_ipdltest15PTestBasicChild17OnMessageReceivedERKN3IPC7MessageE","type":"enum mozilla::ipc::HasResultCodes::Result (const mozilla::_ipdltest::PTestBasicChild::Message &)"}
{"loc":"00075:28-35","source":1,"syntax":"type,use","pretty":"type mozilla::_ipdltest::PTestBasicChild::Message","sym":"T_mozilla::_ipdltest::PTestBasicChild::Message","type":"mozilla::_ipdltest::PTestBasicChild::Message","typesym":"T_IPC::Message"}
{"loc":"00075:37-42","source":1,"syntax":"","pretty":"variable msg__","sym":"V_acdf43854e01e514_a6144ff013,V_a869a20f20b8dc7f_a6144ff013,V_fcf724ceea6bd383_a6144ff013,V_ab88364b8d1dd597_a6144ff013","no_crossref":1,"type":"const mozilla::_ipdltest::PTestBasicChild::Message &"}
{"loc":"00077:4-10","source":1,"syntax":"type,use","pretty":"type mozilla::ipc::HasResultCodes::Result","sym":"T_mozilla::ipc::HasResultCodes::Result","type":"enum mozilla::ipc::HasResultCodes::Result","typesym":"T_mozilla::ipc::HasResultCodes::Result"}
{"loc":"00078:4-21","source":1,"syntax":"decl,function","pretty":"function mozilla::_ipdltest::PTestBasicChild::OnMessageReceived","sym":"_ZN7mozilla9_ipdltest15PTestBasicChild17OnMessageReceivedERKN3IPC7MessageERNS_9UniquePtrIS3_NS_13DefaultDeleteIS3_EEEE","type":"enum mozilla::ipc::HasResultCodes::Result (const mozilla::_ipdltest::PTestBasicChild::Message &, UniquePtr<mozilla::_ipdltest::PTestBasicChild::Message> &)"}
{"loc":"00079:18-25","source":1,"syntax":"type,use","pretty":"type mozilla::_ipdltest::PTestBasicChild::Message","sym":"T_mozilla::_ipdltest::PTestBasicChild::Message","type":"mozilla::_ipdltest::PTestBasicChild::Message","typesym":"T_IPC::Message"}
{"loc":"00079:27-32","source":1,"syntax":"","pretty":"variable msg__","sym":"V_d2f273854e01e514_a6144ff013,V_de7cc20f20b8dc7f_a6144ff013,V_231b44ceea6bd383_a6144ff013,V_d1ab564b8d1dd597_a6144ff013","no_crossref":1,"type":"const mozilla::_ipdltest::PTestBasicChild::Message &"}
{"loc":"00080:12-21","source":1,"syntax":"type,use","pretty":"type mozilla::_ipdltest::PTestBasicChild::UniquePtr","sym":"T_mozilla::_ipdltest::PTestBasicChild::UniquePtr"}
{"loc":"00080:22-29","source":1,"syntax":"type,use","pretty":"type mozilla::_ipdltest::PTestBasicChild::Message","sym":"T_mozilla::_ipdltest::PTestBasicChild::Message","type":"mozilla::_ipdltest::PTestBasicChild::Message","typesym":"T_IPC::Message"}
{"loc":"00080:32-39","source":1,"syntax":"","pretty":"variable reply__","sym":"V_168544854e01e514_f83bfee36b0d,V_121f930f20b8dc7f_f83bfee36b0d,V_66ad15ceea6bd383_f83bfee36b0d,V_153e274b8d1dd597_f83bfee36b0d","no_crossref":1,"type":"UniquePtr<mozilla::_ipdltest::PTestBasicChild::Message> &"}
{"loc":"00082:4-10","source":1,"syntax":"type,use","pretty":"type mozilla::ipc::HasResultCodes::Result","sym":"T_mozilla::ipc::HasResultCodes::Result","type":"enum mozilla::ipc::HasResultCodes::Result","typesym":"T_mozilla::ipc::HasResultCodes::Result"}
{"loc":"00083:4-18","source":1,"syntax":"decl,function","pretty":"function mozilla::_ipdltest::PTestBasicChild::OnCallReceived","sym":"_ZN7mozilla9_ipdltest15PTestBasicChild14OnCallReceivedERKN3IPC7MessageERNS_9UniquePtrIS3_NS_13DefaultDeleteIS3_EEEE","type":"enum mozilla::ipc::HasResultCodes::Result (const mozilla::_ipdltest::PTestBasicChild::Message &, UniquePtr<mozilla::_ipdltest::PTestBasicChild::Message> &)"}
{"loc":"00084:18-25","source":1,"syntax":"type,use","pretty":"type mozilla::_ipdltest::PTestBasicChild::Message","sym":"T_mozilla::_ipdltest::PTestBasicChild::Message","type":"mozilla::_ipdltest::PTestBasicChild::Message","typesym":"T_IPC::Message"}
{"loc":"00084:27-32","source":1,"syntax":"","pretty":"variable msg__","sym":"V_9c9864854e01e514_a6144ff013,V_9822c30f20b8dc7f_a6144ff013,V_ecb045ceea6bd383_a6144ff013,V_9b41574b8d1dd597_a6144ff013","no_crossref":1,"type":"const mozilla::_ipdltest::PTestBasicChild::Message &"}
{"loc":"00085:12-21","source":1,"syntax":"type,use","pretty":"type mozilla::_ipdltest::PTestBasicChild::UniquePtr","sym":"T_mozilla::_ipdltest::PTestBasicChild::UniquePtr"}
{"loc":"00085:22-29","source":1,"syntax":"type,use","pretty":"type mozilla::_ipdltest::PTestBasicChild::Message","sym":"T_mozilla::_ipdltest::PTestBasicChild::Message","type":"mozilla::_ipdltest::PTestBasicChild::Message","typesym":"T_IPC::Message"}
{"loc":"00085:32-39","source":1,"syntax":"","pretty":"variable reply__","sym":"V_646174854e01e514_f83bfee36b0d,V_60fac30f20b8dc7f_f83bfee36b0d,V_b48945ceea6bd383_f83bfee36b0d,V_631a574b8d1dd597_f83bfee36b0d","no_crossref":1,"type":"UniquePtr<mozilla::_ipdltest::PTestBasicChild::Message> &"}
{"loc":"00088:4-18","source":1,"syntax":"decl,function","pretty":"function mozilla::_ipdltest::PTestBasicChild::OnChannelClose","sym":"_ZN7mozilla9_ipdltest15PTestBasicChild14OnChannelCloseEv","type":"void (void)"}
{"loc":"00091:4-18","source":1,"syntax":"decl,function","pretty":"function mozilla::_ipdltest::PTestBasicChild::OnChannelError","sym":"_ZN7mozilla9_ipdltest15PTestBasicChild14OnChannelErrorEv","type":"void (void)"}
{"loc":"00095:4-16","source":1,"syntax":"decl,function","pretty":"function mozilla::_ipdltest::PTestBasicChild::ClearSubtree","sym":"_ZN7mozilla9_ipdltest15PTestBasicChild12ClearSubtreeEv","type":"void (void)"}
{"loc":"00102:10-13","source":1,"syntax":"def,namespace","pretty":"namespace IPC","sym":"NS_IPC","nestingRange":"102:15-116:0"}
{"loc":"00104:7-18","source":1,"syntax":"def,type,use","pretty":"type IPC::ParamTraits","sym":"T_IPC::ParamTraits","nestingRange":"105:0-115:0"}
{"loc":"00104:40-55","source":1,"syntax":"type,use","pretty":"type mozilla::_ipdltest::PTestBasicChild","sym":"T_mozilla::_ipdltest::PTestBasicChild","type":"class mozilla::_ipdltest::PTestBasicChild","typesym":"T_mozilla::_ipdltest::PTestBasicChild"}
{"loc":"00106:32-47","source":1,"syntax":"type,use","pretty":"type mozilla::_ipdltest::PTestBasicChild","sym":"T_mozilla::_ipdltest::PTestBasicChild","type":"class mozilla::_ipdltest::PTestBasicChild","typesym":"T_mozilla::_ipdltest::PTestBasicChild"}
{"loc":"00106:49-58","source":1,"syntax":"def,type","pretty":"type IPC::ParamTraits<mozilla::_ipdltest::PTestBasicChild *>::paramType","sym":"T_IPC::ParamTraits<mozilla::_ipdltest::PTestBasicChild_*>::paramType"}
{"loc":"00108:4-9","source":1,"syntax":"decl,function","pretty":"function IPC::ParamTraits<mozilla::_ipdltest::PTestBasicChild *>::Write","sym":"_ZN3IPC11ParamTraitsIPN7mozilla9_ipdltest15PTestBasicChildEE5WriteEPNS_13MessageWriterERKS4_","type":"void (IPC::MessageWriter *, const IPC::ParamTraits<class mozilla::_ipdltest::PTestBasicChild *>::paramType &)"}
{"loc":"00109:17-30","source":1,"syntax":"type,use","pretty":"type IPC::MessageWriter","sym":"T_IPC::MessageWriter","type":"class IPC::MessageWriter","typesym":"T_IPC::MessageWriter"}
{"loc":"00109:32-39","source":1,"syntax":"","pretty":"variable aWriter","sym":"V_36cad0a54e01e514_369ebc101b0d,V_3254302f20b8dc7f_369ebc101b0d,V_86e2b1eeea6bd383_369ebc101b0d,V_3573c36b8d1dd597_369ebc101b0d","no_crossref":1,"type":"IPC::MessageWriter *"}
{"loc":"00110:18-27","source":1,"syntax":"type,use","pretty":"type IPC::ParamTraits<mozilla::_ipdltest::PTestBasicChild *>::paramType","sym":"T_IPC::ParamTraits<mozilla::_ipdltest::PTestBasicChild_*>::paramType","type":"IPC::ParamTraits<class mozilla::_ipdltest::PTestBasicChild *>::paramType"}
{"loc":"00110:29-33","source":1,"syntax":"","pretty":"variable aVar","sym":"V_165da1a54e01e514_f0bb39c71,V_12e6012f20b8dc7f_f0bb39c71,V_667582eeea6bd383_f0bb39c71,V_1506946b8d1dd597_f0bb39c71","no_crossref":1,"type":"const IPC::ParamTraits<class mozilla::_ipdltest::PTestBasicChild *>::paramType &"}
{"loc":"00112:4-8","source":1,"syntax":"decl,function","pretty":"function IPC::ParamTraits<mozilla::_ipdltest::PTestBasicChild *>::Read","sym":"_ZN3IPC11ParamTraitsIPN7mozilla9_ipdltest15PTestBasicChildEE4ReadEPNS_13MessageReaderEPS4_","type":"_Bool (IPC::MessageReader *, IPC::ParamTraits<class mozilla::_ipdltest::PTestBasicChild *>::paramType *)"}
{"loc":"00113:17-30","source":1,"syntax":"type,use","pretty":"type IPC::MessageReader","sym":"T_IPC::MessageReader","type":"class IPC::MessageReader","typesym":"T_IPC::MessageReader"}
{"loc":"00113:32-39","source":1,"syntax":"","pretty":"variable aReader","sym":"V_e9a7c1a54e01e514_9983235f0b0d,V_e531212f20b8dc7f_9983235f0b0d,V_3acf92eeea6bd383_9983235f0b0d,V_e850b46b8d1dd597_9983235f0b0d","no_crossref":1,"type":"IPC::MessageReader *"}
{"loc":"00114:12-21","source":1,"syntax":"type,use","pretty":"type IPC::ParamTraits<mozilla::_ipdltest::PTestBasicChild *>::paramType","sym":"T_IPC::ParamTraits<mozilla::_ipdltest::PTestBasicChild_*>::paramType","type":"IPC::ParamTraits<class mozilla::_ipdltest::PTestBasicChild *>::paramType"}
{"loc":"00114:23-27","source":1,"syntax":"","pretty":"variable aVar","sym":"V_fd60d1a54e01e514_f0bb39c71,V_f9f9212f20b8dc7f_f0bb39c71,V_4e88a2eeea6bd383_f0bb39c71,V_fc19b46b8d1dd597_f0bb39c71","no_crossref":1,"type":"IPC::ParamTraits<class mozilla::_ipdltest::PTestBasicChild *>::paramType *"}
{"loc":"00104:7-18","structured":1,"pretty":"IPC::ParamTraits","sym":"T_IPC::ParamTraits","kind":"struct","implKind":"","sizeBytes":1,"supers":[],"methods":[{"pretty":"IPC::ParamTraits<mozilla::_ipdltest::PTestBasicChild *>::Write","sym":"_ZN3IPC11ParamTraitsIPN7mozilla9_ipdltest15PTestBasicChildEE5WriteEPNS_13MessageWriterERKS4_","props":["static","user"]},{"pretty":"IPC::ParamTraits<mozilla::_ipdltest::PTestBasicChild *>::Read","sym":"_ZN3IPC11ParamTraitsIPN7mozilla9_ipdltest15PTestBasicChildEE4ReadEPNS_13MessageReaderEPS4_","props":["static","user"]}],"fields":[],"overrides":[],"props":[]}
{"loc":"00021:6-21","structured":1,"pretty":"mozilla::_ipdltest::PTestBasicChild","sym":"T_mozilla::_ipdltest::PTestBasicChild","kind":"class","implKind":"","sizeBytes":368,"supers":[{"pretty":"mozilla::ipc::IToplevelProtocol","sym":"T_mozilla::ipc::IToplevelProtocol","props":[]}],"methods":[{"pretty":"mozilla::_ipdltest::PTestBasicChild::ProcessingError","sym":"_ZN7mozilla9_ipdltest15PTestBasicChild15ProcessingErrorENS_3ipc14HasResultCodes6ResultEPKc","props":["instance","virtual","user"]},{"pretty":"mozilla::_ipdltest::PTestBasicChild::ShouldContinueFromReplyTimeout","sym":"_ZN7mozilla9_ipdltest15PTestBasicChild30ShouldContinueFromReplyTimeoutEv","props":["instance","virtual","user"]},{"pretty":"mozilla::_ipdltest::PTestBasicChild::PTestBasicChild","sym":"_ZN7mozilla9_ipdltest15PTestBasicChildC1Ev","props":["instance","user"]},{"pretty":"mozilla::_ipdltest::PTestBasicChild::~PTestBasicChild","sym":"_ZN7mozilla9_ipdltest15PTestBasicChildD1Ev","props":["instance","virtual","user"]},{"pretty":"mozilla::_ipdltest::PTestBasicChild::AddRef","sym":"_ZN7mozilla9_ipdltest15PTestBasicChild6AddRefEv","props":["instance","virtual","user"]},{"pretty":"mozilla::_ipdltest::PTestBasicChild::Release","sym":"_ZN7mozilla9_ipdltest15PTestBasicChild7ReleaseEv","props":["instance","virtual","user"]},{"pretty":"mozilla::_ipdltest::PTestBasicChild::ActorAlloc","sym":"_ZN7mozilla9_ipdltest15PTestBasicChild10ActorAllocEv","props":["instance","virtual","user"]},{"pretty":"mozilla::_ipdltest::PTestBasicChild::ActorDealloc","sym":"_ZN7mozilla9_ipdltest15PTestBasicChild12ActorDeallocEv","props":["instance","virtual","user"]},{"pretty":"mozilla::_ipdltest::PTestBasicChild::AllManagedActors","sym":"_ZNK7mozilla9_ipdltest15PTestBasicChild16AllManagedActorsER8nsTArrayI6RefPtrINS_3ipc19ActorLifecycleProxyEEE","props":["instance","virtual","user"]},{"pretty":"mozilla::_ipdltest::PTestBasicChild::RemoveManagee","sym":"_ZN7mozilla9_ipdltest15PTestBasicChild13RemoveManageeEiPNS_3ipc9IProtocolE","props":["instance","virtual","user"]},{"pretty":"mozilla::_ipdltest::PTestBasicChild::DeallocManagee","sym":"_ZN7mozilla9_ipdltest15PTestBasicChild14DeallocManageeEiPNS_3ipc9IProtocolE","props":["instance","virtual","user"]},{"pretty":"mozilla::_ipdltest::PTestBasicChild::OnMessageReceived","sym":"_ZN7mozilla9_ipdltest15PTestBasicChild17OnMessageReceivedERKN3IPC7MessageE","props":["instance","virtual","user"]},{"pretty":"mozilla::_ipdltest::PTestBasicChild::OnMessageReceived","sym":"_ZN7mozilla9_ipdltest15PTestBasicChild17OnMessageReceivedERKN3IPC7MessageERNS_9UniquePtrIS3_NS_13DefaultDeleteIS3_EEEE","props":["instance","virtual","user"]},{"pretty":"mozilla::_ipdltest::PTestBasicChild::OnCallReceived","sym":"_ZN7mozilla9_ipdltest15PTestBasicChild14OnCallReceivedERKN3IPC7MessageERNS_9UniquePtrIS3_NS_13DefaultDeleteIS3_EEEE","props":["instance","virtual","user"]},{"pretty":"mozilla::_ipdltest::PTestBasicChild::OnChannelClose","sym":"_ZN7mozilla9_ipdltest15PTestBasicChild14OnChannelCloseEv","props":["instance","virtual","user"]},{"pretty":"mozilla::_ipdltest::PTestBasicChild::OnChannelError","sym":"_ZN7mozilla9_ipdltest15PTestBasicChild14OnChannelErrorEv","props":["instance","virtual","user"]},{"pretty":"mozilla::_ipdltest::PTestBasicChild::ClearSubtree","sym":"_ZN7mozilla9_ipdltest15PTestBasicChild12ClearSubtreeEv","props":["instance","user"]},{"pretty":"mozilla::_ipdltest::PTestBasicChild::PTestBasicChild","sym":"_ZN7mozilla9_ipdltest15PTestBasicChildC1ERKS1_","props":["instance","defaulted","deleted"]},{"pretty":"mozilla::_ipdltest::PTestBasicChild::operator=","sym":"_ZN7mozilla9_ipdltest15PTestBasicChildaSERKS1_","props":["instance","defaulted","deleted"]}],"fields":[],"overrides":[],"props":[],"platforms":["win64"],"variants":[{"structured":1,"pretty":"mozilla::_ipdltest::PTestBasicChild","sym":"T_mozilla::_ipdltest::PTestBasicChild","kind":"class","implKind":"","sizeBytes":352,"supers":[{"pretty":"mozilla::ipc::IToplevelProtocol","sym":"T_mozilla::ipc::IToplevelProtocol","props":[]}],"methods":[{"pretty":"mozilla::_ipdltest::PTestBasicChild::ProcessingError","sym":"_ZN7mozilla9_ipdltest15PTestBasicChild15ProcessingErrorENS_3ipc14HasResultCodes6ResultEPKc","props":["instance","virtual","user"]},{"pretty":"mozilla::_ipdltest::PTestBasicChild::ShouldContinueFromReplyTimeout","sym":"_ZN7mozilla9_ipdltest15PTestBasicChild30ShouldContinueFromReplyTimeoutEv","props":["instance","virtual","user"]},{"pretty":"mozilla::_ipdltest::PTestBasicChild::PTestBasicChild","sym":"_ZN7mozilla9_ipdltest15PTestBasicChildC1Ev","props":["instance","user"]},{"pretty":"mozilla::_ipdltest::PTestBasicChild::~PTestBasicChild","sym":"_ZN7mozilla9_ipdltest15PTestBasicChildD1Ev","props":["instance","virtual","user"]},{"pretty":"mozilla::_ipdltest::PTestBasicChild::AddRef","sym":"_ZN7mozilla9_ipdltest15PTestBasicChild6AddRefEv","props":["instance","virtual","user"]},{"pretty":"mozilla::_ipdltest::PTestBasicChild::Release","sym":"_ZN7mozilla9_ipdltest15PTestBasicChild7ReleaseEv","props":["instance","virtual","user"]},{"pretty":"mozilla::_ipdltest::PTestBasicChild::ActorAlloc","sym":"_ZN7mozilla9_ipdltest15PTestBasicChild10ActorAllocEv","props":["instance","virtual","user"]},{"pretty":"mozilla::_ipdltest::PTestBasicChild::ActorDealloc","sym":"_ZN7mozilla9_ipdltest15PTestBasicChild12ActorDeallocEv","props":["instance","virtual","user"]},{"pretty":"mozilla::_ipdltest::PTestBasicChild::AllManagedActors","sym":"_ZNK7mozilla9_ipdltest15PTestBasicChild16AllManagedActorsER8nsTArrayI6RefPtrINS_3ipc19ActorLifecycleProxyEEE","props":["instance","virtual","user"]},{"pretty":"mozilla::_ipdltest::PTestBasicChild::RemoveManagee","sym":"_ZN7mozilla9_ipdltest15PTestBasicChild13RemoveManageeEiPNS_3ipc9IProtocolE","props":["instance","virtual","user"]},{"pretty":"mozilla::_ipdltest::PTestBasicChild::DeallocManagee","sym":"_ZN7mozilla9_ipdltest15PTestBasicChild14DeallocManageeEiPNS_3ipc9IProtocolE","props":["instance","virtual","user"]},{"pretty":"mozilla::_ipdltest::PTestBasicChild::OnMessageReceived","sym":"_ZN7mozilla9_ipdltest15PTestBasicChild17OnMessageReceivedERKN3IPC7MessageE","props":["instance","virtual","user"]},{"pretty":"mozilla::_ipdltest::PTestBasicChild::OnMessageReceived","sym":"_ZN7mozilla9_ipdltest15PTestBasicChild17OnMessageReceivedERKN3IPC7MessageERNS_9UniquePtrIS3_NS_13DefaultDeleteIS3_EEEE","props":["instance","virtual","user"]},{"pretty":"mozilla::_ipdltest::PTestBasicChild::OnCallReceived","sym":"_ZN7mozilla9_ipdltest15PTestBasicChild14OnCallReceivedERKN3IPC7MessageERNS_9UniquePtrIS3_NS_13DefaultDeleteIS3_EEEE","props":["instance","virtual","user"]},{"pretty":"mozilla::_ipdltest::PTestBasicChild::OnChannelClose","sym":"_ZN7mozilla9_ipdltest15PTestBasicChild14OnChannelCloseEv","props":["instance","virtual","user"]},{"pretty":"mozilla::_ipdltest::PTestBasicChild::OnChannelError","sym":"_ZN7mozilla9_ipdltest15PTestBasicChild14OnChannelErrorEv","props":["instance","virtual","user"]},{"pretty":"mozilla::_ipdltest::PTestBasicChild::ClearSubtree","sym":"_ZN7mozilla9_ipdltest15PTestBasicChild12ClearSubtreeEv","props":["instance","user"]},{"pretty":"mozilla::_ipdltest::PTestBasicChild::PTestBasicChild","sym":"_ZN7mozilla9_ipdltest15PTestBasicChildC1ERKS1_","props":["instance","defaulted","deleted"]},{"pretty":"mozilla::_ipdltest::PTestBasicChild::operator=","sym":"_ZN7mozilla9_ipdltest15PTestBasicChildaSERKS1_","props":["instance","defaulted","deleted"]}],"fields":[],"overrides":[],"props":[],"platforms":["macosx64"]},{"structured":1,"pretty":"mozilla::_ipdltest::PTestBasicChild","sym":"T_mozilla::_ipdltest::PTestBasicChild","kind":"class","implKind":"","sizeBytes":224,"supers":[{"pretty":"mozilla::ipc::IToplevelProtocol","sym":"T_mozilla::ipc::IToplevelProtocol","props":[]}],"methods":[{"pretty":"mozilla::_ipdltest::PTestBasicChild::ProcessingError","sym":"_ZN7mozilla9_ipdltest15PTestBasicChild15ProcessingErrorENS_3ipc14HasResultCodes6ResultEPKc","props":["instance","virtual","user"]},{"pretty":"mozilla::_ipdltest::PTestBasicChild::ShouldContinueFromReplyTimeout","sym":"_ZN7mozilla9_ipdltest15PTestBasicChild30ShouldContinueFromReplyTimeoutEv","props":["instance","virtual","user"]},{"pretty":"mozilla::_ipdltest::PTestBasicChild::PTestBasicChild","sym":"_ZN7mozilla9_ipdltest15PTestBasicChildC1Ev","props":["instance","user"]},{"pretty":"mozilla::_ipdltest::PTestBasicChild::~PTestBasicChild","sym":"_ZN7mozilla9_ipdltest15PTestBasicChildD1Ev","props":["instance","virtual","user"]},{"pretty":"mozilla::_ipdltest::PTestBasicChild::AddRef","sym":"_ZN7mozilla9_ipdltest15PTestBasicChild6AddRefEv","props":["instance","virtual","user"]},{"pretty":"mozilla::_ipdltest::PTestBasicChild::Release","sym":"_ZN7mozilla9_ipdltest15PTestBasicChild7ReleaseEv","props":["instance","virtual","user"]},{"pretty":"mozilla::_ipdltest::PTestBasicChild::ActorAlloc","sym":"_ZN7mozilla9_ipdltest15PTestBasicChild10ActorAllocEv","props":["instance","virtual","user"]},{"pretty":"mozilla::_ipdltest::PTestBasicChild::ActorDealloc","sym":"_ZN7mozilla9_ipdltest15PTestBasicChild12ActorDeallocEv","props":["instance","virtual","user"]},{"pretty":"mozilla::_ipdltest::PTestBasicChild::AllManagedActors","sym":"_ZNK7mozilla9_ipdltest15PTestBasicChild16AllManagedActorsER8nsTArrayI6RefPtrINS_3ipc19ActorLifecycleProxyEEE","props":["instance","virtual","user"]},{"pretty":"mozilla::_ipdltest::PTestBasicChild::RemoveManagee","sym":"_ZN7mozilla9_ipdltest15PTestBasicChild13RemoveManageeEiPNS_3ipc9IProtocolE","props":["instance","virtual","user"]},{"pretty":"mozilla::_ipdltest::PTestBasicChild::DeallocManagee","sym":"_ZN7mozilla9_ipdltest15PTestBasicChild14DeallocManageeEiPNS_3ipc9IProtocolE","props":["instance","virtual","user"]},{"pretty":"mozilla::_ipdltest::PTestBasicChild::OnMessageReceived","sym":"_ZN7mozilla9_ipdltest15PTestBasicChild17OnMessageReceivedERKN3IPC7MessageE","props":["instance","virtual","user"]},{"pretty":"mozilla::_ipdltest::PTestBasicChild::OnMessageReceived","sym":"_ZN7mozilla9_ipdltest15PTestBasicChild17OnMessageReceivedERKN3IPC7MessageERNS_9UniquePtrIS3_NS_13DefaultDeleteIS3_EEEE","props":["instance","virtual","user"]},{"pretty":"mozilla::_ipdltest::PTestBasicChild::OnCallReceived","sym":"_ZN7mozilla9_ipdltest15PTestBasicChild14OnCallReceivedERKN3IPC7MessageERNS_9UniquePtrIS3_NS_13DefaultDeleteIS3_EEEE","props":["instance","virtual","user"]},{"pretty":"mozilla::_ipdltest::PTestBasicChild::OnChannelClose","sym":"_ZN7mozilla9_ipdltest15PTestBasicChild14OnChannelCloseEv","props":["instance","virtual","user"]},{"pretty":"mozilla::_ipdltest::PTestBasicChild::OnChannelError","sym":"_ZN7mozilla9_ipdltest15PTestBasicChild14OnChannelErrorEv","props":["instance","virtual","user"]},{"pretty":"mozilla::_ipdltest::PTestBasicChild::ClearSubtree","sym":"_ZN7mozilla9_ipdltest15PTestBasicChild12ClearSubtreeEv","props":["instance","user"]},{"pretty":"mozilla::_ipdltest::PTestBasicChild::PTestBasicChild","sym":"_ZN7mozilla9_ipdltest15PTestBasicChildC1ERKS1_","props":["instance","defaulted","deleted"]},{"pretty":"mozilla::_ipdltest::PTestBasicChild::operator=","sym":"_ZN7mozilla9_ipdltest15PTestBasicChildaSERKS1_","props":["instance","defaulted","deleted"]}],"fields":[],"overrides":[],"props":[],"platforms":["android-armv7"]},{"structured":1,"pretty":"mozilla::_ipdltest::PTestBasicChild","sym":"T_mozilla::_ipdltest::PTestBasicChild","kind":"class","implKind":"","sizeBytes":376,"supers":[{"pretty":"mozilla::ipc::IToplevelProtocol","sym":"T_mozilla::ipc::IToplevelProtocol","props":[]}],"methods":[{"pretty":"mozilla::_ipdltest::PTestBasicChild::ProcessingError","sym":"_ZN7mozilla9_ipdltest15PTestBasicChild15ProcessingErrorENS_3ipc14HasResultCodes6ResultEPKc","props":["instance","virtual","user"]},{"pretty":"mozilla::_ipdltest::PTestBasicChild::ShouldContinueFromReplyTimeout","sym":"_ZN7mozilla9_ipdltest15PTestBasicChild30ShouldContinueFromReplyTimeoutEv","props":["instance","virtual","user"]},{"pretty":"mozilla::_ipdltest::PTestBasicChild::PTestBasicChild","sym":"_ZN7mozilla9_ipdltest15PTestBasicChildC1Ev","props":["instance","user"]},{"pretty":"mozilla::_ipdltest::PTestBasicChild::~PTestBasicChild","sym":"_ZN7mozilla9_ipdltest15PTestBasicChildD1Ev","props":["instance","virtual","user"]},{"pretty":"mozilla::_ipdltest::PTestBasicChild::AddRef","sym":"_ZN7mozilla9_ipdltest15PTestBasicChild6AddRefEv","props":["instance","virtual","user"]},{"pretty":"mozilla::_ipdltest::PTestBasicChild::Release","sym":"_ZN7mozilla9_ipdltest15PTestBasicChild7ReleaseEv","props":["instance","virtual","user"]},{"pretty":"mozilla::_ipdltest::PTestBasicChild::ActorAlloc","sym":"_ZN7mozilla9_ipdltest15PTestBasicChild10ActorAllocEv","props":["instance","virtual","user"]},{"pretty":"mozilla::_ipdltest::PTestBasicChild::ActorDealloc","sym":"_ZN7mozilla9_ipdltest15PTestBasicChild12ActorDeallocEv","props":["instance","virtual","user"]},{"pretty":"mozilla::_ipdltest::PTestBasicChild::AllManagedActors","sym":"_ZNK7mozilla9_ipdltest15PTestBasicChild16AllManagedActorsER8nsTArrayI6RefPtrINS_3ipc19ActorLifecycleProxyEEE","props":["instance","virtual","user"]},{"pretty":"mozilla::_ipdltest::PTestBasicChild::RemoveManagee","sym":"_ZN7mozilla9_ipdltest15PTestBasicChild13RemoveManageeEiPNS_3ipc9IProtocolE","props":["instance","virtual","user"]},{"pretty":"mozilla::_ipdltest::PTestBasicChild::DeallocManagee","sym":"_ZN7mozilla9_ipdltest15PTestBasicChild14DeallocManageeEiPNS_3ipc9IProtocolE","props":["instance","virtual","user"]},{"pretty":"mozilla::_ipdltest::PTestBasicChild::OnMessageReceived","sym":"_ZN7mozilla9_ipdltest15PTestBasicChild17OnMessageReceivedERKN3IPC7MessageE","props":["instance","virtual","user"]},{"pretty":"mozilla::_ipdltest::PTestBasicChild::OnMessageReceived","sym":"_ZN7mozilla9_ipdltest15PTestBasicChild17OnMessageReceivedERKN3IPC7MessageERNS_9UniquePtrIS3_NS_13DefaultDeleteIS3_EEEE","props":["instance","virtual","user"]},{"pretty":"mozilla::_ipdltest::PTestBasicChild::OnCallReceived","sym":"_ZN7mozilla9_ipdltest15PTestBasicChild14OnCallReceivedERKN3IPC7MessageERNS_9UniquePtrIS3_NS_13DefaultDeleteIS3_EEEE","props":["instance","virtual","user"]},{"pretty":"mozilla::_ipdltest::PTestBasicChild::OnChannelClose","sym":"_ZN7mozilla9_ipdltest15PTestBasicChild14OnChannelCloseEv","props":["instance","virtual","user"]},{"pretty":"mozilla::_ipdltest::PTestBasicChild::OnChannelError","sym":"_ZN7mozilla9_ipdltest15PTestBasicChild14OnChannelErrorEv","props":["instance","virtual","user"]},{"pretty":"mozilla::_ipdltest::PTestBasicChild::ClearSubtree","sym":"_ZN7mozilla9_ipdltest15PTestBasicChild12ClearSubtreeEv","props":["instance","user"]},{"pretty":"mozilla::_ipdltest::PTestBasicChild::PTestBasicChild","sym":"_ZN7mozilla9_ipdltest15PTestBasicChildC1ERKS1_","props":["instance","defaulted","deleted"]},{"pretty":"mozilla::_ipdltest::PTestBasicChild::operator=","sym":"_ZN7mozilla9_ipdltest15PTestBasicChildaSERKS1_","props":["instance","defaulted","deleted"]}],"fields":[],"overrides":[],"props":[],"platforms":["linux64"]}]}
{"loc":"00059:9-19","structured":1,"pretty":"mozilla::_ipdltest::PTestBasicChild::ActorAlloc","sym":"_ZN7mozilla9_ipdltest15PTestBasicChild10ActorAllocEv","kind":"method","parentsym":"T_mozilla::_ipdltest::PTestBasicChild","implKind":"","sizeBytes":null,"supers":[],"methods":[],"fields":[],"overrides":[{"pretty":"mozilla::ipc::IProtocol::ActorAlloc","sym":"_ZN7mozilla3ipc9IProtocol10ActorAllocEv"}],"props":["instance","virtual","user"]}
{"loc":"00060:9-21","structured":1,"pretty":"mozilla::_ipdltest::PTestBasicChild::ActorDealloc","sym":"_ZN7mozilla9_ipdltest15PTestBasicChild12ActorDeallocEv","kind":"method","parentsym":"T_mozilla::_ipdltest::PTestBasicChild","implKind":"","sizeBytes":null,"supers":[],"methods":[],"fields":[],"overrides":[{"pretty":"mozilla::ipc::IProtocol::ActorDealloc","sym":"_ZN7mozilla3ipc9IProtocol12ActorDeallocEv"}],"props":["instance","virtual","user"]}
{"loc":"00057:4-43","structured":1,"pretty":"mozilla::_ipdltest::PTestBasicChild::AddRef","sym":"_ZN7mozilla9_ipdltest15PTestBasicChild6AddRefEv","kind":"method","parentsym":"T_mozilla::_ipdltest::PTestBasicChild","implKind":"","sizeBytes":null,"supers":[],"methods":[],"fields":[],"overrides":[],"props":["instance","virtual","user"]}
{"loc":"00057:4-43","structured":1,"pretty":"mozilla::_ipdltest::PTestBasicChild::Release","sym":"_ZN7mozilla9_ipdltest15PTestBasicChild7ReleaseEv","kind":"method","parentsym":"T_mozilla::_ipdltest::PTestBasicChild","implKind":"","sizeBytes":null,"supers":[],"methods":[],"fields":[],"overrides":[],"props":["instance","virtual","user"]}

```

## tests/tests/mc-analysis/__GENERATED__/ipc/ipdl/_ipdlheaders/mozilla/_ipdltest/PTestBasicParent.h
```
{"loc":"00001:0","target":1,"kind":"def","pretty":"__GENERATED__/ipc/ipdl/_ipdlheaders/mozilla/_ipdltest/PTestBasicParent.h","sym":"FILE_android-armv7@__GENERATED__/ipc/ipdl/_ipdlheaders/mozilla/_ipdltest/PTestBasicParent@2Eh"}
{"loc":"00007:8-26","target":1,"kind":"def","pretty":"PTestBasicParent_h","sym":"M_90e0f433a2843475"}
{"loc":"00009:9-41","target":1,"kind":"use","pretty":"__GENERATED__/ipc/ipdl/_ipdlheaders/mozilla/_ipdltest/PTestBasic.h","sym":"FILE_android-armv7@__GENERATED__/ipc/ipdl/_ipdlheaders/mozilla/_ipdltest/PTestBasic@2Eh"}
{"loc":"00010:7-12","target":1,"kind":"use","pretty":"DEBUG","sym":"M_DEBUG"}
{"loc":"00011:9-18","target":1,"kind":"use","pretty":"__GENERATED__/dist/system_wrappers/prenv.h","sym":"FILE_android-armv7@__GENERATED__/dist/system_wrappers/prenv@2Eh"}
{"loc":"00014:9-29","target":1,"kind":"use","pretty":"mfbt/Tainting.h","sym":"FILE_mfbt/Tainting@2Eh"}
{"loc":"00015:9-39","target":1,"kind":"use","pretty":"ipc/glue/MessageChannel.h","sym":"FILE_ipc/glue/MessageChannel@2Eh"}
{"loc":"00016:9-38","target":1,"kind":"use","pretty":"ipc/glue/ProtocolUtils.h","sym":"FILE_ipc/glue/ProtocolUtils@2Eh"}
{"loc":"00017:6-13","target":1,"kind":"forward","pretty":"nsIFile","sym":"T_nsIFile"}
{"loc":"00019:10-17","target":1,"kind":"def","pretty":"mozilla","sym":"NS_mozilla"}
{"loc":"00020:10-19","target":1,"kind":"def","pretty":"mozilla::_ipdltest","sym":"NS_mozilla::_ipdltest"}
{"loc":"00023:6-22","target":1,"kind":"def","pretty":"mozilla::_ipdltest::PTestBasicParent","sym":"T_mozilla::_ipdltest::PTestBasicParent","peekRange":"23-24"}
{"loc":"00024:25-42","target":1,"kind":"use","pretty":"mozilla::ipc::IToplevelProtocol","sym":"T_mozilla::ipc::IToplevelProtocol","context":"mozilla::_ipdltest::PTestBasicParent","contextsym":"T_mozilla::_ipdltest::PTestBasicParent"}
{"loc":"00028:26-37","target":1,"kind":"use","pretty":"mozilla::ipc::ActorHandle","sym":"T_mozilla::ipc::ActorHandle","context":"mozilla::_ipdltest::PTestBasicParent","contextsym":"T_mozilla::_ipdltest::PTestBasicParent"}
{"loc":"00028:38-49","target":1,"kind":"def","pretty":"mozilla::_ipdltest::PTestBasicParent::ActorHandle","sym":"T_mozilla::_ipdltest::PTestBasicParent::ActorHandle","context":"mozilla::_ipdltest::PTestBasicParent","contextsym":"T_mozilla::_ipdltest::PTestBasicParent"}
{"loc":"00029:26-33","target":1,"kind":"use","pretty":"mozilla::ipc::ByteBuf","sym":"T_mozilla::ipc::ByteBuf","context":"mozilla::_ipdltest::PTestBasicParent","contextsym":"T_mozilla::_ipdltest::PTestBasicParent"}
{"loc":"00029:34-41","target":1,"kind":"def","pretty":"mozilla::_ipdltest::PTestBasicParent::ByteBuf","sym":"T_mozilla::_ipdltest::PTestBasicParent::ByteBuf","context":"mozilla::_ipdltest::PTestBasicParent","contextsym":"T_mozilla::_ipdltest::PTestBasicParent"}
{"loc":"00030:34-42","target":1,"kind":"def","pretty":"mozilla::_ipdltest::PTestBasicParent::Endpoint","sym":"T_mozilla::_ipdltest::PTestBasicParent::Endpoint","context":"mozilla::_ipdltest::PTestBasicParent","contextsym":"T_mozilla::_ipdltest::PTestBasicParent"}
{"loc":"00030:59-67","target":1,"kind":"use","pretty":"mozilla::ipc::Endpoint","sym":"T_mozilla::ipc::Endpoint","context":"mozilla::_ipdltest::PTestBasicParent","contextsym":"T_mozilla::_ipdltest::PTestBasicParent"}
{"loc":"00031:26-40","target":1,"kind":"use","pretty":"mozilla::ipc::FileDescriptor","sym":"T_mozilla::ipc::FileDescriptor","context":"mozilla::_ipdltest::PTestBasicParent","contextsym":"T_mozilla::_ipdltest::PTestBasicParent"}
{"loc":"00031:41-55","target":1,"kind":"def","pretty":"mozilla::_ipdltest::PTestBasicParent::FileDescriptor","sym":"T_mozilla::_ipdltest::PTestBasicParent::FileDescriptor","context":"mozilla::_ipdltest::PTestBasicParent","contextsym":"T_mozilla::_ipdltest::PTestBasicParent"}
{"loc":"00032:34-49","target":1,"kind":"def","pretty":"mozilla::_ipdltest::PTestBasicParent::ManagedEndpoint","sym":"T_mozilla::_ipdltest::PTestBasicParent::ManagedEndpoint","context":"mozilla::_ipdltest::PTestBasicParent","contextsym":"T_mozilla::_ipdltest::PTestBasicParent"}
{"loc":"00032:66-81","target":1,"kind":"use","pretty":"mozilla::ipc::ManagedEndpoint","sym":"T_mozilla::ipc::ManagedEndpoint","context":"mozilla::_ipdltest::PTestBasicParent","contextsym":"T_mozilla::_ipdltest::PTestBasicParent"}
{"loc":"00033:18-27","target":1,"kind":"use","pretty":"base::ProcessId","sym":"T_base::ProcessId","context":"mozilla::_ipdltest::PTestBasicParent","contextsym":"T_mozilla::_ipdltest::PTestBasicParent"}
{"loc":"00033:28-37","target":1,"kind":"def","pretty":"mozilla::_ipdltest::PTestBasicParent::ProcessId","sym":"T_mozilla::_ipdltest::PTestBasicParent::ProcessId","context":"mozilla::_ipdltest::PTestBasicParent","contextsym":"T_mozilla::_ipdltest::PTestBasicParent"}
{"loc":"00034:26-36","target":1,"kind":"use","pretty":"mozilla::ipc::ProtocolId","sym":"T_mozilla::ipc::ProtocolId","context":"mozilla::_ipdltest::PTestBasicParent","contextsym":"T_mozilla::_ipdltest::PTestBasicParent"}
{"loc":"00034:37-47","target":1,"kind":"def","pretty":"mozilla::_ipdltest::PTestBasicParent::ProtocolId","sym":"T_mozilla::_ipdltest::PTestBasicParent::ProtocolId","context":"mozilla::_ipdltest::PTestBasicParent","contextsym":"T_mozilla::_ipdltest::PTestBasicParent"}
{"loc":"00035:26-46","target":1,"kind":"use","pretty":"mozilla::ipc::ResponseRejectReason","sym":"T_mozilla::ipc::ResponseRejectReason","context":"mozilla::_ipdltest::PTestBasicParent","contextsym":"T_mozilla::_ipdltest::PTestBasicParent"}
{"loc":"00035:47-67","target":1,"kind":"def","pretty":"mozilla::_ipdltest::PTestBasicParent::ResponseRejectReason","sym":"T_mozilla::_ipdltest::PTestBasicParent::ResponseRejectReason","context":"mozilla::_ipdltest::PTestBasicParent","contextsym":"T_mozilla::_ipdltest::PTestBasicParent"}
{"loc":"00036:26-31","target":1,"kind":"use","pretty":"mozilla::ipc::Shmem","sym":"T_mozilla::ipc::Shmem","context":"mozilla::_ipdltest::PTestBasicParent","contextsym":"T_mozilla::_ipdltest::PTestBasicParent"}
{"loc":"00036:32-37","target":1,"kind":"def","pretty":"mozilla::_ipdltest::PTestBasicParent::Shmem","sym":"T_mozilla::_ipdltest::PTestBasicParent::Shmem","context":"mozilla::_ipdltest::PTestBasicParent","contextsym":"T_mozilla::_ipdltest::PTestBasicParent"}
{"loc":"00037:28-37","target":1,"kind":"def","pretty":"mozilla::_ipdltest::PTestBasicParent::UniquePtr","sym":"T_mozilla::_ipdltest::PTestBasicParent::UniquePtr","context":"mozilla::_ipdltest::PTestBasicParent","contextsym":"T_mozilla::_ipdltest::PTestBasicParent"}
{"loc":"00037:49-58","target":1,"kind":"use","pretty":"mozilla::UniquePtr","sym":"T_mozilla::UniquePtr","context":"mozilla::_ipdltest::PTestBasicParent","contextsym":"T_mozilla::_ipdltest::PTestBasicParent"}
{"loc":"00041:4-19","target":1,"kind":"decl","pretty":"mozilla::_ipdltest::PTestBasicParent::ProcessingError","sym":"_ZN7mozilla9_ipdltest16PTestBasicParent15ProcessingErrorENS_3ipc14HasResultCodes6ResultEPKc","context":"mozilla::_ipdltest::PTestBasicParent","contextsym":"T_mozilla::_ipdltest::PTestBasicParent","peekRange":"40-43"}
{"loc":"00042:12-18","target":1,"kind":"use","pretty":"mozilla::ipc::HasResultCodes::Result","sym":"T_mozilla::ipc::HasResultCodes::Result","context":"mozilla::_ipdltest::PTestBasicParent::ProcessingError","contextsym":"_ZN7mozilla9_ipdltest16PTestBasicParent15ProcessingErrorENS_3ipc14HasResultCodes6ResultEPKc"}
{"loc":"00045:4-34","target":1,"kind":"decl","pretty":"mozilla::_ipdltest::PTestBasicParent::ShouldContinueFromReplyTimeout","sym":"_ZN7mozilla9_ipdltest16PTestBasicParent30ShouldContinueFromReplyTimeoutEv","context":"mozilla::_ipdltest::PTestBasicParent","contextsym":"T_mozilla::_ipdltest::PTestBasicParent","peekRange":"44-45"}
{"loc":"00048:26-35","target":1,"kind":"use","pretty":"mozilla::ipc::IProtocol","sym":"T_mozilla::ipc::IProtocol","context":"mozilla::_ipdltest::PTestBasicParent","contextsym":"T_mozilla::_ipdltest::PTestBasicParent"}
{"loc":"00048:36-45","target":1,"kind":"def","pretty":"mozilla::_ipdltest::PTestBasicParent::IProtocol","sym":"T_mozilla::_ipdltest::PTestBasicParent::IProtocol","context":"mozilla::_ipdltest::PTestBasicParent","contextsym":"T_mozilla::_ipdltest::PTestBasicParent"}
{"loc":"00049:17-24","target":1,"kind":"use","pretty":"IPC::Message","sym":"T_IPC::Message","context":"mozilla::_ipdltest::PTestBasicParent","contextsym":"T_mozilla::_ipdltest::PTestBasicParent"}
{"loc":"00049:25-32","target":1,"kind":"def","pretty":"mozilla::_ipdltest::PTestBasicParent::Message","sym":"T_mozilla::_ipdltest::PTestBasicParent::Message","context":"mozilla::_ipdltest::PTestBasicParent","contextsym":"T_mozilla::_ipdltest::PTestBasicParent"}
{"loc":"00050:18-31","target":1,"kind":"use","pretty":"base::ProcessHandle","sym":"T_base::ProcessHandle","context":"mozilla::_ipdltest::PTestBasicParent","contextsym":"T_mozilla::_ipdltest::PTestBasicParent"}
{"loc":"00050:32-45","target":1,"kind":"def","pretty":"mozilla::_ipdltest::PTestBasicParent::ProcessHandle","sym":"T_mozilla::_ipdltest::PTestBasicParent::ProcessHandle","context":"mozilla::_ipdltest::PTestBasicParent","contextsym":"T_mozilla::_ipdltest::PTestBasicParent"}
{"loc":"00051:26-40","target":1,"kind":"use","pretty":"mozilla::ipc::MessageChannel","sym":"T_mozilla::ipc::MessageChannel","context":"mozilla::_ipdltest::PTestBasicParent","contextsym":"T_mozilla::_ipdltest::PTestBasicParent"}
{"loc":"00051:41-55","target":1,"kind":"def","pretty":"mozilla::_ipdltest::PTestBasicParent::MessageChannel","sym":"T_mozilla::_ipdltest::PTestBasicParent::MessageChannel","context":"mozilla::_ipdltest::PTestBasicParent","contextsym":"T_mozilla::_ipdltest::PTestBasicParent"}
{"loc":"00052:26-38","target":1,"kind":"use","pretty":"mozilla::ipc::SharedMemory","sym":"T_mozilla::ipc::SharedMemory","context":"mozilla::_ipdltest::PTestBasicParent","contextsym":"T_mozilla::_ipdltest::PTestBasicParent"}
{"loc":"00052:39-51","target":1,"kind":"def","pretty":"mozilla::_ipdltest::PTestBasicParent::SharedMemory","sym":"T_mozilla::_ipdltest::PTestBasicParent::SharedMemory","context":"mozilla::_ipdltest::PTestBasicParent","contextsym":"T_mozilla::_ipdltest::PTestBasicParent"}
{"loc":"00055:17-33","target":1,"kind":"decl","pretty":"mozilla::_ipdltest::PTestBasicParent::PTestBasicParent","sym":"_ZN7mozilla9_ipdltest16PTestBasicParentC1Ev","context":"mozilla::_ipdltest::PTestBasicParent","contextsym":"T_mozilla::_ipdltest::PTestBasicParent"}
{"loc":"00055:4-16","target":1,"kind":"use","pretty":"MOZ_IMPLICIT","sym":"M_5f65895d5a5258f7"}
{"loc":"00057:13-29","target":1,"kind":"decl","pretty":"mozilla::_ipdltest::PTestBasicParent::~PTestBasicParent","sym":"_ZN7mozilla9_ipdltest16PTestBasicParentD1Ev","context":"mozilla::_ipdltest::PTestBasicParent","contextsym":"T_mozilla::_ipdltest::PTestBasicParent","peekRange":"57-57"}
{"loc":"00057:13-29","target":1,"kind":"use","pretty":"mozilla::_ipdltest::PTestBasicParent","sym":"T_mozilla::_ipdltest::PTestBasicParent","context":"mozilla::_ipdltest::PTestBasicParent::~PTestBasicParent","contextsym":"_ZN7mozilla9_ipdltest16PTestBasicParentD1Ev"}
{"loc":"00059:4-43","target":1,"kind":"def","pretty":"mozilla::_ipdltest::PTestBasicParent::AddRef","sym":"_ZN7mozilla9_ipdltest16PTestBasicParent6AddRefEv"}
{"loc":"00059:4-43","target":1,"kind":"def","pretty":"mozilla::_ipdltest::PTestBasicParent::Release","sym":"_ZN7mozilla9_ipdltest16PTestBasicParent7ReleaseEv"}
{"loc":"00059:4-43","target":1,"kind":"use","pretty":"NS_INLINE_DECL_PURE_VIRTUAL_REFCOUNTING","sym":"M_19459f64b289c7f3"}
{"loc":"00061:30-36","target":1,"kind":"use","pretty":"mozilla::_ipdltest::PTestBasicParent::AddRef","sym":"_ZN7mozilla9_ipdltest16PTestBasicParent6AddRefEv","context":"mozilla::_ipdltest::PTestBasicParent::ActorAlloc","contextsym":"_ZN7mozilla9_ipdltest16PTestBasicParent10ActorAllocEv"}
{"loc":"00061:9-19","target":1,"kind":"def","pretty":"mozilla::_ipdltest::PTestBasicParent::ActorAlloc","sym":"_ZN7mozilla9_ipdltest16PTestBasicParent10ActorAllocEv","context":"mozilla::_ipdltest::PTestBasicParent","contextsym":"T_mozilla::_ipdltest::PTestBasicParent","peekRange":"61-61"}
{"loc":"00062:32-39","target":1,"kind":"use","pretty":"mozilla::_ipdltest::PTestBasicParent::Release","sym":"_ZN7mozilla9_ipdltest16PTestBasicParent7ReleaseEv","context":"mozilla::_ipdltest::PTestBasicParent::ActorDealloc","contextsym":"_ZN7mozilla9_ipdltest16PTestBasicParent12ActorDeallocEv"}
{"loc":"00062:9-21","target":1,"kind":"def","pretty":"mozilla::_ipdltest::PTestBasicParent::ActorDealloc","sym":"_ZN7mozilla9_ipdltest16PTestBasicParent12ActorDeallocEv","context":"mozilla::_ipdltest::PTestBasicParent","contextsym":"T_mozilla::_ipdltest::PTestBasicParent","peekRange":"62-62"}
{"loc":"00065:21-29","target":1,"kind":"use","pretty":"nsTArray","sym":"T_nsTArray","context":"mozilla::_ipdltest::PTestBasicParent::AllManagedActors","contextsym":"_ZNK7mozilla9_ipdltest16PTestBasicParent16AllManagedActorsER8nsTArrayI6RefPtrINS_3ipc19ActorLifecycleProxyEEE"}
{"loc":"00065:30-36","target":1,"kind":"use","pretty":"RefPtr","sym":"T_RefPtr","context":"mozilla::_ipdltest::PTestBasicParent::AllManagedActors","contextsym":"_ZNK7mozilla9_ipdltest16PTestBasicParent16AllManagedActorsER8nsTArrayI6RefPtrINS_3ipc19ActorLifecycleProxyEEE"}
{"loc":"00065:4-20","target":1,"kind":"decl","pretty":"mozilla::_ipdltest::PTestBasicParent::AllManagedActors","sym":"_ZNK7mozilla9_ipdltest16PTestBasicParent16AllManagedActorsER8nsTArrayI6RefPtrINS_3ipc19ActorLifecycleProxyEEE","context":"mozilla::_ipdltest::PTestBasicParent","contextsym":"T_mozilla::_ipdltest::PTestBasicParent","peekRange":"64-65"}
{"loc":"00065:51-70","target":1,"kind":"use","pretty":"mozilla::ipc::ActorLifecycleProxy","sym":"T_mozilla::ipc::ActorLifecycleProxy","context":"mozilla::_ipdltest::PTestBasicParent::AllManagedActors","contextsym":"_ZNK7mozilla9_ipdltest16PTestBasicParent16AllManagedActorsER8nsTArrayI6RefPtrINS_3ipc19ActorLifecycleProxyEEE"}
{"loc":"00068:4-13","target":1,"kind":"decl","pretty":"mozilla::_ipdltest::PTestBasicParent::SendHello","sym":"_ZN7mozilla9_ipdltest16PTestBasicParent9SendHelloEv","context":"mozilla::_ipdltest::PTestBasicParent","contextsym":"T_mozilla::_ipdltest::PTestBasicParent","peekRange":"67-68"}
{"loc":"00071:4-17","target":1,"kind":"decl","pretty":"mozilla::_ipdltest::PTestBasicParent::RemoveManagee","sym":"_ZN7mozilla9_ipdltest16PTestBasicParent13RemoveManageeEiPNS_3ipc9IProtocolE","context":"mozilla::_ipdltest::PTestBasicParent","contextsym":"T_mozilla::_ipdltest::PTestBasicParent","peekRange":"70-73"}
{"loc":"00072:12-19","target":1,"kind":"use","pretty":"int32_t","sym":"T_int32_t","context":"mozilla::_ipdltest::PTestBasicParent::RemoveManagee","contextsym":"_ZN7mozilla9_ipdltest16PTestBasicParent13RemoveManageeEiPNS_3ipc9IProtocolE"}
{"loc":"00073:12-21","target":1,"kind":"use","pretty":"mozilla::_ipdltest::PTestBasicParent::IProtocol","sym":"T_mozilla::_ipdltest::PTestBasicParent::IProtocol","context":"mozilla::_ipdltest::PTestBasicParent::RemoveManagee","contextsym":"_ZN7mozilla9_ipdltest16PTestBasicParent13RemoveManageeEiPNS_3ipc9IProtocolE"}
{"loc":"00075:4-18","target":1,"kind":"decl","pretty":"mozilla::_ipdltest::PTestBasicParent::DeallocManagee","sym":"_ZN7mozilla9_ipdltest16PTestBasicParent14DeallocManageeEiPNS_3ipc9IProtocolE","context":"mozilla::_ipdltest::PTestBasicParent","contextsym":"T_mozilla::_ipdltest::PTestBasicParent","peekRange":"74-77"}
{"loc":"00076:12-19","target":1,"kind":"use","pretty":"int32_t","sym":"T_int32_t","context":"mozilla::_ipdltest::PTestBasicParent::DeallocManagee","contextsym":"_ZN7mozilla9_ipdltest16PTestBasicParent14DeallocManageeEiPNS_3ipc9IProtocolE"}
{"loc":"00077:12-21","target":1,"kind":"use","pretty":"mozilla::_ipdltest::PTestBasicParent::IProtocol","sym":"T_mozilla::_ipdltest::PTestBasicParent::IProtocol","context":"mozilla::_ipdltest::PTestBasicParent::DeallocManagee","contextsym":"_ZN7mozilla9_ipdltest16PTestBasicParent14DeallocManageeEiPNS_3ipc9IProtocolE"}
{"loc":"00079:4-10","target":1,"kind":"use","pretty":"mozilla::ipc::HasResultCodes::Result","sym":"T_mozilla::ipc::HasResultCodes::Result","context":"mozilla::_ipdltest::PTestBasicParent::OnMessageReceived","contextsym":"_ZN7mozilla9_ipdltest16PTestBasicParent17OnMessageReceivedERKN3IPC7MessageE"}
{"loc":"00080:28-35","target":1,"kind":"use","pretty":"mozilla::_ipdltest::PTestBasicParent::Message","sym":"T_mozilla::_ipdltest::PTestBasicParent::Message","context":"mozilla::_ipdltest::PTestBasicParent::OnMessageReceived","contextsym":"_ZN7mozilla9_ipdltest16PTestBasicParent17OnMessageReceivedERKN3IPC7MessageE"}
{"loc":"00080:4-21","target":1,"kind":"decl","pretty":"mozilla::_ipdltest::PTestBasicParent::OnMessageReceived","sym":"_ZN7mozilla9_ipdltest16PTestBasicParent17OnMessageReceivedERKN3IPC7MessageE","context":"mozilla::_ipdltest::PTestBasicParent","contextsym":"T_mozilla::_ipdltest::PTestBasicParent","peekRange":"79-80"}
{"loc":"00082:4-10","target":1,"kind":"use","pretty":"mozilla::ipc::HasResultCodes::Result","sym":"T_mozilla::ipc::HasResultCodes::Result","context":"mozilla::_ipdltest::PTestBasicParent::OnMessageReceived","contextsym":"_ZN7mozilla9_ipdltest16PTestBasicParent17OnMessageReceivedERKN3IPC7MessageERNS_9UniquePtrIS3_NS_13DefaultDeleteIS3_EEEE"}
{"loc":"00083:4-21","target":1,"kind":"decl","pretty":"mozilla::_ipdltest::PTestBasicParent::OnMessageReceived","sym":"_ZN7mozilla9_ipdltest16PTestBasicParent17OnMessageReceivedERKN3IPC7MessageERNS_9UniquePtrIS3_NS_13DefaultDeleteIS3_EEEE","context":"mozilla::_ipdltest::PTestBasicParent","contextsym":"T_mozilla::_ipdltest::PTestBasicParent","peekRange":"82-85"}
{"loc":"00084:18-25","target":1,"kind":"use","pretty":"mozilla::_ipdltest::PTestBasicParent::Message","sym":"T_mozilla::_ipdltest::PTestBasicParent::Message","context":"mozilla::_ipdltest::PTestBasicParent::OnMessageReceived","contextsym":"_ZN7mozilla9_ipdltest16PTestBasicParent17OnMessageReceivedERKN3IPC7MessageERNS_9UniquePtrIS3_NS_13DefaultDeleteIS3_EEEE"}
{"loc":"00085:12-21","target":1,"kind":"use","pretty":"mozilla::_ipdltest::PTestBasicParent::UniquePtr","sym":"T_mozilla::_ipdltest::PTestBasicParent::UniquePtr","context":"mozilla::_ipdltest::PTestBasicParent::OnMessageReceived","contextsym":"_ZN7mozilla9_ipdltest16PTestBasicParent17OnMessageReceivedERKN3IPC7MessageERNS_9UniquePtrIS3_NS_13DefaultDeleteIS3_EEEE"}
{"loc":"00085:22-29","target":1,"kind":"use","pretty":"mozilla::_ipdltest::PTestBasicParent::Message","sym":"T_mozilla::_ipdltest::PTestBasicParent::Message","context":"mozilla::_ipdltest::PTestBasicParent::OnMessageReceived","contextsym":"_ZN7mozilla9_ipdltest16PTestBasicParent17OnMessageReceivedERKN3IPC7MessageERNS_9UniquePtrIS3_NS_13DefaultDeleteIS3_EEEE"}
{"loc":"00087:4-10","target":1,"kind":"use","pretty":"mozilla::ipc::HasResultCodes::Result","sym":"T_mozilla::ipc::HasResultCodes::Result","context":"mozilla::_ipdltest::PTestBasicParent::OnCallReceived","contextsym":"_ZN7mozilla9_ipdltest16PTestBasicParent14OnCallReceivedERKN3IPC7MessageERNS_9UniquePtrIS3_NS_13DefaultDeleteIS3_EEEE"}
{"loc":"00088:4-18","target":1,"kind":"decl","pretty":"mozilla::_ipdltest::PTestBasicParent::OnCallReceived","sym":"_ZN7mozilla9_ipdltest16PTestBasicParent14OnCallReceivedERKN3IPC7MessageERNS_9UniquePtrIS3_NS_13DefaultDeleteIS3_EEEE","context":"mozilla::_ipdltest::PTestBasicParent","contextsym":"T_mozilla::_ipdltest::PTestBasicParent","peekRange":"87-90"}
{"loc":"00089:18-25","target":1,"kind":"use","pretty":"mozilla::_ipdltest::PTestBasicParent::Message","sym":"T_mozilla::_ipdltest::PTestBasicParent::Message","context":"mozilla::_ipdltest::PTestBasicParent::OnCallReceived","contextsym":"_ZN7mozilla9_ipdltest16PTestBasicParent14OnCallReceivedERKN3IPC7MessageERNS_9UniquePtrIS3_NS_13DefaultDeleteIS3_EEEE"}
{"loc":"00090:12-21","target":1,"kind":"use","pretty":"mozilla::_ipdltest::PTestBasicParent::UniquePtr","sym":"T_mozilla::_ipdltest::PTestBasicParent::UniquePtr","context":"mozilla::_ipdltest::PTestBasicParent::OnCallReceived","contextsym":"_ZN7mozilla9_ipdltest16PTestBasicParent14OnCallReceivedERKN3IPC7MessageERNS_9UniquePtrIS3_NS_13DefaultDeleteIS3_EEEE"}
{"loc":"00090:22-29","target":1,"kind":"use","pretty":"mozilla::_ipdltest::PTestBasicParent::Message","sym":"T_mozilla::_ipdltest::PTestBasicParent::Message","context":"mozilla::_ipdltest::PTestBasicParent::OnCallReceived","contextsym":"_ZN7mozilla9_ipdltest16PTestBasicParent14OnCallReceivedERKN3IPC7MessageERNS_9UniquePtrIS3_NS_13DefaultDeleteIS3_EEEE"}
{"loc":"00093:4-18","target":1,"kind":"decl","pretty":"mozilla::_ipdltest::PTestBasicParent::OnChannelClose","sym":"_ZN7mozilla9_ipdltest16PTestBasicParent14OnChannelCloseEv","context":"mozilla::_ipdltest::PTestBasicParent","contextsym":"T_mozilla::_ipdltest::PTestBasicParent","peekRange":"92-93"}
{"loc":"00096:4-18","target":1,"kind":"decl","pretty":"mozilla::_ipdltest::PTestBasicParent::OnChannelError","sym":"_ZN7mozilla9_ipdltest16PTestBasicParent14OnChannelErrorEv","context":"mozilla::_ipdltest::PTestBasicParent","contextsym":"T_mozilla::_ipdltest::PTestBasicParent","peekRange":"95-96"}
{"loc":"00100:4-16","target":1,"kind":"decl","pretty":"mozilla::_ipdltest::PTestBasicParent::ClearSubtree","sym":"_ZN7mozilla9_ipdltest16PTestBasicParent12ClearSubtreeEv","context":"mozilla::_ipdltest::PTestBasicParent","contextsym":"T_mozilla::_ipdltest::PTestBasicParent","peekRange":"99-100"}
{"loc":"00107:10-13","target":1,"kind":"def","pretty":"IPC","sym":"NS_IPC"}
{"loc":"00109:40-56","target":1,"kind":"use","pretty":"mozilla::_ipdltest::PTestBasicParent","sym":"T_mozilla::_ipdltest::PTestBasicParent"}
{"loc":"00109:7-18","target":1,"kind":"def","pretty":"IPC::ParamTraits","sym":"T_IPC::ParamTraits","peekRange":"109-109"}
{"loc":"00109:7-18","target":1,"kind":"use","pretty":"IPC::ParamTraits","sym":"T_IPC::ParamTraits"}
{"loc":"00111:32-48","target":1,"kind":"use","pretty":"mozilla::_ipdltest::PTestBasicParent","sym":"T_mozilla::_ipdltest::PTestBasicParent"}
{"loc":"00111:50-59","target":1,"kind":"def","pretty":"IPC::ParamTraits<mozilla::_ipdltest::PTestBasicParent *>::paramType","sym":"T_IPC::ParamTraits<mozilla::_ipdltest::PTestBasicParent_*>::paramType"}
{"loc":"00113:4-9","target":1,"kind":"decl","pretty":"IPC::ParamTraits<mozilla::_ipdltest::PTestBasicParent *>::Write","sym":"_ZN3IPC11ParamTraitsIPN7mozilla9_ipdltest16PTestBasicParentEE5WriteEPNS_13MessageWriterERKS4_","peekRange":"112-115"}
{"loc":"00114:17-30","target":1,"kind":"use","pretty":"IPC::MessageWriter","sym":"T_IPC::MessageWriter","context":"IPC::ParamTraits<mozilla::_ipdltest::PTestBasicParent *>::Write","contextsym":"_ZN3IPC11ParamTraitsIPN7mozilla9_ipdltest16PTestBasicParentEE5WriteEPNS_13MessageWriterERKS4_"}
{"loc":"00115:18-27","target":1,"kind":"use","pretty":"IPC::ParamTraits<mozilla::_ipdltest::PTestBasicParent *>::paramType","sym":"T_IPC::ParamTraits<mozilla::_ipdltest::PTestBasicParent_*>::paramType","context":"IPC::ParamTraits<mozilla::_ipdltest::PTestBasicParent *>::Write","contextsym":"_ZN3IPC11ParamTraitsIPN7mozilla9_ipdltest16PTestBasicParentEE5WriteEPNS_13MessageWriterERKS4_"}
{"loc":"00117:4-8","target":1,"kind":"decl","pretty":"IPC::ParamTraits<mozilla::_ipdltest::PTestBasicParent *>::Read","sym":"_ZN3IPC11ParamTraitsIPN7mozilla9_ipdltest16PTestBasicParentEE4ReadEPNS_13MessageReaderEPS4_","peekRange":"116-119"}
{"loc":"00118:17-30","target":1,"kind":"use","pretty":"IPC::MessageReader","sym":"T_IPC::MessageReader","context":"IPC::ParamTraits<mozilla::_ipdltest::PTestBasicParent *>::Read","contextsym":"_ZN3IPC11ParamTraitsIPN7mozilla9_ipdltest16PTestBasicParentEE4ReadEPNS_13MessageReaderEPS4_"}
{"loc":"00119:12-21","target":1,"kind":"use","pretty":"IPC::ParamTraits<mozilla::_ipdltest::PTestBasicParent *>::paramType","sym":"T_IPC::ParamTraits<mozilla::_ipdltest::PTestBasicParent_*>::paramType","context":"IPC::ParamTraits<mozilla::_ipdltest::PTestBasicParent *>::Read","contextsym":"_ZN3IPC11ParamTraitsIPN7mozilla9_ipdltest16PTestBasicParentEE4ReadEPNS_13MessageReaderEPS4_"}
{"loc":"00001:0","target":1,"kind":"def","pretty":"__GENERATED__/ipc/ipdl/_ipdlheaders/mozilla/_ipdltest/PTestBasicParent.h","sym":"FILE_linux@__GENERATED__/ipc/ipdl/_ipdlheaders/mozilla/_ipdltest/PTestBasicParent@2Eh"}
{"loc":"00007:8-26","target":1,"kind":"def","pretty":"PTestBasicParent_h","sym":"M_e150e2b600d23261"}
{"loc":"00009:9-41","target":1,"kind":"use","pretty":"__GENERATED__/ipc/ipdl/_ipdlheaders/mozilla/_ipdltest/PTestBasic.h","sym":"FILE_linux@__GENERATED__/ipc/ipdl/_ipdlheaders/mozilla/_ipdltest/PTestBasic@2Eh"}
{"loc":"00011:9-18","target":1,"kind":"use","pretty":"__GENERATED__/dist/system_wrappers/prenv.h","sym":"FILE_linux@__GENERATED__/dist/system_wrappers/prenv@2Eh"}
{"loc":"00001:0","target":1,"kind":"def","pretty":"__GENERATED__/ipc/ipdl/_ipdlheaders/mozilla/_ipdltest/PTestBasicParent.h","sym":"FILE_macosx@__GENERATED__/ipc/ipdl/_ipdlheaders/mozilla/_ipdltest/PTestBasicParent@2Eh"}
{"loc":"00007:8-26","target":1,"kind":"def","pretty":"PTestBasicParent_h","sym":"M_9db161f645103b5d"}
{"loc":"00009:9-41","target":1,"kind":"use","pretty":"__GENERATED__/ipc/ipdl/_ipdlheaders/mozilla/_ipdltest/PTestBasic.h","sym":"FILE_macosx@__GENERATED__/ipc/ipdl/_ipdlheaders/mozilla/_ipdltest/PTestBasic@2Eh"}
{"loc":"00011:9-18","target":1,"kind":"use","pretty":"nsprpub/pr/include/prenv.h","sym":"FILE_nsprpub/pr/include/prenv@2Eh"}
{"loc":"00001:0","target":1,"kind":"def","pretty":"__GENERATED__/ipc/ipdl/_ipdlheaders/mozilla/_ipdltest/PTestBasicParent.h","sym":"FILE_windows@__GENERATED__/ipc/ipdl/_ipdlheaders/mozilla/_ipdltest/PTestBasicParent@2Eh"}
{"loc":"00007:8-26","target":1,"kind":"def","pretty":"PTestBasicParent_h","sym":"M_9138027d537834f1"}
{"loc":"00009:9-41","target":1,"kind":"use","pretty":"__GENERATED__/ipc/ipdl/_ipdlheaders/mozilla/_ipdltest/PTestBasic.h","sym":"FILE_windows@__GENERATED__/ipc/ipdl/_ipdlheaders/mozilla/_ipdltest/PTestBasic@2Eh"}
{"loc":"00001:0","source":1,"syntax":"def,file","pretty":"file __GENERATED__/ipc/ipdl/_ipdlheaders/mozilla/_ipdltest/PTestBasicParent.h","sym":"FILE_windows@__GENERATED__/ipc/ipdl/_ipdlheaders/mozilla/_ipdltest/PTestBasicParent@2Eh,FILE_macosx@__GENERATED__/ipc/ipdl/_ipdlheaders/mozilla/_ipdltest/PTestBasicParent@2Eh,FILE_linux@__GENERATED__/ipc/ipdl/_ipdlheaders/mozilla/_ipdltest/PTestBasicParent@2Eh,FILE_android-armv7@__GENERATED__/ipc/ipdl/_ipdlheaders/mozilla/_ipdltest/PTestBasicParent@2Eh"}
{"loc":"00007:8-26","source":1,"syntax":"def,macro","pretty":"macro PTestBasicParent_h","sym":"M_9138027d537834f1,M_9db161f645103b5d,M_e150e2b600d23261,M_90e0f433a2843475"}
{"loc":"00009:9-41","source":1,"syntax":"file,use","pretty":"file __GENERATED__/ipc/ipdl/_ipdlheaders/mozilla/_ipdltest/PTestBasic.h","sym":"FILE_windows@__GENERATED__/ipc/ipdl/_ipdlheaders/mozilla/_ipdltest/PTestBasic@2Eh,FILE_macosx@__GENERATED__/ipc/ipdl/_ipdlheaders/mozilla/_ipdltest/PTestBasic@2Eh,FILE_linux@__GENERATED__/ipc/ipdl/_ipdlheaders/mozilla/_ipdltest/PTestBasic@2Eh,FILE_android-armv7@__GENERATED__/ipc/ipdl/_ipdlheaders/mozilla/_ipdltest/PTestBasic@2Eh"}
{"loc":"00010:7-12","source":1,"syntax":"macro,use","pretty":"macro DEBUG","sym":"M_DEBUG"}
{"loc":"00011:9-18","source":1,"syntax":"file,use","pretty":"file __GENERATED__/dist/system_wrappers/prenv.h","sym":"FILE_linux@__GENERATED__/dist/system_wrappers/prenv@2Eh,FILE_android-armv7@__GENERATED__/dist/system_wrappers/prenv@2Eh"}
{"loc":"00011:9-18","source":1,"syntax":"file,use","pretty":"file nsprpub/pr/include/prenv.h","sym":"FILE_nsprpub/pr/include/prenv@2Eh"}
{"loc":"00014:9-29","source":1,"syntax":"file,use","pretty":"file mfbt/Tainting.h","sym":"FILE_mfbt/Tainting@2Eh"}
{"loc":"00015:9-39","source":1,"syntax":"file,use","pretty":"file ipc/glue/MessageChannel.h","sym":"FILE_ipc/glue/MessageChannel@2Eh"}
{"loc":"00016:9-38","source":1,"syntax":"file,use","pretty":"file ipc/glue/ProtocolUtils.h","sym":"FILE_ipc/glue/ProtocolUtils@2Eh"}
{"loc":"00017:6-13","source":1,"syntax":"forward,type","pretty":"type nsIFile","sym":"T_nsIFile"}
{"loc":"00019:10-17","source":1,"syntax":"def,namespace","pretty":"namespace mozilla","sym":"NS_mozilla","nestingRange":"19:19-106:0"}
{"loc":"00020:10-19","source":1,"syntax":"def,namespace","pretty":"namespace mozilla::_ipdltest","sym":"NS_mozilla::_ipdltest","nestingRange":"20:21-105:0"}
{"loc":"00023:6-22","source":1,"syntax":"def,type","pretty":"type mozilla::_ipdltest::PTestBasicParent","sym":"T_mozilla::_ipdltest::PTestBasicParent","nestingRange":"25:0-102:0"}
{"loc":"00024:25-42","source":1,"syntax":"type,use","pretty":"type mozilla::ipc::IToplevelProtocol","sym":"T_mozilla::ipc::IToplevelProtocol","type":"class mozilla::ipc::IToplevelProtocol","typesym":"T_mozilla::ipc::IToplevelProtocol"}
{"loc":"00028:26-37","source":1,"syntax":"type,use","pretty":"type mozilla::ipc::ActorHandle","sym":"T_mozilla::ipc::ActorHandle","type":"struct mozilla::ipc::ActorHandle","typesym":"T_mozilla::ipc::ActorHandle"}
{"loc":"00028:38-49","source":1,"syntax":"def,type","pretty":"type mozilla::_ipdltest::PTestBasicParent::ActorHandle","sym":"T_mozilla::_ipdltest::PTestBasicParent::ActorHandle"}
{"loc":"00029:26-33","source":1,"syntax":"type,use","pretty":"type mozilla::ipc::ByteBuf","sym":"T_mozilla::ipc::ByteBuf","type":"class mozilla::ipc::ByteBuf","typesym":"T_mozilla::ipc::ByteBuf"}
{"loc":"00029:34-41","source":1,"syntax":"def,type","pretty":"type mozilla::_ipdltest::PTestBasicParent::ByteBuf","sym":"T_mozilla::_ipdltest::PTestBasicParent::ByteBuf"}
{"loc":"00030:34-42","source":1,"syntax":"def,type","pretty":"type mozilla::_ipdltest::PTestBasicParent::Endpoint","sym":"T_mozilla::_ipdltest::PTestBasicParent::Endpoint"}
{"loc":"00030:59-67","source":1,"syntax":"type,use","pretty":"type mozilla::ipc::Endpoint","sym":"T_mozilla::ipc::Endpoint"}
{"loc":"00031:26-40","source":1,"syntax":"type,use","pretty":"type mozilla::ipc::FileDescriptor","sym":"T_mozilla::ipc::FileDescriptor","type":"class mozilla::ipc::FileDescriptor","typesym":"T_mozilla::ipc::FileDescriptor"}
{"loc":"00031:41-55","source":1,"syntax":"def,type","pretty":"type mozilla::_ipdltest::PTestBasicParent::FileDescriptor","sym":"T_mozilla::_ipdltest::PTestBasicParent::FileDescriptor"}
{"loc":"00032:34-49","source":1,"syntax":"def,type","pretty":"type mozilla::_ipdltest::PTestBasicParent::ManagedEndpoint","sym":"T_mozilla::_ipdltest::PTestBasicParent::ManagedEndpoint"}
{"loc":"00032:66-81","source":1,"syntax":"type,use","pretty":"type mozilla::ipc::ManagedEndpoint","sym":"T_mozilla::ipc::ManagedEndpoint"}
{"loc":"00033:18-27","source":1,"syntax":"type,use","pretty":"type base::ProcessId","sym":"T_base::ProcessId","type":"base::ProcessId"}
{"loc":"00033:28-37","source":1,"syntax":"def,type","pretty":"type mozilla::_ipdltest::PTestBasicParent::ProcessId","sym":"T_mozilla::_ipdltest::PTestBasicParent::ProcessId"}
{"loc":"00034:26-36","source":1,"syntax":"type,use","pretty":"type mozilla::ipc::ProtocolId","sym":"T_mozilla::ipc::ProtocolId","type":"mozilla::ipc::ProtocolId","typesym":"T_IPCMessageStart"}
{"loc":"00034:37-47","source":1,"syntax":"def,type","pretty":"type mozilla::_ipdltest::PTestBasicParent::ProtocolId","sym":"T_mozilla::_ipdltest::PTestBasicParent::ProtocolId"}
{"loc":"00035:26-46","source":1,"syntax":"type,use","pretty":"type mozilla::ipc::ResponseRejectReason","sym":"T_mozilla::ipc::ResponseRejectReason","type":"enum mozilla::ipc::ResponseRejectReason","typesym":"T_mozilla::ipc::ResponseRejectReason"}
{"loc":"00035:47-67","source":1,"syntax":"def,type","pretty":"type mozilla::_ipdltest::PTestBasicParent::ResponseRejectReason","sym":"T_mozilla::_ipdltest::PTestBasicParent::ResponseRejectReason"}
{"loc":"00036:26-31","source":1,"syntax":"type,use","pretty":"type mozilla::ipc::Shmem","sym":"T_mozilla::ipc::Shmem","type":"class mozilla::ipc::Shmem","typesym":"T_mozilla::ipc::Shmem"}
{"loc":"00036:32-37","source":1,"syntax":"def,type","pretty":"type mozilla::_ipdltest::PTestBasicParent::Shmem","sym":"T_mozilla::_ipdltest::PTestBasicParent::Shmem"}
{"loc":"00037:28-37","source":1,"syntax":"def,type","pretty":"type mozilla::_ipdltest::PTestBasicParent::UniquePtr","sym":"T_mozilla::_ipdltest::PTestBasicParent::UniquePtr"}
{"loc":"00037:49-58","source":1,"syntax":"type,use","pretty":"type mozilla::UniquePtr","sym":"T_mozilla::UniquePtr"}
{"loc":"00041:4-19","source":1,"syntax":"decl,function","pretty":"function mozilla::_ipdltest::PTestBasicParent::ProcessingError","sym":"_ZN7mozilla9_ipdltest16PTestBasicParent15ProcessingErrorENS_3ipc14HasResultCodes6ResultEPKc","type":"void (enum mozilla::ipc::HasResultCodes::Result, const char *)"}
{"loc":"00042:12-18","source":1,"syntax":"type,use","pretty":"type mozilla::ipc::HasResultCodes::Result","sym":"T_mozilla::ipc::HasResultCodes::Result","type":"enum mozilla::ipc::HasResultCodes::Result","typesym":"T_mozilla::ipc::HasResultCodes::Result"}
{"loc":"00042:19-24","source":1,"syntax":"","pretty":"variable aCode","sym":"V_aa9867bb0fd64b7_1ebe00f013,V_a6a3f1252eb231c8_1ebe00f013,V_f4c4431dd0dc88ad_1ebe00f013,V_a92767d907d4caf3_1ebe00f013","no_crossref":1,"type":"enum mozilla::ipc::HasResultCodes::Result","typesym":"T_mozilla::ipc::HasResultCodes::Result"}
{"loc":"00043:24-31","source":1,"syntax":"","pretty":"variable aReason","sym":"V_726177bb0fd64b7_ea97235f0b0d,V_7e6cf1252eb231c8_ea97235f0b0d,V_cc8d431dd0dc88ad_ea97235f0b0d,V_71ff67d907d4caf3_ea97235f0b0d","no_crossref":1,"type":"const char *"}
{"loc":"00045:4-34","source":1,"syntax":"decl,function","pretty":"function mozilla::_ipdltest::PTestBasicParent::ShouldContinueFromReplyTimeout","sym":"_ZN7mozilla9_ipdltest16PTestBasicParent30ShouldContinueFromReplyTimeoutEv","type":"_Bool (void)"}
{"loc":"00048:26-35","source":1,"syntax":"type,use","pretty":"type mozilla::ipc::IProtocol","sym":"T_mozilla::ipc::IProtocol","type":"class mozilla::ipc::IProtocol","typesym":"T_mozilla::ipc::IProtocol"}
{"loc":"00048:36-45","source":1,"syntax":"def,type","pretty":"type mozilla::_ipdltest::PTestBasicParent::IProtocol","sym":"T_mozilla::_ipdltest::PTestBasicParent::IProtocol"}
{"loc":"00049:17-24","source":1,"syntax":"type,use","pretty":"type IPC::Message","sym":"T_IPC::Message","type":"class IPC::Message","typesym":"T_IPC::Message"}
{"loc":"00049:25-32","source":1,"syntax":"def,type","pretty":"type mozilla::_ipdltest::PTestBasicParent::Message","sym":"T_mozilla::_ipdltest::PTestBasicParent::Message"}
{"loc":"00050:18-31","source":1,"syntax":"type,use","pretty":"type base::ProcessHandle","sym":"T_base::ProcessHandle","type":"base::ProcessHandle"}
{"loc":"00050:32-45","source":1,"syntax":"def,type","pretty":"type mozilla::_ipdltest::PTestBasicParent::ProcessHandle","sym":"T_mozilla::_ipdltest::PTestBasicParent::ProcessHandle"}
{"loc":"00051:26-40","source":1,"syntax":"type,use","pretty":"type mozilla::ipc::MessageChannel","sym":"T_mozilla::ipc::MessageChannel","type":"class mozilla::ipc::MessageChannel","typesym":"T_mozilla::ipc::MessageChannel"}
{"loc":"00051:41-55","source":1,"syntax":"def,type","pretty":"type mozilla::_ipdltest::PTestBasicParent::MessageChannel","sym":"T_mozilla::_ipdltest::PTestBasicParent::MessageChannel"}
{"loc":"00052:26-38","source":1,"syntax":"type,use","pretty":"type mozilla::ipc::SharedMemory","sym":"T_mozilla::ipc::SharedMemory","type":"class mozilla::ipc::SharedMemory","typesym":"T_mozilla::ipc::SharedMemory"}
{"loc":"00052:39-51","source":1,"syntax":"def,type","pretty":"type mozilla::_ipdltest::PTestBasicParent::SharedMemory","sym":"T_mozilla::_ipdltest::PTestBasicParent::SharedMemory"}
{"loc":"00055:4-16","source":1,"syntax":"macro,use","pretty":"macro MOZ_IMPLICIT","sym":"M_5f65895d5a5258f7"}
{"loc":"00055:17-33","source":1,"syntax":"decl,function","pretty":"function mozilla::_ipdltest::PTestBasicParent::PTestBasicParent","sym":"_ZN7mozilla9_ipdltest16PTestBasicParentC1Ev","type":"void (void)"}
{"loc":"00057:13-29","source":1,"syntax":"decl,destructor","pretty":"destructor mozilla::_ipdltest::PTestBasicParent::~PTestBasicParent","sym":"_ZN7mozilla9_ipdltest16PTestBasicParentD1Ev","type":"void (void) noexcept"}
{"loc":"00057:13-29","source":1,"syntax":"type,use","pretty":"type mozilla::_ipdltest::PTestBasicParent","sym":"T_mozilla::_ipdltest::PTestBasicParent","type":"class mozilla::_ipdltest::PTestBasicParent","typesym":"T_mozilla::_ipdltest::PTestBasicParent"}
{"loc":"00059:4-43","source":1,"syntax":"def,function","pretty":"function mozilla::_ipdltest::PTestBasicParent::AddRef","sym":"_ZN7mozilla9_ipdltest16PTestBasicParent6AddRefEv","type":"MozExternalRefCountType (void)"}
{"loc":"00059:4-43","source":1,"syntax":"def,function","pretty":"function mozilla::_ipdltest::PTestBasicParent::Release","sym":"_ZN7mozilla9_ipdltest16PTestBasicParent7ReleaseEv","type":"MozExternalRefCountType (void)"}
{"loc":"00059:4-43","source":1,"syntax":"macro,use","pretty":"macro NS_INLINE_DECL_PURE_VIRTUAL_REFCOUNTING","sym":"M_19459f64b289c7f3"}
{"loc":"00061:9-19","source":1,"syntax":"def,function","pretty":"function mozilla::_ipdltest::PTestBasicParent::ActorAlloc","sym":"_ZN7mozilla9_ipdltest16PTestBasicParent10ActorAllocEv","nestingRange":"61:28-61:40","type":"void (void)"}
{"loc":"00061:30-36","source":1,"syntax":"function,use","pretty":"function mozilla::_ipdltest::PTestBasicParent::AddRef","sym":"_ZN7mozilla9_ipdltest16PTestBasicParent6AddRefEv","type":"MozExternalRefCountType"}
{"loc":"00062:9-21","source":1,"syntax":"def,function","pretty":"function mozilla::_ipdltest::PTestBasicParent::ActorDealloc","sym":"_ZN7mozilla9_ipdltest16PTestBasicParent12ActorDeallocEv","nestingRange":"62:30-62:43","type":"void (void)"}
{"loc":"00062:32-39","source":1,"syntax":"function,use","pretty":"function mozilla::_ipdltest::PTestBasicParent::Release","sym":"_ZN7mozilla9_ipdltest16PTestBasicParent7ReleaseEv","type":"MozExternalRefCountType"}
{"loc":"00065:4-20","source":1,"syntax":"decl,function","pretty":"function mozilla::_ipdltest::PTestBasicParent::AllManagedActors","sym":"_ZNK7mozilla9_ipdltest16PTestBasicParent16AllManagedActorsER8nsTArrayI6RefPtrINS_3ipc19ActorLifecycleProxyEEE","type":"void (nsTArray<RefPtr<mozilla::ipc::ActorLifecycleProxy> > &) const"}
{"loc":"00065:21-29","source":1,"syntax":"type,use","pretty":"type nsTArray","sym":"T_nsTArray"}
{"loc":"00065:30-36","source":1,"syntax":"type,use","pretty":"type RefPtr","sym":"T_RefPtr"}
{"loc":"00065:51-70","source":1,"syntax":"type,use","pretty":"type mozilla::ipc::ActorLifecycleProxy","sym":"T_mozilla::ipc::ActorLifecycleProxy","type":"class mozilla::ipc::ActorLifecycleProxy","typesym":"T_mozilla::ipc::ActorLifecycleProxy"}
{"loc":"00065:74-79","source":1,"syntax":"","pretty":"variable arr__","sym":"V_0906c9bb0fd64b7_8cdba1f013,V_051154252eb231c8_8cdba1f013,V_5332a51dd0dc88ad_8cdba1f013,V_0894c9d907d4caf3_8cdba1f013","no_crossref":1,"type":"nsTArray<RefPtr<mozilla::ipc::ActorLifecycleProxy> > &"}
{"loc":"00068:4-13","source":1,"syntax":"decl,function","pretty":"function mozilla::_ipdltest::PTestBasicParent::SendHello","sym":"_ZN7mozilla9_ipdltest16PTestBasicParent9SendHelloEv","type":"_Bool (void)"}
{"loc":"00071:4-17","source":1,"syntax":"decl,function","pretty":"function mozilla::_ipdltest::PTestBasicParent::RemoveManagee","sym":"_ZN7mozilla9_ipdltest16PTestBasicParent13RemoveManageeEiPNS_3ipc9IProtocolE","type":"void (int32_t, mozilla::_ipdltest::PTestBasicParent::IProtocol *)"}
{"loc":"00072:12-19","source":1,"syntax":"type,use","pretty":"type int32_t","sym":"T_int32_t","type":"int32_t"}
{"loc":"00072:20-31","source":1,"syntax":"","pretty":"variable aProtocolId","sym":"V_543dcabb0fd64b7_542cb52f47aba60c,V_504855252eb231c8_542cb52f47aba60c,V_ae59a61dd0dc88ad_542cb52f47aba60c,V_53cbcad907d4caf3_542cb52f47aba60c","no_crossref":1,"type":"int32_t"}
{"loc":"00073:12-21","source":1,"syntax":"type,use","pretty":"type mozilla::_ipdltest::PTestBasicParent::IProtocol","sym":"T_mozilla::_ipdltest::PTestBasicParent::IProtocol","type":"mozilla::_ipdltest::PTestBasicParent::IProtocol","typesym":"T_mozilla::ipc::IProtocol"}
{"loc":"00073:23-32","source":1,"syntax":"","pretty":"variable aListener","sym":"V_9af5dabb0fd64b7_c681f10e880c773,V_960165252eb231c8_c681f10e880c773,V_e422b61dd0dc88ad_c681f10e880c773,V_9984dad907d4caf3_c681f10e880c773","no_crossref":1,"type":"mozilla::_ipdltest::PTestBasicParent::IProtocol *"}
{"loc":"00075:4-18","source":1,"syntax":"decl,function","pretty":"function mozilla::_ipdltest::PTestBasicParent::DeallocManagee","sym":"_ZN7mozilla9_ipdltest16PTestBasicParent14DeallocManageeEiPNS_3ipc9IProtocolE","type":"void (int32_t, mozilla::_ipdltest::PTestBasicParent::IProtocol *)"}
{"loc":"00076:12-19","source":1,"syntax":"type,use","pretty":"type int32_t","sym":"T_int32_t","type":"int32_t"}
{"loc":"00076:20-31","source":1,"syntax":"","pretty":"variable aProtocolId","sym":"V_9c40fabb0fd64b7_542cb52f47aba60c,V_985b75252eb231c8_542cb52f47aba60c,V_e67cc61dd0dc88ad_542cb52f47aba60c,V_9bdeead907d4caf3_542cb52f47aba60c","no_crossref":1,"type":"int32_t"}
{"loc":"00077:12-21","source":1,"syntax":"type,use","pretty":"type mozilla::_ipdltest::PTestBasicParent::IProtocol","sym":"T_mozilla::_ipdltest::PTestBasicParent::IProtocol","type":"mozilla::_ipdltest::PTestBasicParent::IProtocol","typesym":"T_mozilla::ipc::IProtocol"}
{"loc":"00077:23-32","source":1,"syntax":"","pretty":"variable aListener","sym":"V_d219fabb0fd64b7_c681f10e880c773,V_de1485252eb231c8_c681f10e880c773,V_2d35d61dd0dc88ad_c681f10e880c773,V_d1a7fad907d4caf3_c681f10e880c773","no_crossref":1,"type":"mozilla::_ipdltest::PTestBasicParent::IProtocol *"}
{"loc":"00079:4-10","source":1,"syntax":"type,use","pretty":"type mozilla::ipc::HasResultCodes::Result","sym":"T_mozilla::ipc::HasResultCodes::Result","type":"enum mozilla::ipc::HasResultCodes::Result","typesym":"T_mozilla::ipc::HasResultCodes::Result"}
{"loc":"00080:4-21","source":1,"syntax":"decl,function","pretty":"function mozilla::_ipdltest::PTestBasicParent::OnMessageReceived","sym":"_ZN7mozilla9_ipdltest16PTestBasicParent17OnMessageReceivedERKN3IPC7MessageE","type":"enum mozilla::ipc::HasResultCodes::Result (const mozilla::_ipdltest::PTestBasicParent::Message &)"}
{"loc":"00080:28-35","source":1,"syntax":"type,use","pretty":"type mozilla::_ipdltest::PTestBasicParent::Message","sym":"T_mozilla::_ipdltest::PTestBasicParent::Message","type":"mozilla::_ipdltest::PTestBasicParent::Message","typesym":"T_IPC::Message"}
{"loc":"00080:37-42","source":1,"syntax":"","pretty":"variable msg__","sym":"V_c23ddbbb0fd64b7_a6144ff013,V_ce3866252eb231c8_a6144ff013,V_1d59b71dd0dc88ad_a6144ff013,V_c1cbdbd907d4caf3_a6144ff013","no_crossref":1,"type":"const mozilla::_ipdltest::PTestBasicParent::Message &"}
{"loc":"00082:4-10","source":1,"syntax":"type,use","pretty":"type mozilla::ipc::HasResultCodes::Result","sym":"T_mozilla::ipc::HasResultCodes::Result","type":"enum mozilla::ipc::HasResultCodes::Result","typesym":"T_mozilla::ipc::HasResultCodes::Result"}
{"loc":"00083:4-21","source":1,"syntax":"decl,function","pretty":"function mozilla::_ipdltest::PTestBasicParent::OnMessageReceived","sym":"_ZN7mozilla9_ipdltest16PTestBasicParent17OnMessageReceivedERKN3IPC7MessageERNS_9UniquePtrIS3_NS_13DefaultDeleteIS3_EEEE","type":"enum mozilla::ipc::HasResultCodes::Result (const mozilla::_ipdltest::PTestBasicParent::Message &, UniquePtr<mozilla::_ipdltest::PTestBasicParent::Message> &)"}
{"loc":"00084:18-25","source":1,"syntax":"type,use","pretty":"type mozilla::_ipdltest::PTestBasicParent::Message","sym":"T_mozilla::_ipdltest::PTestBasicParent::Message","type":"mozilla::_ipdltest::PTestBasicParent::Message","typesym":"T_IPC::Message"}
{"loc":"00084:27-32","source":1,"syntax":"","pretty":"variable msg__","sym":"V_f8400cbb0fd64b7_a6144ff013,V_f45b86252eb231c8_a6144ff013,V_437cd71dd0dc88ad_a6144ff013,V_f7defbd907d4caf3_a6144ff013","no_crossref":1,"type":"const mozilla::_ipdltest::PTestBasicParent::Message &"}
{"loc":"00085:12-21","source":1,"syntax":"type,use","pretty":"type mozilla::_ipdltest::PTestBasicParent::UniquePtr","sym":"T_mozilla::_ipdltest::PTestBasicParent::UniquePtr"}
{"loc":"00085:22-29","source":1,"syntax":"type,use","pretty":"type mozilla::_ipdltest::PTestBasicParent::Message","sym":"T_mozilla::_ipdltest::PTestBasicParent::Message","type":"mozilla::_ipdltest::PTestBasicParent::Message","typesym":"T_IPC::Message"}
{"loc":"00085:32-39","source":1,"syntax":"","pretty":"variable reply__","sym":"V_c0190cbb0fd64b7_f83bfee36b0d,V_cc1496252eb231c8_f83bfee36b0d,V_1b35e71dd0dc88ad_f83bfee36b0d,V_cf970cd907d4caf3_f83bfee36b0d","no_crossref":1,"type":"UniquePtr<mozilla::_ipdltest::PTestBasicParent::Message> &"}
{"loc":"00087:4-10","source":1,"syntax":"type,use","pretty":"type mozilla::ipc::HasResultCodes::Result","sym":"T_mozilla::ipc::HasResultCodes::Result","type":"enum mozilla::ipc::HasResultCodes::Result","typesym":"T_mozilla::ipc::HasResultCodes::Result"}
{"loc":"00088:4-18","source":1,"syntax":"decl,function","pretty":"function mozilla::_ipdltest::PTestBasicParent::OnCallReceived","sym":"_ZN7mozilla9_ipdltest16PTestBasicParent14OnCallReceivedERKN3IPC7MessageERNS_9UniquePtrIS3_NS_13DefaultDeleteIS3_EEEE","type":"enum mozilla::ipc::HasResultCodes::Result (const mozilla::_ipdltest::PTestBasicParent::Message &, UniquePtr<mozilla::_ipdltest::PTestBasicParent::Message> &)"}
{"loc":"00089:18-25","source":1,"syntax":"type,use","pretty":"type mozilla::_ipdltest::PTestBasicParent::Message","sym":"T_mozilla::_ipdltest::PTestBasicParent::Message","type":"mozilla::_ipdltest::PTestBasicParent::Message","typesym":"T_IPC::Message"}
{"loc":"00089:27-32","source":1,"syntax":"","pretty":"variable msg__","sym":"V_472c2cbb0fd64b7_a6144ff013,V_4337b6252eb231c8_a6144ff013,V_9158081dd0dc88ad_a6144ff013,V_46ba2cd907d4caf3_a6144ff013","no_crossref":1,"type":"const mozilla::_ipdltest::PTestBasicParent::Message &"}
{"loc":"00090:12-21","source":1,"syntax":"type,use","pretty":"type mozilla::_ipdltest::PTestBasicParent::UniquePtr","sym":"T_mozilla::_ipdltest::PTestBasicParent::UniquePtr"}
{"loc":"00090:22-29","source":1,"syntax":"type,use","pretty":"type mozilla::_ipdltest::PTestBasicParent::Message","sym":"T_mozilla::_ipdltest::PTestBasicParent::Message","type":"mozilla::_ipdltest::PTestBasicParent::Message","typesym":"T_IPC::Message"}
{"loc":"00090:32-39","source":1,"syntax":"","pretty":"variable reply__","sym":"V_8abefcbb0fd64b7_f83bfee36b0d,V_86c987252eb231c8_f83bfee36b0d,V_d4ead81dd0dc88ad_f83bfee36b0d,V_894dfcd907d4caf3_f83bfee36b0d","no_crossref":1,"type":"UniquePtr<mozilla::_ipdltest::PTestBasicParent::Message> &"}
{"loc":"00093:4-18","source":1,"syntax":"decl,function","pretty":"function mozilla::_ipdltest::PTestBasicParent::OnChannelClose","sym":"_ZN7mozilla9_ipdltest16PTestBasicParent14OnChannelCloseEv","type":"void (void)"}
{"loc":"00096:4-18","source":1,"syntax":"decl,function","pretty":"function mozilla::_ipdltest::PTestBasicParent::OnChannelError","sym":"_ZN7mozilla9_ipdltest16PTestBasicParent14OnChannelErrorEv","type":"void (void)"}
{"loc":"00100:4-16","source":1,"syntax":"decl,function","pretty":"function mozilla::_ipdltest::PTestBasicParent::ClearSubtree","sym":"_ZN7mozilla9_ipdltest16PTestBasicParent12ClearSubtreeEv","type":"void (void)"}
{"loc":"00107:10-13","source":1,"syntax":"def,namespace","pretty":"namespace IPC","sym":"NS_IPC","nestingRange":"107:15-121:0"}
{"loc":"00109:7-18","source":1,"syntax":"def,type,use","pretty":"type IPC::ParamTraits","sym":"T_IPC::ParamTraits","nestingRange":"110:0-120:0"}
{"loc":"00109:40-56","source":1,"syntax":"type,use","pretty":"type mozilla::_ipdltest::PTestBasicParent","sym":"T_mozilla::_ipdltest::PTestBasicParent","type":"class mozilla::_ipdltest::PTestBasicParent","typesym":"T_mozilla::_ipdltest::PTestBasicParent"}
{"loc":"00111:32-48","source":1,"syntax":"type,use","pretty":"type mozilla::_ipdltest::PTestBasicParent","sym":"T_mozilla::_ipdltest::PTestBasicParent","type":"class mozilla::_ipdltest::PTestBasicParent","typesym":"T_mozilla::_ipdltest::PTestBasicParent"}
{"loc":"00111:50-59","source":1,"syntax":"def,type","pretty":"type IPC::ParamTraits<mozilla::_ipdltest::PTestBasicParent *>::paramType","sym":"T_IPC::ParamTraits<mozilla::_ipdltest::PTestBasicParent_*>::paramType"}
{"loc":"00113:4-9","source":1,"syntax":"decl,function","pretty":"function IPC::ParamTraits<mozilla::_ipdltest::PTestBasicParent *>::Write","sym":"_ZN3IPC11ParamTraitsIPN7mozilla9_ipdltest16PTestBasicParentEE5WriteEPNS_13MessageWriterERKS4_","type":"void (IPC::MessageWriter *, const IPC::ParamTraits<class mozilla::_ipdltest::PTestBasicParent *>::paramType &)"}
{"loc":"00114:17-30","source":1,"syntax":"type,use","pretty":"type IPC::MessageWriter","sym":"T_IPC::MessageWriter","type":"class IPC::MessageWriter","typesym":"T_IPC::MessageWriter"}
{"loc":"00114:32-39","source":1,"syntax":"","pretty":"variable aWriter","sym":"V_5c1869db0fd64b7_369ebc101b0d,V_5823f3452eb231c8_369ebc101b0d,V_a644453dd0dc88ad_369ebc101b0d,V_5ba669f907d4caf3_369ebc101b0d","no_crossref":1,"type":"IPC::MessageWriter *"}
{"loc":"00115:18-27","source":1,"syntax":"type,use","pretty":"type IPC::ParamTraits<mozilla::_ipdltest::PTestBasicParent *>::paramType","sym":"T_IPC::ParamTraits<mozilla::_ipdltest::PTestBasicParent_*>::paramType","type":"IPC::ParamTraits<class mozilla::_ipdltest::PTestBasicParent *>::paramType"}
{"loc":"00115:29-33","source":1,"syntax":"","pretty":"variable aVar","sym":"V_c0e079db0fd64b7_f0bb39c71,V_ccebf3452eb231c8_f0bb39c71,V_1b0d453dd0dc88ad_f0bb39c71,V_cf6f69f907d4caf3_f0bb39c71","no_crossref":1,"type":"const IPC::ParamTraits<class mozilla::_ipdltest::PTestBasicParent *>::paramType &"}
{"loc":"00117:4-8","source":1,"syntax":"decl,function","pretty":"function IPC::ParamTraits<mozilla::_ipdltest::PTestBasicParent *>::Read","sym":"_ZN3IPC11ParamTraitsIPN7mozilla9_ipdltest16PTestBasicParentEE4ReadEPNS_13MessageReaderEPS4_","type":"_Bool (IPC::MessageReader *, IPC::ParamTraits<class mozilla::_ipdltest::PTestBasicParent *>::paramType *)"}
{"loc":"00118:17-30","source":1,"syntax":"type,use","pretty":"type IPC::MessageReader","sym":"T_IPC::MessageReader","type":"class IPC::MessageReader","typesym":"T_IPC::MessageReader"}
{"loc":"00118:32-39","source":1,"syntax":"","pretty":"variable aReader","sym":"V_943b89db0fd64b7_9983235f0b0d,V_904614452eb231c8_9983235f0b0d,V_ee57653dd0dc88ad_9983235f0b0d,V_93c989f907d4caf3_9983235f0b0d","no_crossref":1,"type":"IPC::MessageReader *"}
{"loc":"00119:12-21","source":1,"syntax":"type,use","pretty":"type IPC::ParamTraits<mozilla::_ipdltest::PTestBasicParent *>::paramType","sym":"T_IPC::ParamTraits<mozilla::_ipdltest::PTestBasicParent_*>::paramType","type":"IPC::ParamTraits<class mozilla::_ipdltest::PTestBasicParent *>::paramType"}
{"loc":"00119:23-27","source":1,"syntax":"","pretty":"variable aVar","sym":"V_a8f399db0fd64b7_f0bb39c71,V_a40f14452eb231c8_f0bb39c71,V_f220753dd0dc88ad_f0bb39c71,V_a78299f907d4caf3_f0bb39c71","no_crossref":1,"type":"IPC::ParamTraits<class mozilla::_ipdltest::PTestBasicParent *>::paramType *"}
{"loc":"00109:7-18","structured":1,"pretty":"IPC::ParamTraits","sym":"T_IPC::ParamTraits","kind":"struct","implKind":"","sizeBytes":1,"supers":[],"methods":[{"pretty":"IPC::ParamTraits<mozilla::_ipdltest::PTestBasicParent *>::Write","sym":"_ZN3IPC11ParamTraitsIPN7mozilla9_ipdltest16PTestBasicParentEE5WriteEPNS_13MessageWriterERKS4_","props":["static","user"]},{"pretty":"IPC::ParamTraits<mozilla::_ipdltest::PTestBasicParent *>::Read","sym":"_ZN3IPC11ParamTraitsIPN7mozilla9_ipdltest16PTestBasicParentEE4ReadEPNS_13MessageReaderEPS4_","props":["static","user"]}],"fields":[],"overrides":[],"props":[]}
{"loc":"00023:6-22","structured":1,"pretty":"mozilla::_ipdltest::PTestBasicParent","sym":"T_mozilla::_ipdltest::PTestBasicParent","kind":"class","implKind":"","sizeBytes":368,"supers":[{"pretty":"mozilla::ipc::IToplevelProtocol","sym":"T_mozilla::ipc::IToplevelProtocol","props":[]}],"methods":[{"pretty":"mozilla::_ipdltest::PTestBasicParent::ProcessingError","sym":"_ZN7mozilla9_ipdltest16PTestBasicParent15ProcessingErrorENS_3ipc14HasResultCodes6ResultEPKc","props":["instance","virtual","user"]},{"pretty":"mozilla::_ipdltest::PTestBasicParent::ShouldContinueFromReplyTimeout","sym":"_ZN7mozilla9_ipdltest16PTestBasicParent30ShouldContinueFromReplyTimeoutEv","props":["instance","virtual","user"]},{"pretty":"mozilla::_ipdltest::PTestBasicParent::PTestBasicParent","sym":"_ZN7mozilla9_ipdltest16PTestBasicParentC1Ev","props":["instance","user"]},{"pretty":"mozilla::_ipdltest::PTestBasicParent::~PTestBasicParent","sym":"_ZN7mozilla9_ipdltest16PTestBasicParentD1Ev","props":["instance","virtual","user"]},{"pretty":"mozilla::_ipdltest::PTestBasicParent::AddRef","sym":"_ZN7mozilla9_ipdltest16PTestBasicParent6AddRefEv","props":["instance","virtual","user"]},{"pretty":"mozilla::_ipdltest::PTestBasicParent::Release","sym":"_ZN7mozilla9_ipdltest16PTestBasicParent7ReleaseEv","props":["instance","virtual","user"]},{"pretty":"mozilla::_ipdltest::PTestBasicParent::ActorAlloc","sym":"_ZN7mozilla9_ipdltest16PTestBasicParent10ActorAllocEv","props":["instance","virtual","user"]},{"pretty":"mozilla::_ipdltest::PTestBasicParent::ActorDealloc","sym":"_ZN7mozilla9_ipdltest16PTestBasicParent12ActorDeallocEv","props":["instance","virtual","user"]},{"pretty":"mozilla::_ipdltest::PTestBasicParent::AllManagedActors","sym":"_ZNK7mozilla9_ipdltest16PTestBasicParent16AllManagedActorsER8nsTArrayI6RefPtrINS_3ipc19ActorLifecycleProxyEEE","props":["instance","virtual","user"]},{"pretty":"mozilla::_ipdltest::PTestBasicParent::SendHello","sym":"_ZN7mozilla9_ipdltest16PTestBasicParent9SendHelloEv","props":["instance","user"]},{"pretty":"mozilla::_ipdltest::PTestBasicParent::RemoveManagee","sym":"_ZN7mozilla9_ipdltest16PTestBasicParent13RemoveManageeEiPNS_3ipc9IProtocolE","props":["instance","virtual","user"]},{"pretty":"mozilla::_ipdltest::PTestBasicParent::DeallocManagee","sym":"_ZN7mozilla9_ipdltest16PTestBasicParent14DeallocManageeEiPNS_3ipc9IProtocolE","props":["instance","virtual","user"]},{"pretty":"mozilla::_ipdltest::PTestBasicParent::OnMessageReceived","sym":"_ZN7mozilla9_ipdltest16PTestBasicParent17OnMessageReceivedERKN3IPC7MessageE","props":["instance","virtual","user"]},{"pretty":"mozilla::_ipdltest::PTestBasicParent::OnMessageReceived","sym":"_ZN7mozilla9_ipdltest16PTestBasicParent17OnMessageReceivedERKN3IPC7MessageERNS_9UniquePtrIS3_NS_13DefaultDeleteIS3_EEEE","props":["instance","virtual","user"]},{"pretty":"mozilla::_ipdltest::PTestBasicParent::OnCallReceived","sym":"_ZN7mozilla9_ipdltest16PTestBasicParent14OnCallReceivedERKN3IPC7MessageERNS_9UniquePtrIS3_NS_13DefaultDeleteIS3_EEEE","props":["instance","virtual","user"]},{"pretty":"mozilla::_ipdltest::PTestBasicParent::OnChannelClose","sym":"_ZN7mozilla9_ipdltest16PTestBasicParent14OnChannelCloseEv","props":["instance","virtual","user"]},{"pretty":"mozilla::_ipdltest::PTestBasicParent::OnChannelError","sym":"_ZN7mozilla9_ipdltest16PTestBasicParent14OnChannelErrorEv","props":["instance","virtual","user"]},{"pretty":"mozilla::_ipdltest::PTestBasicParent::ClearSubtree","sym":"_ZN7mozilla9_ipdltest16PTestBasicParent12ClearSubtreeEv","props":["instance","user"]},{"pretty":"mozilla::_ipdltest::PTestBasicParent::PTestBasicParent","sym":"_ZN7mozilla9_ipdltest16PTestBasicParentC1ERKS1_","props":["instance","defaulted","deleted"]},{"pretty":"mozilla::_ipdltest::PTestBasicParent::operator=","sym":"_ZN7mozilla9_ipdltest16PTestBasicParentaSERKS1_","props":["instance","defaulted","deleted"]}],"fields":[],"overrides":[],"props":[],"platforms":["win64"],"variants":[{"structured":1,"pretty":"mozilla::_ipdltest::PTestBasicParent","sym":"T_mozilla::_ipdltest::PTestBasicParent","kind":"class","implKind":"","sizeBytes":224,"supers":[{"pretty":"mozilla::ipc::IToplevelProtocol","sym":"T_mozilla::ipc::IToplevelProtocol","props":[]}],"methods":[{"pretty":"mozilla::_ipdltest::PTestBasicParent::ProcessingError","sym":"_ZN7mozilla9_ipdltest16PTestBasicParent15ProcessingErrorENS_3ipc14HasResultCodes6ResultEPKc","props":["instance","virtual","user"]},{"pretty":"mozilla::_ipdltest::PTestBasicParent::ShouldContinueFromReplyTimeout","sym":"_ZN7mozilla9_ipdltest16PTestBasicParent30ShouldContinueFromReplyTimeoutEv","props":["instance","virtual","user"]},{"pretty":"mozilla::_ipdltest::PTestBasicParent::PTestBasicParent","sym":"_ZN7mozilla9_ipdltest16PTestBasicParentC1Ev","props":["instance","user"]},{"pretty":"mozilla::_ipdltest::PTestBasicParent::~PTestBasicParent","sym":"_ZN7mozilla9_ipdltest16PTestBasicParentD1Ev","props":["instance","virtual","user"]},{"pretty":"mozilla::_ipdltest::PTestBasicParent::AddRef","sym":"_ZN7mozilla9_ipdltest16PTestBasicParent6AddRefEv","props":["instance","virtual","user"]},{"pretty":"mozilla::_ipdltest::PTestBasicParent::Release","sym":"_ZN7mozilla9_ipdltest16PTestBasicParent7ReleaseEv","props":["instance","virtual","user"]},{"pretty":"mozilla::_ipdltest::PTestBasicParent::ActorAlloc","sym":"_ZN7mozilla9_ipdltest16PTestBasicParent10ActorAllocEv","props":["instance","virtual","user"]},{"pretty":"mozilla::_ipdltest::PTestBasicParent::ActorDealloc","sym":"_ZN7mozilla9_ipdltest16PTestBasicParent12ActorDeallocEv","props":["instance","virtual","user"]},{"pretty":"mozilla::_ipdltest::PTestBasicParent::AllManagedActors","sym":"_ZNK7mozilla9_ipdltest16PTestBasicParent16AllManagedActorsER8nsTArrayI6RefPtrINS_3ipc19ActorLifecycleProxyEEE","props":["instance","virtual","user"]},{"pretty":"mozilla::_ipdltest::PTestBasicParent::SendHello","sym":"_ZN7mozilla9_ipdltest16PTestBasicParent9SendHelloEv","props":["instance","user"]},{"pretty":"mozilla::_ipdltest::PTestBasicParent::RemoveManagee","sym":"_ZN7mozilla9_ipdltest16PTestBasicParent13RemoveManageeEiPNS_3ipc9IProtocolE","props":["instance","virtual","user"]},{"pretty":"mozilla::_ipdltest::PTestBasicParent::DeallocManagee","sym":"_ZN7mozilla9_ipdltest16PTestBasicParent14DeallocManageeEiPNS_3ipc9IProtocolE","props":["instance","virtual","user"]},{"pretty":"mozilla::_ipdltest::PTestBasicParent::OnMessageReceived","sym":"_ZN7mozilla9_ipdltest16PTestBasicParent17OnMessageReceivedERKN3IPC7MessageE","props":["instance","virtual","user"]},{"pretty":"mozilla::_ipdltest::PTestBasicParent::OnMessageReceived","sym":"_ZN7mozilla9_ipdltest16PTestBasicParent17OnMessageReceivedERKN3IPC7MessageERNS_9UniquePtrIS3_NS_13DefaultDeleteIS3_EEEE","props":["instance","virtual","user"]},{"pretty":"mozilla::_ipdltest::PTestBasicParent::OnCallReceived","sym":"_ZN7mozilla9_ipdltest16PTestBasicParent14OnCallReceivedERKN3IPC7MessageERNS_9UniquePtrIS3_NS_13DefaultDeleteIS3_EEEE","props":["instance","virtual","user"]},{"pretty":"mozilla::_ipdltest::PTestBasicParent::OnChannelClose","sym":"_ZN7mozilla9_ipdltest16PTestBasicParent14OnChannelCloseEv","props":["instance","virtual","user"]},{"pretty":"mozilla::_ipdltest::PTestBasicParent::OnChannelError","sym":"_ZN7mozilla9_ipdltest16PTestBasicParent14OnChannelErrorEv","props":["instance","virtual","user"]},{"pretty":"mozilla::_ipdltest::PTestBasicParent::ClearSubtree","sym":"_ZN7mozilla9_ipdltest16PTestBasicParent12ClearSubtreeEv","props":["instance","user"]},{"pretty":"mozilla::_ipdltest::PTestBasicParent::PTestBasicParent","sym":"_ZN7mozilla9_ipdltest16PTestBasicParentC1ERKS1_","props":["instance","defaulted","deleted"]},{"pretty":"mozilla::_ipdltest::PTestBasicParent::operator=","sym":"_ZN7mozilla9_ipdltest16PTestBasicParentaSERKS1_","props":["instance","defaulted","deleted"]}],"fields":[],"overrides":[],"props":[],"platforms":["android-armv7"]},{"structured":1,"pretty":"mozilla::_ipdltest::PTestBasicParent","sym":"T_mozilla::_ipdltest::PTestBasicParent","kind":"class","implKind":"","sizeBytes":376,"supers":[{"pretty":"mozilla::ipc::IToplevelProtocol","sym":"T_mozilla::ipc::IToplevelProtocol","props":[]}],"methods":[{"pretty":"mozilla::_ipdltest::PTestBasicParent::ProcessingError","sym":"_ZN7mozilla9_ipdltest16PTestBasicParent15ProcessingErrorENS_3ipc14HasResultCodes6ResultEPKc","props":["instance","virtual","user"]},{"pretty":"mozilla::_ipdltest::PTestBasicParent::ShouldContinueFromReplyTimeout","sym":"_ZN7mozilla9_ipdltest16PTestBasicParent30ShouldContinueFromReplyTimeoutEv","props":["instance","virtual","user"]},{"pretty":"mozilla::_ipdltest::PTestBasicParent::PTestBasicParent","sym":"_ZN7mozilla9_ipdltest16PTestBasicParentC1Ev","props":["instance","user"]},{"pretty":"mozilla::_ipdltest::PTestBasicParent::~PTestBasicParent","sym":"_ZN7mozilla9_ipdltest16PTestBasicParentD1Ev","props":["instance","virtual","user"]},{"pretty":"mozilla::_ipdltest::PTestBasicParent::AddRef","sym":"_ZN7mozilla9_ipdltest16PTestBasicParent6AddRefEv","props":["instance","virtual","user"]},{"pretty":"mozilla::_ipdltest::PTestBasicParent::Release","sym":"_ZN7mozilla9_ipdltest16PTestBasicParent7ReleaseEv","props":["instance","virtual","user"]},{"pretty":"mozilla::_ipdltest::PTestBasicParent::ActorAlloc","sym":"_ZN7mozilla9_ipdltest16PTestBasicParent10ActorAllocEv","props":["instance","virtual","user"]},{"pretty":"mozilla::_ipdltest::PTestBasicParent::ActorDealloc","sym":"_ZN7mozilla9_ipdltest16PTestBasicParent12ActorDeallocEv","props":["instance","virtual","user"]},{"pretty":"mozilla::_ipdltest::PTestBasicParent::AllManagedActors","sym":"_ZNK7mozilla9_ipdltest16PTestBasicParent16AllManagedActorsER8nsTArrayI6RefPtrINS_3ipc19ActorLifecycleProxyEEE","props":["instance","virtual","user"]},{"pretty":"mozilla::_ipdltest::PTestBasicParent::SendHello","sym":"_ZN7mozilla9_ipdltest16PTestBasicParent9SendHelloEv","props":["instance","user"]},{"pretty":"mozilla::_ipdltest::PTestBasicParent::RemoveManagee","sym":"_ZN7mozilla9_ipdltest16PTestBasicParent13RemoveManageeEiPNS_3ipc9IProtocolE","props":["instance","virtual","user"]},{"pretty":"mozilla::_ipdltest::PTestBasicParent::DeallocManagee","sym":"_ZN7mozilla9_ipdltest16PTestBasicParent14DeallocManageeEiPNS_3ipc9IProtocolE","props":["instance","virtual","user"]},{"pretty":"mozilla::_ipdltest::PTestBasicParent::OnMessageReceived","sym":"_ZN7mozilla9_ipdltest16PTestBasicParent17OnMessageReceivedERKN3IPC7MessageE","props":["instance","virtual","user"]},{"pretty":"mozilla::_ipdltest::PTestBasicParent::OnMessageReceived","sym":"_ZN7mozilla9_ipdltest16PTestBasicParent17OnMessageReceivedERKN3IPC7MessageERNS_9UniquePtrIS3_NS_13DefaultDeleteIS3_EEEE","props":["instance","virtual","user"]},{"pretty":"mozilla::_ipdltest::PTestBasicParent::OnCallReceived","sym":"_ZN7mozilla9_ipdltest16PTestBasicParent14OnCallReceivedERKN3IPC7MessageERNS_9UniquePtrIS3_NS_13DefaultDeleteIS3_EEEE","props":["instance","virtual","user"]},{"pretty":"mozilla::_ipdltest::PTestBasicParent::OnChannelClose","sym":"_ZN7mozilla9_ipdltest16PTestBasicParent14OnChannelCloseEv","props":["instance","virtual","user"]},{"pretty":"mozilla::_ipdltest::PTestBasicParent::OnChannelError","sym":"_ZN7mozilla9_ipdltest16PTestBasicParent14OnChannelErrorEv","props":["instance","virtual","user"]},{"pretty":"mozilla::_ipdltest::PTestBasicParent::ClearSubtree","sym":"_ZN7mozilla9_ipdltest16PTestBasicParent12ClearSubtreeEv","props":["instance","user"]},{"pretty":"mozilla::_ipdltest::PTestBasicParent::PTestBasicParent","sym":"_ZN7mozilla9_ipdltest16PTestBasicParentC1ERKS1_","props":["instance","defaulted","deleted"]},{"pretty":"mozilla::_ipdltest::PTestBasicParent::operator=","sym":"_ZN7mozilla9_ipdltest16PTestBasicParentaSERKS1_","props":["instance","defaulted","deleted"]}],"fields":[],"overrides":[],"props":[],"platforms":["linux64"]},{"structured":1,"pretty":"mozilla::_ipdltest::PTestBasicParent","sym":"T_mozilla::_ipdltest::PTestBasicParent","kind":"class","implKind":"","sizeBytes":352,"supers":[{"pretty":"mozilla::ipc::IToplevelProtocol","sym":"T_mozilla::ipc::IToplevelProtocol","props":[]}],"methods":[{"pretty":"mozilla::_ipdltest::PTestBasicParent::ProcessingError","sym":"_ZN7mozilla9_ipdltest16PTestBasicParent15ProcessingErrorENS_3ipc14HasResultCodes6ResultEPKc","props":["instance","virtual","user"]},{"pretty":"mozilla::_ipdltest::PTestBasicParent::ShouldContinueFromReplyTimeout","sym":"_ZN7mozilla9_ipdltest16PTestBasicParent30ShouldContinueFromReplyTimeoutEv","props":["instance","virtual","user"]},{"pretty":"mozilla::_ipdltest::PTestBasicParent::PTestBasicParent","sym":"_ZN7mozilla9_ipdltest16PTestBasicParentC1Ev","props":["instance","user"]},{"pretty":"mozilla::_ipdltest::PTestBasicParent::~PTestBasicParent","sym":"_ZN7mozilla9_ipdltest16PTestBasicParentD1Ev","props":["instance","virtual","user"]},{"pretty":"mozilla::_ipdltest::PTestBasicParent::AddRef","sym":"_ZN7mozilla9_ipdltest16PTestBasicParent6AddRefEv","props":["instance","virtual","user"]},{"pretty":"mozilla::_ipdltest::PTestBasicParent::Release","sym":"_ZN7mozilla9_ipdltest16PTestBasicParent7ReleaseEv","props":["instance","virtual","user"]},{"pretty":"mozilla::_ipdltest::PTestBasicParent::ActorAlloc","sym":"_ZN7mozilla9_ipdltest16PTestBasicParent10ActorAllocEv","props":["instance","virtual","user"]},{"pretty":"mozilla::_ipdltest::PTestBasicParent::ActorDealloc","sym":"_ZN7mozilla9_ipdltest16PTestBasicParent12ActorDeallocEv","props":["instance","virtual","user"]},{"pretty":"mozilla::_ipdltest::PTestBasicParent::AllManagedActors","sym":"_ZNK7mozilla9_ipdltest16PTestBasicParent16AllManagedActorsER8nsTArrayI6RefPtrINS_3ipc19ActorLifecycleProxyEEE","props":["instance","virtual","user"]},{"pretty":"mozilla::_ipdltest::PTestBasicParent::SendHello","sym":"_ZN7mozilla9_ipdltest16PTestBasicParent9SendHelloEv","props":["instance","user"]},{"pretty":"mozilla::_ipdltest::PTestBasicParent::RemoveManagee","sym":"_ZN7mozilla9_ipdltest16PTestBasicParent13RemoveManageeEiPNS_3ipc9IProtocolE","props":["instance","virtual","user"]},{"pretty":"mozilla::_ipdltest::PTestBasicParent::DeallocManagee","sym":"_ZN7mozilla9_ipdltest16PTestBasicParent14DeallocManageeEiPNS_3ipc9IProtocolE","props":["instance","virtual","user"]},{"pretty":"mozilla::_ipdltest::PTestBasicParent::OnMessageReceived","sym":"_ZN7mozilla9_ipdltest16PTestBasicParent17OnMessageReceivedERKN3IPC7MessageE","props":["instance","virtual","user"]},{"pretty":"mozilla::_ipdltest::PTestBasicParent::OnMessageReceived","sym":"_ZN7mozilla9_ipdltest16PTestBasicParent17OnMessageReceivedERKN3IPC7MessageERNS_9UniquePtrIS3_NS_13DefaultDeleteIS3_EEEE","props":["instance","virtual","user"]},{"pretty":"mozilla::_ipdltest::PTestBasicParent::OnCallReceived","sym":"_ZN7mozilla9_ipdltest16PTestBasicParent14OnCallReceivedERKN3IPC7MessageERNS_9UniquePtrIS3_NS_13DefaultDeleteIS3_EEEE","props":["instance","virtual","user"]},{"pretty":"mozilla::_ipdltest::PTestBasicParent::OnChannelClose","sym":"_ZN7mozilla9_ipdltest16PTestBasicParent14OnChannelCloseEv","props":["instance","virtual","user"]},{"pretty":"mozilla::_ipdltest::PTestBasicParent::OnChannelError","sym":"_ZN7mozilla9_ipdltest16PTestBasicParent14OnChannelErrorEv","props":["instance","virtual","user"]},{"pretty":"mozilla::_ipdltest::PTestBasicParent::ClearSubtree","sym":"_ZN7mozilla9_ipdltest16PTestBasicParent12ClearSubtreeEv","props":["instance","user"]},{"pretty":"mozilla::_ipdltest::PTestBasicParent::PTestBasicParent","sym":"_ZN7mozilla9_ipdltest16PTestBasicParentC1ERKS1_","props":["instance","defaulted","deleted"]},{"pretty":"mozilla::_ipdltest::PTestBasicParent::operator=","sym":"_ZN7mozilla9_ipdltest16PTestBasicParentaSERKS1_","props":["instance","defaulted","deleted"]}],"fields":[],"overrides":[],"props":[],"platforms":["macosx64"]}]}
{"loc":"00061:9-19","structured":1,"pretty":"mozilla::_ipdltest::PTestBasicParent::ActorAlloc","sym":"_ZN7mozilla9_ipdltest16PTestBasicParent10ActorAllocEv","kind":"method","parentsym":"T_mozilla::_ipdltest::PTestBasicParent","implKind":"","sizeBytes":null,"supers":[],"methods":[],"fields":[],"overrides":[{"pretty":"mozilla::ipc::IProtocol::ActorAlloc","sym":"_ZN7mozilla3ipc9IProtocol10ActorAllocEv"}],"props":["instance","virtual","user"]}
{"loc":"00062:9-21","structured":1,"pretty":"mozilla::_ipdltest::PTestBasicParent::ActorDealloc","sym":"_ZN7mozilla9_ipdltest16PTestBasicParent12ActorDeallocEv","kind":"method","parentsym":"T_mozilla::_ipdltest::PTestBasicParent","implKind":"","sizeBytes":null,"supers":[],"methods":[],"fields":[],"overrides":[{"pretty":"mozilla::ipc::IProtocol::ActorDealloc","sym":"_ZN7mozilla3ipc9IProtocol12ActorDeallocEv"}],"props":["instance","virtual","user"]}
{"loc":"00059:4-43","structured":1,"pretty":"mozilla::_ipdltest::PTestBasicParent::AddRef","sym":"_ZN7mozilla9_ipdltest16PTestBasicParent6AddRefEv","kind":"method","parentsym":"T_mozilla::_ipdltest::PTestBasicParent","implKind":"","sizeBytes":null,"supers":[],"methods":[],"fields":[],"overrides":[],"props":["instance","virtual","user"]}
{"loc":"00059:4-43","structured":1,"pretty":"mozilla::_ipdltest::PTestBasicParent::Release","sym":"_ZN7mozilla9_ipdltest16PTestBasicParent7ReleaseEv","kind":"method","parentsym":"T_mozilla::_ipdltest::PTestBasicParent","implKind":"","sizeBytes":null,"supers":[],"methods":[],"fields":[],"overrides":[],"props":["instance","virtual","user"]}

```

## tests/tests/mc-analysis/__GENERATED__/ipdl/PTestBasic.cpp
```
{"loc":"00001:0","target":1,"kind":"def","pretty":"__GENERATED__/ipc/ipdl/PTestBasic.cpp","sym":"FILE_android-armv7@__GENERATED__/ipc/ipdl/PTestBasic@2Ecpp"}
{"loc":"00007:9-41","target":1,"kind":"use","pretty":"__GENERATED__/ipc/ipdl/_ipdlheaders/mozilla/_ipdltest/PTestBasic.h","sym":"FILE_android-armv7@__GENERATED__/ipc/ipdl/_ipdlheaders/mozilla/_ipdltest/PTestBasic@2Eh"}
{"loc":"00008:9-47","target":1,"kind":"use","pretty":"__GENERATED__/ipc/ipdl/_ipdlheaders/mozilla/_ipdltest/PTestBasicParent.h","sym":"FILE_android-armv7@__GENERATED__/ipc/ipdl/_ipdlheaders/mozilla/_ipdltest/PTestBasicParent@2Eh"}
{"loc":"00009:9-46","target":1,"kind":"use","pretty":"__GENERATED__/ipc/ipdl/_ipdlheaders/mozilla/_ipdltest/PTestBasicChild.h","sym":"FILE_android-armv7@__GENERATED__/ipc/ipdl/_ipdlheaders/mozilla/_ipdltest/PTestBasicChild@2Eh"}
{"loc":"00011:9-32","target":1,"kind":"use","pretty":"ipc/glue/IPCMessageUtils.h","sym":"FILE_ipc/glue/IPCMessageUtils@2Eh"}
{"loc":"00012:9-47","target":1,"kind":"use","pretty":"ipc/glue/IPCMessageUtilsSpecializations.h","sym":"FILE_ipc/glue/IPCMessageUtilsSpecializations@2Eh"}
{"loc":"00013:9-20","target":1,"kind":"use","pretty":"__GENERATED__/dist/include/nsIFile.h","sym":"FILE_android-armv7@__GENERATED__/dist/include/nsIFile@2Eh"}
{"loc":"00014:9-33","target":1,"kind":"use","pretty":"ipc/glue/Endpoint.h","sym":"FILE_ipc/glue/Endpoint@2Eh"}
{"loc":"00015:9-45","target":1,"kind":"use","pretty":"ipc/glue/ProtocolMessageUtils.h","sym":"FILE_ipc/glue/ProtocolMessageUtils@2Eh"}
{"loc":"00016:9-38","target":1,"kind":"use","pretty":"ipc/glue/ProtocolUtils.h","sym":"FILE_ipc/glue/ProtocolUtils@2Eh"}
{"loc":"00017:9-42","target":1,"kind":"use","pretty":"ipc/glue/ShmemMessageUtils.h","sym":"FILE_ipc/glue/ShmemMessageUtils@2Eh"}
{"loc":"00018:9-41","target":1,"kind":"use","pretty":"ipc/glue/TaintingIPCUtils.h","sym":"FILE_ipc/glue/TaintingIPCUtils@2Eh"}
{"loc":"00020:10-17","target":1,"kind":"def","pretty":"mozilla","sym":"NS_mozilla"}
{"loc":"00021:10-19","target":1,"kind":"def","pretty":"mozilla::_ipdltest","sym":"NS_mozilla::_ipdltest"}
{"loc":"00022:10-20","target":1,"kind":"def","pretty":"mozilla::_ipdltest::PTestBasic","sym":"NS_mozilla::_ipdltest::PTestBasic"}
{"loc":"00024:0-8","target":1,"kind":"use","pretty":"nsresult","sym":"T_nsresult","context":"mozilla::_ipdltest::PTestBasic::CreateEndpoints","contextsym":"_ZN7mozilla9_ipdltest10PTestBasic15CreateEndpointsEiiPNS_3ipc8EndpointINS0_16PTestBasicParentEEEPNS3_INS0_15PTestBasicChildEEE"}
{"loc":"00025:0-15","target":1,"kind":"def","pretty":"mozilla::_ipdltest::PTestBasic::CreateEndpoints","sym":"_ZN7mozilla9_ipdltest10PTestBasic15CreateEndpointsEiiPNS_3ipc8EndpointINS0_16PTestBasicParentEEEPNS3_INS0_15PTestBasicChildEEE","peekRange":"24-29"}
{"loc":"00026:14-23","target":1,"kind":"use","pretty":"base::ProcessId","sym":"T_base::ProcessId","context":"mozilla::_ipdltest::PTestBasic::CreateEndpoints","contextsym":"_ZN7mozilla9_ipdltest10PTestBasic15CreateEndpointsEiiPNS_3ipc8EndpointINS0_16PTestBasicParentEEEPNS3_INS0_15PTestBasicChildEEE"}
{"loc":"00027:14-23","target":1,"kind":"use","pretty":"base::ProcessId","sym":"T_base::ProcessId","context":"mozilla::_ipdltest::PTestBasic::CreateEndpoints","contextsym":"_ZN7mozilla9_ipdltest10PTestBasic15CreateEndpointsEiiPNS_3ipc8EndpointINS0_16PTestBasicParentEEEPNS3_INS0_15PTestBasicChildEEE"}
{"loc":"00028:22-30","target":1,"kind":"use","pretty":"mozilla::ipc::Endpoint","sym":"T_mozilla::ipc::Endpoint","context":"mozilla::_ipdltest::PTestBasic::CreateEndpoints","contextsym":"_ZN7mozilla9_ipdltest10PTestBasic15CreateEndpointsEiiPNS_3ipc8EndpointINS0_16PTestBasicParentEEEPNS3_INS0_15PTestBasicChildEEE"}
{"loc":"00028:51-67","target":1,"kind":"use","pretty":"mozilla::_ipdltest::PTestBasicParent","sym":"T_mozilla::_ipdltest::PTestBasicParent","context":"mozilla::_ipdltest::PTestBasic::CreateEndpoints","contextsym":"_ZN7mozilla9_ipdltest10PTestBasic15CreateEndpointsEiiPNS_3ipc8EndpointINS0_16PTestBasicParentEEEPNS3_INS0_15PTestBasicChildEEE"}
{"loc":"00029:22-30","target":1,"kind":"use","pretty":"mozilla::ipc::Endpoint","sym":"T_mozilla::ipc::Endpoint","context":"mozilla::_ipdltest::PTestBasic::CreateEndpoints","contextsym":"_ZN7mozilla9_ipdltest10PTestBasic15CreateEndpointsEiiPNS_3ipc8EndpointINS0_16PTestBasicParentEEEPNS3_INS0_15PTestBasicChildEEE"}
{"loc":"00029:51-66","target":1,"kind":"use","pretty":"mozilla::_ipdltest::PTestBasicChild","sym":"T_mozilla::_ipdltest::PTestBasicChild","context":"mozilla::_ipdltest::PTestBasic::CreateEndpoints","contextsym":"_ZN7mozilla9_ipdltest10PTestBasic15CreateEndpointsEiiPNS_3ipc8EndpointINS0_16PTestBasicParentEEEPNS3_INS0_15PTestBasicChildEEE"}
{"loc":"00031:25-40","target":1,"kind":"use","pretty":"mozilla::ipc::CreateEndpoints","sym":"_ZN7mozilla3ipc15CreateEndpointsERKNS0_20PrivateIPDLInterfaceEiiPNS0_8EndpointIT_EEPNS4_IT0_EE","context":"mozilla::_ipdltest::PTestBasic::CreateEndpoints","contextsym":"_ZN7mozilla9_ipdltest10PTestBasic15CreateEndpointsEiiPNS_3ipc8EndpointINS0_16PTestBasicParentEEEPNS3_INS0_15PTestBasicChildEEE"}
{"loc":"00032:22-42","target":1,"kind":"use","pretty":"mozilla::ipc::PrivateIPDLInterface","sym":"T_mozilla::ipc::PrivateIPDLInterface","context":"mozilla::_ipdltest::PTestBasic::CreateEndpoints","contextsym":"_ZN7mozilla9_ipdltest10PTestBasic15CreateEndpointsEiiPNS_3ipc8EndpointINS0_16PTestBasicParentEEEPNS3_INS0_15PTestBasicChildEEE"}
{"loc":"00032:8-15","target":1,"kind":"use","pretty":"mozilla::ipc::PrivateIPDLInterface::PrivateIPDLInterface","sym":"_ZN7mozilla3ipc20PrivateIPDLInterfaceC1Ev","context":"mozilla::_ipdltest::PTestBasic::CreateEndpoints","contextsym":"_ZN7mozilla9_ipdltest10PTestBasic15CreateEndpointsEiiPNS_3ipc8EndpointINS0_16PTestBasicParentEEEPNS3_INS0_15PTestBasicChildEEE"}
{"loc":"00035:0-8","target":1,"kind":"use","pretty":"nsresult","sym":"T_nsresult","context":"mozilla::_ipdltest::PTestBasic::CreateEndpoints","contextsym":"_ZN7mozilla9_ipdltest10PTestBasic15CreateEndpointsEPNS_3ipc8EndpointINS0_16PTestBasicParentEEEPNS3_INS0_15PTestBasicChildEEE"}
{"loc":"00036:0-15","target":1,"kind":"def","pretty":"mozilla::_ipdltest::PTestBasic::CreateEndpoints","sym":"_ZN7mozilla9_ipdltest10PTestBasic15CreateEndpointsEPNS_3ipc8EndpointINS0_16PTestBasicParentEEEPNS3_INS0_15PTestBasicChildEEE","peekRange":"35-38"}
{"loc":"00037:22-30","target":1,"kind":"use","pretty":"mozilla::ipc::Endpoint","sym":"T_mozilla::ipc::Endpoint","context":"mozilla::_ipdltest::PTestBasic::CreateEndpoints","contextsym":"_ZN7mozilla9_ipdltest10PTestBasic15CreateEndpointsEPNS_3ipc8EndpointINS0_16PTestBasicParentEEEPNS3_INS0_15PTestBasicChildEEE"}
{"loc":"00037:51-67","target":1,"kind":"use","pretty":"mozilla::_ipdltest::PTestBasicParent","sym":"T_mozilla::_ipdltest::PTestBasicParent","context":"mozilla::_ipdltest::PTestBasic::CreateEndpoints","contextsym":"_ZN7mozilla9_ipdltest10PTestBasic15CreateEndpointsEPNS_3ipc8EndpointINS0_16PTestBasicParentEEEPNS3_INS0_15PTestBasicChildEEE"}
{"loc":"00038:22-30","target":1,"kind":"use","pretty":"mozilla::ipc::Endpoint","sym":"T_mozilla::ipc::Endpoint","context":"mozilla::_ipdltest::PTestBasic::CreateEndpoints","contextsym":"_ZN7mozilla9_ipdltest10PTestBasic15CreateEndpointsEPNS_3ipc8EndpointINS0_16PTestBasicParentEEEPNS3_INS0_15PTestBasicChildEEE"}
{"loc":"00038:51-66","target":1,"kind":"use","pretty":"mozilla::_ipdltest::PTestBasicChild","sym":"T_mozilla::_ipdltest::PTestBasicChild","context":"mozilla::_ipdltest::PTestBasic::CreateEndpoints","contextsym":"_ZN7mozilla9_ipdltest10PTestBasic15CreateEndpointsEPNS_3ipc8EndpointINS0_16PTestBasicParentEEEPNS3_INS0_15PTestBasicChildEEE"}
{"loc":"00040:25-40","target":1,"kind":"use","pretty":"mozilla::ipc::CreateEndpoints","sym":"_ZN7mozilla3ipc15CreateEndpointsERKNS0_20PrivateIPDLInterfaceEPNS0_8EndpointIT_EEPNS4_IT0_EE","context":"mozilla::_ipdltest::PTestBasic::CreateEndpoints","contextsym":"_ZN7mozilla9_ipdltest10PTestBasic15CreateEndpointsEPNS_3ipc8EndpointINS0_16PTestBasicParentEEEPNS3_INS0_15PTestBasicChildEEE"}
{"loc":"00041:22-42","target":1,"kind":"use","pretty":"mozilla::ipc::PrivateIPDLInterface","sym":"T_mozilla::ipc::PrivateIPDLInterface","context":"mozilla::_ipdltest::PTestBasic::CreateEndpoints","contextsym":"_ZN7mozilla9_ipdltest10PTestBasic15CreateEndpointsEPNS_3ipc8EndpointINS0_16PTestBasicParentEEEPNS3_INS0_15PTestBasicChildEEE"}
{"loc":"00041:8-15","target":1,"kind":"use","pretty":"mozilla::ipc::PrivateIPDLInterface::PrivateIPDLInterface","sym":"_ZN7mozilla3ipc20PrivateIPDLInterfaceC1Ev","context":"mozilla::_ipdltest::PTestBasic::CreateEndpoints","contextsym":"_ZN7mozilla9_ipdltest10PTestBasic15CreateEndpointsEPNS_3ipc8EndpointINS0_16PTestBasicParentEEEPNS3_INS0_15PTestBasicChildEEE"}
{"loc":"00044:24-31","target":1,"kind":"use","pretty":"IPC::Message","sym":"T_IPC::Message","context":"mozilla::_ipdltest::PTestBasic::Msg_Hello","contextsym":"_ZN7mozilla9_ipdltest10PTestBasic9Msg_HelloEi"}
{"loc":"00044:9-18","target":1,"kind":"use","pretty":"mozilla::UniquePtr","sym":"T_mozilla::UniquePtr","context":"mozilla::_ipdltest::PTestBasic::Msg_Hello","contextsym":"_ZN7mozilla9_ipdltest10PTestBasic9Msg_HelloEi"}
{"loc":"00045:0-9","target":1,"kind":"def","pretty":"mozilla::_ipdltest::PTestBasic::Msg_Hello","sym":"_ZN7mozilla9_ipdltest10PTestBasic9Msg_HelloEi","peekRange":"44-45"}
{"loc":"00045:10-17","target":1,"kind":"use","pretty":"int32_t","sym":"T_int32_t","context":"mozilla::_ipdltest::PTestBasic::Msg_Hello","contextsym":"_ZN7mozilla9_ipdltest10PTestBasic9Msg_HelloEi"}
{"loc":"00047:106-116","target":1,"kind":"use","pretty":"IPC::Message::NestedLevel::NOT_NESTED","sym":"E_<T_IPC::Message::NestedLevel>_NOT_NESTED","context":"mozilla::_ipdltest::PTestBasic::Msg_Hello","contextsym":"_ZN7mozilla9_ipdltest10PTestBasic9Msg_HelloEi"}
{"loc":"00047:123-130","target":1,"kind":"use","pretty":"IPC::Message","sym":"T_IPC::Message","context":"mozilla::_ipdltest::PTestBasic::Msg_Hello","contextsym":"_ZN7mozilla9_ipdltest10PTestBasic9Msg_HelloEi"}
{"loc":"00047:132-147","target":1,"kind":"use","pretty":"IPC::Message::PriorityValue::NORMAL_PRIORITY","sym":"E_<T_IPC::Message::PriorityValue>_NORMAL_PRIORITY","context":"mozilla::_ipdltest::PTestBasic::Msg_Hello","contextsym":"_ZN7mozilla9_ipdltest10PTestBasic9Msg_HelloEi"}
{"loc":"00047:154-161","target":1,"kind":"use","pretty":"IPC::Message","sym":"T_IPC::Message","context":"mozilla::_ipdltest::PTestBasic::Msg_Hello","contextsym":"_ZN7mozilla9_ipdltest10PTestBasic9Msg_HelloEi"}
{"loc":"00047:16-23","target":1,"kind":"use","pretty":"IPC::Message","sym":"T_IPC::Message","context":"mozilla::_ipdltest::PTestBasic::Msg_Hello","contextsym":"_ZN7mozilla9_ipdltest10PTestBasic9Msg_HelloEi"}
{"loc":"00047:163-179","target":1,"kind":"use","pretty":"IPC::Message::MessageCompression::COMPRESSION_NONE","sym":"E_<T_IPC::Message::MessageCompression>_COMPRESSION_NONE","context":"mozilla::_ipdltest::PTestBasic::Msg_Hello","contextsym":"_ZN7mozilla9_ipdltest10PTestBasic9Msg_HelloEi"}
{"loc":"00047:186-193","target":1,"kind":"use","pretty":"IPC::Message","sym":"T_IPC::Message","context":"mozilla::_ipdltest::PTestBasic::Msg_Hello","contextsym":"_ZN7mozilla9_ipdltest10PTestBasic9Msg_HelloEi"}
{"loc":"00047:195-210","target":1,"kind":"use","pretty":"IPC::Message::Constructor::NOT_CONSTRUCTOR","sym":"E_<T_IPC::Message::Constructor>_NOT_CONSTRUCTOR","context":"mozilla::_ipdltest::PTestBasic::Msg_Hello","contextsym":"_ZN7mozilla9_ipdltest10PTestBasic9Msg_HelloEi"}
{"loc":"00047:217-224","target":1,"kind":"use","pretty":"IPC::Message","sym":"T_IPC::Message","context":"mozilla::_ipdltest::PTestBasic::Msg_Hello","contextsym":"_ZN7mozilla9_ipdltest10PTestBasic9Msg_HelloEi"}
{"loc":"00047:226-231","target":1,"kind":"use","pretty":"IPC::Message::Sync::ASYNC","sym":"E_<T_IPC::Message::Sync>_ASYNC","context":"mozilla::_ipdltest::PTestBasic::Msg_Hello","contextsym":"_ZN7mozilla9_ipdltest10PTestBasic9Msg_HelloEi"}
{"loc":"00047:238-245","target":1,"kind":"use","pretty":"IPC::Message","sym":"T_IPC::Message","context":"mozilla::_ipdltest::PTestBasic::Msg_Hello","contextsym":"_ZN7mozilla9_ipdltest10PTestBasic9Msg_HelloEi"}
{"loc":"00047:247-256","target":1,"kind":"use","pretty":"IPC::Message::Reply::NOT_REPLY","sym":"E_<T_IPC::Message::Reply>_NOT_REPLY","context":"mozilla::_ipdltest::PTestBasic::Msg_Hello","contextsym":"_ZN7mozilla9_ipdltest10PTestBasic9Msg_HelloEi"}
{"loc":"00047:25-36","target":1,"kind":"use","pretty":"IPC::Message::IPDLMessage","sym":"_ZN3IPC7Message11IPDLMessageEijjNS0_11HeaderFlagsE","context":"mozilla::_ipdltest::PTestBasic::Msg_Hello","contextsym":"_ZN7mozilla9_ipdltest10PTestBasic9Msg_HelloEi"}
{"loc":"00047:48-61","target":1,"kind":"use","pretty":"mozilla::_ipdltest::PTestBasic::MessageType::Msg_Hello__ID","sym":"E_<T_mozilla::_ipdltest::PTestBasic::MessageType>_Msg_Hello__ID","context":"mozilla::_ipdltest::PTestBasic::Msg_Hello","contextsym":"_ZN7mozilla9_ipdltest10PTestBasic9Msg_HelloEi"}
{"loc":"00047:66-69","target":1,"kind":"use","pretty":"IPC::Message::HeaderFlags::HeaderFlags","sym":"_ZN3IPC7Message11HeaderFlagsC1ENS0_11NestedLevelENS0_13PriorityValueENS0_18MessageCompressionENS0_11ConstructorENS0_4SyncENS0_5ReplyE","context":"mozilla::_ipdltest::PTestBasic::Msg_Hello","contextsym":"_ZN7mozilla9_ipdltest10PTestBasic9Msg_HelloEi"}
{"loc":"00047:71-78","target":1,"kind":"use","pretty":"IPC::Message","sym":"T_IPC::Message","context":"mozilla::_ipdltest::PTestBasic::Msg_Hello","contextsym":"_ZN7mozilla9_ipdltest10PTestBasic9Msg_HelloEi"}
{"loc":"00047:80-91","target":1,"kind":"use","pretty":"IPC::Message::HeaderFlags","sym":"T_IPC::Message::HeaderFlags","context":"mozilla::_ipdltest::PTestBasic::Msg_Hello","contextsym":"_ZN7mozilla9_ipdltest10PTestBasic9Msg_HelloEi"}
{"loc":"00047:97-104","target":1,"kind":"use","pretty":"IPC::Message","sym":"T_IPC::Message","context":"mozilla::_ipdltest::PTestBasic::Msg_Hello","contextsym":"_ZN7mozilla9_ipdltest10PTestBasic9Msg_HelloEi"}
{"loc":"00001:0","target":1,"kind":"def","pretty":"__GENERATED__/ipc/ipdl/PTestBasic.cpp","sym":"FILE_linux@__GENERATED__/ipc/ipdl/PTestBasic@2Ecpp"}
{"loc":"00007:9-41","target":1,"kind":"use","pretty":"__GENERATED__/ipc/ipdl/_ipdlheaders/mozilla/_ipdltest/PTestBasic.h","sym":"FILE_linux@__GENERATED__/ipc/ipdl/_ipdlheaders/mozilla/_ipdltest/PTestBasic@2Eh"}
{"loc":"00008:9-47","target":1,"kind":"use","pretty":"__GENERATED__/ipc/ipdl/_ipdlheaders/mozilla/_ipdltest/PTestBasicParent.h","sym":"FILE_linux@__GENERATED__/ipc/ipdl/_ipdlheaders/mozilla/_ipdltest/PTestBasicParent@2Eh"}
{"loc":"00009:9-46","target":1,"kind":"use","pretty":"__GENERATED__/ipc/ipdl/_ipdlheaders/mozilla/_ipdltest/PTestBasicChild.h","sym":"FILE_linux@__GENERATED__/ipc/ipdl/_ipdlheaders/mozilla/_ipdltest/PTestBasicChild@2Eh"}
{"loc":"00013:9-20","target":1,"kind":"use","pretty":"__GENERATED__/dist/include/nsIFile.h","sym":"FILE_linux@__GENERATED__/dist/include/nsIFile@2Eh"}
{"loc":"00001:0","target":1,"kind":"def","pretty":"__GENERATED__/ipc/ipdl/PTestBasic.cpp","sym":"FILE_macosx@__GENERATED__/ipc/ipdl/PTestBasic@2Ecpp"}
{"loc":"00007:9-41","target":1,"kind":"use","pretty":"__GENERATED__/ipc/ipdl/_ipdlheaders/mozilla/_ipdltest/PTestBasic.h","sym":"FILE_macosx@__GENERATED__/ipc/ipdl/_ipdlheaders/mozilla/_ipdltest/PTestBasic@2Eh"}
{"loc":"00008:9-47","target":1,"kind":"use","pretty":"__GENERATED__/ipc/ipdl/_ipdlheaders/mozilla/_ipdltest/PTestBasicParent.h","sym":"FILE_macosx@__GENERATED__/ipc/ipdl/_ipdlheaders/mozilla/_ipdltest/PTestBasicParent@2Eh"}
{"loc":"00009:9-46","target":1,"kind":"use","pretty":"__GENERATED__/ipc/ipdl/_ipdlheaders/mozilla/_ipdltest/PTestBasicChild.h","sym":"FILE_macosx@__GENERATED__/ipc/ipdl/_ipdlheaders/mozilla/_ipdltest/PTestBasicChild@2Eh"}
{"loc":"00013:9-20","target":1,"kind":"use","pretty":"__GENERATED__/dist/include/nsIFile.h","sym":"FILE_macosx@__GENERATED__/dist/include/nsIFile@2Eh"}
{"loc":"00001:0","target":1,"kind":"def","pretty":"__GENERATED__/ipc/ipdl/PTestBasic.cpp","sym":"FILE_windows@__GENERATED__/ipc/ipdl/PTestBasic@2Ecpp"}
{"loc":"00007:9-41","target":1,"kind":"use","pretty":"__GENERATED__/ipc/ipdl/_ipdlheaders/mozilla/_ipdltest/PTestBasic.h","sym":"FILE_windows@__GENERATED__/ipc/ipdl/_ipdlheaders/mozilla/_ipdltest/PTestBasic@2Eh"}
{"loc":"00008:9-47","target":1,"kind":"use","pretty":"__GENERATED__/ipc/ipdl/_ipdlheaders/mozilla/_ipdltest/PTestBasicParent.h","sym":"FILE_windows@__GENERATED__/ipc/ipdl/_ipdlheaders/mozilla/_ipdltest/PTestBasicParent@2Eh"}
{"loc":"00009:9-46","target":1,"kind":"use","pretty":"__GENERATED__/ipc/ipdl/_ipdlheaders/mozilla/_ipdltest/PTestBasicChild.h","sym":"FILE_windows@__GENERATED__/ipc/ipdl/_ipdlheaders/mozilla/_ipdltest/PTestBasicChild@2Eh"}
{"loc":"00013:9-20","target":1,"kind":"use","pretty":"__GENERATED__/dist/include/nsIFile.h","sym":"FILE_windows@__GENERATED__/dist/include/nsIFile@2Eh"}
{"loc":"00024:0-8","target":1,"kind":"use","pretty":"nsresult","sym":"T_nsresult","context":"mozilla::_ipdltest::PTestBasic::CreateEndpoints","contextsym":"_ZN7mozilla9_ipdltest10PTestBasic15CreateEndpointsEmmPNS_3ipc8EndpointINS0_16PTestBasicParentEEEPNS3_INS0_15PTestBasicChildEEE"}
{"loc":"00025:0-15","target":1,"kind":"def","pretty":"mozilla::_ipdltest::PTestBasic::CreateEndpoints","sym":"_ZN7mozilla9_ipdltest10PTestBasic15CreateEndpointsEmmPNS_3ipc8EndpointINS0_16PTestBasicParentEEEPNS3_INS0_15PTestBasicChildEEE","peekRange":"24-29"}
{"loc":"00026:14-23","target":1,"kind":"use","pretty":"base::ProcessId","sym":"T_base::ProcessId","context":"mozilla::_ipdltest::PTestBasic::CreateEndpoints","contextsym":"_ZN7mozilla9_ipdltest10PTestBasic15CreateEndpointsEmmPNS_3ipc8EndpointINS0_16PTestBasicParentEEEPNS3_INS0_15PTestBasicChildEEE"}
{"loc":"00027:14-23","target":1,"kind":"use","pretty":"base::ProcessId","sym":"T_base::ProcessId","context":"mozilla::_ipdltest::PTestBasic::CreateEndpoints","contextsym":"_ZN7mozilla9_ipdltest10PTestBasic15CreateEndpointsEmmPNS_3ipc8EndpointINS0_16PTestBasicParentEEEPNS3_INS0_15PTestBasicChildEEE"}
{"loc":"00028:22-30","target":1,"kind":"use","pretty":"mozilla::ipc::Endpoint","sym":"T_mozilla::ipc::Endpoint","context":"mozilla::_ipdltest::PTestBasic::CreateEndpoints","contextsym":"_ZN7mozilla9_ipdltest10PTestBasic15CreateEndpointsEmmPNS_3ipc8EndpointINS0_16PTestBasicParentEEEPNS3_INS0_15PTestBasicChildEEE"}
{"loc":"00028:51-67","target":1,"kind":"use","pretty":"mozilla::_ipdltest::PTestBasicParent","sym":"T_mozilla::_ipdltest::PTestBasicParent","context":"mozilla::_ipdltest::PTestBasic::CreateEndpoints","contextsym":"_ZN7mozilla9_ipdltest10PTestBasic15CreateEndpointsEmmPNS_3ipc8EndpointINS0_16PTestBasicParentEEEPNS3_INS0_15PTestBasicChildEEE"}
{"loc":"00029:22-30","target":1,"kind":"use","pretty":"mozilla::ipc::Endpoint","sym":"T_mozilla::ipc::Endpoint","context":"mozilla::_ipdltest::PTestBasic::CreateEndpoints","contextsym":"_ZN7mozilla9_ipdltest10PTestBasic15CreateEndpointsEmmPNS_3ipc8EndpointINS0_16PTestBasicParentEEEPNS3_INS0_15PTestBasicChildEEE"}
{"loc":"00029:51-66","target":1,"kind":"use","pretty":"mozilla::_ipdltest::PTestBasicChild","sym":"T_mozilla::_ipdltest::PTestBasicChild","context":"mozilla::_ipdltest::PTestBasic::CreateEndpoints","contextsym":"_ZN7mozilla9_ipdltest10PTestBasic15CreateEndpointsEmmPNS_3ipc8EndpointINS0_16PTestBasicParentEEEPNS3_INS0_15PTestBasicChildEEE"}
{"loc":"00031:25-40","target":1,"kind":"use","pretty":"mozilla::ipc::CreateEndpoints","sym":"_ZN7mozilla3ipc15CreateEndpointsERKNS0_20PrivateIPDLInterfaceEmmPNS0_8EndpointIT_EEPNS4_IT0_EE","context":"mozilla::_ipdltest::PTestBasic::CreateEndpoints","contextsym":"_ZN7mozilla9_ipdltest10PTestBasic15CreateEndpointsEmmPNS_3ipc8EndpointINS0_16PTestBasicParentEEEPNS3_INS0_15PTestBasicChildEEE"}
{"loc":"00032:22-42","target":1,"kind":"use","pretty":"mozilla::ipc::PrivateIPDLInterface","sym":"T_mozilla::ipc::PrivateIPDLInterface","context":"mozilla::_ipdltest::PTestBasic::CreateEndpoints","contextsym":"_ZN7mozilla9_ipdltest10PTestBasic15CreateEndpointsEmmPNS_3ipc8EndpointINS0_16PTestBasicParentEEEPNS3_INS0_15PTestBasicChildEEE"}
{"loc":"00032:8-15","target":1,"kind":"use","pretty":"mozilla::ipc::PrivateIPDLInterface::PrivateIPDLInterface","sym":"_ZN7mozilla3ipc20PrivateIPDLInterfaceC1Ev","context":"mozilla::_ipdltest::PTestBasic::CreateEndpoints","contextsym":"_ZN7mozilla9_ipdltest10PTestBasic15CreateEndpointsEmmPNS_3ipc8EndpointINS0_16PTestBasicParentEEEPNS3_INS0_15PTestBasicChildEEE"}
{"loc":"00001:0","source":1,"syntax":"def,file","pretty":"file __GENERATED__/ipc/ipdl/PTestBasic.cpp","sym":"FILE_windows@__GENERATED__/ipc/ipdl/PTestBasic@2Ecpp,FILE_macosx@__GENERATED__/ipc/ipdl/PTestBasic@2Ecpp,FILE_linux@__GENERATED__/ipc/ipdl/PTestBasic@2Ecpp,FILE_android-armv7@__GENERATED__/ipc/ipdl/PTestBasic@2Ecpp"}
{"loc":"00007:9-41","source":1,"syntax":"file,use","pretty":"file __GENERATED__/ipc/ipdl/_ipdlheaders/mozilla/_ipdltest/PTestBasic.h","sym":"FILE_windows@__GENERATED__/ipc/ipdl/_ipdlheaders/mozilla/_ipdltest/PTestBasic@2Eh,FILE_macosx@__GENERATED__/ipc/ipdl/_ipdlheaders/mozilla/_ipdltest/PTestBasic@2Eh,FILE_linux@__GENERATED__/ipc/ipdl/_ipdlheaders/mozilla/_ipdltest/PTestBasic@2Eh,FILE_android-armv7@__GENERATED__/ipc/ipdl/_ipdlheaders/mozilla/_ipdltest/PTestBasic@2Eh"}
{"loc":"00008:9-47","source":1,"syntax":"file,use","pretty":"file __GENERATED__/ipc/ipdl/_ipdlheaders/mozilla/_ipdltest/PTestBasicParent.h","sym":"FILE_windows@__GENERATED__/ipc/ipdl/_ipdlheaders/mozilla/_ipdltest/PTestBasicParent@2Eh,FILE_macosx@__GENERATED__/ipc/ipdl/_ipdlheaders/mozilla/_ipdltest/PTestBasicParent@2Eh,FILE_linux@__GENERATED__/ipc/ipdl/_ipdlheaders/mozilla/_ipdltest/PTestBasicParent@2Eh,FILE_android-armv7@__GENERATED__/ipc/ipdl/_ipdlheaders/mozilla/_ipdltest/PTestBasicParent@2Eh"}
{"loc":"00009:9-46","source":1,"syntax":"file,use","pretty":"file __GENERATED__/ipc/ipdl/_ipdlheaders/mozilla/_ipdltest/PTestBasicChild.h","sym":"FILE_windows@__GENERATED__/ipc/ipdl/_ipdlheaders/mozilla/_ipdltest/PTestBasicChild@2Eh,FILE_macosx@__GENERATED__/ipc/ipdl/_ipdlheaders/mozilla/_ipdltest/PTestBasicChild@2Eh,FILE_linux@__GENERATED__/ipc/ipdl/_ipdlheaders/mozilla/_ipdltest/PTestBasicChild@2Eh,FILE_android-armv7@__GENERATED__/ipc/ipdl/_ipdlheaders/mozilla/_ipdltest/PTestBasicChild@2Eh"}
{"loc":"00011:9-32","source":1,"syntax":"file,use","pretty":"file ipc/glue/IPCMessageUtils.h","sym":"FILE_ipc/glue/IPCMessageUtils@2Eh"}
{"loc":"00012:9-47","source":1,"syntax":"file,use","pretty":"file ipc/glue/IPCMessageUtilsSpecializations.h","sym":"FILE_ipc/glue/IPCMessageUtilsSpecializations@2Eh"}
{"loc":"00013:9-20","source":1,"syntax":"file,use","pretty":"file __GENERATED__/dist/include/nsIFile.h","sym":"FILE_windows@__GENERATED__/dist/include/nsIFile@2Eh,FILE_macosx@__GENERATED__/dist/include/nsIFile@2Eh,FILE_linux@__GENERATED__/dist/include/nsIFile@2Eh,FILE_android-armv7@__GENERATED__/dist/include/nsIFile@2Eh"}
{"loc":"00014:9-33","source":1,"syntax":"file,use","pretty":"file ipc/glue/Endpoint.h","sym":"FILE_ipc/glue/Endpoint@2Eh"}
{"loc":"00015:9-45","source":1,"syntax":"file,use","pretty":"file ipc/glue/ProtocolMessageUtils.h","sym":"FILE_ipc/glue/ProtocolMessageUtils@2Eh"}
{"loc":"00016:9-38","source":1,"syntax":"file,use","pretty":"file ipc/glue/ProtocolUtils.h","sym":"FILE_ipc/glue/ProtocolUtils@2Eh"}
{"loc":"00017:9-42","source":1,"syntax":"file,use","pretty":"file ipc/glue/ShmemMessageUtils.h","sym":"FILE_ipc/glue/ShmemMessageUtils@2Eh"}
{"loc":"00018:9-41","source":1,"syntax":"file,use","pretty":"file ipc/glue/TaintingIPCUtils.h","sym":"FILE_ipc/glue/TaintingIPCUtils@2Eh"}
{"loc":"00020:10-17","source":1,"syntax":"def,namespace","pretty":"namespace mozilla","sym":"NS_mozilla","nestingRange":"20:19-52:0"}
{"loc":"00021:10-19","source":1,"syntax":"def,namespace","pretty":"namespace mozilla::_ipdltest","sym":"NS_mozilla::_ipdltest","nestingRange":"21:21-51:0"}
{"loc":"00022:10-20","source":1,"syntax":"def,namespace","pretty":"namespace mozilla::_ipdltest::PTestBasic","sym":"NS_mozilla::_ipdltest::PTestBasic","nestingRange":"22:22-50:0"}
{"loc":"00024:0-8","source":1,"syntax":"type,use","pretty":"type nsresult","sym":"T_nsresult","type":"enum nsresult","typesym":"T_nsresult"}
{"loc":"00025:0-15","source":1,"syntax":"def,function","pretty":"function mozilla::_ipdltest::PTestBasic::CreateEndpoints","sym":"_ZN7mozilla9_ipdltest10PTestBasic15CreateEndpointsEmmPNS_3ipc8EndpointINS0_16PTestBasicParentEEEPNS3_INS0_15PTestBasicChildEEE,_ZN7mozilla9_ipdltest10PTestBasic15CreateEndpointsEiiPNS_3ipc8EndpointINS0_16PTestBasicParentEEEPNS3_INS0_15PTestBasicChildEEE","nestingRange":"30:0-34:0","type":"enum nsresult (base::ProcessId, base::ProcessId, mozilla::ipc::Endpoint<mozilla::_ipdltest::PTestBasicParent> *, mozilla::ipc::Endpoint<mozilla::_ipdltest::PTestBasicChild> *)"}
{"loc":"00026:14-23","source":1,"syntax":"type,use","pretty":"type base::ProcessId","sym":"T_base::ProcessId","type":"base::ProcessId"}
{"loc":"00026:24-38","source":1,"syntax":"","pretty":"variable aParentDestPid","sym":"V_86052b1c8f905ba9_d1259d65da7e98a3,V_829b7740f87a9861_d1259d65da7e98a3,V_d29b37a6c76a5f4d_d1259d65da7e98a3,V_85f91e7333aa66e3_d1259d65da7e98a3","no_crossref":1,"type":"base::ProcessId"}
{"loc":"00027:14-23","source":1,"syntax":"type,use","pretty":"type base::ProcessId","sym":"T_base::ProcessId","type":"base::ProcessId"}
{"loc":"00027:24-37","source":1,"syntax":"","pretty":"variable aChildDestPid","sym":"V_9ccd2b1c8f905ba9_797885af5cc6dac7,V_98548740f87a9861_797885af5cc6dac7,V_e85447a6c76a5f4d_797885af5cc6dac7,V_9bb22e7333aa66e3_797885af5cc6dac7","no_crossref":1,"type":"base::ProcessId"}
{"loc":"00028:22-30","source":1,"syntax":"type,use","pretty":"type mozilla::ipc::Endpoint","sym":"T_mozilla::ipc::Endpoint"}
{"loc":"00028:51-67","source":1,"syntax":"type,use","pretty":"type mozilla::_ipdltest::PTestBasicParent","sym":"T_mozilla::_ipdltest::PTestBasicParent","type":"class mozilla::_ipdltest::PTestBasicParent","typesym":"T_mozilla::_ipdltest::PTestBasicParent"}
{"loc":"00028:70-77","source":1,"syntax":"","pretty":"variable aParent","sym":"V_bc963b1c8f905ba9_03dd840f0b0d,V_b82d8740f87a9861_03dd840f0b0d,V_092d47a6c76a5f4d_03dd840f0b0d,V_bb8b2e7333aa66e3_03dd840f0b0d","no_crossref":1,"type":"mozilla::ipc::Endpoint<mozilla::_ipdltest::PTestBasicParent> *"}
{"loc":"00029:22-30","source":1,"syntax":"type,use","pretty":"type mozilla::ipc::Endpoint","sym":"T_mozilla::ipc::Endpoint"}
{"loc":"00029:51-66","source":1,"syntax":"type,use","pretty":"type mozilla::_ipdltest::PTestBasicChild","sym":"T_mozilla::_ipdltest::PTestBasicChild","type":"class mozilla::_ipdltest::PTestBasicChild","typesym":"T_mozilla::_ipdltest::PTestBasicChild"}
{"loc":"00029:69-75","source":1,"syntax":"","pretty":"variable aChild","sym":"V_416f3b1c8f905ba9_ae7aa1fe256,V_4de59740f87a9861_ae7aa1fe256,V_9de557a6c76a5f4d_ae7aa1fe256,V_40543e7333aa66e3_ae7aa1fe256","no_crossref":1,"type":"mozilla::ipc::Endpoint<mozilla::_ipdltest::PTestBasicChild> *"}
{"loc":"00031:25-40","source":1,"syntax":"function,use","pretty":"function mozilla::ipc::CreateEndpoints","sym":"_ZN7mozilla3ipc15CreateEndpointsERKNS0_20PrivateIPDLInterfaceEmmPNS0_8EndpointIT_EEPNS4_IT0_EE,_ZN7mozilla3ipc15CreateEndpointsERKNS0_20PrivateIPDLInterfaceEiiPNS0_8EndpointIT_EEPNS4_IT0_EE","type":"enum nsresult (const struct mozilla::ipc::PrivateIPDLInterface &, base::ProcessId, base::ProcessId, Endpoint<class mozilla::_ipdltest::PTestBasicParent> *, Endpoint<class mozilla::_ipdltest::PTestBasicChild> *)"}
{"loc":"00032:8-15","source":1,"syntax":"constructor,use","pretty":"constructor mozilla::ipc::PrivateIPDLInterface::PrivateIPDLInterface","sym":"_ZN7mozilla3ipc20PrivateIPDLInterfaceC1Ev"}
{"loc":"00032:22-42","source":1,"syntax":"type,use","pretty":"type mozilla::ipc::PrivateIPDLInterface","sym":"T_mozilla::ipc::PrivateIPDLInterface","type":"struct mozilla::ipc::PrivateIPDLInterface","typesym":"T_mozilla::ipc::PrivateIPDLInterface"}
{"loc":"00033:8-22","source":1,"syntax":"","pretty":"variable aParentDestPid","sym":"V_86052b1c8f905ba9_d1259d65da7e98a3,V_829b7740f87a9861_d1259d65da7e98a3,V_d29b37a6c76a5f4d_d1259d65da7e98a3,V_85f91e7333aa66e3_d1259d65da7e98a3","no_crossref":1,"type":"base::ProcessId"}
{"loc":"00033:24-37","source":1,"syntax":"","pretty":"variable aChildDestPid","sym":"V_9ccd2b1c8f905ba9_797885af5cc6dac7,V_98548740f87a9861_797885af5cc6dac7,V_e85447a6c76a5f4d_797885af5cc6dac7,V_9bb22e7333aa66e3_797885af5cc6dac7","no_crossref":1,"type":"base::ProcessId"}
{"loc":"00033:39-46","source":1,"syntax":"","pretty":"variable aParent","sym":"V_bc963b1c8f905ba9_03dd840f0b0d,V_b82d8740f87a9861_03dd840f0b0d,V_092d47a6c76a5f4d_03dd840f0b0d,V_bb8b2e7333aa66e3_03dd840f0b0d","no_crossref":1,"type":"mozilla::ipc::Endpoint<mozilla::_ipdltest::PTestBasicParent> *"}
{"loc":"00033:48-54","source":1,"syntax":"","pretty":"variable aChild","sym":"V_416f3b1c8f905ba9_ae7aa1fe256,V_4de59740f87a9861_ae7aa1fe256,V_9de557a6c76a5f4d_ae7aa1fe256,V_40543e7333aa66e3_ae7aa1fe256","no_crossref":1,"type":"mozilla::ipc::Endpoint<mozilla::_ipdltest::PTestBasicChild> *"}
{"loc":"00035:0-8","source":1,"syntax":"type,use","pretty":"type nsresult","sym":"T_nsresult","type":"enum nsresult","typesym":"T_nsresult"}
{"loc":"00036:0-15","source":1,"syntax":"def,function","pretty":"function mozilla::_ipdltest::PTestBasic::CreateEndpoints","sym":"_ZN7mozilla9_ipdltest10PTestBasic15CreateEndpointsEPNS_3ipc8EndpointINS0_16PTestBasicParentEEEPNS3_INS0_15PTestBasicChildEEE","nestingRange":"39:0-43:0","type":"enum nsresult (mozilla::ipc::Endpoint<mozilla::_ipdltest::PTestBasicParent> *, mozilla::ipc::Endpoint<mozilla::_ipdltest::PTestBasicChild> *)"}
{"loc":"00037:22-30","source":1,"syntax":"type,use","pretty":"type mozilla::ipc::Endpoint","sym":"T_mozilla::ipc::Endpoint"}
{"loc":"00037:51-67","source":1,"syntax":"type,use","pretty":"type mozilla::_ipdltest::PTestBasicParent","sym":"T_mozilla::_ipdltest::PTestBasicParent","type":"class mozilla::_ipdltest::PTestBasicParent","typesym":"T_mozilla::_ipdltest::PTestBasicParent"}
{"loc":"00037:70-77","source":1,"syntax":"","pretty":"variable aParent","sym":"V_be5f4c1c8f905ba9_03dd840f0b0d,V_bae5a840f87a9861_03dd840f0b0d,V_0be568a6c76a5f4d_03dd840f0b0d,V_bd444f7333aa66e3_03dd840f0b0d","no_crossref":1,"type":"mozilla::ipc::Endpoint<mozilla::_ipdltest::PTestBasicParent> *"}
{"loc":"00038:22-30","source":1,"syntax":"type,use","pretty":"type mozilla::ipc::Endpoint","sym":"T_mozilla::ipc::Endpoint"}
{"loc":"00038:51-66","source":1,"syntax":"type,use","pretty":"type mozilla::_ipdltest::PTestBasicChild","sym":"T_mozilla::_ipdltest::PTestBasicChild","type":"class mozilla::_ipdltest::PTestBasicChild","typesym":"T_mozilla::_ipdltest::PTestBasicChild"}
{"loc":"00038:69-75","source":1,"syntax":"","pretty":"variable aChild","sym":"V_43285c1c8f905ba9_ae7aa1fe256,V_4faea840f87a9861_ae7aa1fe256,V_9fae68a6c76a5f4d_ae7aa1fe256,V_421d4f7333aa66e3_ae7aa1fe256","no_crossref":1,"type":"mozilla::ipc::Endpoint<mozilla::_ipdltest::PTestBasicChild> *"}
{"loc":"00040:25-40","source":1,"syntax":"function,use","pretty":"function mozilla::ipc::CreateEndpoints","sym":"_ZN7mozilla3ipc15CreateEndpointsERKNS0_20PrivateIPDLInterfaceEPNS0_8EndpointIT_EEPNS4_IT0_EE","type":"enum nsresult (const struct mozilla::ipc::PrivateIPDLInterface &, Endpoint<class mozilla::_ipdltest::PTestBasicParent> *, Endpoint<class mozilla::_ipdltest::PTestBasicChild> *)"}
{"loc":"00041:8-15","source":1,"syntax":"constructor,use","pretty":"constructor mozilla::ipc::PrivateIPDLInterface::PrivateIPDLInterface","sym":"_ZN7mozilla3ipc20PrivateIPDLInterfaceC1Ev"}
{"loc":"00041:22-42","source":1,"syntax":"type,use","pretty":"type mozilla::ipc::PrivateIPDLInterface","sym":"T_mozilla::ipc::PrivateIPDLInterface","type":"struct mozilla::ipc::PrivateIPDLInterface","typesym":"T_mozilla::ipc::PrivateIPDLInterface"}
{"loc":"00042:8-15","source":1,"syntax":"","pretty":"variable aParent","sym":"V_be5f4c1c8f905ba9_03dd840f0b0d,V_bae5a840f87a9861_03dd840f0b0d,V_0be568a6c76a5f4d_03dd840f0b0d,V_bd444f7333aa66e3_03dd840f0b0d","no_crossref":1,"type":"mozilla::ipc::Endpoint<mozilla::_ipdltest::PTestBasicParent> *"}
{"loc":"00042:17-23","source":1,"syntax":"","pretty":"variable aChild","sym":"V_43285c1c8f905ba9_ae7aa1fe256,V_4faea840f87a9861_ae7aa1fe256,V_9fae68a6c76a5f4d_ae7aa1fe256,V_421d4f7333aa66e3_ae7aa1fe256","no_crossref":1,"type":"mozilla::ipc::Endpoint<mozilla::_ipdltest::PTestBasicChild> *"}
{"loc":"00044:9-18","source":1,"syntax":"type,use","pretty":"type mozilla::UniquePtr","sym":"T_mozilla::UniquePtr"}
{"loc":"00044:24-31","source":1,"syntax":"type,use","pretty":"type IPC::Message","sym":"T_IPC::Message","type":"class IPC::Message","typesym":"T_IPC::Message"}
{"loc":"00045:0-9","source":1,"syntax":"def,function","pretty":"function mozilla::_ipdltest::PTestBasic::Msg_Hello","sym":"_ZN7mozilla9_ipdltest10PTestBasic9Msg_HelloEi","nestingRange":"46:0-48:0","type":"mozilla::UniquePtr<IPC::Message> (int32_t)"}
{"loc":"00045:10-17","source":1,"syntax":"type,use","pretty":"type int32_t","sym":"T_int32_t","type":"int32_t"}
{"loc":"00045:18-27","source":1,"syntax":"","pretty":"variable routingId","sym":"V_ce4f5d1c8f905ba9_a5670b686a7d773,V_cad5b940f87a9861_a5670b686a7d773,V_1bd579a6c76a5f4d_a5670b686a7d773,V_cd34508333aa66e3_a5670b686a7d773","no_crossref":1,"type":"int32_t"}
{"loc":"00047:16-23","source":1,"syntax":"type,use","pretty":"type IPC::Message","sym":"T_IPC::Message","type":"class IPC::Message","typesym":"T_IPC::Message"}
{"loc":"00047:25-36","source":1,"syntax":"function,use","pretty":"function IPC::Message::IPDLMessage","sym":"_ZN3IPC7Message11IPDLMessageEijjNS0_11HeaderFlagsE","type":"mozilla::UniquePtr<Message> (int32_t, IPC::Message::msgid_t, uint32_t, class IPC::Message::HeaderFlags)"}
{"loc":"00047:37-46","source":1,"syntax":"","pretty":"variable routingId","sym":"V_ce4f5d1c8f905ba9_a5670b686a7d773,V_cad5b940f87a9861_a5670b686a7d773,V_1bd579a6c76a5f4d_a5670b686a7d773,V_cd34508333aa66e3_a5670b686a7d773","no_crossref":1,"type":"int32_t"}
{"loc":"00047:48-61","source":1,"syntax":"enum,use","pretty":"enum mozilla::_ipdltest::PTestBasic::MessageType::Msg_Hello__ID","sym":"E_<T_mozilla::_ipdltest::PTestBasic::MessageType>_Msg_Hello__ID","type":"enum mozilla::_ipdltest::PTestBasic::MessageType","typesym":"T_mozilla::_ipdltest::PTestBasic::MessageType"}
{"loc":"00047:66-69","source":1,"syntax":"constructor,use","pretty":"constructor IPC::Message::HeaderFlags::HeaderFlags","sym":"_ZN3IPC7Message11HeaderFlagsC1ENS0_11NestedLevelENS0_13PriorityValueENS0_18MessageCompressionENS0_11ConstructorENS0_4SyncENS0_5ReplyE"}
{"loc":"00047:71-78","source":1,"syntax":"type,use","pretty":"type IPC::Message","sym":"T_IPC::Message","type":"class IPC::Message","typesym":"T_IPC::Message"}
{"loc":"00047:80-91","source":1,"syntax":"type,use","pretty":"type IPC::Message::HeaderFlags","sym":"T_IPC::Message::HeaderFlags","type":"class IPC::Message::HeaderFlags","typesym":"T_IPC::Message::HeaderFlags"}
{"loc":"00047:97-104","source":1,"syntax":"type,use","pretty":"type IPC::Message","sym":"T_IPC::Message","type":"class IPC::Message","typesym":"T_IPC::Message"}
{"loc":"00047:106-116","source":1,"syntax":"enum,use","pretty":"enum IPC::Message::NestedLevel::NOT_NESTED","sym":"E_<T_IPC::Message::NestedLevel>_NOT_NESTED","type":"enum IPC::Message::NestedLevel","typesym":"T_IPC::Message::NestedLevel"}
{"loc":"00047:123-130","source":1,"syntax":"type,use","pretty":"type IPC::Message","sym":"T_IPC::Message","type":"class IPC::Message","typesym":"T_IPC::Message"}
{"loc":"00047:132-147","source":1,"syntax":"enum,use","pretty":"enum IPC::Message::PriorityValue::NORMAL_PRIORITY","sym":"E_<T_IPC::Message::PriorityValue>_NORMAL_PRIORITY","type":"enum IPC::Message::PriorityValue","typesym":"T_IPC::Message::PriorityValue"}
{"loc":"00047:154-161","source":1,"syntax":"type,use","pretty":"type IPC::Message","sym":"T_IPC::Message","type":"class IPC::Message","typesym":"T_IPC::Message"}
{"loc":"00047:163-179","source":1,"syntax":"enum,use","pretty":"enum IPC::Message::MessageCompression::COMPRESSION_NONE","sym":"E_<T_IPC::Message::MessageCompression>_COMPRESSION_NONE","type":"enum IPC::Message::MessageCompression","typesym":"T_IPC::Message::MessageCompression"}
{"loc":"00047:186-193","source":1,"syntax":"type,use","pretty":"type IPC::Message","sym":"T_IPC::Message","type":"class IPC::Message","typesym":"T_IPC::Message"}
{"loc":"00047:195-210","source":1,"syntax":"enum,use","pretty":"enum IPC::Message::Constructor::NOT_CONSTRUCTOR","sym":"E_<T_IPC::Message::Constructor>_NOT_CONSTRUCTOR","type":"enum IPC::Message::Constructor","typesym":"T_IPC::Message::Constructor"}
{"loc":"00047:217-224","source":1,"syntax":"type,use","pretty":"type IPC::Message","sym":"T_IPC::Message","type":"class IPC::Message","typesym":"T_IPC::Message"}
{"loc":"00047:226-231","source":1,"syntax":"enum,use","pretty":"enum IPC::Message::Sync::ASYNC","sym":"E_<T_IPC::Message::Sync>_ASYNC","type":"enum IPC::Message::Sync","typesym":"T_IPC::Message::Sync"}
{"loc":"00047:238-245","source":1,"syntax":"type,use","pretty":"type IPC::Message","sym":"T_IPC::Message","type":"class IPC::Message","typesym":"T_IPC::Message"}
{"loc":"00047:247-256","source":1,"syntax":"enum,use","pretty":"enum IPC::Message::Reply::NOT_REPLY","sym":"E_<T_IPC::Message::Reply>_NOT_REPLY","type":"enum IPC::Message::Reply","typesym":"T_IPC::Message::Reply"}
{"loc":"00036:0-15","structured":1,"pretty":"mozilla::_ipdltest::PTestBasic::CreateEndpoints","sym":"_ZN7mozilla9_ipdltest10PTestBasic15CreateEndpointsEPNS_3ipc8EndpointINS0_16PTestBasicParentEEEPNS3_INS0_15PTestBasicChildEEE","kind":"function","implKind":"","sizeBytes":null,"supers":[],"methods":[],"fields":[],"overrides":[],"props":[]}
{"loc":"00025:0-15","structured":1,"pretty":"mozilla::_ipdltest::PTestBasic::CreateEndpoints","sym":"_ZN7mozilla9_ipdltest10PTestBasic15CreateEndpointsEiiPNS_3ipc8EndpointINS0_16PTestBasicParentEEEPNS3_INS0_15PTestBasicChildEEE","kind":"function","implKind":"","sizeBytes":null,"supers":[],"methods":[],"fields":[],"overrides":[],"props":[]}
{"loc":"00025:0-15","structured":1,"pretty":"mozilla::_ipdltest::PTestBasic::CreateEndpoints","sym":"_ZN7mozilla9_ipdltest10PTestBasic15CreateEndpointsEmmPNS_3ipc8EndpointINS0_16PTestBasicParentEEEPNS3_INS0_15PTestBasicChildEEE","kind":"function","implKind":"","sizeBytes":null,"supers":[],"methods":[],"fields":[],"overrides":[],"props":[]}
{"loc":"00045:0-9","structured":1,"pretty":"mozilla::_ipdltest::PTestBasic::Msg_Hello","sym":"_ZN7mozilla9_ipdltest10PTestBasic9Msg_HelloEi","kind":"function","implKind":"","sizeBytes":null,"supers":[],"methods":[],"fields":[],"overrides":[],"props":[]}

```

## tests/tests/mc-analysis/__GENERATED__/ipdl/PTestBasicChild.cpp
```
{"loc":"00001:0","target":1,"kind":"def","pretty":"__GENERATED__/ipc/ipdl/PTestBasicChild.cpp","sym":"FILE_android-armv7@__GENERATED__/ipc/ipdl/PTestBasicChild@2Ecpp"}
{"loc":"00007:9-46","target":1,"kind":"use","pretty":"__GENERATED__/ipc/ipdl/_ipdlheaders/mozilla/_ipdltest/PTestBasicChild.h","sym":"FILE_android-armv7@__GENERATED__/ipc/ipdl/_ipdlheaders/mozilla/_ipdltest/PTestBasicChild@2Eh"}
{"loc":"00008:9-35","target":1,"kind":"use","pretty":"tools/profiler/public/ProfilerLabels.h","sym":"FILE_tools/profiler/public/ProfilerLabels@2Eh"}
{"loc":"00009:9-45","target":1,"kind":"use","pretty":"ipc/ipdl/test/gtest/TestBasicChild.h","sym":"FILE_ipc/ipdl/test/gtest/TestBasicChild@2Eh"}
{"loc":"00012:9-32","target":1,"kind":"use","pretty":"ipc/glue/IPCMessageUtils.h","sym":"FILE_ipc/glue/IPCMessageUtils@2Eh"}
{"loc":"00013:9-47","target":1,"kind":"use","pretty":"ipc/glue/IPCMessageUtilsSpecializations.h","sym":"FILE_ipc/glue/IPCMessageUtilsSpecializations@2Eh"}
{"loc":"00014:9-20","target":1,"kind":"use","pretty":"__GENERATED__/dist/include/nsIFile.h","sym":"FILE_android-armv7@__GENERATED__/dist/include/nsIFile@2Eh"}
{"loc":"00015:9-33","target":1,"kind":"use","pretty":"ipc/glue/Endpoint.h","sym":"FILE_ipc/glue/Endpoint@2Eh"}
{"loc":"00016:9-45","target":1,"kind":"use","pretty":"ipc/glue/ProtocolMessageUtils.h","sym":"FILE_ipc/glue/ProtocolMessageUtils@2Eh"}
{"loc":"00017:9-38","target":1,"kind":"use","pretty":"ipc/glue/ProtocolUtils.h","sym":"FILE_ipc/glue/ProtocolUtils@2Eh"}
{"loc":"00018:9-42","target":1,"kind":"use","pretty":"ipc/glue/ShmemMessageUtils.h","sym":"FILE_ipc/glue/ShmemMessageUtils@2Eh"}
{"loc":"00019:9-41","target":1,"kind":"use","pretty":"ipc/glue/TaintingIPCUtils.h","sym":"FILE_ipc/glue/TaintingIPCUtils@2Eh"}
{"loc":"00021:10-17","target":1,"kind":"def","pretty":"mozilla","sym":"NS_mozilla"}
{"loc":"00022:10-19","target":1,"kind":"def","pretty":"mozilla::_ipdltest","sym":"NS_mozilla::_ipdltest"}
{"loc":"00025:22-37","target":1,"kind":"def","pretty":"mozilla::_ipdltest::PTestBasicChild::ProcessingError","sym":"_ZN7mozilla9_ipdltest15PTestBasicChild15ProcessingErrorENS_3ipc14HasResultCodes6ResultEPKc","peekRange":"25-27"}
{"loc":"00025:5-20","target":1,"kind":"use","pretty":"mozilla::_ipdltest::PTestBasicChild","sym":"T_mozilla::_ipdltest::PTestBasicChild","context":"mozilla::_ipdltest::PTestBasicChild::ProcessingError","contextsym":"_ZN7mozilla9_ipdltest15PTestBasicChild15ProcessingErrorENS_3ipc14HasResultCodes6ResultEPKc"}
{"loc":"00026:8-14","target":1,"kind":"use","pretty":"mozilla::ipc::HasResultCodes::Result","sym":"T_mozilla::ipc::HasResultCodes::Result","context":"mozilla::_ipdltest::PTestBasicChild::ProcessingError","contextsym":"_ZN7mozilla9_ipdltest15PTestBasicChild15ProcessingErrorENS_3ipc14HasResultCodes6ResultEPKc"}
{"loc":"00031:22-52","target":1,"kind":"def","pretty":"mozilla::_ipdltest::PTestBasicChild::ShouldContinueFromReplyTimeout","sym":"_ZN7mozilla9_ipdltest15PTestBasicChild30ShouldContinueFromReplyTimeoutEv","peekRange":"31-31"}
{"loc":"00031:5-20","target":1,"kind":"use","pretty":"mozilla::_ipdltest::PTestBasicChild","sym":"T_mozilla::_ipdltest::PTestBasicChild","context":"mozilla::_ipdltest::PTestBasicChild::ShouldContinueFromReplyTimeout","contextsym":"_ZN7mozilla9_ipdltest15PTestBasicChild30ShouldContinueFromReplyTimeoutEv"}
{"loc":"00036:0-12","target":1,"kind":"use","pretty":"MOZ_IMPLICIT","sym":"M_5f65895d5a5258f7"}
{"loc":"00036:13-28","target":1,"kind":"use","pretty":"mozilla::_ipdltest::PTestBasicChild","sym":"T_mozilla::_ipdltest::PTestBasicChild","context":"mozilla::_ipdltest::PTestBasicChild::PTestBasicChild","contextsym":"_ZN7mozilla9_ipdltest15PTestBasicChildC1Ev"}
{"loc":"00036:30-45","target":1,"kind":"def","pretty":"mozilla::_ipdltest::PTestBasicChild::PTestBasicChild","sym":"_ZN7mozilla9_ipdltest15PTestBasicChildC1Ev"}
{"loc":"00037:18-35","target":1,"kind":"use","pretty":"mozilla::ipc::IToplevelProtocol","sym":"T_mozilla::ipc::IToplevelProtocol","context":"mozilla::_ipdltest::PTestBasicChild::PTestBasicChild","contextsym":"_ZN7mozilla9_ipdltest15PTestBasicChildC1Ev"}
{"loc":"00037:4-11","target":1,"kind":"use","pretty":"mozilla::ipc::IToplevelProtocol::IToplevelProtocol","sym":"_ZN7mozilla3ipc17IToplevelProtocolC1EPKc15IPCMessageStartNS0_4SideE","context":"mozilla::_ipdltest::PTestBasicChild::PTestBasicChild","contextsym":"_ZN7mozilla9_ipdltest15PTestBasicChildC1Ev"}
{"loc":"00037:55-73","target":1,"kind":"use","pretty":"IPCMessageStart::PTestBasicMsgStart","sym":"E_<T_IPCMessageStart>_PTestBasicMsgStart","context":"mozilla::_ipdltest::PTestBasicChild::PTestBasicChild","contextsym":"_ZN7mozilla9_ipdltest15PTestBasicChildC1Ev"}
{"loc":"00037:89-98","target":1,"kind":"use","pretty":"mozilla::ipc::Side::ChildSide","sym":"E_<T_mozilla::ipc::Side>_ChildSide","context":"mozilla::_ipdltest::PTestBasicChild::PTestBasicChild","contextsym":"_ZN7mozilla9_ipdltest15PTestBasicChildC1Ev"}
{"loc":"00039:19-34","target":1,"kind":"use","pretty":"mozilla::_ipdltest::PTestBasicChild","sym":"T_mozilla::_ipdltest::PTestBasicChild","context":"mozilla::_ipdltest::PTestBasicChild::PTestBasicChild","contextsym":"_ZN7mozilla9_ipdltest15PTestBasicChildC1Ev"}
{"loc":"00039:4-18","target":1,"kind":"use","pretty":"MOZ_COUNT_CTOR","sym":"M_d9d58f5149d9f0f2"}
{"loc":"00042:0-15","target":1,"kind":"use","pretty":"mozilla::_ipdltest::PTestBasicChild","sym":"T_mozilla::_ipdltest::PTestBasicChild","context":"mozilla::_ipdltest::PTestBasicChild::~PTestBasicChild","contextsym":"_ZN7mozilla9_ipdltest15PTestBasicChildD1Ev"}
{"loc":"00042:18-33","target":1,"kind":"def","pretty":"mozilla::_ipdltest::PTestBasicChild::~PTestBasicChild","sym":"_ZN7mozilla9_ipdltest15PTestBasicChildD1Ev","peekRange":"42-42"}
{"loc":"00042:18-33","target":1,"kind":"use","pretty":"mozilla::_ipdltest::PTestBasicChild","sym":"T_mozilla::_ipdltest::PTestBasicChild","context":"mozilla::_ipdltest::PTestBasicChild::~PTestBasicChild","contextsym":"_ZN7mozilla9_ipdltest15PTestBasicChildD1Ev"}
{"loc":"00044:19-34","target":1,"kind":"use","pretty":"mozilla::_ipdltest::PTestBasicChild","sym":"T_mozilla::_ipdltest::PTestBasicChild","context":"mozilla::_ipdltest::PTestBasicChild::~PTestBasicChild","contextsym":"_ZN7mozilla9_ipdltest15PTestBasicChildD1Ev"}
{"loc":"00044:4-18","target":1,"kind":"use","pretty":"MOZ_COUNT_DTOR","sym":"M_f9e8c16149d9f0f2"}
{"loc":"00047:22-38","target":1,"kind":"def","pretty":"mozilla::_ipdltest::PTestBasicChild::AllManagedActors","sym":"_ZNK7mozilla9_ipdltest15PTestBasicChild16AllManagedActorsER8nsTArrayI6RefPtrINS_3ipc19ActorLifecycleProxyEEE","peekRange":"47-47"}
{"loc":"00047:39-47","target":1,"kind":"use","pretty":"nsTArray","sym":"T_nsTArray","context":"mozilla::_ipdltest::PTestBasicChild::AllManagedActors","contextsym":"_ZNK7mozilla9_ipdltest15PTestBasicChild16AllManagedActorsER8nsTArrayI6RefPtrINS_3ipc19ActorLifecycleProxyEEE"}
{"loc":"00047:48-54","target":1,"kind":"use","pretty":"RefPtr","sym":"T_RefPtr","context":"mozilla::_ipdltest::PTestBasicChild::AllManagedActors","contextsym":"_ZNK7mozilla9_ipdltest15PTestBasicChild16AllManagedActorsER8nsTArrayI6RefPtrINS_3ipc19ActorLifecycleProxyEEE"}
{"loc":"00047:5-20","target":1,"kind":"use","pretty":"mozilla::_ipdltest::PTestBasicChild","sym":"T_mozilla::_ipdltest::PTestBasicChild","context":"mozilla::_ipdltest::PTestBasicChild::AllManagedActors","contextsym":"_ZNK7mozilla9_ipdltest15PTestBasicChild16AllManagedActorsER8nsTArrayI6RefPtrINS_3ipc19ActorLifecycleProxyEEE"}
{"loc":"00047:69-88","target":1,"kind":"use","pretty":"mozilla::ipc::ActorLifecycleProxy","sym":"T_mozilla::ipc::ActorLifecycleProxy","context":"mozilla::_ipdltest::PTestBasicChild::AllManagedActors","contextsym":"_ZNK7mozilla9_ipdltest15PTestBasicChild16AllManagedActorsER8nsTArrayI6RefPtrINS_3ipc19ActorLifecycleProxyEEE"}
{"loc":"00049:4-12","target":1,"kind":"use","pretty":"uint32_t","sym":"T_uint32_t","context":"mozilla::_ipdltest::PTestBasicChild::AllManagedActors","contextsym":"_ZNK7mozilla9_ipdltest15PTestBasicChild16AllManagedActorsER8nsTArrayI6RefPtrINS_3ipc19ActorLifecycleProxyEEE"}
{"loc":"00050:10-21","target":1,"kind":"use","pretty":"nsTArray_Impl::SetCapacity","sym":"_ZN13nsTArray_Impl11SetCapacityEN13nsTArray_baseIT0_N27nsTArray_RelocationStrategyIT_E4TypeEE9size_typeE","context":"mozilla::_ipdltest::PTestBasicChild::AllManagedActors","contextsym":"_ZNK7mozilla9_ipdltest15PTestBasicChild16AllManagedActorsER8nsTArrayI6RefPtrINS_3ipc19ActorLifecycleProxyEEE"}
{"loc":"00054:22-35","target":1,"kind":"def","pretty":"mozilla::_ipdltest::PTestBasicChild::RemoveManagee","sym":"_ZN7mozilla9_ipdltest15PTestBasicChild13RemoveManageeEiPNS_3ipc9IProtocolE","peekRange":"54-56"}
{"loc":"00054:5-20","target":1,"kind":"use","pretty":"mozilla::_ipdltest::PTestBasicChild","sym":"T_mozilla::_ipdltest::PTestBasicChild","context":"mozilla::_ipdltest::PTestBasicChild::RemoveManagee","contextsym":"_ZN7mozilla9_ipdltest15PTestBasicChild13RemoveManageeEiPNS_3ipc9IProtocolE"}
{"loc":"00055:8-15","target":1,"kind":"use","pretty":"int32_t","sym":"T_int32_t","context":"mozilla::_ipdltest::PTestBasicChild::RemoveManagee","contextsym":"_ZN7mozilla9_ipdltest15PTestBasicChild13RemoveManageeEiPNS_3ipc9IProtocolE"}
{"loc":"00056:8-17","target":1,"kind":"use","pretty":"mozilla::_ipdltest::PTestBasicChild::IProtocol","sym":"T_mozilla::_ipdltest::PTestBasicChild::IProtocol","context":"mozilla::_ipdltest::PTestBasicChild::RemoveManagee","contextsym":"_ZN7mozilla9_ipdltest15PTestBasicChild13RemoveManageeEiPNS_3ipc9IProtocolE"}
{"loc":"00058:4-14","target":1,"kind":"use","pretty":"mozilla::ipc::IProtocol::FatalError","sym":"_ZNK7mozilla3ipc9IProtocol10FatalErrorEPKc","context":"mozilla::_ipdltest::PTestBasicChild::RemoveManagee","contextsym":"_ZN7mozilla9_ipdltest15PTestBasicChild13RemoveManageeEiPNS_3ipc9IProtocolE"}
{"loc":"00062:22-36","target":1,"kind":"def","pretty":"mozilla::_ipdltest::PTestBasicChild::DeallocManagee","sym":"_ZN7mozilla9_ipdltest15PTestBasicChild14DeallocManageeEiPNS_3ipc9IProtocolE","peekRange":"62-64"}
{"loc":"00062:5-20","target":1,"kind":"use","pretty":"mozilla::_ipdltest::PTestBasicChild","sym":"T_mozilla::_ipdltest::PTestBasicChild","context":"mozilla::_ipdltest::PTestBasicChild::DeallocManagee","contextsym":"_ZN7mozilla9_ipdltest15PTestBasicChild14DeallocManageeEiPNS_3ipc9IProtocolE"}
{"loc":"00063:8-15","target":1,"kind":"use","pretty":"int32_t","sym":"T_int32_t","context":"mozilla::_ipdltest::PTestBasicChild::DeallocManagee","contextsym":"_ZN7mozilla9_ipdltest15PTestBasicChild14DeallocManageeEiPNS_3ipc9IProtocolE"}
{"loc":"00064:8-17","target":1,"kind":"use","pretty":"mozilla::_ipdltest::PTestBasicChild::IProtocol","sym":"T_mozilla::_ipdltest::PTestBasicChild::IProtocol","context":"mozilla::_ipdltest::PTestBasicChild::DeallocManagee","contextsym":"_ZN7mozilla9_ipdltest15PTestBasicChild14DeallocManageeEiPNS_3ipc9IProtocolE"}
{"loc":"00066:4-14","target":1,"kind":"use","pretty":"mozilla::ipc::IProtocol::FatalError","sym":"_ZNK7mozilla3ipc9IProtocol10FatalErrorEPKc","context":"mozilla::_ipdltest::PTestBasicChild::DeallocManagee","contextsym":"_ZN7mozilla9_ipdltest15PTestBasicChild14DeallocManageeEiPNS_3ipc9IProtocolE"}
{"loc":"00070:22-39","target":1,"kind":"def","pretty":"mozilla::_ipdltest::PTestBasicChild::OnMessageReceived","sym":"_ZN7mozilla9_ipdltest15PTestBasicChild17OnMessageReceivedERKN3IPC7MessageE","peekRange":"70-70"}
{"loc":"00070:46-53","target":1,"kind":"use","pretty":"mozilla::_ipdltest::PTestBasicChild::Message","sym":"T_mozilla::_ipdltest::PTestBasicChild::Message","context":"mozilla::_ipdltest::PTestBasicChild::OnMessageReceived","contextsym":"_ZN7mozilla9_ipdltest15PTestBasicChild17OnMessageReceivedERKN3IPC7MessageE"}
{"loc":"00070:5-20","target":1,"kind":"use","pretty":"mozilla::_ipdltest::PTestBasicChild","sym":"T_mozilla::_ipdltest::PTestBasicChild","context":"mozilla::_ipdltest::PTestBasicChild::OnMessageReceived","contextsym":"_ZN7mozilla9_ipdltest15PTestBasicChild17OnMessageReceivedERKN3IPC7MessageE"}
{"loc":"00070:65-80","target":1,"kind":"use","pretty":"mozilla::_ipdltest::PTestBasicChild","sym":"T_mozilla::_ipdltest::PTestBasicChild","context":"mozilla::_ipdltest::PTestBasicChild::OnMessageReceived","contextsym":"_ZN7mozilla9_ipdltest15PTestBasicChild17OnMessageReceivedERKN3IPC7MessageE"}
{"loc":"00070:82-88","target":1,"kind":"use","pretty":"mozilla::ipc::HasResultCodes::Result","sym":"T_mozilla::ipc::HasResultCodes::Result","context":"mozilla::_ipdltest::PTestBasicChild::OnMessageReceived","contextsym":"_ZN7mozilla9_ipdltest15PTestBasicChild17OnMessageReceivedERKN3IPC7MessageE"}
{"loc":"00072:18-22","target":1,"kind":"use","pretty":"IPC::Message::type","sym":"_ZNK3IPC7Message4typeEv","context":"mozilla::_ipdltest::PTestBasicChild::OnMessageReceived","contextsym":"_ZN7mozilla9_ipdltest15PTestBasicChild17OnMessageReceivedERKN3IPC7MessageE"}
{"loc":"00073:21-34","target":1,"kind":"use","pretty":"mozilla::_ipdltest::PTestBasic::MessageType::Msg_Hello__ID","sym":"E_<T_mozilla::_ipdltest::PTestBasic::MessageType>_Msg_Hello__ID","context":"mozilla::_ipdltest::PTestBasicChild::OnMessageReceived","contextsym":"_ZN7mozilla9_ipdltest15PTestBasicChild17OnMessageReceivedERKN3IPC7MessageE"}
{"loc":"00075:30-47","target":1,"kind":"use","pretty":"mozilla::ipc::LoggingEnabledFor","sym":"_ZN7mozilla3ipc17LoggingEnabledForEPKc","context":"mozilla::_ipdltest::PTestBasicChild::OnMessageReceived","contextsym":"_ZN7mozilla9_ipdltest15PTestBasicChild17OnMessageReceivedERKN3IPC7MessageE"}
{"loc":"00076:30-51","target":1,"kind":"use","pretty":"mozilla::ipc::LogMessageForProtocol","sym":"_ZN7mozilla3ipc21LogMessageForProtocolEPKciS2_jNS0_16MessageDirectionE","context":"mozilla::_ipdltest::PTestBasicChild::OnMessageReceived","contextsym":"_ZN7mozilla9_ipdltest15PTestBasicChild17OnMessageReceivedERKN3IPC7MessageE"}
{"loc":"00078:26-42","target":1,"kind":"use","pretty":"mozilla::ipc::IProtocol::ToplevelProtocol","sym":"_ZN7mozilla3ipc9IProtocol16ToplevelProtocolEv","context":"mozilla::_ipdltest::PTestBasicChild::OnMessageReceived","contextsym":"_ZN7mozilla9_ipdltest15PTestBasicChild17OnMessageReceivedERKN3IPC7MessageE"}
{"loc":"00078:46-66","target":1,"kind":"use","pretty":"mozilla::ipc::IToplevelProtocol::OtherPidMaybeInvalid","sym":"_ZNK7mozilla3ipc17IToplevelProtocol20OtherPidMaybeInvalidEv","context":"mozilla::_ipdltest::PTestBasicChild::OnMessageReceived","contextsym":"_ZN7mozilla9_ipdltest15PTestBasicChild17OnMessageReceivedERKN3IPC7MessageE"}
{"loc":"00080:32-36","target":1,"kind":"use","pretty":"IPC::Message::type","sym":"_ZNK3IPC7Message4typeEv","context":"mozilla::_ipdltest::PTestBasicChild::OnMessageReceived","contextsym":"_ZN7mozilla9_ipdltest15PTestBasicChild17OnMessageReceivedERKN3IPC7MessageE"}
{"loc":"00081:34-50","target":1,"kind":"use","pretty":"mozilla::ipc::MessageDirection","sym":"T_mozilla::ipc::MessageDirection","context":"mozilla::_ipdltest::PTestBasicChild::OnMessageReceived","contextsym":"_ZN7mozilla9_ipdltest15PTestBasicChild17OnMessageReceivedERKN3IPC7MessageE"}
{"loc":"00081:52-62","target":1,"kind":"use","pretty":"mozilla::ipc::MessageDirection::eReceiving","sym":"E_<T_mozilla::ipc::MessageDirection>_eReceiving","context":"mozilla::_ipdltest::PTestBasicChild::OnMessageReceived","contextsym":"_ZN7mozilla9_ipdltest15PTestBasicChild17OnMessageReceivedERKN3IPC7MessageE"}
{"loc":"00083:12-31","target":1,"kind":"use","pretty":"AUTO_PROFILER_LABEL","sym":"M_99e0335dc782f2d6"}
{"loc":"00083:57-62","target":1,"kind":"use","pretty":"JS::ProfilingCategoryPair::OTHER","sym":"E_<T_JS::ProfilingCategoryPair>_OTHER","context":"mozilla::_ipdltest::PTestBasicChild::OnMessageReceived","contextsym":"_ZN7mozilla9_ipdltest15PTestBasicChild17OnMessageReceivedERKN3IPC7MessageE"}
{"loc":"00085:26-35","target":1,"kind":"use","pretty":"mozilla::ipc::IPCResult","sym":"T_mozilla::ipc::IPCResult","context":"mozilla::_ipdltest::PTestBasicChild::OnMessageReceived","contextsym":"_ZN7mozilla9_ipdltest15PTestBasicChild17OnMessageReceivedERKN3IPC7MessageE"}
{"loc":"00085:56-70","target":1,"kind":"use","pretty":"mozilla::_ipdltest::TestBasicChild","sym":"T_mozilla::_ipdltest::TestBasicChild","context":"mozilla::_ipdltest::PTestBasicChild::OnMessageReceived","contextsym":"_ZN7mozilla9_ipdltest15PTestBasicChild17OnMessageReceivedERKN3IPC7MessageE"}
{"loc":"00085:81-90","target":1,"kind":"use","pretty":"mozilla::_ipdltest::TestBasicChild::RecvHello","sym":"_ZN7mozilla9_ipdltest14TestBasicChild9RecvHelloEv","context":"mozilla::_ipdltest::PTestBasicChild::OnMessageReceived","contextsym":"_ZN7mozilla9_ipdltest15PTestBasicChild17OnMessageReceivedERKN3IPC7MessageE"}
{"loc":"00087:30-53","target":1,"kind":"use","pretty":"mozilla::ipc::ProtocolErrorBreakpoint","sym":"_ZN7mozilla3ipc23ProtocolErrorBreakpointEPKc","context":"mozilla::_ipdltest::PTestBasicChild::OnMessageReceived","contextsym":"_ZN7mozilla9_ipdltest15PTestBasicChild17OnMessageReceivedERKN3IPC7MessageE"}
{"loc":"00089:23-41","target":1,"kind":"use","pretty":"mozilla::ipc::HasResultCodes::Result::MsgProcessingError","sym":"E_<T_mozilla::ipc::HasResultCodes::Result>_MsgProcessingError","context":"mozilla::_ipdltest::PTestBasicChild::OnMessageReceived","contextsym":"_ZN7mozilla9_ipdltest15PTestBasicChild17OnMessageReceivedERKN3IPC7MessageE"}
{"loc":"00092:19-31","target":1,"kind":"use","pretty":"mozilla::ipc::HasResultCodes::Result::MsgProcessed","sym":"E_<T_mozilla::ipc::HasResultCodes::Result>_MsgProcessed","context":"mozilla::_ipdltest::PTestBasicChild::OnMessageReceived","contextsym":"_ZN7mozilla9_ipdltest15PTestBasicChild17OnMessageReceivedERKN3IPC7MessageE"}
{"loc":"00095:15-26","target":1,"kind":"use","pretty":"mozilla::ipc::HasResultCodes::Result::MsgNotKnown","sym":"E_<T_mozilla::ipc::HasResultCodes::Result>_MsgNotKnown","context":"mozilla::_ipdltest::PTestBasicChild::OnMessageReceived","contextsym":"_ZN7mozilla9_ipdltest15PTestBasicChild17OnMessageReceivedERKN3IPC7MessageE"}
{"loc":"00096:9-35","target":1,"kind":"use","pretty":"SHMEM_CREATED_MESSAGE_TYPE","sym":"E_<T_b078e220e89184de>_SHMEM_CREATED_MESSAGE_TYPE","context":"mozilla::_ipdltest::PTestBasicChild::OnMessageReceived","contextsym":"_ZN7mozilla9_ipdltest15PTestBasicChild17OnMessageReceivedERKN3IPC7MessageE"}
{"loc":"00098:12-22","target":1,"kind":"use","pretty":"mozilla::ipc::IProtocol::FatalError","sym":"_ZNK7mozilla3ipc9IProtocol10FatalErrorEPKc","context":"mozilla::_ipdltest::PTestBasicChild::OnMessageReceived","contextsym":"_ZN7mozilla9_ipdltest15PTestBasicChild17OnMessageReceivedERKN3IPC7MessageE"}
{"loc":"00099:19-30","target":1,"kind":"use","pretty":"mozilla::ipc::HasResultCodes::Result::MsgNotKnown","sym":"E_<T_mozilla::ipc::HasResultCodes::Result>_MsgNotKnown","context":"mozilla::_ipdltest::PTestBasicChild::OnMessageReceived","contextsym":"_ZN7mozilla9_ipdltest15PTestBasicChild17OnMessageReceivedERKN3IPC7MessageE"}
{"loc":"00101:9-37","target":1,"kind":"use","pretty":"SHMEM_DESTROYED_MESSAGE_TYPE","sym":"E_<T_b078e220e89184de>_SHMEM_DESTROYED_MESSAGE_TYPE","context":"mozilla::_ipdltest::PTestBasicChild::OnMessageReceived","contextsym":"_ZN7mozilla9_ipdltest15PTestBasicChild17OnMessageReceivedERKN3IPC7MessageE"}
{"loc":"00103:12-22","target":1,"kind":"use","pretty":"mozilla::ipc::IProtocol::FatalError","sym":"_ZNK7mozilla3ipc9IProtocol10FatalErrorEPKc","context":"mozilla::_ipdltest::PTestBasicChild::OnMessageReceived","contextsym":"_ZN7mozilla9_ipdltest15PTestBasicChild17OnMessageReceivedERKN3IPC7MessageE"}
{"loc":"00104:19-30","target":1,"kind":"use","pretty":"mozilla::ipc::HasResultCodes::Result::MsgNotKnown","sym":"E_<T_mozilla::ipc::HasResultCodes::Result>_MsgNotKnown","context":"mozilla::_ipdltest::PTestBasicChild::OnMessageReceived","contextsym":"_ZN7mozilla9_ipdltest15PTestBasicChild17OnMessageReceivedERKN3IPC7MessageE"}
{"loc":"00109:22-39","target":1,"kind":"def","pretty":"mozilla::_ipdltest::PTestBasicChild::OnMessageReceived","sym":"_ZN7mozilla9_ipdltest15PTestBasicChild17OnMessageReceivedERKN3IPC7MessageERNS_9UniquePtrIS3_NS_13DefaultDeleteIS3_EEEE","peekRange":"109-111"}
{"loc":"00109:5-20","target":1,"kind":"use","pretty":"mozilla::_ipdltest::PTestBasicChild","sym":"T_mozilla::_ipdltest::PTestBasicChild","context":"mozilla::_ipdltest::PTestBasicChild::OnMessageReceived","contextsym":"_ZN7mozilla9_ipdltest15PTestBasicChild17OnMessageReceivedERKN3IPC7MessageERNS_9UniquePtrIS3_NS_13DefaultDeleteIS3_EEEE"}
{"loc":"00110:14-21","target":1,"kind":"use","pretty":"mozilla::_ipdltest::PTestBasicChild::Message","sym":"T_mozilla::_ipdltest::PTestBasicChild::Message","context":"mozilla::_ipdltest::PTestBasicChild::OnMessageReceived","contextsym":"_ZN7mozilla9_ipdltest15PTestBasicChild17OnMessageReceivedERKN3IPC7MessageERNS_9UniquePtrIS3_NS_13DefaultDeleteIS3_EEEE"}
{"loc":"00111:18-25","target":1,"kind":"use","pretty":"mozilla::_ipdltest::PTestBasicChild::Message","sym":"T_mozilla::_ipdltest::PTestBasicChild::Message","context":"mozilla::_ipdltest::PTestBasicChild::OnMessageReceived","contextsym":"_ZN7mozilla9_ipdltest15PTestBasicChild17OnMessageReceivedERKN3IPC7MessageERNS_9UniquePtrIS3_NS_13DefaultDeleteIS3_EEEE"}
{"loc":"00111:40-55","target":1,"kind":"use","pretty":"mozilla::_ipdltest::PTestBasicChild","sym":"T_mozilla::_ipdltest::PTestBasicChild","context":"mozilla::_ipdltest::PTestBasicChild::OnMessageReceived","contextsym":"_ZN7mozilla9_ipdltest15PTestBasicChild17OnMessageReceivedERKN3IPC7MessageERNS_9UniquePtrIS3_NS_13DefaultDeleteIS3_EEEE"}
{"loc":"00111:57-63","target":1,"kind":"use","pretty":"mozilla::ipc::HasResultCodes::Result","sym":"T_mozilla::ipc::HasResultCodes::Result","context":"mozilla::_ipdltest::PTestBasicChild::OnMessageReceived","contextsym":"_ZN7mozilla9_ipdltest15PTestBasicChild17OnMessageReceivedERKN3IPC7MessageERNS_9UniquePtrIS3_NS_13DefaultDeleteIS3_EEEE"}
{"loc":"00111:8-17","target":1,"kind":"use","pretty":"mozilla::_ipdltest::PTestBasicChild::UniquePtr","sym":"T_mozilla::_ipdltest::PTestBasicChild::UniquePtr","context":"mozilla::_ipdltest::PTestBasicChild::OnMessageReceived","contextsym":"_ZN7mozilla9_ipdltest15PTestBasicChild17OnMessageReceivedERKN3IPC7MessageERNS_9UniquePtrIS3_NS_13DefaultDeleteIS3_EEEE"}
{"loc":"00113:4-26","target":1,"kind":"use","pretty":"MOZ_ASSERT_UNREACHABLE","sym":"M_0d9562839add515c"}
{"loc":"00114:11-22","target":1,"kind":"use","pretty":"mozilla::ipc::HasResultCodes::Result::MsgNotKnown","sym":"E_<T_mozilla::ipc::HasResultCodes::Result>_MsgNotKnown","context":"mozilla::_ipdltest::PTestBasicChild::OnMessageReceived","contextsym":"_ZN7mozilla9_ipdltest15PTestBasicChild17OnMessageReceivedERKN3IPC7MessageERNS_9UniquePtrIS3_NS_13DefaultDeleteIS3_EEEE"}
{"loc":"00117:22-36","target":1,"kind":"def","pretty":"mozilla::_ipdltest::PTestBasicChild::OnCallReceived","sym":"_ZN7mozilla9_ipdltest15PTestBasicChild14OnCallReceivedERKN3IPC7MessageERNS_9UniquePtrIS3_NS_13DefaultDeleteIS3_EEEE","peekRange":"117-119"}
{"loc":"00117:5-20","target":1,"kind":"use","pretty":"mozilla::_ipdltest::PTestBasicChild","sym":"T_mozilla::_ipdltest::PTestBasicChild","context":"mozilla::_ipdltest::PTestBasicChild::OnCallReceived","contextsym":"_ZN7mozilla9_ipdltest15PTestBasicChild14OnCallReceivedERKN3IPC7MessageERNS_9UniquePtrIS3_NS_13DefaultDeleteIS3_EEEE"}
{"loc":"00118:14-21","target":1,"kind":"use","pretty":"mozilla::_ipdltest::PTestBasicChild::Message","sym":"T_mozilla::_ipdltest::PTestBasicChild::Message","context":"mozilla::_ipdltest::PTestBasicChild::OnCallReceived","contextsym":"_ZN7mozilla9_ipdltest15PTestBasicChild14OnCallReceivedERKN3IPC7MessageERNS_9UniquePtrIS3_NS_13DefaultDeleteIS3_EEEE"}
{"loc":"00119:18-25","target":1,"kind":"use","pretty":"mozilla::_ipdltest::PTestBasicChild::Message","sym":"T_mozilla::_ipdltest::PTestBasicChild::Message","context":"mozilla::_ipdltest::PTestBasicChild::OnCallReceived","contextsym":"_ZN7mozilla9_ipdltest15PTestBasicChild14OnCallReceivedERKN3IPC7MessageERNS_9UniquePtrIS3_NS_13DefaultDeleteIS3_EEEE"}
{"loc":"00119:40-55","target":1,"kind":"use","pretty":"mozilla::_ipdltest::PTestBasicChild","sym":"T_mozilla::_ipdltest::PTestBasicChild","context":"mozilla::_ipdltest::PTestBasicChild::OnCallReceived","contextsym":"_ZN7mozilla9_ipdltest15PTestBasicChild14OnCallReceivedERKN3IPC7MessageERNS_9UniquePtrIS3_NS_13DefaultDeleteIS3_EEEE"}
{"loc":"00119:57-63","target":1,"kind":"use","pretty":"mozilla::ipc::HasResultCodes::Result","sym":"T_mozilla::ipc::HasResultCodes::Result","context":"mozilla::_ipdltest::PTestBasicChild::OnCallReceived","contextsym":"_ZN7mozilla9_ipdltest15PTestBasicChild14OnCallReceivedERKN3IPC7MessageERNS_9UniquePtrIS3_NS_13DefaultDeleteIS3_EEEE"}
{"loc":"00119:8-17","target":1,"kind":"use","pretty":"mozilla::_ipdltest::PTestBasicChild::UniquePtr","sym":"T_mozilla::_ipdltest::PTestBasicChild::UniquePtr","context":"mozilla::_ipdltest::PTestBasicChild::OnCallReceived","contextsym":"_ZN7mozilla9_ipdltest15PTestBasicChild14OnCallReceivedERKN3IPC7MessageERNS_9UniquePtrIS3_NS_13DefaultDeleteIS3_EEEE"}
{"loc":"00121:4-26","target":1,"kind":"use","pretty":"MOZ_ASSERT_UNREACHABLE","sym":"M_0d9562839add515c"}
{"loc":"00122:11-22","target":1,"kind":"use","pretty":"mozilla::ipc::HasResultCodes::Result::MsgNotKnown","sym":"E_<T_mozilla::ipc::HasResultCodes::Result>_MsgNotKnown","context":"mozilla::_ipdltest::PTestBasicChild::OnCallReceived","contextsym":"_ZN7mozilla9_ipdltest15PTestBasicChild14OnCallReceivedERKN3IPC7MessageERNS_9UniquePtrIS3_NS_13DefaultDeleteIS3_EEEE"}
{"loc":"00125:22-36","target":1,"kind":"def","pretty":"mozilla::_ipdltest::PTestBasicChild::OnChannelClose","sym":"_ZN7mozilla9_ipdltest15PTestBasicChild14OnChannelCloseEv","peekRange":"125-125"}
{"loc":"00125:5-20","target":1,"kind":"use","pretty":"mozilla::_ipdltest::PTestBasicChild","sym":"T_mozilla::_ipdltest::PTestBasicChild","context":"mozilla::_ipdltest::PTestBasicChild::OnChannelClose","contextsym":"_ZN7mozilla9_ipdltest15PTestBasicChild14OnChannelCloseEv"}
{"loc":"00127:19-33","target":1,"kind":"use","pretty":"mozilla::ipc::IProtocol::ActorDestroyReason::NormalShutdown","sym":"E_<T_mozilla::ipc::IProtocol::ActorDestroyReason>_NormalShutdown","context":"mozilla::_ipdltest::PTestBasicChild::OnChannelClose","contextsym":"_ZN7mozilla9_ipdltest15PTestBasicChild14OnChannelCloseEv"}
{"loc":"00127:4-18","target":1,"kind":"use","pretty":"mozilla::ipc::IProtocol::DestroySubtree","sym":"_ZN7mozilla3ipc9IProtocol14DestroySubtreeENS1_18ActorDestroyReasonE","context":"mozilla::_ipdltest::PTestBasicChild::OnChannelClose","contextsym":"_ZN7mozilla9_ipdltest15PTestBasicChild14OnChannelCloseEv"}
{"loc":"00128:4-16","target":1,"kind":"use","pretty":"mozilla::_ipdltest::PTestBasicChild::ClearSubtree","sym":"_ZN7mozilla9_ipdltest15PTestBasicChild12ClearSubtreeEv","context":"mozilla::_ipdltest::PTestBasicChild::OnChannelClose","contextsym":"_ZN7mozilla9_ipdltest15PTestBasicChild14OnChannelCloseEv"}
{"loc":"00129:4-17","target":1,"kind":"use","pretty":"mozilla::ipc::IToplevelProtocol::DeallocShmems","sym":"_ZN7mozilla3ipc17IToplevelProtocol13DeallocShmemsEv","context":"mozilla::_ipdltest::PTestBasicChild::OnChannelClose","contextsym":"_ZN7mozilla9_ipdltest15PTestBasicChild14OnChannelCloseEv"}
{"loc":"00130:8-25","target":1,"kind":"use","pretty":"mozilla::ipc::IProtocol::GetLifecycleProxy","sym":"_ZN7mozilla3ipc9IProtocol17GetLifecycleProxyEv","context":"mozilla::_ipdltest::PTestBasicChild::OnChannelClose","contextsym":"_ZN7mozilla9_ipdltest15PTestBasicChild14OnChannelCloseEv"}
{"loc":"00131:29-36","target":1,"kind":"use","pretty":"mozilla::ipc::ActorLifecycleProxy::Release","sym":"_ZN7mozilla3ipc19ActorLifecycleProxy7ReleaseEv","context":"mozilla::_ipdltest::PTestBasicChild::OnChannelClose","contextsym":"_ZN7mozilla9_ipdltest15PTestBasicChild14OnChannelCloseEv"}
{"loc":"00131:8-25","target":1,"kind":"use","pretty":"mozilla::ipc::IProtocol::GetLifecycleProxy","sym":"_ZN7mozilla3ipc9IProtocol17GetLifecycleProxyEv","context":"mozilla::_ipdltest::PTestBasicChild::OnChannelClose","contextsym":"_ZN7mozilla9_ipdltest15PTestBasicChild14OnChannelCloseEv"}
{"loc":"00135:22-36","target":1,"kind":"def","pretty":"mozilla::_ipdltest::PTestBasicChild::OnChannelError","sym":"_ZN7mozilla9_ipdltest15PTestBasicChild14OnChannelErrorEv","peekRange":"135-135"}
{"loc":"00135:5-20","target":1,"kind":"use","pretty":"mozilla::_ipdltest::PTestBasicChild","sym":"T_mozilla::_ipdltest::PTestBasicChild","context":"mozilla::_ipdltest::PTestBasicChild::OnChannelError","contextsym":"_ZN7mozilla9_ipdltest15PTestBasicChild14OnChannelErrorEv"}
{"loc":"00137:19-35","target":1,"kind":"use","pretty":"mozilla::ipc::IProtocol::ActorDestroyReason::AbnormalShutdown","sym":"E_<T_mozilla::ipc::IProtocol::ActorDestroyReason>_AbnormalShutdown","context":"mozilla::_ipdltest::PTestBasicChild::OnChannelError","contextsym":"_ZN7mozilla9_ipdltest15PTestBasicChild14OnChannelErrorEv"}
{"loc":"00137:4-18","target":1,"kind":"use","pretty":"mozilla::ipc::IProtocol::DestroySubtree","sym":"_ZN7mozilla3ipc9IProtocol14DestroySubtreeENS1_18ActorDestroyReasonE","context":"mozilla::_ipdltest::PTestBasicChild::OnChannelError","contextsym":"_ZN7mozilla9_ipdltest15PTestBasicChild14OnChannelErrorEv"}
{"loc":"00138:4-16","target":1,"kind":"use","pretty":"mozilla::_ipdltest::PTestBasicChild::ClearSubtree","sym":"_ZN7mozilla9_ipdltest15PTestBasicChild12ClearSubtreeEv","context":"mozilla::_ipdltest::PTestBasicChild::OnChannelError","contextsym":"_ZN7mozilla9_ipdltest15PTestBasicChild14OnChannelErrorEv"}
{"loc":"00139:4-17","target":1,"kind":"use","pretty":"mozilla::ipc::IToplevelProtocol::DeallocShmems","sym":"_ZN7mozilla3ipc17IToplevelProtocol13DeallocShmemsEv","context":"mozilla::_ipdltest::PTestBasicChild::OnChannelError","contextsym":"_ZN7mozilla9_ipdltest15PTestBasicChild14OnChannelErrorEv"}
{"loc":"00140:8-25","target":1,"kind":"use","pretty":"mozilla::ipc::IProtocol::GetLifecycleProxy","sym":"_ZN7mozilla3ipc9IProtocol17GetLifecycleProxyEv","context":"mozilla::_ipdltest::PTestBasicChild::OnChannelError","contextsym":"_ZN7mozilla9_ipdltest15PTestBasicChild14OnChannelErrorEv"}
{"loc":"00141:29-36","target":1,"kind":"use","pretty":"mozilla::ipc::ActorLifecycleProxy::Release","sym":"_ZN7mozilla3ipc19ActorLifecycleProxy7ReleaseEv","context":"mozilla::_ipdltest::PTestBasicChild::OnChannelError","contextsym":"_ZN7mozilla9_ipdltest15PTestBasicChild14OnChannelErrorEv"}
{"loc":"00141:8-25","target":1,"kind":"use","pretty":"mozilla::ipc::IProtocol::GetLifecycleProxy","sym":"_ZN7mozilla3ipc9IProtocol17GetLifecycleProxyEv","context":"mozilla::_ipdltest::PTestBasicChild::OnChannelError","contextsym":"_ZN7mozilla9_ipdltest15PTestBasicChild14OnChannelErrorEv"}
{"loc":"00145:22-34","target":1,"kind":"def","pretty":"mozilla::_ipdltest::PTestBasicChild::ClearSubtree","sym":"_ZN7mozilla9_ipdltest15PTestBasicChild12ClearSubtreeEv","peekRange":"145-145"}
{"loc":"00145:5-20","target":1,"kind":"use","pretty":"mozilla::_ipdltest::PTestBasicChild","sym":"T_mozilla::_ipdltest::PTestBasicChild","context":"mozilla::_ipdltest::PTestBasicChild::ClearSubtree","contextsym":"_ZN7mozilla9_ipdltest15PTestBasicChild12ClearSubtreeEv"}
{"loc":"00153:10-13","target":1,"kind":"def","pretty":"IPC","sym":"NS_IPC"}
{"loc":"00154:37-52","target":1,"kind":"use","pretty":"mozilla::_ipdltest::PTestBasicChild","sym":"T_mozilla::_ipdltest::PTestBasicChild","context":"IPC::ParamTraits<mozilla::_ipdltest::PTestBasicChild *>::Write","contextsym":"_ZN3IPC11ParamTraitsIPN7mozilla9_ipdltest15PTestBasicChildEE5WriteEPNS_13MessageWriterERKS4_"}
{"loc":"00154:5-16","target":1,"kind":"use","pretty":"IPC::ParamTraits","sym":"T_IPC::ParamTraits","context":"IPC::ParamTraits<mozilla::_ipdltest::PTestBasicChild *>::Write","contextsym":"_ZN3IPC11ParamTraitsIPN7mozilla9_ipdltest15PTestBasicChildEE5WriteEPNS_13MessageWriterERKS4_"}
{"loc":"00154:56-61","target":1,"kind":"def","pretty":"IPC::ParamTraits<mozilla::_ipdltest::PTestBasicChild *>::Write","sym":"_ZN3IPC11ParamTraitsIPN7mozilla9_ipdltest15PTestBasicChildEE5WriteEPNS_13MessageWriterERKS4_","peekRange":"154-156"}
{"loc":"00155:13-26","target":1,"kind":"use","pretty":"IPC::MessageWriter","sym":"T_IPC::MessageWriter","context":"IPC::ParamTraits<mozilla::_ipdltest::PTestBasicChild *>::Write","contextsym":"_ZN3IPC11ParamTraitsIPN7mozilla9_ipdltest15PTestBasicChildEE5WriteEPNS_13MessageWriterERKS4_"}
{"loc":"00156:14-23","target":1,"kind":"use","pretty":"IPC::ParamTraits<mozilla::_ipdltest::PTestBasicChild *>::paramType","sym":"T_IPC::ParamTraits<mozilla::_ipdltest::PTestBasicChild_*>::paramType","context":"IPC::ParamTraits<mozilla::_ipdltest::PTestBasicChild *>::Write","contextsym":"_ZN3IPC11ParamTraitsIPN7mozilla9_ipdltest15PTestBasicChildEE5WriteEPNS_13MessageWriterERKS4_"}
{"loc":"00158:4-22","target":1,"kind":"use","pretty":"MOZ_RELEASE_ASSERT","sym":"M_96f821839add515c"}
{"loc":"00159:17-25","target":1,"kind":"use","pretty":"IPC::MessageWriter::GetActor","sym":"_ZNK3IPC13MessageWriter8GetActorEv","context":"IPC::ParamTraits<mozilla::_ipdltest::PTestBasicChild *>::Write","contextsym":"_ZN3IPC11ParamTraitsIPN7mozilla9_ipdltest15PTestBasicChildEE5WriteEPNS_13MessageWriterERKS4_"}
{"loc":"00162:4-11","target":1,"kind":"use","pretty":"int32_t","sym":"T_int32_t","context":"IPC::ParamTraits<mozilla::_ipdltest::PTestBasicChild *>::Write","contextsym":"_ZN3IPC11ParamTraitsIPN7mozilla9_ipdltest15PTestBasicChildEE5WriteEPNS_13MessageWriterERKS4_"}
{"loc":"00166:19-21","target":1,"kind":"use","pretty":"mozilla::ipc::IProtocol::Id","sym":"_ZNK7mozilla3ipc9IProtocol2IdEv","context":"IPC::ParamTraits<mozilla::_ipdltest::PTestBasicChild *>::Write","contextsym":"_ZN3IPC11ParamTraitsIPN7mozilla9_ipdltest15PTestBasicChildEE5WriteEPNS_13MessageWriterERKS4_"}
{"loc":"00168:18-28","target":1,"kind":"use","pretty":"mozilla::ipc::IProtocol::FatalError","sym":"_ZNK7mozilla3ipc9IProtocol10FatalErrorEPKc","context":"IPC::ParamTraits<mozilla::_ipdltest::PTestBasicChild *>::Write","contextsym":"_ZN3IPC11ParamTraitsIPN7mozilla9_ipdltest15PTestBasicChildEE5WriteEPNS_13MessageWriterERKS4_"}
{"loc":"00170:8-26","target":1,"kind":"use","pretty":"MOZ_RELEASE_ASSERT","sym":"M_96f821839add515c"}
{"loc":"00171:21-29","target":1,"kind":"use","pretty":"IPC::MessageWriter::GetActor","sym":"_ZNK3IPC13MessageWriter8GetActorEv","context":"IPC::ParamTraits<mozilla::_ipdltest::PTestBasicChild *>::Write","contextsym":"_ZN3IPC11ParamTraitsIPN7mozilla9_ipdltest15PTestBasicChildEE5WriteEPNS_13MessageWriterERKS4_"}
{"loc":"00171:33-46","target":1,"kind":"use","pretty":"mozilla::ipc::IProtocol::GetIPCChannel","sym":"_ZN7mozilla3ipc9IProtocol13GetIPCChannelEv","context":"IPC::ParamTraits<mozilla::_ipdltest::PTestBasicChild *>::Write","contextsym":"_ZN3IPC11ParamTraitsIPN7mozilla9_ipdltest15PTestBasicChildEE5WriteEPNS_13MessageWriterERKS4_"}
{"loc":"00171:58-71","target":1,"kind":"use","pretty":"mozilla::ipc::IToplevelProtocol::GetIPCChannel","sym":"_ZN7mozilla3ipc17IToplevelProtocol13GetIPCChannelEv","context":"IPC::ParamTraits<mozilla::_ipdltest::PTestBasicChild *>::Write","contextsym":"_ZN3IPC11ParamTraitsIPN7mozilla9_ipdltest15PTestBasicChildEE5WriteEPNS_13MessageWriterERKS4_"}
{"loc":"00174:8-26","target":1,"kind":"use","pretty":"MOZ_RELEASE_ASSERT","sym":"M_96f821839add515c"}
{"loc":"00175:18-25","target":1,"kind":"use","pretty":"mozilla::ipc::IProtocol::CanSend","sym":"_ZNK7mozilla3ipc9IProtocol7CanSendEv","context":"IPC::ParamTraits<mozilla::_ipdltest::PTestBasicChild *>::Write","contextsym":"_ZN3IPC11ParamTraitsIPN7mozilla9_ipdltest15PTestBasicChildEE5WriteEPNS_13MessageWriterERKS4_"}
{"loc":"00179:9-19","target":1,"kind":"use","pretty":"IPC::WriteParam","sym":"_ZN3IPCL10WriteParamEPNS_13MessageWriterEOT_","context":"IPC::ParamTraits<mozilla::_ipdltest::PTestBasicChild *>::Write","contextsym":"_ZN3IPC11ParamTraitsIPN7mozilla9_ipdltest15PTestBasicChildEE5WriteEPNS_13MessageWriterERKS4_"}
{"loc":"00182:37-52","target":1,"kind":"use","pretty":"mozilla::_ipdltest::PTestBasicChild","sym":"T_mozilla::_ipdltest::PTestBasicChild","context":"IPC::ParamTraits<mozilla::_ipdltest::PTestBasicChild *>::Read","contextsym":"_ZN3IPC11ParamTraitsIPN7mozilla9_ipdltest15PTestBasicChildEE4ReadEPNS_13MessageReaderEPS4_"}
{"loc":"00182:5-16","target":1,"kind":"use","pretty":"IPC::ParamTraits","sym":"T_IPC::ParamTraits","context":"IPC::ParamTraits<mozilla::_ipdltest::PTestBasicChild *>::Read","contextsym":"_ZN3IPC11ParamTraitsIPN7mozilla9_ipdltest15PTestBasicChildEE4ReadEPNS_13MessageReaderEPS4_"}
{"loc":"00182:56-60","target":1,"kind":"def","pretty":"IPC::ParamTraits<mozilla::_ipdltest::PTestBasicChild *>::Read","sym":"_ZN3IPC11ParamTraitsIPN7mozilla9_ipdltest15PTestBasicChildEE4ReadEPNS_13MessageReaderEPS4_","peekRange":"182-184"}
{"loc":"00183:13-26","target":1,"kind":"use","pretty":"IPC::MessageReader","sym":"T_IPC::MessageReader","context":"IPC::ParamTraits<mozilla::_ipdltest::PTestBasicChild *>::Read","contextsym":"_ZN3IPC11ParamTraitsIPN7mozilla9_ipdltest15PTestBasicChildEE4ReadEPNS_13MessageReaderEPS4_"}
{"loc":"00184:8-17","target":1,"kind":"use","pretty":"IPC::ParamTraits<mozilla::_ipdltest::PTestBasicChild *>::paramType","sym":"T_IPC::ParamTraits<mozilla::_ipdltest::PTestBasicChild_*>::paramType","context":"IPC::ParamTraits<mozilla::_ipdltest::PTestBasicChild *>::Read","contextsym":"_ZN3IPC11ParamTraitsIPN7mozilla9_ipdltest15PTestBasicChildEE4ReadEPNS_13MessageReaderEPS4_"}
{"loc":"00186:4-22","target":1,"kind":"use","pretty":"MOZ_RELEASE_ASSERT","sym":"M_96f821839add515c"}
{"loc":"00187:17-25","target":1,"kind":"use","pretty":"IPC::MessageReader::GetActor","sym":"_ZNK3IPC13MessageReader8GetActorEv","context":"IPC::ParamTraits<mozilla::_ipdltest::PTestBasicChild *>::Read","contextsym":"_ZN3IPC11ParamTraitsIPN7mozilla9_ipdltest15PTestBasicChildEE4ReadEPNS_13MessageReaderEPS4_"}
{"loc":"00190:13-18","target":1,"kind":"use","pretty":"mozilla::Maybe","sym":"T_mozilla::Maybe","context":"IPC::ParamTraits<mozilla::_ipdltest::PTestBasicChild *>::Read","contextsym":"_ZN3IPC11ParamTraitsIPN7mozilla9_ipdltest15PTestBasicChildEE4ReadEPNS_13MessageReaderEPS4_"}
{"loc":"00190:33-42","target":1,"kind":"use","pretty":"mozilla::ipc::IProtocol","sym":"T_mozilla::ipc::IProtocol","context":"IPC::ParamTraits<mozilla::_ipdltest::PTestBasicChild *>::Read","contextsym":"_ZN3IPC11ParamTraitsIPN7mozilla9_ipdltest15PTestBasicChildEE4ReadEPNS_13MessageReaderEPS4_"}
{"loc":"00191:17-25","target":1,"kind":"use","pretty":"IPC::MessageReader::GetActor","sym":"_ZNK3IPC13MessageReader8GetActorEv","context":"IPC::ParamTraits<mozilla::_ipdltest::PTestBasicChild *>::Read","contextsym":"_ZN3IPC11ParamTraitsIPN7mozilla9_ipdltest15PTestBasicChildEE4ReadEPNS_13MessageReaderEPS4_"}
{"loc":"00191:29-38","target":1,"kind":"use","pretty":"mozilla::ipc::IProtocol::ReadActor","sym":"_ZN7mozilla3ipc9IProtocol9ReadActorEPN3IPC13MessageReaderEbPKci","context":"IPC::ParamTraits<mozilla::_ipdltest::PTestBasicChild *>::Read","contextsym":"_ZN3IPC11ParamTraitsIPN7mozilla9_ipdltest15PTestBasicChildEE4ReadEPNS_13MessageReaderEPS4_"}
{"loc":"00191:68-86","target":1,"kind":"use","pretty":"IPCMessageStart::PTestBasicMsgStart","sym":"E_<T_IPCMessageStart>_PTestBasicMsgStart","context":"IPC::ParamTraits<mozilla::_ipdltest::PTestBasicChild *>::Read","contextsym":"_ZN3IPC11ParamTraitsIPN7mozilla9_ipdltest15PTestBasicChildEE4ReadEPNS_13MessageReaderEPS4_"}
{"loc":"00192:14-23","target":1,"kind":"use","pretty":"mozilla::Maybe::isNothing","sym":"_ZNK7mozilla5Maybe9isNothingEv","context":"IPC::ParamTraits<mozilla::_ipdltest::PTestBasicChild *>::Read","contextsym":"_ZN3IPC11ParamTraitsIPN7mozilla9_ipdltest15PTestBasicChildEE4ReadEPNS_13MessageReaderEPS4_"}
{"loc":"00196:44-59","target":1,"kind":"use","pretty":"mozilla::_ipdltest::PTestBasicChild","sym":"T_mozilla::_ipdltest::PTestBasicChild","context":"IPC::ParamTraits<mozilla::_ipdltest::PTestBasicChild *>::Read","contextsym":"_ZN3IPC11ParamTraitsIPN7mozilla9_ipdltest15PTestBasicChildEE4ReadEPNS_13MessageReaderEPS4_"}
{"loc":"00196:68-73","target":1,"kind":"use","pretty":"mozilla::Maybe::value","sym":"_ZNKR7mozilla5Maybe5valueEv","context":"IPC::ParamTraits<mozilla::_ipdltest::PTestBasicChild *>::Read","contextsym":"_ZN3IPC11ParamTraitsIPN7mozilla9_ipdltest15PTestBasicChildEE4ReadEPNS_13MessageReaderEPS4_"}
{"loc":"00001:0","target":1,"kind":"def","pretty":"__GENERATED__/ipc/ipdl/PTestBasicChild.cpp","sym":"FILE_linux@__GENERATED__/ipc/ipdl/PTestBasicChild@2Ecpp"}
{"loc":"00007:9-46","target":1,"kind":"use","pretty":"__GENERATED__/ipc/ipdl/_ipdlheaders/mozilla/_ipdltest/PTestBasicChild.h","sym":"FILE_linux@__GENERATED__/ipc/ipdl/_ipdlheaders/mozilla/_ipdltest/PTestBasicChild@2Eh"}
{"loc":"00014:9-20","target":1,"kind":"use","pretty":"__GENERATED__/dist/include/nsIFile.h","sym":"FILE_linux@__GENERATED__/dist/include/nsIFile@2Eh"}
{"loc":"00001:0","target":1,"kind":"def","pretty":"__GENERATED__/ipc/ipdl/PTestBasicChild.cpp","sym":"FILE_macosx@__GENERATED__/ipc/ipdl/PTestBasicChild@2Ecpp"}
{"loc":"00007:9-46","target":1,"kind":"use","pretty":"__GENERATED__/ipc/ipdl/_ipdlheaders/mozilla/_ipdltest/PTestBasicChild.h","sym":"FILE_macosx@__GENERATED__/ipc/ipdl/_ipdlheaders/mozilla/_ipdltest/PTestBasicChild@2Eh"}
{"loc":"00014:9-20","target":1,"kind":"use","pretty":"__GENERATED__/dist/include/nsIFile.h","sym":"FILE_macosx@__GENERATED__/dist/include/nsIFile@2Eh"}
{"loc":"00001:0","target":1,"kind":"def","pretty":"__GENERATED__/ipc/ipdl/PTestBasicChild.cpp","sym":"FILE_windows@__GENERATED__/ipc/ipdl/PTestBasicChild@2Ecpp"}
{"loc":"00007:9-46","target":1,"kind":"use","pretty":"__GENERATED__/ipc/ipdl/_ipdlheaders/mozilla/_ipdltest/PTestBasicChild.h","sym":"FILE_windows@__GENERATED__/ipc/ipdl/_ipdlheaders/mozilla/_ipdltest/PTestBasicChild@2Eh"}
{"loc":"00014:9-20","target":1,"kind":"use","pretty":"__GENERATED__/dist/include/nsIFile.h","sym":"FILE_windows@__GENERATED__/dist/include/nsIFile@2Eh"}
{"loc":"00076:30-51","target":1,"kind":"use","pretty":"mozilla::ipc::LogMessageForProtocol","sym":"_ZN7mozilla3ipc21LogMessageForProtocolEPKcmS2_jNS0_16MessageDirectionE","context":"mozilla::_ipdltest::PTestBasicChild::OnMessageReceived","contextsym":"_ZN7mozilla9_ipdltest15PTestBasicChild17OnMessageReceivedERKN3IPC7MessageE"}
{"loc":"00001:0","source":1,"syntax":"def,file","pretty":"file __GENERATED__/ipc/ipdl/PTestBasicChild.cpp","sym":"FILE_windows@__GENERATED__/ipc/ipdl/PTestBasicChild@2Ecpp,FILE_macosx@__GENERATED__/ipc/ipdl/PTestBasicChild@2Ecpp,FILE_linux@__GENERATED__/ipc/ipdl/PTestBasicChild@2Ecpp,FILE_android-armv7@__GENERATED__/ipc/ipdl/PTestBasicChild@2Ecpp"}
{"loc":"00007:9-46","source":1,"syntax":"file,use","pretty":"file __GENERATED__/ipc/ipdl/_ipdlheaders/mozilla/_ipdltest/PTestBasicChild.h","sym":"FILE_windows@__GENERATED__/ipc/ipdl/_ipdlheaders/mozilla/_ipdltest/PTestBasicChild@2Eh,FILE_macosx@__GENERATED__/ipc/ipdl/_ipdlheaders/mozilla/_ipdltest/PTestBasicChild@2Eh,FILE_linux@__GENERATED__/ipc/ipdl/_ipdlheaders/mozilla/_ipdltest/PTestBasicChild@2Eh,FILE_android-armv7@__GENERATED__/ipc/ipdl/_ipdlheaders/mozilla/_ipdltest/PTestBasicChild@2Eh"}
{"loc":"00008:9-35","source":1,"syntax":"file,use","pretty":"file tools/profiler/public/ProfilerLabels.h","sym":"FILE_tools/profiler/public/ProfilerLabels@2Eh"}
{"loc":"00009:9-45","source":1,"syntax":"file,use","pretty":"file ipc/ipdl/test/gtest/TestBasicChild.h","sym":"FILE_ipc/ipdl/test/gtest/TestBasicChild@2Eh"}
{"loc":"00012:9-32","source":1,"syntax":"file,use","pretty":"file ipc/glue/IPCMessageUtils.h","sym":"FILE_ipc/glue/IPCMessageUtils@2Eh"}
{"loc":"00013:9-47","source":1,"syntax":"file,use","pretty":"file ipc/glue/IPCMessageUtilsSpecializations.h","sym":"FILE_ipc/glue/IPCMessageUtilsSpecializations@2Eh"}
{"loc":"00014:9-20","source":1,"syntax":"file,use","pretty":"file __GENERATED__/dist/include/nsIFile.h","sym":"FILE_windows@__GENERATED__/dist/include/nsIFile@2Eh,FILE_macosx@__GENERATED__/dist/include/nsIFile@2Eh,FILE_linux@__GENERATED__/dist/include/nsIFile@2Eh,FILE_android-armv7@__GENERATED__/dist/include/nsIFile@2Eh"}
{"loc":"00015:9-33","source":1,"syntax":"file,use","pretty":"file ipc/glue/Endpoint.h","sym":"FILE_ipc/glue/Endpoint@2Eh"}
{"loc":"00016:9-45","source":1,"syntax":"file,use","pretty":"file ipc/glue/ProtocolMessageUtils.h","sym":"FILE_ipc/glue/ProtocolMessageUtils@2Eh"}
{"loc":"00017:9-38","source":1,"syntax":"file,use","pretty":"file ipc/glue/ProtocolUtils.h","sym":"FILE_ipc/glue/ProtocolUtils@2Eh"}
{"loc":"00018:9-42","source":1,"syntax":"file,use","pretty":"file ipc/glue/ShmemMessageUtils.h","sym":"FILE_ipc/glue/ShmemMessageUtils@2Eh"}
{"loc":"00019:9-41","source":1,"syntax":"file,use","pretty":"file ipc/glue/TaintingIPCUtils.h","sym":"FILE_ipc/glue/TaintingIPCUtils@2Eh"}
{"loc":"00021:10-17","source":1,"syntax":"def,namespace","pretty":"namespace mozilla","sym":"NS_mozilla","nestingRange":"21:19-152:0"}
{"loc":"00022:10-19","source":1,"syntax":"def,namespace","pretty":"namespace mozilla::_ipdltest","sym":"NS_mozilla::_ipdltest","nestingRange":"22:21-151:0"}
{"loc":"00025:5-20","source":1,"syntax":"type,use","pretty":"type mozilla::_ipdltest::PTestBasicChild","sym":"T_mozilla::_ipdltest::PTestBasicChild","type":"class mozilla::_ipdltest::PTestBasicChild","typesym":"T_mozilla::_ipdltest::PTestBasicChild"}
{"loc":"00025:22-37","source":1,"syntax":"def,function","pretty":"function mozilla::_ipdltest::PTestBasicChild::ProcessingError","sym":"_ZN7mozilla9_ipdltest15PTestBasicChild15ProcessingErrorENS_3ipc14HasResultCodes6ResultEPKc","nestingRange":"28:0-29:0","type":"auto (enum mozilla::ipc::HasResultCodes::Result, const char *) -> void"}
{"loc":"00026:8-14","source":1,"syntax":"type,use","pretty":"type mozilla::ipc::HasResultCodes::Result","sym":"T_mozilla::ipc::HasResultCodes::Result","type":"enum mozilla::ipc::HasResultCodes::Result","typesym":"T_mozilla::ipc::HasResultCodes::Result"}
{"loc":"00026:15-20","source":1,"syntax":"","pretty":"variable aCode","sym":"V_c40c7ee2992e95a8_1ebe00f013,V_c01ac2d45ee2a132_1ebe00f013,V_13c625cddce69744_1ebe00f013,V_c35edacf7266e66e_1ebe00f013","no_crossref":1,"type":"enum mozilla::ipc::HasResultCodes::Result","typesym":"T_mozilla::ipc::HasResultCodes::Result"}
{"loc":"00027:20-27","source":1,"syntax":"","pretty":"variable aReason","sym":"V_9cc48ee2992e95a8_ea97235f0b0d,V_98d2d2d45ee2a132_ea97235f0b0d,V_ea8f25cddce69744_ea97235f0b0d,V_9b17eacf7266e66e_ea97235f0b0d","no_crossref":1,"type":"const char *"}
{"loc":"00031:5-20","source":1,"syntax":"type,use","pretty":"type mozilla::_ipdltest::PTestBasicChild","sym":"T_mozilla::_ipdltest::PTestBasicChild","type":"class mozilla::_ipdltest::PTestBasicChild","typesym":"T_mozilla::_ipdltest::PTestBasicChild"}
{"loc":"00031:22-52","source":1,"syntax":"def,function","pretty":"function mozilla::_ipdltest::PTestBasicChild::ShouldContinueFromReplyTimeout","sym":"_ZN7mozilla9_ipdltest15PTestBasicChild30ShouldContinueFromReplyTimeoutEv","nestingRange":"32:0-34:0","type":"auto (void) -> _Bool"}
{"loc":"00036:0-12","source":1,"syntax":"macro,use","pretty":"macro MOZ_IMPLICIT","sym":"M_5f65895d5a5258f7"}
{"loc":"00036:13-28","source":1,"syntax":"type,use","pretty":"type mozilla::_ipdltest::PTestBasicChild","sym":"T_mozilla::_ipdltest::PTestBasicChild","type":"class mozilla::_ipdltest::PTestBasicChild","typesym":"T_mozilla::_ipdltest::PTestBasicChild"}
{"loc":"00036:30-45","source":1,"syntax":"def,function","pretty":"function mozilla::_ipdltest::PTestBasicChild::PTestBasicChild","sym":"_ZN7mozilla9_ipdltest15PTestBasicChildC1Ev","nestingRange":"38:0-40:0","type":"void (void)"}
{"loc":"00037:4-11","source":1,"syntax":"constructor,use","pretty":"constructor mozilla::ipc::IToplevelProtocol::IToplevelProtocol","sym":"_ZN7mozilla3ipc17IToplevelProtocolC1EPKc15IPCMessageStartNS0_4SideE"}
{"loc":"00037:18-35","source":1,"syntax":"type,use","pretty":"type mozilla::ipc::IToplevelProtocol","sym":"T_mozilla::ipc::IToplevelProtocol","type":"class mozilla::ipc::IToplevelProtocol","typesym":"T_mozilla::ipc::IToplevelProtocol"}
{"loc":"00037:55-73","source":1,"syntax":"enum,use","pretty":"enum IPCMessageStart::PTestBasicMsgStart","sym":"E_<T_IPCMessageStart>_PTestBasicMsgStart","type":"enum IPCMessageStart","typesym":"T_IPCMessageStart"}
{"loc":"00037:89-98","source":1,"syntax":"enum,use","pretty":"enum mozilla::ipc::Side::ChildSide","sym":"E_<T_mozilla::ipc::Side>_ChildSide","type":"enum mozilla::ipc::Side","typesym":"T_mozilla::ipc::Side"}
{"loc":"00039:4-18","source":1,"syntax":"macro,use","pretty":"macro MOZ_COUNT_CTOR","sym":"M_d9d58f5149d9f0f2"}
{"loc":"00039:19-34","source":1,"syntax":"type,use","pretty":"type mozilla::_ipdltest::PTestBasicChild","sym":"T_mozilla::_ipdltest::PTestBasicChild","type":"class mozilla::_ipdltest::PTestBasicChild","typesym":"T_mozilla::_ipdltest::PTestBasicChild"}
{"loc":"00042:0-15","source":1,"syntax":"type,use","pretty":"type mozilla::_ipdltest::PTestBasicChild","sym":"T_mozilla::_ipdltest::PTestBasicChild","type":"class mozilla::_ipdltest::PTestBasicChild","typesym":"T_mozilla::_ipdltest::PTestBasicChild"}
{"loc":"00042:18-33","source":1,"syntax":"def,destructor","pretty":"destructor mozilla::_ipdltest::PTestBasicChild::~PTestBasicChild","sym":"_ZN7mozilla9_ipdltest15PTestBasicChildD1Ev","nestingRange":"43:0-45:0","type":"void (void) noexcept"}
{"loc":"00042:18-33","source":1,"syntax":"type,use","pretty":"type mozilla::_ipdltest::PTestBasicChild","sym":"T_mozilla::_ipdltest::PTestBasicChild","type":"class mozilla::_ipdltest::PTestBasicChild","typesym":"T_mozilla::_ipdltest::PTestBasicChild"}
{"loc":"00044:4-18","source":1,"syntax":"macro,use","pretty":"macro MOZ_COUNT_DTOR","sym":"M_f9e8c16149d9f0f2"}
{"loc":"00044:19-34","source":1,"syntax":"type,use","pretty":"type mozilla::_ipdltest::PTestBasicChild","sym":"T_mozilla::_ipdltest::PTestBasicChild","type":"class mozilla::_ipdltest::PTestBasicChild","typesym":"T_mozilla::_ipdltest::PTestBasicChild"}
{"loc":"00047:5-20","source":1,"syntax":"type,use","pretty":"type mozilla::_ipdltest::PTestBasicChild","sym":"T_mozilla::_ipdltest::PTestBasicChild","type":"class mozilla::_ipdltest::PTestBasicChild","typesym":"T_mozilla::_ipdltest::PTestBasicChild"}
{"loc":"00047:22-38","source":1,"syntax":"def,function","pretty":"function mozilla::_ipdltest::PTestBasicChild::AllManagedActors","sym":"_ZNK7mozilla9_ipdltest15PTestBasicChild16AllManagedActorsER8nsTArrayI6RefPtrINS_3ipc19ActorLifecycleProxyEEE","nestingRange":"48:0-52:0","type":"auto (nsTArray<RefPtr<mozilla::ipc::ActorLifecycleProxy> > &) const -> void"}
{"loc":"00047:39-47","source":1,"syntax":"type,use","pretty":"type nsTArray","sym":"T_nsTArray"}
{"loc":"00047:48-54","source":1,"syntax":"type,use","pretty":"type RefPtr","sym":"T_RefPtr"}
{"loc":"00047:69-88","source":1,"syntax":"type,use","pretty":"type mozilla::ipc::ActorLifecycleProxy","sym":"T_mozilla::ipc::ActorLifecycleProxy","type":"class mozilla::ipc::ActorLifecycleProxy","typesym":"T_mozilla::ipc::ActorLifecycleProxy"}
{"loc":"00047:92-97","source":1,"syntax":"","pretty":"variable arr__","sym":"V_4be7c0f2992e95a8_8cdba1f013,V_47f515d45ee2a132_8cdba1f013,V_99a277cddce69744_8cdba1f013,V_4a3a2dcf7266e66e_8cdba1f013","no_crossref":1,"type":"nsTArray<RefPtr<mozilla::ipc::ActorLifecycleProxy> > &"}
{"loc":"00049:4-12","source":1,"syntax":"type,use","pretty":"type uint32_t","sym":"T_uint32_t","type":"uint32_t"}
{"loc":"00049:13-18","source":1,"syntax":"","pretty":"variable total","sym":"V_f669d0f2992e95a8_903f070113,V_f27725d45ee2a132_903f070113,V_452487cddce69744_903f070113,V_f5bb3dcf7266e66e_903f070113","no_crossref":1,"type":"uint32_t"}
{"loc":"00050:4-9","source":1,"syntax":"","pretty":"variable arr__","sym":"V_4be7c0f2992e95a8_8cdba1f013,V_47f515d45ee2a132_8cdba1f013,V_99a277cddce69744_8cdba1f013,V_4a3a2dcf7266e66e_8cdba1f013","no_crossref":1,"type":"nsTArray<RefPtr<mozilla::ipc::ActorLifecycleProxy> > &"}
{"loc":"00050:10-21","source":1,"syntax":"function,use","pretty":"function nsTArray_Impl::SetCapacity","sym":"_ZN13nsTArray_Impl11SetCapacityEN13nsTArray_baseIT0_N27nsTArray_RelocationStrategyIT_E4TypeEE9size_typeE","type":"typename struct nsTArrayInfallibleAllocator::ResultType"}
{"loc":"00050:22-27","source":1,"syntax":"","pretty":"variable total","sym":"V_f669d0f2992e95a8_903f070113,V_f27725d45ee2a132_903f070113,V_452487cddce69744_903f070113,V_f5bb3dcf7266e66e_903f070113","no_crossref":1,"type":"uint32_t"}
{"loc":"00054:5-20","source":1,"syntax":"type,use","pretty":"type mozilla::_ipdltest::PTestBasicChild","sym":"T_mozilla::_ipdltest::PTestBasicChild","type":"class mozilla::_ipdltest::PTestBasicChild","typesym":"T_mozilla::_ipdltest::PTestBasicChild"}
{"loc":"00054:22-35","source":1,"syntax":"def,function","pretty":"function mozilla::_ipdltest::PTestBasicChild::RemoveManagee","sym":"_ZN7mozilla9_ipdltest15PTestBasicChild13RemoveManageeEiPNS_3ipc9IProtocolE","nestingRange":"57:0-60:0","type":"auto (int32_t, mozilla::_ipdltest::PTestBasicChild::IProtocol *) -> void"}
{"loc":"00055:8-15","source":1,"syntax":"type,use","pretty":"type int32_t","sym":"T_int32_t","type":"int32_t"}
{"loc":"00055:16-27","source":1,"syntax":"","pretty":"variable aProtocolId","sym":"V_f6d7d1f2992e95a8_542cb52f47aba60c,V_f2e526d45ee2a132_542cb52f47aba60c,V_459288cddce69744_542cb52f47aba60c,V_f52a3ecf7266e66e_542cb52f47aba60c","no_crossref":1,"type":"int32_t"}
{"loc":"00056:8-17","source":1,"syntax":"type,use","pretty":"type mozilla::_ipdltest::PTestBasicChild::IProtocol","sym":"T_mozilla::_ipdltest::PTestBasicChild::IProtocol","type":"mozilla::_ipdltest::PTestBasicChild::IProtocol","typesym":"T_mozilla::ipc::IProtocol"}
{"loc":"00056:19-28","source":1,"syntax":"","pretty":"variable aListener","sym":"V_3d90e1f2992e95a8_c681f10e880c773,V_39ae26d45ee2a132_c681f10e880c773,V_8b5b88cddce69744_c681f10e880c773,V_3ce24ecf7266e66e_c681f10e880c773","no_crossref":1,"type":"mozilla::_ipdltest::PTestBasicChild::IProtocol *"}
{"loc":"00058:4-14","source":1,"syntax":"function,use","pretty":"function mozilla::ipc::IProtocol::FatalError","sym":"_ZNK7mozilla3ipc9IProtocol10FatalErrorEPKc","type":"void"}
{"loc":"00062:5-20","source":1,"syntax":"type,use","pretty":"type mozilla::_ipdltest::PTestBasicChild","sym":"T_mozilla::_ipdltest::PTestBasicChild","type":"class mozilla::_ipdltest::PTestBasicChild","typesym":"T_mozilla::_ipdltest::PTestBasicChild"}
{"loc":"00062:22-36","source":1,"syntax":"def,function","pretty":"function mozilla::_ipdltest::PTestBasicChild::DeallocManagee","sym":"_ZN7mozilla9_ipdltest15PTestBasicChild14DeallocManageeEiPNS_3ipc9IProtocolE","nestingRange":"65:0-68:0","type":"auto (int32_t, mozilla::_ipdltest::PTestBasicChild::IProtocol *) -> void"}
{"loc":"00063:8-15","source":1,"syntax":"type,use","pretty":"type int32_t","sym":"T_int32_t","type":"int32_t"}
{"loc":"00063:16-27","source":1,"syntax":"","pretty":"variable aProtocolId","sym":"V_e2d7e2f2992e95a8_542cb52f47aba60c,V_eed537d45ee2a132_542cb52f47aba60c,V_319299cddce69744_542cb52f47aba60c,V_e12a4fcf7266e66e_542cb52f47aba60c","no_crossref":1,"type":"int32_t"}
{"loc":"00064:8-17","source":1,"syntax":"type,use","pretty":"type mozilla::_ipdltest::PTestBasicChild::IProtocol","sym":"T_mozilla::_ipdltest::PTestBasicChild::IProtocol","type":"mozilla::_ipdltest::PTestBasicChild::IProtocol","typesym":"T_mozilla::ipc::IProtocol"}
{"loc":"00064:19-28","source":1,"syntax":"","pretty":"variable aListener","sym":"V_2990f2f2992e95a8_c681f10e880c773,V_25ae37d45ee2a132_c681f10e880c773,V_775b99cddce69744_c681f10e880c773,V_28e25fcf7266e66e_c681f10e880c773","no_crossref":1,"type":"mozilla::_ipdltest::PTestBasicChild::IProtocol *"}
{"loc":"00066:4-14","source":1,"syntax":"function,use","pretty":"function mozilla::ipc::IProtocol::FatalError","sym":"_ZNK7mozilla3ipc9IProtocol10FatalErrorEPKc","type":"void"}
{"loc":"00070:5-20","source":1,"syntax":"type,use","pretty":"type mozilla::_ipdltest::PTestBasicChild","sym":"T_mozilla::_ipdltest::PTestBasicChild","type":"class mozilla::_ipdltest::PTestBasicChild","typesym":"T_mozilla::_ipdltest::PTestBasicChild"}
{"loc":"00070:22-39","source":1,"syntax":"def,function","pretty":"function mozilla::_ipdltest::PTestBasicChild::OnMessageReceived","sym":"_ZN7mozilla9_ipdltest15PTestBasicChild17OnMessageReceivedERKN3IPC7MessageE","nestingRange":"71:0-107:0","type":"auto (const mozilla::_ipdltest::PTestBasicChild::Message &) -> class PTestBasicChild::Result"}
{"loc":"00070:46-53","source":1,"syntax":"type,use","pretty":"type mozilla::_ipdltest::PTestBasicChild::Message","sym":"T_mozilla::_ipdltest::PTestBasicChild::Message","type":"mozilla::_ipdltest::PTestBasicChild::Message","typesym":"T_IPC::Message"}
{"loc":"00070:55-60","source":1,"syntax":"","pretty":"variable msg__","sym":"V_f01fe3f2992e95a8_a6144ff013,V_fc1d38d45ee2a132_a6144ff013,V_4fc99acddce69744_a6144ff013,V_ff5150df7266e66e_a6144ff013","no_crossref":1,"type":"const mozilla::_ipdltest::PTestBasicChild::Message &"}
{"loc":"00070:65-80","source":1,"syntax":"type,use","pretty":"type mozilla::_ipdltest::PTestBasicChild","sym":"T_mozilla::_ipdltest::PTestBasicChild","type":"class mozilla::_ipdltest::PTestBasicChild","typesym":"T_mozilla::_ipdltest::PTestBasicChild"}
{"loc":"00070:82-88","source":1,"syntax":"type,use","pretty":"type mozilla::ipc::HasResultCodes::Result","sym":"T_mozilla::ipc::HasResultCodes::Result","type":"enum mozilla::ipc::HasResultCodes::Result","typesym":"T_mozilla::ipc::HasResultCodes::Result"}
{"loc":"00072:12-17","source":1,"syntax":"","pretty":"variable msg__","sym":"V_f01fe3f2992e95a8_a6144ff013,V_fc1d38d45ee2a132_a6144ff013,V_4fc99acddce69744_a6144ff013,V_ff5150df7266e66e_a6144ff013","no_crossref":1,"type":"const mozilla::_ipdltest::PTestBasicChild::Message &"}
{"loc":"00072:18-22","source":1,"syntax":"function,use","pretty":"function IPC::Message::type","sym":"_ZNK3IPC7Message4typeEv","type":"IPC::Message::msgid_t"}
{"loc":"00073:21-34","source":1,"syntax":"enum,use","pretty":"enum mozilla::_ipdltest::PTestBasic::MessageType::Msg_Hello__ID","sym":"E_<T_mozilla::_ipdltest::PTestBasic::MessageType>_Msg_Hello__ID","type":"enum mozilla::_ipdltest::PTestBasic::MessageType","typesym":"T_mozilla::_ipdltest::PTestBasic::MessageType"}
{"loc":"00075:30-47","source":1,"syntax":"function,use","pretty":"function mozilla::ipc::LoggingEnabledFor","sym":"_ZN7mozilla3ipc17LoggingEnabledForEPKc","type":"_Bool (const char *)"}
{"loc":"00076:30-51","source":1,"syntax":"function,use","pretty":"function mozilla::ipc::LogMessageForProtocol","sym":"_ZN7mozilla3ipc21LogMessageForProtocolEPKcmS2_jNS0_16MessageDirectionE,_ZN7mozilla3ipc21LogMessageForProtocolEPKciS2_jNS0_16MessageDirectionE","type":"void (const char *, base::ProcessId, const char *, uint32_t, enum mozilla::ipc::MessageDirection)"}
{"loc":"00078:26-42","source":1,"syntax":"function,use","pretty":"function mozilla::ipc::IProtocol::ToplevelProtocol","sym":"_ZN7mozilla3ipc9IProtocol16ToplevelProtocolEv","type":"class mozilla::ipc::IToplevelProtocol *"}
{"loc":"00078:46-66","source":1,"syntax":"function,use","pretty":"function mozilla::ipc::IToplevelProtocol::OtherPidMaybeInvalid","sym":"_ZNK7mozilla3ipc17IToplevelProtocol20OtherPidMaybeInvalidEv","type":"base::ProcessId"}
{"loc":"00080:23-28","source":1,"syntax":"","pretty":"variable msg__","sym":"V_f01fe3f2992e95a8_a6144ff013,V_fc1d38d45ee2a132_a6144ff013,V_4fc99acddce69744_a6144ff013,V_ff5150df7266e66e_a6144ff013","no_crossref":1,"type":"const mozilla::_ipdltest::PTestBasicChild::Message &"}
{"loc":"00080:32-36","source":1,"syntax":"function,use","pretty":"function IPC::Message::type","sym":"_ZNK3IPC7Message4typeEv","type":"IPC::Message::msgid_t"}
{"loc":"00081:34-50","source":1,"syntax":"type,use","pretty":"type mozilla::ipc::MessageDirection","sym":"T_mozilla::ipc::MessageDirection","type":"enum mozilla::ipc::MessageDirection","typesym":"T_mozilla::ipc::MessageDirection"}
{"loc":"00081:52-62","source":1,"syntax":"enum,use","pretty":"enum mozilla::ipc::MessageDirection::eReceiving","sym":"E_<T_mozilla::ipc::MessageDirection>_eReceiving","type":"enum mozilla::ipc::MessageDirection","typesym":"T_mozilla::ipc::MessageDirection"}
{"loc":"00083:12-31","source":1,"syntax":"macro,use","pretty":"macro AUTO_PROFILER_LABEL","sym":"M_99e0335dc782f2d6"}
{"loc":"00083:12-31","source":1,"syntax":"","pretty":"variable raiiObject83","sym":"V_5e5b2_c869d3b76a0990ad","no_crossref":1,"type":"mozilla::AutoProfilerLabel","typesym":"T_mozilla::AutoProfilerLabel"}
{"loc":"00083:57-62","source":1,"syntax":"enum,use","pretty":"enum JS::ProfilingCategoryPair::OTHER","sym":"E_<T_JS::ProfilingCategoryPair>_OTHER","type":"enum JS::ProfilingCategoryPair","typesym":"T_JS::ProfilingCategoryPair"}
{"loc":"00085:26-35","source":1,"syntax":"type,use","pretty":"type mozilla::ipc::IPCResult","sym":"T_mozilla::ipc::IPCResult","type":"class mozilla::ipc::IPCResult","typesym":"T_mozilla::ipc::IPCResult"}
{"loc":"00085:36-40","source":1,"syntax":"","pretty":"variable __ok","sym":"V_437c35f2992e95a8_d5ac29c71,V_4f7a89d45ee2a132_d5ac29c71,V_9137ebcddce69744_d5ac29c71,V_42ce91df7266e66e_d5ac29c71","no_crossref":1,"type":"mozilla::ipc::IPCResult","typesym":"T_mozilla::ipc::IPCResult"}
{"loc":"00085:56-70","source":1,"syntax":"type,use","pretty":"type mozilla::_ipdltest::TestBasicChild","sym":"T_mozilla::_ipdltest::TestBasicChild","type":"class mozilla::_ipdltest::TestBasicChild","typesym":"T_mozilla::_ipdltest::TestBasicChild"}
{"loc":"00085:81-90","source":1,"syntax":"function,use","pretty":"function mozilla::_ipdltest::TestBasicChild::RecvHello","sym":"_ZN7mozilla9_ipdltest14TestBasicChild9RecvHelloEv","type":"mozilla::ipc::IPCResult","typesym":"T_mozilla::ipc::IPCResult"}
{"loc":"00086:19-23","source":1,"syntax":"","pretty":"variable __ok","sym":"V_437c35f2992e95a8_d5ac29c71,V_4f7a89d45ee2a132_d5ac29c71,V_9137ebcddce69744_d5ac29c71,V_42ce91df7266e66e_d5ac29c71","no_crossref":1,"type":"mozilla::ipc::IPCResult","typesym":"T_mozilla::ipc::IPCResult"}
{"loc":"00087:30-53","source":1,"syntax":"function,use","pretty":"function mozilla::ipc::ProtocolErrorBreakpoint","sym":"_ZN7mozilla3ipc23ProtocolErrorBreakpointEPKc","type":"void (const char *)"}
{"loc":"00089:23-41","source":1,"syntax":"enum,use","pretty":"enum mozilla::ipc::HasResultCodes::Result::MsgProcessingError","sym":"E_<T_mozilla::ipc::HasResultCodes::Result>_MsgProcessingError","type":"enum mozilla::ipc::HasResultCodes::Result","typesym":"T_mozilla::ipc::HasResultCodes::Result"}
{"loc":"00092:19-31","source":1,"syntax":"enum,use","pretty":"enum mozilla::ipc::HasResultCodes::Result::MsgProcessed","sym":"E_<T_mozilla::ipc::HasResultCodes::Result>_MsgProcessed","type":"enum mozilla::ipc::HasResultCodes::Result","typesym":"T_mozilla::ipc::HasResultCodes::Result"}
{"loc":"00095:15-26","source":1,"syntax":"enum,use","pretty":"enum mozilla::ipc::HasResultCodes::Result::MsgNotKnown","sym":"E_<T_mozilla::ipc::HasResultCodes::Result>_MsgNotKnown","type":"enum mozilla::ipc::HasResultCodes::Result","typesym":"T_mozilla::ipc::HasResultCodes::Result"}
{"loc":"00096:9-35","source":1,"syntax":"enum,use","pretty":"enum SHMEM_CREATED_MESSAGE_TYPE","sym":"E_<T_b078e220e89184de>_SHMEM_CREATED_MESSAGE_TYPE","type":"enum (anonymous namespace)::(unnamed at /builds/worker/workspace/obj-build/dist/include/mozilla/ipc/ProtocolUtils.h:57:1)","typesym":"T_b078e220e89184de"}
{"loc":"00098:12-22","source":1,"syntax":"function,use","pretty":"function mozilla::ipc::IProtocol::FatalError","sym":"_ZNK7mozilla3ipc9IProtocol10FatalErrorEPKc","type":"void"}
{"loc":"00099:19-30","source":1,"syntax":"enum,use","pretty":"enum mozilla::ipc::HasResultCodes::Result::MsgNotKnown","sym":"E_<T_mozilla::ipc::HasResultCodes::Result>_MsgNotKnown","type":"enum mozilla::ipc::HasResultCodes::Result","typesym":"T_mozilla::ipc::HasResultCodes::Result"}
{"loc":"00101:9-37","source":1,"syntax":"enum,use","pretty":"enum SHMEM_DESTROYED_MESSAGE_TYPE","sym":"E_<T_b078e220e89184de>_SHMEM_DESTROYED_MESSAGE_TYPE","type":"enum (anonymous namespace)::(unnamed at /builds/worker/workspace/obj-build/dist/include/mozilla/ipc/ProtocolUtils.h:57:1)","typesym":"T_b078e220e89184de"}
{"loc":"00103:12-22","source":1,"syntax":"function,use","pretty":"function mozilla::ipc::IProtocol::FatalError","sym":"_ZNK7mozilla3ipc9IProtocol10FatalErrorEPKc","type":"void"}
{"loc":"00104:19-30","source":1,"syntax":"enum,use","pretty":"enum mozilla::ipc::HasResultCodes::Result::MsgNotKnown","sym":"E_<T_mozilla::ipc::HasResultCodes::Result>_MsgNotKnown","type":"enum mozilla::ipc::HasResultCodes::Result","typesym":"T_mozilla::ipc::HasResultCodes::Result"}
{"loc":"00109:5-20","source":1,"syntax":"type,use","pretty":"type mozilla::_ipdltest::PTestBasicChild","sym":"T_mozilla::_ipdltest::PTestBasicChild","type":"class mozilla::_ipdltest::PTestBasicChild","typesym":"T_mozilla::_ipdltest::PTestBasicChild"}
{"loc":"00109:22-39","source":1,"syntax":"def,function","pretty":"function mozilla::_ipdltest::PTestBasicChild::OnMessageReceived","sym":"_ZN7mozilla9_ipdltest15PTestBasicChild17OnMessageReceivedERKN3IPC7MessageERNS_9UniquePtrIS3_NS_13DefaultDeleteIS3_EEEE","nestingRange":"112:0-115:0","type":"auto (const mozilla::_ipdltest::PTestBasicChild::Message &, UniquePtr<mozilla::_ipdltest::PTestBasicChild::Message> &) -> class PTestBasicChild::Result"}
{"loc":"00110:14-21","source":1,"syntax":"type,use","pretty":"type mozilla::_ipdltest::PTestBasicChild::Message","sym":"T_mozilla::_ipdltest::PTestBasicChild::Message","type":"mozilla::_ipdltest::PTestBasicChild::Message","typesym":"T_IPC::Message"}
{"loc":"00110:23-28","source":1,"syntax":"","pretty":"variable msg__","sym":"V_54687213992e95a8_a6144ff013,V_5076c6f45ee2a132_a6144ff013,V_a22329eddce69744_a6144ff013,V_53badeef7266e66e_a6144ff013","no_crossref":1,"type":"const mozilla::_ipdltest::PTestBasicChild::Message &"}
{"loc":"00111:8-17","source":1,"syntax":"type,use","pretty":"type mozilla::_ipdltest::PTestBasicChild::UniquePtr","sym":"T_mozilla::_ipdltest::PTestBasicChild::UniquePtr"}
{"loc":"00111:18-25","source":1,"syntax":"type,use","pretty":"type mozilla::_ipdltest::PTestBasicChild::Message","sym":"T_mozilla::_ipdltest::PTestBasicChild::Message","type":"mozilla::_ipdltest::PTestBasicChild::Message","typesym":"T_IPC::Message"}
{"loc":"00111:28-35","source":1,"syntax":"","pretty":"variable reply__","sym":"V_ba218213992e95a8_f83bfee36b0d,V_b63fc6f45ee2a132_f83bfee36b0d,V_09eb29eddce69744_f83bfee36b0d,V_b973eeef7266e66e_f83bfee36b0d","no_crossref":1,"type":"UniquePtr<mozilla::_ipdltest::PTestBasicChild::Message> &"}
{"loc":"00111:40-55","source":1,"syntax":"type,use","pretty":"type mozilla::_ipdltest::PTestBasicChild","sym":"T_mozilla::_ipdltest::PTestBasicChild","type":"class mozilla::_ipdltest::PTestBasicChild","typesym":"T_mozilla::_ipdltest::PTestBasicChild"}
{"loc":"00111:57-63","source":1,"syntax":"type,use","pretty":"type mozilla::ipc::HasResultCodes::Result","sym":"T_mozilla::ipc::HasResultCodes::Result","type":"enum mozilla::ipc::HasResultCodes::Result","typesym":"T_mozilla::ipc::HasResultCodes::Result"}
{"loc":"00113:4-26","source":1,"syntax":"macro,use","pretty":"macro MOZ_ASSERT_UNREACHABLE","sym":"M_0d9562839add515c"}
{"loc":"00114:11-22","source":1,"syntax":"enum,use","pretty":"enum mozilla::ipc::HasResultCodes::Result::MsgNotKnown","sym":"E_<T_mozilla::ipc::HasResultCodes::Result>_MsgNotKnown","type":"enum mozilla::ipc::HasResultCodes::Result","typesym":"T_mozilla::ipc::HasResultCodes::Result"}
{"loc":"00117:5-20","source":1,"syntax":"type,use","pretty":"type mozilla::_ipdltest::PTestBasicChild","sym":"T_mozilla::_ipdltest::PTestBasicChild","type":"class mozilla::_ipdltest::PTestBasicChild","typesym":"T_mozilla::_ipdltest::PTestBasicChild"}
{"loc":"00117:22-36","source":1,"syntax":"def,function","pretty":"function mozilla::_ipdltest::PTestBasicChild::OnCallReceived","sym":"_ZN7mozilla9_ipdltest15PTestBasicChild14OnCallReceivedERKN3IPC7MessageERNS_9UniquePtrIS3_NS_13DefaultDeleteIS3_EEEE","nestingRange":"120:0-123:0","type":"auto (const mozilla::_ipdltest::PTestBasicChild::Message &, UniquePtr<mozilla::_ipdltest::PTestBasicChild::Message> &) -> class PTestBasicChild::Result"}
{"loc":"00118:14-21","source":1,"syntax":"type,use","pretty":"type mozilla::_ipdltest::PTestBasicChild::Message","sym":"T_mozilla::_ipdltest::PTestBasicChild::Message","type":"mozilla::_ipdltest::PTestBasicChild::Message","typesym":"T_IPC::Message"}
{"loc":"00118:23-28","source":1,"syntax":"","pretty":"variable msg__","sym":"V_d49eb213992e95a8_a6144ff013,V_d0ac07f45ee2a132_a6144ff013,V_235969eddce69744_a6144ff013,V_d3e02fef7266e66e_a6144ff013","no_crossref":1,"type":"const mozilla::_ipdltest::PTestBasicChild::Message &"}
{"loc":"00119:8-17","source":1,"syntax":"type,use","pretty":"type mozilla::_ipdltest::PTestBasicChild::UniquePtr","sym":"T_mozilla::_ipdltest::PTestBasicChild::UniquePtr"}
{"loc":"00119:18-25","source":1,"syntax":"type,use","pretty":"type mozilla::_ipdltest::PTestBasicChild::Message","sym":"T_mozilla::_ipdltest::PTestBasicChild::Message","type":"mozilla::_ipdltest::PTestBasicChild::Message","typesym":"T_IPC::Message"}
{"loc":"00119:28-35","source":1,"syntax":"","pretty":"variable reply__","sym":"V_3b57c213992e95a8_f83bfee36b0d,V_376517f45ee2a132_f83bfee36b0d,V_891279eddce69744_f83bfee36b0d,V_3aa92fef7266e66e_f83bfee36b0d","no_crossref":1,"type":"UniquePtr<mozilla::_ipdltest::PTestBasicChild::Message> &"}
{"loc":"00119:40-55","source":1,"syntax":"type,use","pretty":"type mozilla::_ipdltest::PTestBasicChild","sym":"T_mozilla::_ipdltest::PTestBasicChild","type":"class mozilla::_ipdltest::PTestBasicChild","typesym":"T_mozilla::_ipdltest::PTestBasicChild"}
{"loc":"00119:57-63","source":1,"syntax":"type,use","pretty":"type mozilla::ipc::HasResultCodes::Result","sym":"T_mozilla::ipc::HasResultCodes::Result","type":"enum mozilla::ipc::HasResultCodes::Result","typesym":"T_mozilla::ipc::HasResultCodes::Result"}
{"loc":"00121:4-26","source":1,"syntax":"macro,use","pretty":"macro MOZ_ASSERT_UNREACHABLE","sym":"M_0d9562839add515c"}
{"loc":"00122:11-22","source":1,"syntax":"enum,use","pretty":"enum mozilla::ipc::HasResultCodes::Result::MsgNotKnown","sym":"E_<T_mozilla::ipc::HasResultCodes::Result>_MsgNotKnown","type":"enum mozilla::ipc::HasResultCodes::Result","typesym":"T_mozilla::ipc::HasResultCodes::Result"}
{"loc":"00125:5-20","source":1,"syntax":"type,use","pretty":"type mozilla::_ipdltest::PTestBasicChild","sym":"T_mozilla::_ipdltest::PTestBasicChild","type":"class mozilla::_ipdltest::PTestBasicChild","typesym":"T_mozilla::_ipdltest::PTestBasicChild"}
{"loc":"00125:22-36","source":1,"syntax":"def,function","pretty":"function mozilla::_ipdltest::PTestBasicChild::OnChannelClose","sym":"_ZN7mozilla9_ipdltest15PTestBasicChild14OnChannelCloseEv","nestingRange":"126:0-133:0","type":"auto (void) -> void"}
{"loc":"00127:4-18","source":1,"syntax":"function,use","pretty":"function mozilla::ipc::IProtocol::DestroySubtree","sym":"_ZN7mozilla3ipc9IProtocol14DestroySubtreeENS1_18ActorDestroyReasonE","type":"void"}
{"loc":"00127:19-33","source":1,"syntax":"enum,use","pretty":"enum mozilla::ipc::IProtocol::ActorDestroyReason::NormalShutdown","sym":"E_<T_mozilla::ipc::IProtocol::ActorDestroyReason>_NormalShutdown","type":"enum mozilla::ipc::IProtocol::ActorDestroyReason","typesym":"T_mozilla::ipc::IProtocol::ActorDestroyReason"}
{"loc":"00128:4-16","source":1,"syntax":"function,use","pretty":"function mozilla::_ipdltest::PTestBasicChild::ClearSubtree","sym":"_ZN7mozilla9_ipdltest15PTestBasicChild12ClearSubtreeEv","type":"void"}
{"loc":"00129:4-17","source":1,"syntax":"function,use","pretty":"function mozilla::ipc::IToplevelProtocol::DeallocShmems","sym":"_ZN7mozilla3ipc17IToplevelProtocol13DeallocShmemsEv","type":"void"}
{"loc":"00130:8-25","source":1,"syntax":"function,use","pretty":"function mozilla::ipc::IProtocol::GetLifecycleProxy","sym":"_ZN7mozilla3ipc9IProtocol17GetLifecycleProxyEv","type":"class mozilla::ipc::ActorLifecycleProxy *"}
{"loc":"00131:8-25","source":1,"syntax":"function,use","pretty":"function mozilla::ipc::IProtocol::GetLifecycleProxy","sym":"_ZN7mozilla3ipc9IProtocol17GetLifecycleProxyEv","type":"class mozilla::ipc::ActorLifecycleProxy *"}
{"loc":"00131:29-36","source":1,"syntax":"function,use","pretty":"function mozilla::ipc::ActorLifecycleProxy::Release","sym":"_ZN7mozilla3ipc19ActorLifecycleProxy7ReleaseEv","type":"MozExternalRefCountType"}
{"loc":"00135:5-20","source":1,"syntax":"type,use","pretty":"type mozilla::_ipdltest::PTestBasicChild","sym":"T_mozilla::_ipdltest::PTestBasicChild","type":"class mozilla::_ipdltest::PTestBasicChild","typesym":"T_mozilla::_ipdltest::PTestBasicChild"}
{"loc":"00135:22-36","source":1,"syntax":"def,function","pretty":"function mozilla::_ipdltest::PTestBasicChild::OnChannelError","sym":"_ZN7mozilla9_ipdltest15PTestBasicChild14OnChannelErrorEv","nestingRange":"136:0-143:0","type":"auto (void) -> void"}
{"loc":"00137:4-18","source":1,"syntax":"function,use","pretty":"function mozilla::ipc::IProtocol::DestroySubtree","sym":"_ZN7mozilla3ipc9IProtocol14DestroySubtreeENS1_18ActorDestroyReasonE","type":"void"}
{"loc":"00137:19-35","source":1,"syntax":"enum,use","pretty":"enum mozilla::ipc::IProtocol::ActorDestroyReason::AbnormalShutdown","sym":"E_<T_mozilla::ipc::IProtocol::ActorDestroyReason>_AbnormalShutdown","type":"enum mozilla::ipc::IProtocol::ActorDestroyReason","typesym":"T_mozilla::ipc::IProtocol::ActorDestroyReason"}
{"loc":"00138:4-16","source":1,"syntax":"function,use","pretty":"function mozilla::_ipdltest::PTestBasicChild::ClearSubtree","sym":"_ZN7mozilla9_ipdltest15PTestBasicChild12ClearSubtreeEv","type":"void"}
{"loc":"00139:4-17","source":1,"syntax":"function,use","pretty":"function mozilla::ipc::IToplevelProtocol::DeallocShmems","sym":"_ZN7mozilla3ipc17IToplevelProtocol13DeallocShmemsEv","type":"void"}
{"loc":"00140:8-25","source":1,"syntax":"function,use","pretty":"function mozilla::ipc::IProtocol::GetLifecycleProxy","sym":"_ZN7mozilla3ipc9IProtocol17GetLifecycleProxyEv","type":"class mozilla::ipc::ActorLifecycleProxy *"}
{"loc":"00141:8-25","source":1,"syntax":"function,use","pretty":"function mozilla::ipc::IProtocol::GetLifecycleProxy","sym":"_ZN7mozilla3ipc9IProtocol17GetLifecycleProxyEv","type":"class mozilla::ipc::ActorLifecycleProxy *"}
{"loc":"00141:29-36","source":1,"syntax":"function,use","pretty":"function mozilla::ipc::ActorLifecycleProxy::Release","sym":"_ZN7mozilla3ipc19ActorLifecycleProxy7ReleaseEv","type":"MozExternalRefCountType"}
{"loc":"00145:5-20","source":1,"syntax":"type,use","pretty":"type mozilla::_ipdltest::PTestBasicChild","sym":"T_mozilla::_ipdltest::PTestBasicChild","type":"class mozilla::_ipdltest::PTestBasicChild","typesym":"T_mozilla::_ipdltest::PTestBasicChild"}
{"loc":"00145:22-34","source":1,"syntax":"def,function","pretty":"function mozilla::_ipdltest::PTestBasicChild::ClearSubtree","sym":"_ZN7mozilla9_ipdltest15PTestBasicChild12ClearSubtreeEv","nestingRange":"146:0-147:0","type":"auto (void) -> void"}
{"loc":"00153:10-13","source":1,"syntax":"def,namespace","pretty":"namespace IPC","sym":"NS_IPC","nestingRange":"153:15-200:0"}
{"loc":"00154:5-16","source":1,"syntax":"type,use","pretty":"type IPC::ParamTraits","sym":"T_IPC::ParamTraits"}
{"loc":"00154:37-52","source":1,"syntax":"type,use","pretty":"type mozilla::_ipdltest::PTestBasicChild","sym":"T_mozilla::_ipdltest::PTestBasicChild","type":"class mozilla::_ipdltest::PTestBasicChild","typesym":"T_mozilla::_ipdltest::PTestBasicChild"}
{"loc":"00154:56-61","source":1,"syntax":"def,function","pretty":"function IPC::ParamTraits<mozilla::_ipdltest::PTestBasicChild *>::Write","sym":"_ZN3IPC11ParamTraitsIPN7mozilla9_ipdltest15PTestBasicChildEE5WriteEPNS_13MessageWriterERKS4_","nestingRange":"157:0-180:0","type":"auto (IPC::MessageWriter *, const IPC::ParamTraits<class mozilla::_ipdltest::PTestBasicChild *>::paramType &) -> void"}
{"loc":"00155:13-26","source":1,"syntax":"type,use","pretty":"type IPC::MessageWriter","sym":"T_IPC::MessageWriter","type":"class IPC::MessageWriter","typesym":"T_IPC::MessageWriter"}
{"loc":"00155:28-35","source":1,"syntax":"","pretty":"variable aWriter","sym":"V_336a2713992e95a8_369ebc101b0d,V_3f687bf45ee2a132_369ebc101b0d,V_8125ddeddce69744_369ebc101b0d,V_32bc83ff7266e66e_369ebc101b0d","no_crossref":1,"type":"IPC::MessageWriter *"}
{"loc":"00156:14-23","source":1,"syntax":"type,use","pretty":"type IPC::ParamTraits<mozilla::_ipdltest::PTestBasicChild *>::paramType","sym":"T_IPC::ParamTraits<mozilla::_ipdltest::PTestBasicChild_*>::paramType","type":"IPC::ParamTraits<class mozilla::_ipdltest::PTestBasicChild *>::paramType"}
{"loc":"00156:25-29","source":1,"syntax":"","pretty":"variable aVar","sym":"V_19233713992e95a8_f0bb39c71,V_15318bf45ee2a132_f0bb39c71,V_67edddeddce69744_f0bb39c71,V_187593ff7266e66e_f0bb39c71","no_crossref":1,"type":"const IPC::ParamTraits<class mozilla::_ipdltest::PTestBasicChild *>::paramType &"}
{"loc":"00158:4-22","source":1,"syntax":"macro,use","pretty":"macro MOZ_RELEASE_ASSERT","sym":"M_96f821839add515c"}
{"loc":"00159:8-15","source":1,"syntax":"","pretty":"variable aWriter","sym":"V_336a2713992e95a8_369ebc101b0d,V_3f687bf45ee2a132_369ebc101b0d,V_8125ddeddce69744_369ebc101b0d,V_32bc83ff7266e66e_369ebc101b0d","no_crossref":1,"type":"IPC::MessageWriter *"}
{"loc":"00159:17-25","source":1,"syntax":"function,use","pretty":"function IPC::MessageWriter::GetActor","sym":"_ZNK3IPC13MessageWriter8GetActorEv","type":"mozilla::ipc::IProtocol *"}
{"loc":"00162:4-11","source":1,"syntax":"type,use","pretty":"type int32_t","sym":"T_int32_t","type":"int32_t"}
{"loc":"00162:12-14","source":1,"syntax":"","pretty":"variable id","sym":"V_a6913813992e95a8_238795,V_a2af7cf45ee2a132_238795,V_f45cdeeddce69744_238795,V_a5e394ff7266e66e_238795","no_crossref":1,"type":"int32_t"}
{"loc":"00163:9-13","source":1,"syntax":"","pretty":"variable aVar","sym":"V_19233713992e95a8_f0bb39c71,V_15318bf45ee2a132_f0bb39c71,V_67edddeddce69744_f0bb39c71,V_187593ff7266e66e_f0bb39c71","no_crossref":1,"type":"const IPC::ParamTraits<class mozilla::_ipdltest::PTestBasicChild *>::paramType &"}
{"loc":"00164:8-10","source":1,"syntax":"","pretty":"variable id","sym":"V_a6913813992e95a8_238795,V_a2af7cf45ee2a132_238795,V_f45cdeeddce69744_238795,V_a5e394ff7266e66e_238795","no_crossref":1,"type":"int32_t"}
{"loc":"00166:8-10","source":1,"syntax":"","pretty":"variable id","sym":"V_a6913813992e95a8_238795,V_a2af7cf45ee2a132_238795,V_f45cdeeddce69744_238795,V_a5e394ff7266e66e_238795","no_crossref":1,"type":"int32_t"}
{"loc":"00166:13-17","source":1,"syntax":"","pretty":"variable aVar","sym":"V_19233713992e95a8_f0bb39c71,V_15318bf45ee2a132_f0bb39c71,V_67edddeddce69744_f0bb39c71,V_187593ff7266e66e_f0bb39c71","no_crossref":1,"type":"const IPC::ParamTraits<class mozilla::_ipdltest::PTestBasicChild *>::paramType &"}
{"loc":"00166:19-21","source":1,"syntax":"function,use","pretty":"function mozilla::ipc::IProtocol::Id","sym":"_ZNK7mozilla3ipc9IProtocol2IdEv","type":"int32_t"}
{"loc":"00167:12-14","source":1,"syntax":"","pretty":"variable id","sym":"V_a6913813992e95a8_238795,V_a2af7cf45ee2a132_238795,V_f45cdeeddce69744_238795,V_a5e394ff7266e66e_238795","no_crossref":1,"type":"int32_t"}
{"loc":"00168:12-16","source":1,"syntax":"","pretty":"variable aVar","sym":"V_19233713992e95a8_f0bb39c71,V_15318bf45ee2a132_f0bb39c71,V_67edddeddce69744_f0bb39c71,V_187593ff7266e66e_f0bb39c71","no_crossref":1,"type":"const IPC::ParamTraits<class mozilla::_ipdltest::PTestBasicChild *>::paramType &"}
{"loc":"00168:18-28","source":1,"syntax":"function,use","pretty":"function mozilla::ipc::IProtocol::FatalError","sym":"_ZNK7mozilla3ipc9IProtocol10FatalErrorEPKc","type":"void"}
{"loc":"00170:8-26","source":1,"syntax":"macro,use","pretty":"macro MOZ_RELEASE_ASSERT","sym":"M_96f821839add515c"}
{"loc":"00171:12-19","source":1,"syntax":"","pretty":"variable aWriter","sym":"V_336a2713992e95a8_369ebc101b0d,V_3f687bf45ee2a132_369ebc101b0d,V_8125ddeddce69744_369ebc101b0d,V_32bc83ff7266e66e_369ebc101b0d","no_crossref":1,"type":"IPC::MessageWriter *"}
{"loc":"00171:21-29","source":1,"syntax":"function,use","pretty":"function IPC::MessageWriter::GetActor","sym":"_ZNK3IPC13MessageWriter8GetActorEv","type":"mozilla::ipc::IProtocol *"}
{"loc":"00171:33-46","source":1,"syntax":"function,use","pretty":"function mozilla::ipc::IProtocol::GetIPCChannel","sym":"_ZN7mozilla3ipc9IProtocol13GetIPCChannelEv","type":"class mozilla::ipc::MessageChannel *"}
{"loc":"00171:52-56","source":1,"syntax":"","pretty":"variable aVar","sym":"V_19233713992e95a8_f0bb39c71,V_15318bf45ee2a132_f0bb39c71,V_67edddeddce69744_f0bb39c71,V_187593ff7266e66e_f0bb39c71","no_crossref":1,"type":"const IPC::ParamTraits<class mozilla::_ipdltest::PTestBasicChild *>::paramType &"}
{"loc":"00171:58-71","source":1,"syntax":"function,use","pretty":"function mozilla::ipc::IToplevelProtocol::GetIPCChannel","sym":"_ZN7mozilla3ipc17IToplevelProtocol13GetIPCChannelEv","type":"class mozilla::ipc::MessageChannel *"}
{"loc":"00174:8-26","source":1,"syntax":"macro,use","pretty":"macro MOZ_RELEASE_ASSERT","sym":"M_96f821839add515c"}
{"loc":"00175:12-16","source":1,"syntax":"","pretty":"variable aVar","sym":"V_19233713992e95a8_f0bb39c71,V_15318bf45ee2a132_f0bb39c71,V_67edddeddce69744_f0bb39c71,V_187593ff7266e66e_f0bb39c71","no_crossref":1,"type":"const IPC::ParamTraits<class mozilla::_ipdltest::PTestBasicChild *>::paramType &"}
{"loc":"00175:18-25","source":1,"syntax":"function,use","pretty":"function mozilla::ipc::IProtocol::CanSend","sym":"_ZNK7mozilla3ipc9IProtocol7CanSendEv","type":"_Bool"}
{"loc":"00179:9-19","source":1,"syntax":"function,use","pretty":"function IPC::WriteParam","sym":"_ZN3IPCL10WriteParamEPNS_13MessageWriterEOT_","type":"void (class IPC::MessageWriter *, int &)"}
{"loc":"00179:20-27","source":1,"syntax":"","pretty":"variable aWriter","sym":"V_336a2713992e95a8_369ebc101b0d,V_3f687bf45ee2a132_369ebc101b0d,V_8125ddeddce69744_369ebc101b0d,V_32bc83ff7266e66e_369ebc101b0d","no_crossref":1,"type":"IPC::MessageWriter *"}
{"loc":"00179:29-31","source":1,"syntax":"","pretty":"variable id","sym":"V_a6913813992e95a8_238795,V_a2af7cf45ee2a132_238795,V_f45cdeeddce69744_238795,V_a5e394ff7266e66e_238795","no_crossref":1,"type":"int32_t"}
{"loc":"00182:5-16","source":1,"syntax":"type,use","pretty":"type IPC::ParamTraits","sym":"T_IPC::ParamTraits"}
{"loc":"00182:37-52","source":1,"syntax":"type,use","pretty":"type mozilla::_ipdltest::PTestBasicChild","sym":"T_mozilla::_ipdltest::PTestBasicChild","type":"class mozilla::_ipdltest::PTestBasicChild","typesym":"T_mozilla::_ipdltest::PTestBasicChild"}
{"loc":"00182:56-60","source":1,"syntax":"def,function","pretty":"function IPC::ParamTraits<mozilla::_ipdltest::PTestBasicChild *>::Read","sym":"_ZN3IPC11ParamTraitsIPN7mozilla9_ipdltest15PTestBasicChildEE4ReadEPNS_13MessageReaderEPS4_","nestingRange":"185:0-198:0","type":"auto (IPC::MessageReader *, IPC::ParamTraits<class mozilla::_ipdltest::PTestBasicChild *>::paramType *) -> _Bool"}
{"loc":"00183:13-26","source":1,"syntax":"type,use","pretty":"type IPC::MessageReader","sym":"T_IPC::MessageReader","type":"class IPC::MessageReader","typesym":"T_IPC::MessageReader"}
{"loc":"00183:28-35","source":1,"syntax":"","pretty":"variable aReader","sym":"V_4f6d7a13992e95a8_9983235f0b0d,V_4b7bcef45ee2a132_9983235f0b0d,V_9d2821fddce69744_9983235f0b0d,V_4ebfd6ff7266e66e_9983235f0b0d","no_crossref":1,"type":"IPC::MessageReader *"}
{"loc":"00184:8-17","source":1,"syntax":"type,use","pretty":"type IPC::ParamTraits<mozilla::_ipdltest::PTestBasicChild *>::paramType","sym":"T_IPC::ParamTraits<mozilla::_ipdltest::PTestBasicChild_*>::paramType","type":"IPC::ParamTraits<class mozilla::_ipdltest::PTestBasicChild *>::paramType"}
{"loc":"00184:19-23","source":1,"syntax":"","pretty":"variable aVar","sym":"V_53368a13992e95a8_f0bb39c71,V_5f34def45ee2a132_f0bb39c71,V_a1f031fddce69744_f0bb39c71,V_5288e6ff7266e66e_f0bb39c71","no_crossref":1,"type":"IPC::ParamTraits<class mozilla::_ipdltest::PTestBasicChild *>::paramType *"}
{"loc":"00186:4-22","source":1,"syntax":"macro,use","pretty":"macro MOZ_RELEASE_ASSERT","sym":"M_96f821839add515c"}
{"loc":"00187:8-15","source":1,"syntax":"","pretty":"variable aReader","sym":"V_4f6d7a13992e95a8_9983235f0b0d,V_4b7bcef45ee2a132_9983235f0b0d,V_9d2821fddce69744_9983235f0b0d,V_4ebfd6ff7266e66e_9983235f0b0d","no_crossref":1,"type":"IPC::MessageReader *"}
{"loc":"00187:17-25","source":1,"syntax":"function,use","pretty":"function IPC::MessageReader::GetActor","sym":"_ZNK3IPC13MessageReader8GetActorEv","type":"mozilla::ipc::IProtocol *"}
{"loc":"00190:13-18","source":1,"syntax":"type,use","pretty":"type mozilla::Maybe","sym":"T_mozilla::Maybe"}
{"loc":"00190:33-42","source":1,"syntax":"type,use","pretty":"type mozilla::ipc::IProtocol","sym":"T_mozilla::ipc::IProtocol","type":"class mozilla::ipc::IProtocol","typesym":"T_mozilla::ipc::IProtocol"}
{"loc":"00190:45-50","source":1,"syntax":"","pretty":"variable actor","sym":"V_19a48b13992e95a8_ebe821f013,V_15b2dff45ee2a132_ebe821f013,V_676f22fddce69744_ebe821f013,V_18f6e7ff7266e66e_ebe821f013","no_crossref":1,"type":"mozilla::Maybe<mozilla::ipc::IProtocol *>","typesym":"T_mozilla::Maybe"}
{"loc":"00191:8-15","source":1,"syntax":"","pretty":"variable aReader","sym":"V_4f6d7a13992e95a8_9983235f0b0d,V_4b7bcef45ee2a132_9983235f0b0d,V_9d2821fddce69744_9983235f0b0d,V_4ebfd6ff7266e66e_9983235f0b0d","no_crossref":1,"type":"IPC::MessageReader *"}
{"loc":"00191:17-25","source":1,"syntax":"function,use","pretty":"function IPC::MessageReader::GetActor","sym":"_ZNK3IPC13MessageReader8GetActorEv","type":"mozilla::ipc::IProtocol *"}
{"loc":"00191:29-38","source":1,"syntax":"function,use","pretty":"function mozilla::ipc::IProtocol::ReadActor","sym":"_ZN7mozilla3ipc9IProtocol9ReadActorEPN3IPC13MessageReaderEbPKci","type":"Maybe<class mozilla::ipc::IProtocol *>","typesym":"T_mozilla::Maybe"}
{"loc":"00191:39-46","source":1,"syntax":"","pretty":"variable aReader","sym":"V_4f6d7a13992e95a8_9983235f0b0d,V_4b7bcef45ee2a132_9983235f0b0d,V_9d2821fddce69744_9983235f0b0d,V_4ebfd6ff7266e66e_9983235f0b0d","no_crossref":1,"type":"IPC::MessageReader *"}
{"loc":"00191:68-86","source":1,"syntax":"enum,use","pretty":"enum IPCMessageStart::PTestBasicMsgStart","sym":"E_<T_IPCMessageStart>_PTestBasicMsgStart","type":"enum IPCMessageStart","typesym":"T_IPCMessageStart"}
{"loc":"00192:8-13","source":1,"syntax":"","pretty":"variable actor","sym":"V_19a48b13992e95a8_ebe821f013,V_15b2dff45ee2a132_ebe821f013,V_676f22fddce69744_ebe821f013,V_18f6e7ff7266e66e_ebe821f013","no_crossref":1,"type":"mozilla::Maybe<mozilla::ipc::IProtocol *>","typesym":"T_mozilla::Maybe"}
{"loc":"00192:14-23","source":1,"syntax":"function,use","pretty":"function mozilla::Maybe::isNothing","sym":"_ZNK7mozilla5Maybe9isNothingEv","type":"_Bool"}
{"loc":"00196:5-9","source":1,"syntax":"","pretty":"variable aVar","sym":"V_53368a13992e95a8_f0bb39c71,V_5f34def45ee2a132_f0bb39c71,V_a1f031fddce69744_f0bb39c71,V_5288e6ff7266e66e_f0bb39c71","no_crossref":1,"type":"IPC::ParamTraits<class mozilla::_ipdltest::PTestBasicChild *>::paramType *"}
{"loc":"00196:44-59","source":1,"syntax":"type,use","pretty":"type mozilla::_ipdltest::PTestBasicChild","sym":"T_mozilla::_ipdltest::PTestBasicChild","type":"class mozilla::_ipdltest::PTestBasicChild","typesym":"T_mozilla::_ipdltest::PTestBasicChild"}
{"loc":"00196:62-67","source":1,"syntax":"","pretty":"variable actor","sym":"V_19a48b13992e95a8_ebe821f013,V_15b2dff45ee2a132_ebe821f013,V_676f22fddce69744_ebe821f013,V_18f6e7ff7266e66e_ebe821f013","no_crossref":1,"type":"mozilla::Maybe<mozilla::ipc::IProtocol *>","typesym":"T_mozilla::Maybe"}
{"loc":"00196:68-73","source":1,"syntax":"function,use","pretty":"function mozilla::Maybe::value","sym":"_ZNKR7mozilla5Maybe5valueEv","type":"class mozilla::ipc::IProtocol *"}
{"loc":"00182:56-60","structured":1,"pretty":"IPC::ParamTraits<mozilla::_ipdltest::PTestBasicChild *>::Read","sym":"_ZN3IPC11ParamTraitsIPN7mozilla9_ipdltest15PTestBasicChildEE4ReadEPNS_13MessageReaderEPS4_","kind":"method","parentsym":"T_IPC::ParamTraits","implKind":"","sizeBytes":null,"supers":[],"methods":[],"fields":[],"overrides":[],"props":["static","user"]}
{"loc":"00154:56-61","structured":1,"pretty":"IPC::ParamTraits<mozilla::_ipdltest::PTestBasicChild *>::Write","sym":"_ZN3IPC11ParamTraitsIPN7mozilla9_ipdltest15PTestBasicChildEE5WriteEPNS_13MessageWriterERKS4_","kind":"method","parentsym":"T_IPC::ParamTraits","implKind":"","sizeBytes":null,"supers":[],"methods":[],"fields":[],"overrides":[],"props":["static","user"]}
{"loc":"00145:22-34","structured":1,"pretty":"mozilla::_ipdltest::PTestBasicChild::ClearSubtree","sym":"_ZN7mozilla9_ipdltest15PTestBasicChild12ClearSubtreeEv","kind":"method","parentsym":"T_mozilla::_ipdltest::PTestBasicChild","implKind":"","sizeBytes":null,"supers":[],"methods":[],"fields":[],"overrides":[],"props":["instance","user"]}
{"loc":"00054:22-35","structured":1,"pretty":"mozilla::_ipdltest::PTestBasicChild::RemoveManagee","sym":"_ZN7mozilla9_ipdltest15PTestBasicChild13RemoveManageeEiPNS_3ipc9IProtocolE","kind":"method","parentsym":"T_mozilla::_ipdltest::PTestBasicChild","implKind":"","sizeBytes":null,"supers":[],"methods":[],"fields":[],"overrides":[{"pretty":"mozilla::ipc::IProtocol::RemoveManagee","sym":"_ZN7mozilla3ipc9IProtocol13RemoveManageeEiPS1_"}],"props":["instance","virtual","user"]}
{"loc":"00062:22-36","structured":1,"pretty":"mozilla::_ipdltest::PTestBasicChild::DeallocManagee","sym":"_ZN7mozilla9_ipdltest15PTestBasicChild14DeallocManageeEiPNS_3ipc9IProtocolE","kind":"method","parentsym":"T_mozilla::_ipdltest::PTestBasicChild","implKind":"","sizeBytes":null,"supers":[],"methods":[],"fields":[],"overrides":[{"pretty":"mozilla::ipc::IProtocol::DeallocManagee","sym":"_ZN7mozilla3ipc9IProtocol14DeallocManageeEiPS1_"}],"props":["instance","virtual","user"]}
{"loc":"00117:22-36","structured":1,"pretty":"mozilla::_ipdltest::PTestBasicChild::OnCallReceived","sym":"_ZN7mozilla9_ipdltest15PTestBasicChild14OnCallReceivedERKN3IPC7MessageERNS_9UniquePtrIS3_NS_13DefaultDeleteIS3_EEEE","kind":"method","parentsym":"T_mozilla::_ipdltest::PTestBasicChild","implKind":"","sizeBytes":null,"supers":[],"methods":[],"fields":[],"overrides":[{"pretty":"mozilla::ipc::IProtocol::OnCallReceived","sym":"_ZN7mozilla3ipc9IProtocol14OnCallReceivedERKN3IPC7MessageERNS_9UniquePtrIS3_NS_13DefaultDeleteIS3_EEEE"}],"props":["instance","virtual","user"]}
{"loc":"00125:22-36","structured":1,"pretty":"mozilla::_ipdltest::PTestBasicChild::OnChannelClose","sym":"_ZN7mozilla9_ipdltest15PTestBasicChild14OnChannelCloseEv","kind":"method","parentsym":"T_mozilla::_ipdltest::PTestBasicChild","implKind":"","sizeBytes":null,"supers":[],"methods":[],"fields":[],"overrides":[{"pretty":"mozilla::ipc::IToplevelProtocol::OnChannelClose","sym":"_ZN7mozilla3ipc17IToplevelProtocol14OnChannelCloseEv"}],"props":["instance","virtual","user"]}
{"loc":"00135:22-36","structured":1,"pretty":"mozilla::_ipdltest::PTestBasicChild::OnChannelError","sym":"_ZN7mozilla9_ipdltest15PTestBasicChild14OnChannelErrorEv","kind":"method","parentsym":"T_mozilla::_ipdltest::PTestBasicChild","implKind":"","sizeBytes":null,"supers":[],"methods":[],"fields":[],"overrides":[{"pretty":"mozilla::ipc::IToplevelProtocol::OnChannelError","sym":"_ZN7mozilla3ipc17IToplevelProtocol14OnChannelErrorEv"}],"props":["instance","virtual","user"]}
{"loc":"00025:22-37","structured":1,"pretty":"mozilla::_ipdltest::PTestBasicChild::ProcessingError","sym":"_ZN7mozilla9_ipdltest15PTestBasicChild15ProcessingErrorENS_3ipc14HasResultCodes6ResultEPKc","kind":"method","parentsym":"T_mozilla::_ipdltest::PTestBasicChild","implKind":"","sizeBytes":null,"supers":[],"methods":[],"fields":[],"overrides":[{"pretty":"mozilla::ipc::IToplevelProtocol::ProcessingError","sym":"_ZN7mozilla3ipc17IToplevelProtocol15ProcessingErrorENS0_14HasResultCodes6ResultEPKc"}],"props":["instance","virtual","user"]}
{"loc":"00070:22-39","structured":1,"pretty":"mozilla::_ipdltest::PTestBasicChild::OnMessageReceived","sym":"_ZN7mozilla9_ipdltest15PTestBasicChild17OnMessageReceivedERKN3IPC7MessageE","kind":"method","parentsym":"T_mozilla::_ipdltest::PTestBasicChild","implKind":"","sizeBytes":null,"supers":[],"methods":[],"fields":[],"overrides":[{"pretty":"mozilla::ipc::IProtocol::OnMessageReceived","sym":"_ZN7mozilla3ipc9IProtocol17OnMessageReceivedERKN3IPC7MessageE"}],"props":["instance","virtual","user"]}
{"loc":"00109:22-39","structured":1,"pretty":"mozilla::_ipdltest::PTestBasicChild::OnMessageReceived","sym":"_ZN7mozilla9_ipdltest15PTestBasicChild17OnMessageReceivedERKN3IPC7MessageERNS_9UniquePtrIS3_NS_13DefaultDeleteIS3_EEEE","kind":"method","parentsym":"T_mozilla::_ipdltest::PTestBasicChild","implKind":"","sizeBytes":null,"supers":[],"methods":[],"fields":[],"overrides":[{"pretty":"mozilla::ipc::IProtocol::OnMessageReceived","sym":"_ZN7mozilla3ipc9IProtocol17OnMessageReceivedERKN3IPC7MessageERNS_9UniquePtrIS3_NS_13DefaultDeleteIS3_EEEE"}],"props":["instance","virtual","user"]}
{"loc":"00031:22-52","structured":1,"pretty":"mozilla::_ipdltest::PTestBasicChild::ShouldContinueFromReplyTimeout","sym":"_ZN7mozilla9_ipdltest15PTestBasicChild30ShouldContinueFromReplyTimeoutEv","kind":"method","parentsym":"T_mozilla::_ipdltest::PTestBasicChild","implKind":"","sizeBytes":null,"supers":[],"methods":[],"fields":[],"overrides":[{"pretty":"mozilla::ipc::IToplevelProtocol::ShouldContinueFromReplyTimeout","sym":"_ZN7mozilla3ipc17IToplevelProtocol30ShouldContinueFromReplyTimeoutEv"}],"props":["instance","virtual","user"]}
{"loc":"00036:30-45","structured":1,"pretty":"mozilla::_ipdltest::PTestBasicChild::PTestBasicChild","sym":"_ZN7mozilla9_ipdltest15PTestBasicChildC1Ev","kind":"method","parentsym":"T_mozilla::_ipdltest::PTestBasicChild","implKind":"","sizeBytes":null,"supers":[],"methods":[],"fields":[],"overrides":[],"props":["instance","user"]}
{"loc":"00042:18-33","structured":1,"pretty":"mozilla::_ipdltest::PTestBasicChild::~PTestBasicChild","sym":"_ZN7mozilla9_ipdltest15PTestBasicChildD1Ev","kind":"method","parentsym":"T_mozilla::_ipdltest::PTestBasicChild","implKind":"","sizeBytes":null,"supers":[],"methods":[],"fields":[],"overrides":[{"pretty":"mozilla::ipc::IToplevelProtocol::~IToplevelProtocol","sym":"_ZN7mozilla3ipc17IToplevelProtocolD1Ev"}],"props":["instance","virtual","user"]}
{"loc":"00047:22-38","structured":1,"pretty":"mozilla::_ipdltest::PTestBasicChild::AllManagedActors","sym":"_ZNK7mozilla9_ipdltest15PTestBasicChild16AllManagedActorsER8nsTArrayI6RefPtrINS_3ipc19ActorLifecycleProxyEEE","kind":"method","parentsym":"T_mozilla::_ipdltest::PTestBasicChild","implKind":"","sizeBytes":null,"supers":[],"methods":[],"fields":[],"overrides":[{"pretty":"mozilla::ipc::IProtocol::AllManagedActors","sym":"_ZNK7mozilla3ipc9IProtocol16AllManagedActorsER8nsTArrayI6RefPtrINS0_19ActorLifecycleProxyEEE"}],"props":["instance","virtual","user"]}

```

## tests/tests/mc-analysis/__GENERATED__/ipdl/PTestBasicParent.cpp
```
{"loc":"00001:0","target":1,"kind":"def","pretty":"__GENERATED__/ipc/ipdl/PTestBasicParent.cpp","sym":"FILE_android-armv7@__GENERATED__/ipc/ipdl/PTestBasicParent@2Ecpp"}
{"loc":"00007:9-47","target":1,"kind":"use","pretty":"__GENERATED__/ipc/ipdl/_ipdlheaders/mozilla/_ipdltest/PTestBasicParent.h","sym":"FILE_android-armv7@__GENERATED__/ipc/ipdl/_ipdlheaders/mozilla/_ipdltest/PTestBasicParent@2Eh"}
{"loc":"00008:9-35","target":1,"kind":"use","pretty":"tools/profiler/public/ProfilerLabels.h","sym":"FILE_tools/profiler/public/ProfilerLabels@2Eh"}
{"loc":"00009:9-46","target":1,"kind":"use","pretty":"ipc/ipdl/test/gtest/TestBasicParent.h","sym":"FILE_ipc/ipdl/test/gtest/TestBasicParent@2Eh"}
{"loc":"00012:9-32","target":1,"kind":"use","pretty":"ipc/glue/IPCMessageUtils.h","sym":"FILE_ipc/glue/IPCMessageUtils@2Eh"}
{"loc":"00013:9-47","target":1,"kind":"use","pretty":"ipc/glue/IPCMessageUtilsSpecializations.h","sym":"FILE_ipc/glue/IPCMessageUtilsSpecializations@2Eh"}
{"loc":"00014:9-20","target":1,"kind":"use","pretty":"__GENERATED__/dist/include/nsIFile.h","sym":"FILE_android-armv7@__GENERATED__/dist/include/nsIFile@2Eh"}
{"loc":"00015:9-33","target":1,"kind":"use","pretty":"ipc/glue/Endpoint.h","sym":"FILE_ipc/glue/Endpoint@2Eh"}
{"loc":"00016:9-45","target":1,"kind":"use","pretty":"ipc/glue/ProtocolMessageUtils.h","sym":"FILE_ipc/glue/ProtocolMessageUtils@2Eh"}
{"loc":"00017:9-38","target":1,"kind":"use","pretty":"ipc/glue/ProtocolUtils.h","sym":"FILE_ipc/glue/ProtocolUtils@2Eh"}
{"loc":"00018:9-42","target":1,"kind":"use","pretty":"ipc/glue/ShmemMessageUtils.h","sym":"FILE_ipc/glue/ShmemMessageUtils@2Eh"}
{"loc":"00019:9-41","target":1,"kind":"use","pretty":"ipc/glue/TaintingIPCUtils.h","sym":"FILE_ipc/glue/TaintingIPCUtils@2Eh"}
{"loc":"00021:10-17","target":1,"kind":"def","pretty":"mozilla","sym":"NS_mozilla"}
{"loc":"00022:10-19","target":1,"kind":"def","pretty":"mozilla::_ipdltest","sym":"NS_mozilla::_ipdltest"}
{"loc":"00025:23-38","target":1,"kind":"def","pretty":"mozilla::_ipdltest::PTestBasicParent::ProcessingError","sym":"_ZN7mozilla9_ipdltest16PTestBasicParent15ProcessingErrorENS_3ipc14HasResultCodes6ResultEPKc","peekRange":"25-27"}
{"loc":"00025:5-21","target":1,"kind":"use","pretty":"mozilla::_ipdltest::PTestBasicParent","sym":"T_mozilla::_ipdltest::PTestBasicParent","context":"mozilla::_ipdltest::PTestBasicParent::ProcessingError","contextsym":"_ZN7mozilla9_ipdltest16PTestBasicParent15ProcessingErrorENS_3ipc14HasResultCodes6ResultEPKc"}
{"loc":"00026:8-14","target":1,"kind":"use","pretty":"mozilla::ipc::HasResultCodes::Result","sym":"T_mozilla::ipc::HasResultCodes::Result","context":"mozilla::_ipdltest::PTestBasicParent::ProcessingError","contextsym":"_ZN7mozilla9_ipdltest16PTestBasicParent15ProcessingErrorENS_3ipc14HasResultCodes6ResultEPKc"}
{"loc":"00031:23-53","target":1,"kind":"def","pretty":"mozilla::_ipdltest::PTestBasicParent::ShouldContinueFromReplyTimeout","sym":"_ZN7mozilla9_ipdltest16PTestBasicParent30ShouldContinueFromReplyTimeoutEv","peekRange":"31-31"}
{"loc":"00031:5-21","target":1,"kind":"use","pretty":"mozilla::_ipdltest::PTestBasicParent","sym":"T_mozilla::_ipdltest::PTestBasicParent","context":"mozilla::_ipdltest::PTestBasicParent::ShouldContinueFromReplyTimeout","contextsym":"_ZN7mozilla9_ipdltest16PTestBasicParent30ShouldContinueFromReplyTimeoutEv"}
{"loc":"00036:0-12","target":1,"kind":"use","pretty":"MOZ_IMPLICIT","sym":"M_5f65895d5a5258f7"}
{"loc":"00036:13-29","target":1,"kind":"use","pretty":"mozilla::_ipdltest::PTestBasicParent","sym":"T_mozilla::_ipdltest::PTestBasicParent","context":"mozilla::_ipdltest::PTestBasicParent::PTestBasicParent","contextsym":"_ZN7mozilla9_ipdltest16PTestBasicParentC1Ev"}
{"loc":"00036:31-47","target":1,"kind":"def","pretty":"mozilla::_ipdltest::PTestBasicParent::PTestBasicParent","sym":"_ZN7mozilla9_ipdltest16PTestBasicParentC1Ev"}
{"loc":"00037:18-35","target":1,"kind":"use","pretty":"mozilla::ipc::IToplevelProtocol","sym":"T_mozilla::ipc::IToplevelProtocol","context":"mozilla::_ipdltest::PTestBasicParent::PTestBasicParent","contextsym":"_ZN7mozilla9_ipdltest16PTestBasicParentC1Ev"}
{"loc":"00037:4-11","target":1,"kind":"use","pretty":"mozilla::ipc::IToplevelProtocol::IToplevelProtocol","sym":"_ZN7mozilla3ipc17IToplevelProtocolC1EPKc15IPCMessageStartNS0_4SideE","context":"mozilla::_ipdltest::PTestBasicParent::PTestBasicParent","contextsym":"_ZN7mozilla9_ipdltest16PTestBasicParentC1Ev"}
{"loc":"00037:56-74","target":1,"kind":"use","pretty":"IPCMessageStart::PTestBasicMsgStart","sym":"E_<T_IPCMessageStart>_PTestBasicMsgStart","context":"mozilla::_ipdltest::PTestBasicParent::PTestBasicParent","contextsym":"_ZN7mozilla9_ipdltest16PTestBasicParentC1Ev"}
{"loc":"00037:90-100","target":1,"kind":"use","pretty":"mozilla::ipc::Side::ParentSide","sym":"E_<T_mozilla::ipc::Side>_ParentSide","context":"mozilla::_ipdltest::PTestBasicParent::PTestBasicParent","contextsym":"_ZN7mozilla9_ipdltest16PTestBasicParentC1Ev"}
{"loc":"00039:19-35","target":1,"kind":"use","pretty":"mozilla::_ipdltest::PTestBasicParent","sym":"T_mozilla::_ipdltest::PTestBasicParent","context":"mozilla::_ipdltest::PTestBasicParent::PTestBasicParent","contextsym":"_ZN7mozilla9_ipdltest16PTestBasicParentC1Ev"}
{"loc":"00039:4-18","target":1,"kind":"use","pretty":"MOZ_COUNT_CTOR","sym":"M_d9d58f5149d9f0f2"}
{"loc":"00042:0-16","target":1,"kind":"use","pretty":"mozilla::_ipdltest::PTestBasicParent","sym":"T_mozilla::_ipdltest::PTestBasicParent","context":"mozilla::_ipdltest::PTestBasicParent::~PTestBasicParent","contextsym":"_ZN7mozilla9_ipdltest16PTestBasicParentD1Ev"}
{"loc":"00042:19-35","target":1,"kind":"def","pretty":"mozilla::_ipdltest::PTestBasicParent::~PTestBasicParent","sym":"_ZN7mozilla9_ipdltest16PTestBasicParentD1Ev","peekRange":"42-42"}
{"loc":"00042:19-35","target":1,"kind":"use","pretty":"mozilla::_ipdltest::PTestBasicParent","sym":"T_mozilla::_ipdltest::PTestBasicParent","context":"mozilla::_ipdltest::PTestBasicParent::~PTestBasicParent","contextsym":"_ZN7mozilla9_ipdltest16PTestBasicParentD1Ev"}
{"loc":"00044:19-35","target":1,"kind":"use","pretty":"mozilla::_ipdltest::PTestBasicParent","sym":"T_mozilla::_ipdltest::PTestBasicParent","context":"mozilla::_ipdltest::PTestBasicParent::~PTestBasicParent","contextsym":"_ZN7mozilla9_ipdltest16PTestBasicParentD1Ev"}
{"loc":"00044:4-18","target":1,"kind":"use","pretty":"MOZ_COUNT_DTOR","sym":"M_f9e8c16149d9f0f2"}
{"loc":"00047:23-39","target":1,"kind":"def","pretty":"mozilla::_ipdltest::PTestBasicParent::AllManagedActors","sym":"_ZNK7mozilla9_ipdltest16PTestBasicParent16AllManagedActorsER8nsTArrayI6RefPtrINS_3ipc19ActorLifecycleProxyEEE","peekRange":"47-47"}
{"loc":"00047:40-48","target":1,"kind":"use","pretty":"nsTArray","sym":"T_nsTArray","context":"mozilla::_ipdltest::PTestBasicParent::AllManagedActors","contextsym":"_ZNK7mozilla9_ipdltest16PTestBasicParent16AllManagedActorsER8nsTArrayI6RefPtrINS_3ipc19ActorLifecycleProxyEEE"}
{"loc":"00047:49-55","target":1,"kind":"use","pretty":"RefPtr","sym":"T_RefPtr","context":"mozilla::_ipdltest::PTestBasicParent::AllManagedActors","contextsym":"_ZNK7mozilla9_ipdltest16PTestBasicParent16AllManagedActorsER8nsTArrayI6RefPtrINS_3ipc19ActorLifecycleProxyEEE"}
{"loc":"00047:5-21","target":1,"kind":"use","pretty":"mozilla::_ipdltest::PTestBasicParent","sym":"T_mozilla::_ipdltest::PTestBasicParent","context":"mozilla::_ipdltest::PTestBasicParent::AllManagedActors","contextsym":"_ZNK7mozilla9_ipdltest16PTestBasicParent16AllManagedActorsER8nsTArrayI6RefPtrINS_3ipc19ActorLifecycleProxyEEE"}
{"loc":"00047:70-89","target":1,"kind":"use","pretty":"mozilla::ipc::ActorLifecycleProxy","sym":"T_mozilla::ipc::ActorLifecycleProxy","context":"mozilla::_ipdltest::PTestBasicParent::AllManagedActors","contextsym":"_ZNK7mozilla9_ipdltest16PTestBasicParent16AllManagedActorsER8nsTArrayI6RefPtrINS_3ipc19ActorLifecycleProxyEEE"}
{"loc":"00049:4-12","target":1,"kind":"use","pretty":"uint32_t","sym":"T_uint32_t","context":"mozilla::_ipdltest::PTestBasicParent::AllManagedActors","contextsym":"_ZNK7mozilla9_ipdltest16PTestBasicParent16AllManagedActorsER8nsTArrayI6RefPtrINS_3ipc19ActorLifecycleProxyEEE"}
{"loc":"00050:10-21","target":1,"kind":"use","pretty":"nsTArray_Impl::SetCapacity","sym":"_ZN13nsTArray_Impl11SetCapacityEN13nsTArray_baseIT0_N27nsTArray_RelocationStrategyIT_E4TypeEE9size_typeE","context":"mozilla::_ipdltest::PTestBasicParent::AllManagedActors","contextsym":"_ZNK7mozilla9_ipdltest16PTestBasicParent16AllManagedActorsER8nsTArrayI6RefPtrINS_3ipc19ActorLifecycleProxyEEE"}
{"loc":"00054:23-32","target":1,"kind":"def","pretty":"mozilla::_ipdltest::PTestBasicParent::SendHello","sym":"_ZN7mozilla9_ipdltest16PTestBasicParent9SendHelloEv","peekRange":"54-54"}
{"loc":"00054:5-21","target":1,"kind":"use","pretty":"mozilla::_ipdltest::PTestBasicParent","sym":"T_mozilla::_ipdltest::PTestBasicParent","context":"mozilla::_ipdltest::PTestBasicParent::SendHello","contextsym":"_ZN7mozilla9_ipdltest16PTestBasicParent9SendHelloEv"}
{"loc":"00056:19-26","target":1,"kind":"use","pretty":"IPC::Message","sym":"T_IPC::Message","context":"mozilla::_ipdltest::PTestBasicParent::SendHello","contextsym":"_ZN7mozilla9_ipdltest16PTestBasicParent9SendHelloEv"}
{"loc":"00056:4-13","target":1,"kind":"use","pretty":"mozilla::_ipdltest::PTestBasicParent::UniquePtr","sym":"T_mozilla::_ipdltest::PTestBasicParent::UniquePtr","context":"mozilla::_ipdltest::PTestBasicParent::SendHello","contextsym":"_ZN7mozilla9_ipdltest16PTestBasicParent9SendHelloEv"}
{"loc":"00056:48-57","target":1,"kind":"use","pretty":"mozilla::_ipdltest::PTestBasic::Msg_Hello","sym":"_ZN7mozilla9_ipdltest10PTestBasic9Msg_HelloEi","context":"mozilla::_ipdltest::PTestBasicParent::SendHello","contextsym":"_ZN7mozilla9_ipdltest16PTestBasicParent9SendHelloEv"}
{"loc":"00056:58-77","target":1,"kind":"use","pretty":"SpecialRoutingIDs::MSG_ROUTING_CONTROL","sym":"E_<T_SpecialRoutingIDs>_MSG_ROUTING_CONTROL","context":"mozilla::_ipdltest::PTestBasicParent::SendHello","contextsym":"_ZN7mozilla9_ipdltest16PTestBasicParent9SendHelloEv"}
{"loc":"00057:23-31","target":1,"kind":"use","pretty":"IPC::MessageWriter::MessageWriter","sym":"_ZN3IPC13MessageWriterC1ERNS_7MessageEPN7mozilla3ipc9IProtocolE","context":"mozilla::_ipdltest::PTestBasicParent::SendHello","contextsym":"_ZN7mozilla9_ipdltest16PTestBasicParent9SendHelloEv"}
{"loc":"00057:9-22","target":1,"kind":"use","pretty":"IPC::MessageWriter","sym":"T_IPC::MessageWriter","context":"mozilla::_ipdltest::PTestBasicParent::SendHello","contextsym":"_ZN7mozilla9_ipdltest16PTestBasicParent9SendHelloEv"}
{"loc":"00058:13-14","target":1,"kind":"use","pretty":"mozilla::UniquePtr::operator*","sym":"_ZNK7mozilla9UniquePtrdeEv","context":"mozilla::_ipdltest::PTestBasicParent::SendHello","contextsym":"_ZN7mozilla9_ipdltest16PTestBasicParent9SendHelloEv"}
{"loc":"00065:22-39","target":1,"kind":"use","pretty":"mozilla::ipc::LoggingEnabledFor","sym":"_ZN7mozilla3ipc17LoggingEnabledForEPKc","context":"mozilla::_ipdltest::PTestBasicParent::SendHello","contextsym":"_ZN7mozilla9_ipdltest16PTestBasicParent9SendHelloEv"}
{"loc":"00066:22-43","target":1,"kind":"use","pretty":"mozilla::ipc::LogMessageForProtocol","sym":"_ZN7mozilla3ipc21LogMessageForProtocolEPKciS2_jNS0_16MessageDirectionE","context":"mozilla::_ipdltest::PTestBasicParent::SendHello","contextsym":"_ZN7mozilla9_ipdltest16PTestBasicParent9SendHelloEv"}
{"loc":"00068:18-34","target":1,"kind":"use","pretty":"mozilla::ipc::IProtocol::ToplevelProtocol","sym":"_ZN7mozilla3ipc9IProtocol16ToplevelProtocolEv","context":"mozilla::_ipdltest::PTestBasicParent::SendHello","contextsym":"_ZN7mozilla9_ipdltest16PTestBasicParent9SendHelloEv"}
{"loc":"00068:38-58","target":1,"kind":"use","pretty":"mozilla::ipc::IToplevelProtocol::OtherPidMaybeInvalid","sym":"_ZNK7mozilla3ipc17IToplevelProtocol20OtherPidMaybeInvalidEv","context":"mozilla::_ipdltest::PTestBasicParent::SendHello","contextsym":"_ZN7mozilla9_ipdltest16PTestBasicParent9SendHelloEv"}
{"loc":"00070:17-19","target":1,"kind":"use","pretty":"mozilla::UniquePtr::operator->","sym":"_ZNK7mozilla9UniquePtrptEv","context":"mozilla::_ipdltest::PTestBasicParent::SendHello","contextsym":"_ZN7mozilla9_ipdltest16PTestBasicParent9SendHelloEv"}
{"loc":"00070:19-23","target":1,"kind":"use","pretty":"IPC::Message::type","sym":"_ZNK3IPC7Message4typeEv","context":"mozilla::_ipdltest::PTestBasicParent::SendHello","contextsym":"_ZN7mozilla9_ipdltest16PTestBasicParent9SendHelloEv"}
{"loc":"00071:26-42","target":1,"kind":"use","pretty":"mozilla::ipc::MessageDirection","sym":"T_mozilla::ipc::MessageDirection","context":"mozilla::_ipdltest::PTestBasicParent::SendHello","contextsym":"_ZN7mozilla9_ipdltest16PTestBasicParent9SendHelloEv"}
{"loc":"00071:44-52","target":1,"kind":"use","pretty":"mozilla::ipc::MessageDirection::eSending","sym":"E_<T_mozilla::ipc::MessageDirection>_eSending","context":"mozilla::_ipdltest::PTestBasicParent::SendHello","contextsym":"_ZN7mozilla9_ipdltest16PTestBasicParent9SendHelloEv"}
{"loc":"00073:4-23","target":1,"kind":"use","pretty":"AUTO_PROFILER_LABEL","sym":"M_99e0335dc782f2d6"}
{"loc":"00073:49-54","target":1,"kind":"use","pretty":"JS::ProfilingCategoryPair::OTHER","sym":"E_<T_JS::ProfilingCategoryPair>_OTHER","context":"mozilla::_ipdltest::PTestBasicParent::SendHello","contextsym":"_ZN7mozilla9_ipdltest16PTestBasicParent9SendHelloEv"}
{"loc":"00075:20-31","target":1,"kind":"use","pretty":"mozilla::ipc::IProtocol::ChannelSend","sym":"_ZN7mozilla3ipc9IProtocol11ChannelSendENS_9UniquePtrIN3IPC7MessageENS_13DefaultDeleteIS4_EEEE","context":"mozilla::_ipdltest::PTestBasicParent::SendHello","contextsym":"_ZN7mozilla9_ipdltest16PTestBasicParent9SendHelloEv"}
{"loc":"00075:32-35","target":1,"kind":"use","pretty":"mozilla::UniquePtr::UniquePtr<T, D>","sym":"_ZN7mozilla9UniquePtrC1EONS_9UniquePtrIT_T0_EE","context":"mozilla::_ipdltest::PTestBasicParent::SendHello","contextsym":"_ZN7mozilla9_ipdltest16PTestBasicParent9SendHelloEv"}
{"loc":"00075:37-41","target":1,"kind":"use","pretty":"std::move","sym":"_ZNSt6__ndk14moveEOT_","context":"mozilla::_ipdltest::PTestBasicParent::SendHello","contextsym":"_ZN7mozilla9_ipdltest16PTestBasicParent9SendHelloEv"}
{"loc":"00079:23-36","target":1,"kind":"def","pretty":"mozilla::_ipdltest::PTestBasicParent::RemoveManagee","sym":"_ZN7mozilla9_ipdltest16PTestBasicParent13RemoveManageeEiPNS_3ipc9IProtocolE","peekRange":"79-81"}
{"loc":"00079:5-21","target":1,"kind":"use","pretty":"mozilla::_ipdltest::PTestBasicParent","sym":"T_mozilla::_ipdltest::PTestBasicParent","context":"mozilla::_ipdltest::PTestBasicParent::RemoveManagee","contextsym":"_ZN7mozilla9_ipdltest16PTestBasicParent13RemoveManageeEiPNS_3ipc9IProtocolE"}
{"loc":"00080:8-15","target":1,"kind":"use","pretty":"int32_t","sym":"T_int32_t","context":"mozilla::_ipdltest::PTestBasicParent::RemoveManagee","contextsym":"_ZN7mozilla9_ipdltest16PTestBasicParent13RemoveManageeEiPNS_3ipc9IProtocolE"}
{"loc":"00081:8-17","target":1,"kind":"use","pretty":"mozilla::_ipdltest::PTestBasicParent::IProtocol","sym":"T_mozilla::_ipdltest::PTestBasicParent::IProtocol","context":"mozilla::_ipdltest::PTestBasicParent::RemoveManagee","contextsym":"_ZN7mozilla9_ipdltest16PTestBasicParent13RemoveManageeEiPNS_3ipc9IProtocolE"}
{"loc":"00083:4-14","target":1,"kind":"use","pretty":"mozilla::ipc::IProtocol::FatalError","sym":"_ZNK7mozilla3ipc9IProtocol10FatalErrorEPKc","context":"mozilla::_ipdltest::PTestBasicParent::RemoveManagee","contextsym":"_ZN7mozilla9_ipdltest16PTestBasicParent13RemoveManageeEiPNS_3ipc9IProtocolE"}
{"loc":"00087:23-37","target":1,"kind":"def","pretty":"mozilla::_ipdltest::PTestBasicParent::DeallocManagee","sym":"_ZN7mozilla9_ipdltest16PTestBasicParent14DeallocManageeEiPNS_3ipc9IProtocolE","peekRange":"87-89"}
{"loc":"00087:5-21","target":1,"kind":"use","pretty":"mozilla::_ipdltest::PTestBasicParent","sym":"T_mozilla::_ipdltest::PTestBasicParent","context":"mozilla::_ipdltest::PTestBasicParent::DeallocManagee","contextsym":"_ZN7mozilla9_ipdltest16PTestBasicParent14DeallocManageeEiPNS_3ipc9IProtocolE"}
{"loc":"00088:8-15","target":1,"kind":"use","pretty":"int32_t","sym":"T_int32_t","context":"mozilla::_ipdltest::PTestBasicParent::DeallocManagee","contextsym":"_ZN7mozilla9_ipdltest16PTestBasicParent14DeallocManageeEiPNS_3ipc9IProtocolE"}
{"loc":"00089:8-17","target":1,"kind":"use","pretty":"mozilla::_ipdltest::PTestBasicParent::IProtocol","sym":"T_mozilla::_ipdltest::PTestBasicParent::IProtocol","context":"mozilla::_ipdltest::PTestBasicParent::DeallocManagee","contextsym":"_ZN7mozilla9_ipdltest16PTestBasicParent14DeallocManageeEiPNS_3ipc9IProtocolE"}
{"loc":"00091:4-14","target":1,"kind":"use","pretty":"mozilla::ipc::IProtocol::FatalError","sym":"_ZNK7mozilla3ipc9IProtocol10FatalErrorEPKc","context":"mozilla::_ipdltest::PTestBasicParent::DeallocManagee","contextsym":"_ZN7mozilla9_ipdltest16PTestBasicParent14DeallocManageeEiPNS_3ipc9IProtocolE"}
{"loc":"00095:23-40","target":1,"kind":"def","pretty":"mozilla::_ipdltest::PTestBasicParent::OnMessageReceived","sym":"_ZN7mozilla9_ipdltest16PTestBasicParent17OnMessageReceivedERKN3IPC7MessageE","peekRange":"95-95"}
{"loc":"00095:47-54","target":1,"kind":"use","pretty":"mozilla::_ipdltest::PTestBasicParent::Message","sym":"T_mozilla::_ipdltest::PTestBasicParent::Message","context":"mozilla::_ipdltest::PTestBasicParent::OnMessageReceived","contextsym":"_ZN7mozilla9_ipdltest16PTestBasicParent17OnMessageReceivedERKN3IPC7MessageE"}
{"loc":"00095:5-21","target":1,"kind":"use","pretty":"mozilla::_ipdltest::PTestBasicParent","sym":"T_mozilla::_ipdltest::PTestBasicParent","context":"mozilla::_ipdltest::PTestBasicParent::OnMessageReceived","contextsym":"_ZN7mozilla9_ipdltest16PTestBasicParent17OnMessageReceivedERKN3IPC7MessageE"}
{"loc":"00095:66-82","target":1,"kind":"use","pretty":"mozilla::_ipdltest::PTestBasicParent","sym":"T_mozilla::_ipdltest::PTestBasicParent","context":"mozilla::_ipdltest::PTestBasicParent::OnMessageReceived","contextsym":"_ZN7mozilla9_ipdltest16PTestBasicParent17OnMessageReceivedERKN3IPC7MessageE"}
{"loc":"00095:84-90","target":1,"kind":"use","pretty":"mozilla::ipc::HasResultCodes::Result","sym":"T_mozilla::ipc::HasResultCodes::Result","context":"mozilla::_ipdltest::PTestBasicParent::OnMessageReceived","contextsym":"_ZN7mozilla9_ipdltest16PTestBasicParent17OnMessageReceivedERKN3IPC7MessageE"}
{"loc":"00097:18-22","target":1,"kind":"use","pretty":"IPC::Message::type","sym":"_ZNK3IPC7Message4typeEv","context":"mozilla::_ipdltest::PTestBasicParent::OnMessageReceived","contextsym":"_ZN7mozilla9_ipdltest16PTestBasicParent17OnMessageReceivedERKN3IPC7MessageE"}
{"loc":"00099:15-26","target":1,"kind":"use","pretty":"mozilla::ipc::HasResultCodes::Result::MsgNotKnown","sym":"E_<T_mozilla::ipc::HasResultCodes::Result>_MsgNotKnown","context":"mozilla::_ipdltest::PTestBasicParent::OnMessageReceived","contextsym":"_ZN7mozilla9_ipdltest16PTestBasicParent17OnMessageReceivedERKN3IPC7MessageE"}
{"loc":"00100:9-35","target":1,"kind":"use","pretty":"SHMEM_CREATED_MESSAGE_TYPE","sym":"E_<T_b078e220e89184de>_SHMEM_CREATED_MESSAGE_TYPE","context":"mozilla::_ipdltest::PTestBasicParent::OnMessageReceived","contextsym":"_ZN7mozilla9_ipdltest16PTestBasicParent17OnMessageReceivedERKN3IPC7MessageE"}
{"loc":"00102:12-22","target":1,"kind":"use","pretty":"mozilla::ipc::IProtocol::FatalError","sym":"_ZNK7mozilla3ipc9IProtocol10FatalErrorEPKc","context":"mozilla::_ipdltest::PTestBasicParent::OnMessageReceived","contextsym":"_ZN7mozilla9_ipdltest16PTestBasicParent17OnMessageReceivedERKN3IPC7MessageE"}
{"loc":"00103:19-30","target":1,"kind":"use","pretty":"mozilla::ipc::HasResultCodes::Result::MsgNotKnown","sym":"E_<T_mozilla::ipc::HasResultCodes::Result>_MsgNotKnown","context":"mozilla::_ipdltest::PTestBasicParent::OnMessageReceived","contextsym":"_ZN7mozilla9_ipdltest16PTestBasicParent17OnMessageReceivedERKN3IPC7MessageE"}
{"loc":"00105:9-37","target":1,"kind":"use","pretty":"SHMEM_DESTROYED_MESSAGE_TYPE","sym":"E_<T_b078e220e89184de>_SHMEM_DESTROYED_MESSAGE_TYPE","context":"mozilla::_ipdltest::PTestBasicParent::OnMessageReceived","contextsym":"_ZN7mozilla9_ipdltest16PTestBasicParent17OnMessageReceivedERKN3IPC7MessageE"}
{"loc":"00107:12-22","target":1,"kind":"use","pretty":"mozilla::ipc::IProtocol::FatalError","sym":"_ZNK7mozilla3ipc9IProtocol10FatalErrorEPKc","context":"mozilla::_ipdltest::PTestBasicParent::OnMessageReceived","contextsym":"_ZN7mozilla9_ipdltest16PTestBasicParent17OnMessageReceivedERKN3IPC7MessageE"}
{"loc":"00108:19-30","target":1,"kind":"use","pretty":"mozilla::ipc::HasResultCodes::Result::MsgNotKnown","sym":"E_<T_mozilla::ipc::HasResultCodes::Result>_MsgNotKnown","context":"mozilla::_ipdltest::PTestBasicParent::OnMessageReceived","contextsym":"_ZN7mozilla9_ipdltest16PTestBasicParent17OnMessageReceivedERKN3IPC7MessageE"}
{"loc":"00113:23-40","target":1,"kind":"def","pretty":"mozilla::_ipdltest::PTestBasicParent::OnMessageReceived","sym":"_ZN7mozilla9_ipdltest16PTestBasicParent17OnMessageReceivedERKN3IPC7MessageERNS_9UniquePtrIS3_NS_13DefaultDeleteIS3_EEEE","peekRange":"113-115"}
{"loc":"00113:5-21","target":1,"kind":"use","pretty":"mozilla::_ipdltest::PTestBasicParent","sym":"T_mozilla::_ipdltest::PTestBasicParent","context":"mozilla::_ipdltest::PTestBasicParent::OnMessageReceived","contextsym":"_ZN7mozilla9_ipdltest16PTestBasicParent17OnMessageReceivedERKN3IPC7MessageERNS_9UniquePtrIS3_NS_13DefaultDeleteIS3_EEEE"}
{"loc":"00114:14-21","target":1,"kind":"use","pretty":"mozilla::_ipdltest::PTestBasicParent::Message","sym":"T_mozilla::_ipdltest::PTestBasicParent::Message","context":"mozilla::_ipdltest::PTestBasicParent::OnMessageReceived","contextsym":"_ZN7mozilla9_ipdltest16PTestBasicParent17OnMessageReceivedERKN3IPC7MessageERNS_9UniquePtrIS3_NS_13DefaultDeleteIS3_EEEE"}
{"loc":"00115:18-25","target":1,"kind":"use","pretty":"mozilla::_ipdltest::PTestBasicParent::Message","sym":"T_mozilla::_ipdltest::PTestBasicParent::Message","context":"mozilla::_ipdltest::PTestBasicParent::OnMessageReceived","contextsym":"_ZN7mozilla9_ipdltest16PTestBasicParent17OnMessageReceivedERKN3IPC7MessageERNS_9UniquePtrIS3_NS_13DefaultDeleteIS3_EEEE"}
{"loc":"00115:40-56","target":1,"kind":"use","pretty":"mozilla::_ipdltest::PTestBasicParent","sym":"T_mozilla::_ipdltest::PTestBasicParent","context":"mozilla::_ipdltest::PTestBasicParent::OnMessageReceived","contextsym":"_ZN7mozilla9_ipdltest16PTestBasicParent17OnMessageReceivedERKN3IPC7MessageERNS_9UniquePtrIS3_NS_13DefaultDeleteIS3_EEEE"}
{"loc":"00115:58-64","target":1,"kind":"use","pretty":"mozilla::ipc::HasResultCodes::Result","sym":"T_mozilla::ipc::HasResultCodes::Result","context":"mozilla::_ipdltest::PTestBasicParent::OnMessageReceived","contextsym":"_ZN7mozilla9_ipdltest16PTestBasicParent17OnMessageReceivedERKN3IPC7MessageERNS_9UniquePtrIS3_NS_13DefaultDeleteIS3_EEEE"}
{"loc":"00115:8-17","target":1,"kind":"use","pretty":"mozilla::_ipdltest::PTestBasicParent::UniquePtr","sym":"T_mozilla::_ipdltest::PTestBasicParent::UniquePtr","context":"mozilla::_ipdltest::PTestBasicParent::OnMessageReceived","contextsym":"_ZN7mozilla9_ipdltest16PTestBasicParent17OnMessageReceivedERKN3IPC7MessageERNS_9UniquePtrIS3_NS_13DefaultDeleteIS3_EEEE"}
{"loc":"00117:4-26","target":1,"kind":"use","pretty":"MOZ_ASSERT_UNREACHABLE","sym":"M_0d9562839add515c"}
{"loc":"00118:11-22","target":1,"kind":"use","pretty":"mozilla::ipc::HasResultCodes::Result::MsgNotKnown","sym":"E_<T_mozilla::ipc::HasResultCodes::Result>_MsgNotKnown","context":"mozilla::_ipdltest::PTestBasicParent::OnMessageReceived","contextsym":"_ZN7mozilla9_ipdltest16PTestBasicParent17OnMessageReceivedERKN3IPC7MessageERNS_9UniquePtrIS3_NS_13DefaultDeleteIS3_EEEE"}
{"loc":"00121:23-37","target":1,"kind":"def","pretty":"mozilla::_ipdltest::PTestBasicParent::OnCallReceived","sym":"_ZN7mozilla9_ipdltest16PTestBasicParent14OnCallReceivedERKN3IPC7MessageERNS_9UniquePtrIS3_NS_13DefaultDeleteIS3_EEEE","peekRange":"121-123"}
{"loc":"00121:5-21","target":1,"kind":"use","pretty":"mozilla::_ipdltest::PTestBasicParent","sym":"T_mozilla::_ipdltest::PTestBasicParent","context":"mozilla::_ipdltest::PTestBasicParent::OnCallReceived","contextsym":"_ZN7mozilla9_ipdltest16PTestBasicParent14OnCallReceivedERKN3IPC7MessageERNS_9UniquePtrIS3_NS_13DefaultDeleteIS3_EEEE"}
{"loc":"00122:14-21","target":1,"kind":"use","pretty":"mozilla::_ipdltest::PTestBasicParent::Message","sym":"T_mozilla::_ipdltest::PTestBasicParent::Message","context":"mozilla::_ipdltest::PTestBasicParent::OnCallReceived","contextsym":"_ZN7mozilla9_ipdltest16PTestBasicParent14OnCallReceivedERKN3IPC7MessageERNS_9UniquePtrIS3_NS_13DefaultDeleteIS3_EEEE"}
{"loc":"00123:18-25","target":1,"kind":"use","pretty":"mozilla::_ipdltest::PTestBasicParent::Message","sym":"T_mozilla::_ipdltest::PTestBasicParent::Message","context":"mozilla::_ipdltest::PTestBasicParent::OnCallReceived","contextsym":"_ZN7mozilla9_ipdltest16PTestBasicParent14OnCallReceivedERKN3IPC7MessageERNS_9UniquePtrIS3_NS_13DefaultDeleteIS3_EEEE"}
{"loc":"00123:40-56","target":1,"kind":"use","pretty":"mozilla::_ipdltest::PTestBasicParent","sym":"T_mozilla::_ipdltest::PTestBasicParent","context":"mozilla::_ipdltest::PTestBasicParent::OnCallReceived","contextsym":"_ZN7mozilla9_ipdltest16PTestBasicParent14OnCallReceivedERKN3IPC7MessageERNS_9UniquePtrIS3_NS_13DefaultDeleteIS3_EEEE"}
{"loc":"00123:58-64","target":1,"kind":"use","pretty":"mozilla::ipc::HasResultCodes::Result","sym":"T_mozilla::ipc::HasResultCodes::Result","context":"mozilla::_ipdltest::PTestBasicParent::OnCallReceived","contextsym":"_ZN7mozilla9_ipdltest16PTestBasicParent14OnCallReceivedERKN3IPC7MessageERNS_9UniquePtrIS3_NS_13DefaultDeleteIS3_EEEE"}
{"loc":"00123:8-17","target":1,"kind":"use","pretty":"mozilla::_ipdltest::PTestBasicParent::UniquePtr","sym":"T_mozilla::_ipdltest::PTestBasicParent::UniquePtr","context":"mozilla::_ipdltest::PTestBasicParent::OnCallReceived","contextsym":"_ZN7mozilla9_ipdltest16PTestBasicParent14OnCallReceivedERKN3IPC7MessageERNS_9UniquePtrIS3_NS_13DefaultDeleteIS3_EEEE"}
{"loc":"00125:4-26","target":1,"kind":"use","pretty":"MOZ_ASSERT_UNREACHABLE","sym":"M_0d9562839add515c"}
{"loc":"00126:11-22","target":1,"kind":"use","pretty":"mozilla::ipc::HasResultCodes::Result::MsgNotKnown","sym":"E_<T_mozilla::ipc::HasResultCodes::Result>_MsgNotKnown","context":"mozilla::_ipdltest::PTestBasicParent::OnCallReceived","contextsym":"_ZN7mozilla9_ipdltest16PTestBasicParent14OnCallReceivedERKN3IPC7MessageERNS_9UniquePtrIS3_NS_13DefaultDeleteIS3_EEEE"}
{"loc":"00129:23-37","target":1,"kind":"def","pretty":"mozilla::_ipdltest::PTestBasicParent::OnChannelClose","sym":"_ZN7mozilla9_ipdltest16PTestBasicParent14OnChannelCloseEv","peekRange":"129-129"}
{"loc":"00129:5-21","target":1,"kind":"use","pretty":"mozilla::_ipdltest::PTestBasicParent","sym":"T_mozilla::_ipdltest::PTestBasicParent","context":"mozilla::_ipdltest::PTestBasicParent::OnChannelClose","contextsym":"_ZN7mozilla9_ipdltest16PTestBasicParent14OnChannelCloseEv"}
{"loc":"00131:19-33","target":1,"kind":"use","pretty":"mozilla::ipc::IProtocol::ActorDestroyReason::NormalShutdown","sym":"E_<T_mozilla::ipc::IProtocol::ActorDestroyReason>_NormalShutdown","context":"mozilla::_ipdltest::PTestBasicParent::OnChannelClose","contextsym":"_ZN7mozilla9_ipdltest16PTestBasicParent14OnChannelCloseEv"}
{"loc":"00131:4-18","target":1,"kind":"use","pretty":"mozilla::ipc::IProtocol::DestroySubtree","sym":"_ZN7mozilla3ipc9IProtocol14DestroySubtreeENS1_18ActorDestroyReasonE","context":"mozilla::_ipdltest::PTestBasicParent::OnChannelClose","contextsym":"_ZN7mozilla9_ipdltest16PTestBasicParent14OnChannelCloseEv"}
{"loc":"00132:4-16","target":1,"kind":"use","pretty":"mozilla::_ipdltest::PTestBasicParent::ClearSubtree","sym":"_ZN7mozilla9_ipdltest16PTestBasicParent12ClearSubtreeEv","context":"mozilla::_ipdltest::PTestBasicParent::OnChannelClose","contextsym":"_ZN7mozilla9_ipdltest16PTestBasicParent14OnChannelCloseEv"}
{"loc":"00133:4-17","target":1,"kind":"use","pretty":"mozilla::ipc::IToplevelProtocol::DeallocShmems","sym":"_ZN7mozilla3ipc17IToplevelProtocol13DeallocShmemsEv","context":"mozilla::_ipdltest::PTestBasicParent::OnChannelClose","contextsym":"_ZN7mozilla9_ipdltest16PTestBasicParent14OnChannelCloseEv"}
{"loc":"00134:8-25","target":1,"kind":"use","pretty":"mozilla::ipc::IProtocol::GetLifecycleProxy","sym":"_ZN7mozilla3ipc9IProtocol17GetLifecycleProxyEv","context":"mozilla::_ipdltest::PTestBasicParent::OnChannelClose","contextsym":"_ZN7mozilla9_ipdltest16PTestBasicParent14OnChannelCloseEv"}
{"loc":"00135:29-36","target":1,"kind":"use","pretty":"mozilla::ipc::ActorLifecycleProxy::Release","sym":"_ZN7mozilla3ipc19ActorLifecycleProxy7ReleaseEv","context":"mozilla::_ipdltest::PTestBasicParent::OnChannelClose","contextsym":"_ZN7mozilla9_ipdltest16PTestBasicParent14OnChannelCloseEv"}
{"loc":"00135:8-25","target":1,"kind":"use","pretty":"mozilla::ipc::IProtocol::GetLifecycleProxy","sym":"_ZN7mozilla3ipc9IProtocol17GetLifecycleProxyEv","context":"mozilla::_ipdltest::PTestBasicParent::OnChannelClose","contextsym":"_ZN7mozilla9_ipdltest16PTestBasicParent14OnChannelCloseEv"}
{"loc":"00139:23-37","target":1,"kind":"def","pretty":"mozilla::_ipdltest::PTestBasicParent::OnChannelError","sym":"_ZN7mozilla9_ipdltest16PTestBasicParent14OnChannelErrorEv","peekRange":"139-139"}
{"loc":"00139:5-21","target":1,"kind":"use","pretty":"mozilla::_ipdltest::PTestBasicParent","sym":"T_mozilla::_ipdltest::PTestBasicParent","context":"mozilla::_ipdltest::PTestBasicParent::OnChannelError","contextsym":"_ZN7mozilla9_ipdltest16PTestBasicParent14OnChannelErrorEv"}
{"loc":"00141:19-35","target":1,"kind":"use","pretty":"mozilla::ipc::IProtocol::ActorDestroyReason::AbnormalShutdown","sym":"E_<T_mozilla::ipc::IProtocol::ActorDestroyReason>_AbnormalShutdown","context":"mozilla::_ipdltest::PTestBasicParent::OnChannelError","contextsym":"_ZN7mozilla9_ipdltest16PTestBasicParent14OnChannelErrorEv"}
{"loc":"00141:4-18","target":1,"kind":"use","pretty":"mozilla::ipc::IProtocol::DestroySubtree","sym":"_ZN7mozilla3ipc9IProtocol14DestroySubtreeENS1_18ActorDestroyReasonE","context":"mozilla::_ipdltest::PTestBasicParent::OnChannelError","contextsym":"_ZN7mozilla9_ipdltest16PTestBasicParent14OnChannelErrorEv"}
{"loc":"00142:4-16","target":1,"kind":"use","pretty":"mozilla::_ipdltest::PTestBasicParent::ClearSubtree","sym":"_ZN7mozilla9_ipdltest16PTestBasicParent12ClearSubtreeEv","context":"mozilla::_ipdltest::PTestBasicParent::OnChannelError","contextsym":"_ZN7mozilla9_ipdltest16PTestBasicParent14OnChannelErrorEv"}
{"loc":"00143:4-17","target":1,"kind":"use","pretty":"mozilla::ipc::IToplevelProtocol::DeallocShmems","sym":"_ZN7mozilla3ipc17IToplevelProtocol13DeallocShmemsEv","context":"mozilla::_ipdltest::PTestBasicParent::OnChannelError","contextsym":"_ZN7mozilla9_ipdltest16PTestBasicParent14OnChannelErrorEv"}
{"loc":"00144:8-25","target":1,"kind":"use","pretty":"mozilla::ipc::IProtocol::GetLifecycleProxy","sym":"_ZN7mozilla3ipc9IProtocol17GetLifecycleProxyEv","context":"mozilla::_ipdltest::PTestBasicParent::OnChannelError","contextsym":"_ZN7mozilla9_ipdltest16PTestBasicParent14OnChannelErrorEv"}
{"loc":"00145:29-36","target":1,"kind":"use","pretty":"mozilla::ipc::ActorLifecycleProxy::Release","sym":"_ZN7mozilla3ipc19ActorLifecycleProxy7ReleaseEv","context":"mozilla::_ipdltest::PTestBasicParent::OnChannelError","contextsym":"_ZN7mozilla9_ipdltest16PTestBasicParent14OnChannelErrorEv"}
{"loc":"00145:8-25","target":1,"kind":"use","pretty":"mozilla::ipc::IProtocol::GetLifecycleProxy","sym":"_ZN7mozilla3ipc9IProtocol17GetLifecycleProxyEv","context":"mozilla::_ipdltest::PTestBasicParent::OnChannelError","contextsym":"_ZN7mozilla9_ipdltest16PTestBasicParent14OnChannelErrorEv"}
{"loc":"00149:23-35","target":1,"kind":"def","pretty":"mozilla::_ipdltest::PTestBasicParent::ClearSubtree","sym":"_ZN7mozilla9_ipdltest16PTestBasicParent12ClearSubtreeEv","peekRange":"149-149"}
{"loc":"00149:5-21","target":1,"kind":"use","pretty":"mozilla::_ipdltest::PTestBasicParent","sym":"T_mozilla::_ipdltest::PTestBasicParent","context":"mozilla::_ipdltest::PTestBasicParent::ClearSubtree","contextsym":"_ZN7mozilla9_ipdltest16PTestBasicParent12ClearSubtreeEv"}
{"loc":"00157:10-13","target":1,"kind":"def","pretty":"IPC","sym":"NS_IPC"}
{"loc":"00158:37-53","target":1,"kind":"use","pretty":"mozilla::_ipdltest::PTestBasicParent","sym":"T_mozilla::_ipdltest::PTestBasicParent","context":"IPC::ParamTraits<mozilla::_ipdltest::PTestBasicParent *>::Write","contextsym":"_ZN3IPC11ParamTraitsIPN7mozilla9_ipdltest16PTestBasicParentEE5WriteEPNS_13MessageWriterERKS4_"}
{"loc":"00158:5-16","target":1,"kind":"use","pretty":"IPC::ParamTraits","sym":"T_IPC::ParamTraits","context":"IPC::ParamTraits<mozilla::_ipdltest::PTestBasicParent *>::Write","contextsym":"_ZN3IPC11ParamTraitsIPN7mozilla9_ipdltest16PTestBasicParentEE5WriteEPNS_13MessageWriterERKS4_"}
{"loc":"00158:57-62","target":1,"kind":"def","pretty":"IPC::ParamTraits<mozilla::_ipdltest::PTestBasicParent *>::Write","sym":"_ZN3IPC11ParamTraitsIPN7mozilla9_ipdltest16PTestBasicParentEE5WriteEPNS_13MessageWriterERKS4_","peekRange":"158-160"}
{"loc":"00159:13-26","target":1,"kind":"use","pretty":"IPC::MessageWriter","sym":"T_IPC::MessageWriter","context":"IPC::ParamTraits<mozilla::_ipdltest::PTestBasicParent *>::Write","contextsym":"_ZN3IPC11ParamTraitsIPN7mozilla9_ipdltest16PTestBasicParentEE5WriteEPNS_13MessageWriterERKS4_"}
{"loc":"00160:14-23","target":1,"kind":"use","pretty":"IPC::ParamTraits<mozilla::_ipdltest::PTestBasicParent *>::paramType","sym":"T_IPC::ParamTraits<mozilla::_ipdltest::PTestBasicParent_*>::paramType","context":"IPC::ParamTraits<mozilla::_ipdltest::PTestBasicParent *>::Write","contextsym":"_ZN3IPC11ParamTraitsIPN7mozilla9_ipdltest16PTestBasicParentEE5WriteEPNS_13MessageWriterERKS4_"}
{"loc":"00162:4-22","target":1,"kind":"use","pretty":"MOZ_RELEASE_ASSERT","sym":"M_96f821839add515c"}
{"loc":"00163:17-25","target":1,"kind":"use","pretty":"IPC::MessageWriter::GetActor","sym":"_ZNK3IPC13MessageWriter8GetActorEv","context":"IPC::ParamTraits<mozilla::_ipdltest::PTestBasicParent *>::Write","contextsym":"_ZN3IPC11ParamTraitsIPN7mozilla9_ipdltest16PTestBasicParentEE5WriteEPNS_13MessageWriterERKS4_"}
{"loc":"00166:4-11","target":1,"kind":"use","pretty":"int32_t","sym":"T_int32_t","context":"IPC::ParamTraits<mozilla::_ipdltest::PTestBasicParent *>::Write","contextsym":"_ZN3IPC11ParamTraitsIPN7mozilla9_ipdltest16PTestBasicParentEE5WriteEPNS_13MessageWriterERKS4_"}
{"loc":"00170:19-21","target":1,"kind":"use","pretty":"mozilla::ipc::IProtocol::Id","sym":"_ZNK7mozilla3ipc9IProtocol2IdEv","context":"IPC::ParamTraits<mozilla::_ipdltest::PTestBasicParent *>::Write","contextsym":"_ZN3IPC11ParamTraitsIPN7mozilla9_ipdltest16PTestBasicParentEE5WriteEPNS_13MessageWriterERKS4_"}
{"loc":"00172:18-28","target":1,"kind":"use","pretty":"mozilla::ipc::IProtocol::FatalError","sym":"_ZNK7mozilla3ipc9IProtocol10FatalErrorEPKc","context":"IPC::ParamTraits<mozilla::_ipdltest::PTestBasicParent *>::Write","contextsym":"_ZN3IPC11ParamTraitsIPN7mozilla9_ipdltest16PTestBasicParentEE5WriteEPNS_13MessageWriterERKS4_"}
{"loc":"00174:8-26","target":1,"kind":"use","pretty":"MOZ_RELEASE_ASSERT","sym":"M_96f821839add515c"}
{"loc":"00175:21-29","target":1,"kind":"use","pretty":"IPC::MessageWriter::GetActor","sym":"_ZNK3IPC13MessageWriter8GetActorEv","context":"IPC::ParamTraits<mozilla::_ipdltest::PTestBasicParent *>::Write","contextsym":"_ZN3IPC11ParamTraitsIPN7mozilla9_ipdltest16PTestBasicParentEE5WriteEPNS_13MessageWriterERKS4_"}
{"loc":"00175:33-46","target":1,"kind":"use","pretty":"mozilla::ipc::IProtocol::GetIPCChannel","sym":"_ZN7mozilla3ipc9IProtocol13GetIPCChannelEv","context":"IPC::ParamTraits<mozilla::_ipdltest::PTestBasicParent *>::Write","contextsym":"_ZN3IPC11ParamTraitsIPN7mozilla9_ipdltest16PTestBasicParentEE5WriteEPNS_13MessageWriterERKS4_"}
{"loc":"00175:58-71","target":1,"kind":"use","pretty":"mozilla::ipc::IToplevelProtocol::GetIPCChannel","sym":"_ZN7mozilla3ipc17IToplevelProtocol13GetIPCChannelEv","context":"IPC::ParamTraits<mozilla::_ipdltest::PTestBasicParent *>::Write","contextsym":"_ZN3IPC11ParamTraitsIPN7mozilla9_ipdltest16PTestBasicParentEE5WriteEPNS_13MessageWriterERKS4_"}
{"loc":"00178:8-26","target":1,"kind":"use","pretty":"MOZ_RELEASE_ASSERT","sym":"M_96f821839add515c"}
{"loc":"00179:18-25","target":1,"kind":"use","pretty":"mozilla::ipc::IProtocol::CanSend","sym":"_ZNK7mozilla3ipc9IProtocol7CanSendEv","context":"IPC::ParamTraits<mozilla::_ipdltest::PTestBasicParent *>::Write","contextsym":"_ZN3IPC11ParamTraitsIPN7mozilla9_ipdltest16PTestBasicParentEE5WriteEPNS_13MessageWriterERKS4_"}
{"loc":"00183:9-19","target":1,"kind":"use","pretty":"IPC::WriteParam","sym":"_ZN3IPCL10WriteParamEPNS_13MessageWriterEOT_","context":"IPC::ParamTraits<mozilla::_ipdltest::PTestBasicParent *>::Write","contextsym":"_ZN3IPC11ParamTraitsIPN7mozilla9_ipdltest16PTestBasicParentEE5WriteEPNS_13MessageWriterERKS4_"}
{"loc":"00186:37-53","target":1,"kind":"use","pretty":"mozilla::_ipdltest::PTestBasicParent","sym":"T_mozilla::_ipdltest::PTestBasicParent","context":"IPC::ParamTraits<mozilla::_ipdltest::PTestBasicParent *>::Read","contextsym":"_ZN3IPC11ParamTraitsIPN7mozilla9_ipdltest16PTestBasicParentEE4ReadEPNS_13MessageReaderEPS4_"}
{"loc":"00186:5-16","target":1,"kind":"use","pretty":"IPC::ParamTraits","sym":"T_IPC::ParamTraits","context":"IPC::ParamTraits<mozilla::_ipdltest::PTestBasicParent *>::Read","contextsym":"_ZN3IPC11ParamTraitsIPN7mozilla9_ipdltest16PTestBasicParentEE4ReadEPNS_13MessageReaderEPS4_"}
{"loc":"00186:57-61","target":1,"kind":"def","pretty":"IPC::ParamTraits<mozilla::_ipdltest::PTestBasicParent *>::Read","sym":"_ZN3IPC11ParamTraitsIPN7mozilla9_ipdltest16PTestBasicParentEE4ReadEPNS_13MessageReaderEPS4_","peekRange":"186-188"}
{"loc":"00187:13-26","target":1,"kind":"use","pretty":"IPC::MessageReader","sym":"T_IPC::MessageReader","context":"IPC::ParamTraits<mozilla::_ipdltest::PTestBasicParent *>::Read","contextsym":"_ZN3IPC11ParamTraitsIPN7mozilla9_ipdltest16PTestBasicParentEE4ReadEPNS_13MessageReaderEPS4_"}
{"loc":"00188:8-17","target":1,"kind":"use","pretty":"IPC::ParamTraits<mozilla::_ipdltest::PTestBasicParent *>::paramType","sym":"T_IPC::ParamTraits<mozilla::_ipdltest::PTestBasicParent_*>::paramType","context":"IPC::ParamTraits<mozilla::_ipdltest::PTestBasicParent *>::Read","contextsym":"_ZN3IPC11ParamTraitsIPN7mozilla9_ipdltest16PTestBasicParentEE4ReadEPNS_13MessageReaderEPS4_"}
{"loc":"00190:4-22","target":1,"kind":"use","pretty":"MOZ_RELEASE_ASSERT","sym":"M_96f821839add515c"}
{"loc":"00191:17-25","target":1,"kind":"use","pretty":"IPC::MessageReader::GetActor","sym":"_ZNK3IPC13MessageReader8GetActorEv","context":"IPC::ParamTraits<mozilla::_ipdltest::PTestBasicParent *>::Read","contextsym":"_ZN3IPC11ParamTraitsIPN7mozilla9_ipdltest16PTestBasicParentEE4ReadEPNS_13MessageReaderEPS4_"}
{"loc":"00194:13-18","target":1,"kind":"use","pretty":"mozilla::Maybe","sym":"T_mozilla::Maybe","context":"IPC::ParamTraits<mozilla::_ipdltest::PTestBasicParent *>::Read","contextsym":"_ZN3IPC11ParamTraitsIPN7mozilla9_ipdltest16PTestBasicParentEE4ReadEPNS_13MessageReaderEPS4_"}
{"loc":"00194:33-42","target":1,"kind":"use","pretty":"mozilla::ipc::IProtocol","sym":"T_mozilla::ipc::IProtocol","context":"IPC::ParamTraits<mozilla::_ipdltest::PTestBasicParent *>::Read","contextsym":"_ZN3IPC11ParamTraitsIPN7mozilla9_ipdltest16PTestBasicParentEE4ReadEPNS_13MessageReaderEPS4_"}
{"loc":"00195:17-25","target":1,"kind":"use","pretty":"IPC::MessageReader::GetActor","sym":"_ZNK3IPC13MessageReader8GetActorEv","context":"IPC::ParamTraits<mozilla::_ipdltest::PTestBasicParent *>::Read","contextsym":"_ZN3IPC11ParamTraitsIPN7mozilla9_ipdltest16PTestBasicParentEE4ReadEPNS_13MessageReaderEPS4_"}
{"loc":"00195:29-38","target":1,"kind":"use","pretty":"mozilla::ipc::IProtocol::ReadActor","sym":"_ZN7mozilla3ipc9IProtocol9ReadActorEPN3IPC13MessageReaderEbPKci","context":"IPC::ParamTraits<mozilla::_ipdltest::PTestBasicParent *>::Read","contextsym":"_ZN3IPC11ParamTraitsIPN7mozilla9_ipdltest16PTestBasicParentEE4ReadEPNS_13MessageReaderEPS4_"}
{"loc":"00195:68-86","target":1,"kind":"use","pretty":"IPCMessageStart::PTestBasicMsgStart","sym":"E_<T_IPCMessageStart>_PTestBasicMsgStart","context":"IPC::ParamTraits<mozilla::_ipdltest::PTestBasicParent *>::Read","contextsym":"_ZN3IPC11ParamTraitsIPN7mozilla9_ipdltest16PTestBasicParentEE4ReadEPNS_13MessageReaderEPS4_"}
{"loc":"00196:14-23","target":1,"kind":"use","pretty":"mozilla::Maybe::isNothing","sym":"_ZNK7mozilla5Maybe9isNothingEv","context":"IPC::ParamTraits<mozilla::_ipdltest::PTestBasicParent *>::Read","contextsym":"_ZN3IPC11ParamTraitsIPN7mozilla9_ipdltest16PTestBasicParentEE4ReadEPNS_13MessageReaderEPS4_"}
{"loc":"00200:44-60","target":1,"kind":"use","pretty":"mozilla::_ipdltest::PTestBasicParent","sym":"T_mozilla::_ipdltest::PTestBasicParent","context":"IPC::ParamTraits<mozilla::_ipdltest::PTestBasicParent *>::Read","contextsym":"_ZN3IPC11ParamTraitsIPN7mozilla9_ipdltest16PTestBasicParentEE4ReadEPNS_13MessageReaderEPS4_"}
{"loc":"00200:69-74","target":1,"kind":"use","pretty":"mozilla::Maybe::value","sym":"_ZNKR7mozilla5Maybe5valueEv","context":"IPC::ParamTraits<mozilla::_ipdltest::PTestBasicParent *>::Read","contextsym":"_ZN3IPC11ParamTraitsIPN7mozilla9_ipdltest16PTestBasicParentEE4ReadEPNS_13MessageReaderEPS4_"}
{"loc":"00001:0","target":1,"kind":"def","pretty":"__GENERATED__/ipc/ipdl/PTestBasicParent.cpp","sym":"FILE_linux@__GENERATED__/ipc/ipdl/PTestBasicParent@2Ecpp"}
{"loc":"00007:9-47","target":1,"kind":"use","pretty":"__GENERATED__/ipc/ipdl/_ipdlheaders/mozilla/_ipdltest/PTestBasicParent.h","sym":"FILE_linux@__GENERATED__/ipc/ipdl/_ipdlheaders/mozilla/_ipdltest/PTestBasicParent@2Eh"}
{"loc":"00014:9-20","target":1,"kind":"use","pretty":"__GENERATED__/dist/include/nsIFile.h","sym":"FILE_linux@__GENERATED__/dist/include/nsIFile@2Eh"}
{"loc":"00075:37-41","target":1,"kind":"use","pretty":"std::move","sym":"_ZSt4moveOT_","context":"mozilla::_ipdltest::PTestBasicParent::SendHello","contextsym":"_ZN7mozilla9_ipdltest16PTestBasicParent9SendHelloEv"}
{"loc":"00001:0","target":1,"kind":"def","pretty":"__GENERATED__/ipc/ipdl/PTestBasicParent.cpp","sym":"FILE_macosx@__GENERATED__/ipc/ipdl/PTestBasicParent@2Ecpp"}
{"loc":"00007:9-47","target":1,"kind":"use","pretty":"__GENERATED__/ipc/ipdl/_ipdlheaders/mozilla/_ipdltest/PTestBasicParent.h","sym":"FILE_macosx@__GENERATED__/ipc/ipdl/_ipdlheaders/mozilla/_ipdltest/PTestBasicParent@2Eh"}
{"loc":"00014:9-20","target":1,"kind":"use","pretty":"__GENERATED__/dist/include/nsIFile.h","sym":"FILE_macosx@__GENERATED__/dist/include/nsIFile@2Eh"}
{"loc":"00075:37-41","target":1,"kind":"use","pretty":"std::move","sym":"_ZNSt3__14moveEOT_","context":"mozilla::_ipdltest::PTestBasicParent::SendHello","contextsym":"_ZN7mozilla9_ipdltest16PTestBasicParent9SendHelloEv"}
{"loc":"00001:0","target":1,"kind":"def","pretty":"__GENERATED__/ipc/ipdl/PTestBasicParent.cpp","sym":"FILE_windows@__GENERATED__/ipc/ipdl/PTestBasicParent@2Ecpp"}
{"loc":"00007:9-47","target":1,"kind":"use","pretty":"__GENERATED__/ipc/ipdl/_ipdlheaders/mozilla/_ipdltest/PTestBasicParent.h","sym":"FILE_windows@__GENERATED__/ipc/ipdl/_ipdlheaders/mozilla/_ipdltest/PTestBasicParent@2Eh"}
{"loc":"00014:9-20","target":1,"kind":"use","pretty":"__GENERATED__/dist/include/nsIFile.h","sym":"FILE_windows@__GENERATED__/dist/include/nsIFile@2Eh"}
{"loc":"00066:22-43","target":1,"kind":"use","pretty":"mozilla::ipc::LogMessageForProtocol","sym":"_ZN7mozilla3ipc21LogMessageForProtocolEPKcmS2_jNS0_16MessageDirectionE","context":"mozilla::_ipdltest::PTestBasicParent::SendHello","contextsym":"_ZN7mozilla9_ipdltest16PTestBasicParent9SendHelloEv"}
{"loc":"00001:0","source":1,"syntax":"def,file","pretty":"file __GENERATED__/ipc/ipdl/PTestBasicParent.cpp","sym":"FILE_windows@__GENERATED__/ipc/ipdl/PTestBasicParent@2Ecpp,FILE_macosx@__GENERATED__/ipc/ipdl/PTestBasicParent@2Ecpp,FILE_linux@__GENERATED__/ipc/ipdl/PTestBasicParent@2Ecpp,FILE_android-armv7@__GENERATED__/ipc/ipdl/PTestBasicParent@2Ecpp"}
{"loc":"00007:9-47","source":1,"syntax":"file,use","pretty":"file __GENERATED__/ipc/ipdl/_ipdlheaders/mozilla/_ipdltest/PTestBasicParent.h","sym":"FILE_windows@__GENERATED__/ipc/ipdl/_ipdlheaders/mozilla/_ipdltest/PTestBasicParent@2Eh,FILE_macosx@__GENERATED__/ipc/ipdl/_ipdlheaders/mozilla/_ipdltest/PTestBasicParent@2Eh,FILE_linux@__GENERATED__/ipc/ipdl/_ipdlheaders/mozilla/_ipdltest/PTestBasicParent@2Eh,FILE_android-armv7@__GENERATED__/ipc/ipdl/_ipdlheaders/mozilla/_ipdltest/PTestBasicParent@2Eh"}
{"loc":"00008:9-35","source":1,"syntax":"file,use","pretty":"file tools/profiler/public/ProfilerLabels.h","sym":"FILE_tools/profiler/public/ProfilerLabels@2Eh"}
{"loc":"00009:9-46","source":1,"syntax":"file,use","pretty":"file ipc/ipdl/test/gtest/TestBasicParent.h","sym":"FILE_ipc/ipdl/test/gtest/TestBasicParent@2Eh"}
{"loc":"00012:9-32","source":1,"syntax":"file,use","pretty":"file ipc/glue/IPCMessageUtils.h","sym":"FILE_ipc/glue/IPCMessageUtils@2Eh"}
{"loc":"00013:9-47","source":1,"syntax":"file,use","pretty":"file ipc/glue/IPCMessageUtilsSpecializations.h","sym":"FILE_ipc/glue/IPCMessageUtilsSpecializations@2Eh"}
{"loc":"00014:9-20","source":1,"syntax":"file,use","pretty":"file __GENERATED__/dist/include/nsIFile.h","sym":"FILE_windows@__GENERATED__/dist/include/nsIFile@2Eh,FILE_macosx@__GENERATED__/dist/include/nsIFile@2Eh,FILE_linux@__GENERATED__/dist/include/nsIFile@2Eh,FILE_android-armv7@__GENERATED__/dist/include/nsIFile@2Eh"}
{"loc":"00015:9-33","source":1,"syntax":"file,use","pretty":"file ipc/glue/Endpoint.h","sym":"FILE_ipc/glue/Endpoint@2Eh"}
{"loc":"00016:9-45","source":1,"syntax":"file,use","pretty":"file ipc/glue/ProtocolMessageUtils.h","sym":"FILE_ipc/glue/ProtocolMessageUtils@2Eh"}
{"loc":"00017:9-38","source":1,"syntax":"file,use","pretty":"file ipc/glue/ProtocolUtils.h","sym":"FILE_ipc/glue/ProtocolUtils@2Eh"}
{"loc":"00018:9-42","source":1,"syntax":"file,use","pretty":"file ipc/glue/ShmemMessageUtils.h","sym":"FILE_ipc/glue/ShmemMessageUtils@2Eh"}
{"loc":"00019:9-41","source":1,"syntax":"file,use","pretty":"file ipc/glue/TaintingIPCUtils.h","sym":"FILE_ipc/glue/TaintingIPCUtils@2Eh"}
{"loc":"00021:10-17","source":1,"syntax":"def,namespace","pretty":"namespace mozilla","sym":"NS_mozilla","nestingRange":"21:19-156:0"}
{"loc":"00022:10-19","source":1,"syntax":"def,namespace","pretty":"namespace mozilla::_ipdltest","sym":"NS_mozilla::_ipdltest","nestingRange":"22:21-155:0"}
{"loc":"00025:5-21","source":1,"syntax":"type,use","pretty":"type mozilla::_ipdltest::PTestBasicParent","sym":"T_mozilla::_ipdltest::PTestBasicParent","type":"class mozilla::_ipdltest::PTestBasicParent","typesym":"T_mozilla::_ipdltest::PTestBasicParent"}
{"loc":"00025:23-38","source":1,"syntax":"def,function","pretty":"function mozilla::_ipdltest::PTestBasicParent::ProcessingError","sym":"_ZN7mozilla9_ipdltest16PTestBasicParent15ProcessingErrorENS_3ipc14HasResultCodes6ResultEPKc","nestingRange":"28:0-29:0","type":"auto (enum mozilla::ipc::HasResultCodes::Result, const char *) -> void"}
{"loc":"00026:8-14","source":1,"syntax":"type,use","pretty":"type mozilla::ipc::HasResultCodes::Result","sym":"T_mozilla::ipc::HasResultCodes::Result","type":"enum mozilla::ipc::HasResultCodes::Result","typesym":"T_mozilla::ipc::HasResultCodes::Result"}
{"loc":"00026:15-20","source":1,"syntax":"","pretty":"variable aCode","sym":"V_272dfa605a27c74b_1ebe00f013,V_23bc09de47846456_1ebe00f013,V_7faff616f658d82b_1ebe00f013,V_2659a3d8d0861239_1ebe00f013","no_crossref":1,"type":"enum mozilla::ipc::HasResultCodes::Result","typesym":"T_mozilla::ipc::HasResultCodes::Result"}
{"loc":"00027:20-27","source":1,"syntax":"","pretty":"variable aReason","sym":"V_fee50b605a27c74b_ea97235f0b0d,V_fa7519de47846456_ea97235f0b0d,V_47780716f658d82b_ea97235f0b0d,V_fd12b3d8d0861239_ea97235f0b0d","no_crossref":1,"type":"const char *"}
{"loc":"00031:5-21","source":1,"syntax":"type,use","pretty":"type mozilla::_ipdltest::PTestBasicParent","sym":"T_mozilla::_ipdltest::PTestBasicParent","type":"class mozilla::_ipdltest::PTestBasicParent","typesym":"T_mozilla::_ipdltest::PTestBasicParent"}
{"loc":"00031:23-53","source":1,"syntax":"def,function","pretty":"function mozilla::_ipdltest::PTestBasicParent::ShouldContinueFromReplyTimeout","sym":"_ZN7mozilla9_ipdltest16PTestBasicParent30ShouldContinueFromReplyTimeoutEv","nestingRange":"32:0-34:0","type":"auto (void) -> _Bool"}
{"loc":"00036:0-12","source":1,"syntax":"macro,use","pretty":"macro MOZ_IMPLICIT","sym":"M_5f65895d5a5258f7"}
{"loc":"00036:13-29","source":1,"syntax":"type,use","pretty":"type mozilla::_ipdltest::PTestBasicParent","sym":"T_mozilla::_ipdltest::PTestBasicParent","type":"class mozilla::_ipdltest::PTestBasicParent","typesym":"T_mozilla::_ipdltest::PTestBasicParent"}
{"loc":"00036:31-47","source":1,"syntax":"def,function","pretty":"function mozilla::_ipdltest::PTestBasicParent::PTestBasicParent","sym":"_ZN7mozilla9_ipdltest16PTestBasicParentC1Ev","nestingRange":"38:0-40:0","type":"void (void)"}
{"loc":"00037:4-11","source":1,"syntax":"constructor,use","pretty":"constructor mozilla::ipc::IToplevelProtocol::IToplevelProtocol","sym":"_ZN7mozilla3ipc17IToplevelProtocolC1EPKc15IPCMessageStartNS0_4SideE"}
{"loc":"00037:18-35","source":1,"syntax":"type,use","pretty":"type mozilla::ipc::IToplevelProtocol","sym":"T_mozilla::ipc::IToplevelProtocol","type":"class mozilla::ipc::IToplevelProtocol","typesym":"T_mozilla::ipc::IToplevelProtocol"}
{"loc":"00037:56-74","source":1,"syntax":"enum,use","pretty":"enum IPCMessageStart::PTestBasicMsgStart","sym":"E_<T_IPCMessageStart>_PTestBasicMsgStart","type":"enum IPCMessageStart","typesym":"T_IPCMessageStart"}
{"loc":"00037:90-100","source":1,"syntax":"enum,use","pretty":"enum mozilla::ipc::Side::ParentSide","sym":"E_<T_mozilla::ipc::Side>_ParentSide","type":"enum mozilla::ipc::Side","typesym":"T_mozilla::ipc::Side"}
{"loc":"00039:4-18","source":1,"syntax":"macro,use","pretty":"macro MOZ_COUNT_CTOR","sym":"M_d9d58f5149d9f0f2"}
{"loc":"00039:19-35","source":1,"syntax":"type,use","pretty":"type mozilla::_ipdltest::PTestBasicParent","sym":"T_mozilla::_ipdltest::PTestBasicParent","type":"class mozilla::_ipdltest::PTestBasicParent","typesym":"T_mozilla::_ipdltest::PTestBasicParent"}
{"loc":"00042:0-16","source":1,"syntax":"type,use","pretty":"type mozilla::_ipdltest::PTestBasicParent","sym":"T_mozilla::_ipdltest::PTestBasicParent","type":"class mozilla::_ipdltest::PTestBasicParent","typesym":"T_mozilla::_ipdltest::PTestBasicParent"}
{"loc":"00042:19-35","source":1,"syntax":"def,destructor","pretty":"destructor mozilla::_ipdltest::PTestBasicParent::~PTestBasicParent","sym":"_ZN7mozilla9_ipdltest16PTestBasicParentD1Ev","nestingRange":"43:0-45:0","type":"void (void) noexcept"}
{"loc":"00042:19-35","source":1,"syntax":"type,use","pretty":"type mozilla::_ipdltest::PTestBasicParent","sym":"T_mozilla::_ipdltest::PTestBasicParent","type":"class mozilla::_ipdltest::PTestBasicParent","typesym":"T_mozilla::_ipdltest::PTestBasicParent"}
{"loc":"00044:4-18","source":1,"syntax":"macro,use","pretty":"macro MOZ_COUNT_DTOR","sym":"M_f9e8c16149d9f0f2"}
{"loc":"00044:19-35","source":1,"syntax":"type,use","pretty":"type mozilla::_ipdltest::PTestBasicParent","sym":"T_mozilla::_ipdltest::PTestBasicParent","type":"class mozilla::_ipdltest::PTestBasicParent","typesym":"T_mozilla::_ipdltest::PTestBasicParent"}
{"loc":"00047:5-21","source":1,"syntax":"type,use","pretty":"type mozilla::_ipdltest::PTestBasicParent","sym":"T_mozilla::_ipdltest::PTestBasicParent","type":"class mozilla::_ipdltest::PTestBasicParent","typesym":"T_mozilla::_ipdltest::PTestBasicParent"}
{"loc":"00047:23-39","source":1,"syntax":"def,function","pretty":"function mozilla::_ipdltest::PTestBasicParent::AllManagedActors","sym":"_ZNK7mozilla9_ipdltest16PTestBasicParent16AllManagedActorsER8nsTArrayI6RefPtrINS_3ipc19ActorLifecycleProxyEEE","nestingRange":"48:0-52:0","type":"auto (nsTArray<RefPtr<mozilla::ipc::ActorLifecycleProxy> > &) const -> void"}
{"loc":"00047:40-48","source":1,"syntax":"type,use","pretty":"type nsTArray","sym":"T_nsTArray"}
{"loc":"00047:49-55","source":1,"syntax":"type,use","pretty":"type RefPtr","sym":"T_RefPtr"}
{"loc":"00047:70-89","source":1,"syntax":"type,use","pretty":"type mozilla::ipc::ActorLifecycleProxy","sym":"T_mozilla::ipc::ActorLifecycleProxy","type":"class mozilla::ipc::ActorLifecycleProxy","typesym":"T_mozilla::ipc::ActorLifecycleProxy"}
{"loc":"00047:93-98","source":1,"syntax":"","pretty":"variable arr__","sym":"V_bd094d605a27c74b_8cdba1f013,V_b9985bde47846456_8cdba1f013,V_069b4916f658d82b_8cdba1f013,V_bc35f5d8d0861239_8cdba1f013","no_crossref":1,"type":"nsTArray<RefPtr<mozilla::ipc::ActorLifecycleProxy> > &"}
{"loc":"00049:4-12","source":1,"syntax":"type,use","pretty":"type uint32_t","sym":"T_uint32_t","type":"uint32_t"}
{"loc":"00049:13-18","source":1,"syntax":"","pretty":"variable total","sym":"V_598a5d605a27c74b_903f070113,V_551a6bde47846456_903f070113,V_a11d5916f658d82b_903f070113,V_58b606d8d0861239_903f070113","no_crossref":1,"type":"uint32_t"}
{"loc":"00050:4-9","source":1,"syntax":"","pretty":"variable arr__","sym":"V_bd094d605a27c74b_8cdba1f013,V_b9985bde47846456_8cdba1f013,V_069b4916f658d82b_8cdba1f013,V_bc35f5d8d0861239_8cdba1f013","no_crossref":1,"type":"nsTArray<RefPtr<mozilla::ipc::ActorLifecycleProxy> > &"}
{"loc":"00050:10-21","source":1,"syntax":"function,use","pretty":"function nsTArray_Impl::SetCapacity","sym":"_ZN13nsTArray_Impl11SetCapacityEN13nsTArray_baseIT0_N27nsTArray_RelocationStrategyIT_E4TypeEE9size_typeE","type":"typename struct nsTArrayInfallibleAllocator::ResultType"}
{"loc":"00050:22-27","source":1,"syntax":"","pretty":"variable total","sym":"V_598a5d605a27c74b_903f070113,V_551a6bde47846456_903f070113,V_a11d5916f658d82b_903f070113,V_58b606d8d0861239_903f070113","no_crossref":1,"type":"uint32_t"}
{"loc":"00054:5-21","source":1,"syntax":"type,use","pretty":"type mozilla::_ipdltest::PTestBasicParent","sym":"T_mozilla::_ipdltest::PTestBasicParent","type":"class mozilla::_ipdltest::PTestBasicParent","typesym":"T_mozilla::_ipdltest::PTestBasicParent"}
{"loc":"00054:23-32","source":1,"syntax":"def,function","pretty":"function mozilla::_ipdltest::PTestBasicParent::SendHello","sym":"_ZN7mozilla9_ipdltest16PTestBasicParent9SendHelloEv","nestingRange":"55:0-77:0","type":"auto (void) -> _Bool"}
{"loc":"00056:4-13","source":1,"syntax":"type,use","pretty":"type mozilla::_ipdltest::PTestBasicParent::UniquePtr","sym":"T_mozilla::_ipdltest::PTestBasicParent::UniquePtr"}
{"loc":"00056:19-26","source":1,"syntax":"type,use","pretty":"type IPC::Message","sym":"T_IPC::Message","type":"class IPC::Message","typesym":"T_IPC::Message"}
{"loc":"00056:28-33","source":1,"syntax":"","pretty":"variable msg__","sym":"V_91c16e605a27c74b_a6144ff013,V_9d417cde47846456_a6144ff013,V_e9446a16f658d82b_a6144ff013,V_90fd07d8d0861239_a6144ff013","no_crossref":1,"type":"UniquePtr<IPC::Message>","typesym":"T_mozilla::UniquePtr"}
{"loc":"00056:48-57","source":1,"syntax":"function,use","pretty":"function mozilla::_ipdltest::PTestBasic::Msg_Hello","sym":"_ZN7mozilla9_ipdltest10PTestBasic9Msg_HelloEi","type":"mozilla::UniquePtr<IPC::Message> (int32_t)"}
{"loc":"00056:58-77","source":1,"syntax":"enum,use","pretty":"enum SpecialRoutingIDs::MSG_ROUTING_CONTROL","sym":"E_<T_SpecialRoutingIDs>_MSG_ROUTING_CONTROL","type":"enum SpecialRoutingIDs","typesym":"T_SpecialRoutingIDs"}
{"loc":"00057:9-22","source":1,"syntax":"type,use","pretty":"type IPC::MessageWriter","sym":"T_IPC::MessageWriter","type":"class IPC::MessageWriter","typesym":"T_IPC::MessageWriter"}
{"loc":"00057:23-31","source":1,"syntax":"constructor,use","pretty":"constructor IPC::MessageWriter::MessageWriter","sym":"_ZN3IPC13MessageWriterC1ERNS_7MessageEPN7mozilla3ipc9IProtocolE"}
{"loc":"00057:23-31","source":1,"syntax":"","pretty":"variable writer__","sym":"V_578a6e605a27c74b_0e4661393b7ea1,V_531a7cde47846456_0e4661393b7ea1,V_af0d6a16f658d82b_0e4661393b7ea1,V_56b617d8d0861239_0e4661393b7ea1","no_crossref":1,"type":"IPC::MessageWriter","typesym":"T_IPC::MessageWriter"}
{"loc":"00058:13-14","source":1,"syntax":"function,use","pretty":"function mozilla::UniquePtr::operator*","sym":"_ZNK7mozilla9UniquePtrdeEv","type":"std::add_lvalue_reference_t<Message>"}
{"loc":"00058:15-20","source":1,"syntax":"","pretty":"variable msg__","sym":"V_91c16e605a27c74b_a6144ff013,V_9d417cde47846456_a6144ff013,V_e9446a16f658d82b_a6144ff013,V_90fd07d8d0861239_a6144ff013","no_crossref":1,"type":"UniquePtr<IPC::Message>","typesym":"T_mozilla::UniquePtr"}
{"loc":"00065:22-39","source":1,"syntax":"function,use","pretty":"function mozilla::ipc::LoggingEnabledFor","sym":"_ZN7mozilla3ipc17LoggingEnabledForEPKc","type":"_Bool (const char *)"}
{"loc":"00066:22-43","source":1,"syntax":"function,use","pretty":"function mozilla::ipc::LogMessageForProtocol","sym":"_ZN7mozilla3ipc21LogMessageForProtocolEPKcmS2_jNS0_16MessageDirectionE,_ZN7mozilla3ipc21LogMessageForProtocolEPKciS2_jNS0_16MessageDirectionE","type":"void (const char *, base::ProcessId, const char *, uint32_t, enum mozilla::ipc::MessageDirection)"}
{"loc":"00068:18-34","source":1,"syntax":"function,use","pretty":"function mozilla::ipc::IProtocol::ToplevelProtocol","sym":"_ZN7mozilla3ipc9IProtocol16ToplevelProtocolEv","type":"class mozilla::ipc::IToplevelProtocol *"}
{"loc":"00068:38-58","source":1,"syntax":"function,use","pretty":"function mozilla::ipc::IToplevelProtocol::OtherPidMaybeInvalid","sym":"_ZNK7mozilla3ipc17IToplevelProtocol20OtherPidMaybeInvalidEv","type":"base::ProcessId"}
{"loc":"00070:12-17","source":1,"syntax":"","pretty":"variable msg__","sym":"V_91c16e605a27c74b_a6144ff013,V_9d417cde47846456_a6144ff013,V_e9446a16f658d82b_a6144ff013,V_90fd07d8d0861239_a6144ff013","no_crossref":1,"type":"UniquePtr<IPC::Message>","typesym":"T_mozilla::UniquePtr"}
{"loc":"00070:17-19","source":1,"syntax":"function,use","pretty":"function mozilla::UniquePtr::operator->","sym":"_ZNK7mozilla9UniquePtrptEv","type":"mozilla::UniquePtr<class IPC::Message>::Pointer"}
{"loc":"00070:19-23","source":1,"syntax":"function,use","pretty":"function IPC::Message::type","sym":"_ZNK3IPC7Message4typeEv","type":"IPC::Message::msgid_t"}
{"loc":"00071:26-42","source":1,"syntax":"type,use","pretty":"type mozilla::ipc::MessageDirection","sym":"T_mozilla::ipc::MessageDirection","type":"enum mozilla::ipc::MessageDirection","typesym":"T_mozilla::ipc::MessageDirection"}
{"loc":"00071:44-52","source":1,"syntax":"enum,use","pretty":"enum mozilla::ipc::MessageDirection::eSending","sym":"E_<T_mozilla::ipc::MessageDirection>_eSending","type":"enum mozilla::ipc::MessageDirection","typesym":"T_mozilla::ipc::MessageDirection"}
{"loc":"00073:4-23","source":1,"syntax":"macro,use","pretty":"macro AUTO_PROFILER_LABEL","sym":"M_99e0335dc782f2d6"}
{"loc":"00073:4-23","source":1,"syntax":"","pretty":"variable raiiObject73","sym":"V_5e5b2_b669d3b76a0990ad","no_crossref":1,"type":"mozilla::AutoProfilerLabel","typesym":"T_mozilla::AutoProfilerLabel"}
{"loc":"00073:49-54","source":1,"syntax":"enum,use","pretty":"enum JS::ProfilingCategoryPair::OTHER","sym":"E_<T_JS::ProfilingCategoryPair>_OTHER","type":"enum JS::ProfilingCategoryPair","typesym":"T_JS::ProfilingCategoryPair"}
{"loc":"00075:9-17","source":1,"syntax":"","pretty":"variable sendok__","sym":"V_948b2b0f6c935d97_7ac97e30887ea1,V_90997fe031685921_7ac97e30887ea1,V_e246d1e9bf5c4f33_7ac97e30887ea1,V_93dd87eb55db9e5d_7ac97e30887ea1","no_crossref":1,"type":"_Bool"}
{"loc":"00075:20-31","source":1,"syntax":"function,use","pretty":"function mozilla::ipc::IProtocol::ChannelSend","sym":"_ZN7mozilla3ipc9IProtocol11ChannelSendENS_9UniquePtrIN3IPC7MessageENS_13DefaultDeleteIS4_EEEE","type":"_Bool"}
{"loc":"00075:32-35","source":1,"syntax":"constructor,use","pretty":"constructor mozilla::UniquePtr::UniquePtr<T, D>","sym":"_ZN7mozilla9UniquePtrC1EONS_9UniquePtrIT_T0_EE"}
{"loc":"00075:37-41","source":1,"syntax":"function,use","pretty":"function std::move","sym":"_ZSt4moveOT_,_ZNSt3__14moveEOT_,_ZNSt6__ndk14moveEOT_","type":"typename remove_reference<class UniquePtr<class Message> &>::type &&(class mozilla::UniquePtr<class IPC::Message> &) noexcept"}
{"loc":"00075:42-47","source":1,"syntax":"","pretty":"variable msg__","sym":"V_91c16e605a27c74b_a6144ff013,V_9d417cde47846456_a6144ff013,V_e9446a16f658d82b_a6144ff013,V_90fd07d8d0861239_a6144ff013","no_crossref":1,"type":"UniquePtr<IPC::Message>","typesym":"T_mozilla::UniquePtr"}
{"loc":"00076:11-19","source":1,"syntax":"","pretty":"variable sendok__","sym":"V_948b2b0f6c935d97_7ac97e30887ea1,V_90997fe031685921_7ac97e30887ea1,V_e246d1e9bf5c4f33_7ac97e30887ea1,V_93dd87eb55db9e5d_7ac97e30887ea1","no_crossref":1,"type":"_Bool"}
{"loc":"00079:5-21","source":1,"syntax":"type,use","pretty":"type mozilla::_ipdltest::PTestBasicParent","sym":"T_mozilla::_ipdltest::PTestBasicParent","type":"class mozilla::_ipdltest::PTestBasicParent","typesym":"T_mozilla::_ipdltest::PTestBasicParent"}
{"loc":"00079:23-36","source":1,"syntax":"def,function","pretty":"function mozilla::_ipdltest::PTestBasicParent::RemoveManagee","sym":"_ZN7mozilla9_ipdltest16PTestBasicParent13RemoveManageeEiPNS_3ipc9IProtocolE","nestingRange":"82:0-85:0","type":"auto (int32_t, mozilla::_ipdltest::PTestBasicParent::IProtocol *) -> void"}
{"loc":"00080:8-15","source":1,"syntax":"type,use","pretty":"type int32_t","sym":"T_int32_t","type":"int32_t"}
{"loc":"00080:16-27","source":1,"syntax":"","pretty":"variable aProtocolId","sym":"V_33b191705a27c74b_542cb52f47aba60c,V_3f31afde47846456_542cb52f47aba60c,V_8b349d16f658d82b_542cb52f47aba60c,V_32ed3ad8d0861239_542cb52f47aba60c","no_crossref":1,"type":"int32_t"}
{"loc":"00081:8-17","source":1,"syntax":"type,use","pretty":"type mozilla::_ipdltest::PTestBasicParent::IProtocol","sym":"T_mozilla::_ipdltest::PTestBasicParent::IProtocol","type":"mozilla::_ipdltest::PTestBasicParent::IProtocol","typesym":"T_mozilla::ipc::IProtocol"}
{"loc":"00081:19-28","source":1,"syntax":"","pretty":"variable aListener","sym":"V_797a91705a27c74b_c681f10e880c773,V_750aafde47846456_c681f10e880c773,V_c10d9d16f658d82b_c681f10e880c773,V_78a64ad8d0861239_c681f10e880c773","no_crossref":1,"type":"mozilla::_ipdltest::PTestBasicParent::IProtocol *"}
{"loc":"00083:4-14","source":1,"syntax":"function,use","pretty":"function mozilla::ipc::IProtocol::FatalError","sym":"_ZNK7mozilla3ipc9IProtocol10FatalErrorEPKc","type":"void"}
{"loc":"00087:5-21","source":1,"syntax":"type,use","pretty":"type mozilla::_ipdltest::PTestBasicParent","sym":"T_mozilla::_ipdltest::PTestBasicParent","type":"class mozilla::_ipdltest::PTestBasicParent","typesym":"T_mozilla::_ipdltest::PTestBasicParent"}
{"loc":"00087:23-37","source":1,"syntax":"def,function","pretty":"function mozilla::_ipdltest::PTestBasicParent::DeallocManagee","sym":"_ZN7mozilla9_ipdltest16PTestBasicParent14DeallocManageeEiPNS_3ipc9IProtocolE","nestingRange":"90:0-93:0","type":"auto (int32_t, mozilla::_ipdltest::PTestBasicParent::IProtocol *) -> void"}
{"loc":"00088:8-15","source":1,"syntax":"type,use","pretty":"type int32_t","sym":"T_int32_t","type":"int32_t"}
{"loc":"00088:16-27","source":1,"syntax":"","pretty":"variable aProtocolId","sym":"V_b3e7d1705a27c74b_542cb52f47aba60c,V_bf67efde47846456_542cb52f47aba60c,V_0c6add16f658d82b_542cb52f47aba60c,V_b2148ad8d0861239_542cb52f47aba60c","no_crossref":1,"type":"int32_t"}
{"loc":"00089:8-17","source":1,"syntax":"type,use","pretty":"type mozilla::_ipdltest::PTestBasicParent::IProtocol","sym":"T_mozilla::_ipdltest::PTestBasicParent::IProtocol","type":"mozilla::_ipdltest::PTestBasicParent::IProtocol","typesym":"T_mozilla::ipc::IProtocol"}
{"loc":"00089:19-28","source":1,"syntax":"","pretty":"variable aListener","sym":"V_f9a0e1705a27c74b_c681f10e880c773,V_f530ffde47846456_c681f10e880c773,V_4233ed16f658d82b_c681f10e880c773,V_f8dc8ad8d0861239_c681f10e880c773","no_crossref":1,"type":"mozilla::_ipdltest::PTestBasicParent::IProtocol *"}
{"loc":"00091:4-14","source":1,"syntax":"function,use","pretty":"function mozilla::ipc::IProtocol::FatalError","sym":"_ZNK7mozilla3ipc9IProtocol10FatalErrorEPKc","type":"void"}
{"loc":"00095:5-21","source":1,"syntax":"type,use","pretty":"type mozilla::_ipdltest::PTestBasicParent","sym":"T_mozilla::_ipdltest::PTestBasicParent","type":"class mozilla::_ipdltest::PTestBasicParent","typesym":"T_mozilla::_ipdltest::PTestBasicParent"}
{"loc":"00095:23-40","source":1,"syntax":"def,function","pretty":"function mozilla::_ipdltest::PTestBasicParent::OnMessageReceived","sym":"_ZN7mozilla9_ipdltest16PTestBasicParent17OnMessageReceivedERKN3IPC7MessageE","nestingRange":"96:0-111:0","type":"auto (const mozilla::_ipdltest::PTestBasicParent::Message &) -> class PTestBasicParent::Result"}
{"loc":"00095:47-54","source":1,"syntax":"type,use","pretty":"type mozilla::_ipdltest::PTestBasicParent::Message","sym":"T_mozilla::_ipdltest::PTestBasicParent::Message","type":"mozilla::_ipdltest::PTestBasicParent::Message","typesym":"T_IPC::Message"}
{"loc":"00095:56-61","source":1,"syntax":"","pretty":"variable msg__","sym":"V_d12fd2705a27c74b_a6144ff013,V_ddaee0ee47846456_a6144ff013,V_2aa1ee16f658d82b_a6144ff013,V_d05b8bd8d0861239_a6144ff013","no_crossref":1,"type":"const mozilla::_ipdltest::PTestBasicParent::Message &"}
{"loc":"00095:66-82","source":1,"syntax":"type,use","pretty":"type mozilla::_ipdltest::PTestBasicParent","sym":"T_mozilla::_ipdltest::PTestBasicParent","type":"class mozilla::_ipdltest::PTestBasicParent","typesym":"T_mozilla::_ipdltest::PTestBasicParent"}
{"loc":"00095:84-90","source":1,"syntax":"type,use","pretty":"type mozilla::ipc::HasResultCodes::Result","sym":"T_mozilla::ipc::HasResultCodes::Result","type":"enum mozilla::ipc::HasResultCodes::Result","typesym":"T_mozilla::ipc::HasResultCodes::Result"}
{"loc":"00097:12-17","source":1,"syntax":"","pretty":"variable msg__","sym":"V_d12fd2705a27c74b_a6144ff013,V_ddaee0ee47846456_a6144ff013,V_2aa1ee16f658d82b_a6144ff013,V_d05b8bd8d0861239_a6144ff013","no_crossref":1,"type":"const mozilla::_ipdltest::PTestBasicParent::Message &"}
{"loc":"00097:18-22","source":1,"syntax":"function,use","pretty":"function IPC::Message::type","sym":"_ZNK3IPC7Message4typeEv","type":"IPC::Message::msgid_t"}
{"loc":"00099:15-26","source":1,"syntax":"enum,use","pretty":"enum mozilla::ipc::HasResultCodes::Result::MsgNotKnown","sym":"E_<T_mozilla::ipc::HasResultCodes::Result>_MsgNotKnown","type":"enum mozilla::ipc::HasResultCodes::Result","typesym":"T_mozilla::ipc::HasResultCodes::Result"}
{"loc":"00100:9-35","source":1,"syntax":"enum,use","pretty":"enum SHMEM_CREATED_MESSAGE_TYPE","sym":"E_<T_b078e220e89184de>_SHMEM_CREATED_MESSAGE_TYPE","type":"enum (anonymous namespace)::(unnamed at /builds/worker/workspace/obj-build/dist/include/mozilla/ipc/ProtocolUtils.h:57:1)","typesym":"T_b078e220e89184de"}
{"loc":"00102:12-22","source":1,"syntax":"function,use","pretty":"function mozilla::ipc::IProtocol::FatalError","sym":"_ZNK7mozilla3ipc9IProtocol10FatalErrorEPKc","type":"void"}
{"loc":"00103:19-30","source":1,"syntax":"enum,use","pretty":"enum mozilla::ipc::HasResultCodes::Result::MsgNotKnown","sym":"E_<T_mozilla::ipc::HasResultCodes::Result>_MsgNotKnown","type":"enum mozilla::ipc::HasResultCodes::Result","typesym":"T_mozilla::ipc::HasResultCodes::Result"}
{"loc":"00105:9-37","source":1,"syntax":"enum,use","pretty":"enum SHMEM_DESTROYED_MESSAGE_TYPE","sym":"E_<T_b078e220e89184de>_SHMEM_DESTROYED_MESSAGE_TYPE","type":"enum (anonymous namespace)::(unnamed at /builds/worker/workspace/obj-build/dist/include/mozilla/ipc/ProtocolUtils.h:57:1)","typesym":"T_b078e220e89184de"}
{"loc":"00107:12-22","source":1,"syntax":"function,use","pretty":"function mozilla::ipc::IProtocol::FatalError","sym":"_ZNK7mozilla3ipc9IProtocol10FatalErrorEPKc","type":"void"}
{"loc":"00108:19-30","source":1,"syntax":"enum,use","pretty":"enum mozilla::ipc::HasResultCodes::Result::MsgNotKnown","sym":"E_<T_mozilla::ipc::HasResultCodes::Result>_MsgNotKnown","type":"enum mozilla::ipc::HasResultCodes::Result","typesym":"T_mozilla::ipc::HasResultCodes::Result"}
{"loc":"00113:5-21","source":1,"syntax":"type,use","pretty":"type mozilla::_ipdltest::PTestBasicParent","sym":"T_mozilla::_ipdltest::PTestBasicParent","type":"class mozilla::_ipdltest::PTestBasicParent","typesym":"T_mozilla::_ipdltest::PTestBasicParent"}
{"loc":"00113:23-40","source":1,"syntax":"def,function","pretty":"function mozilla::_ipdltest::PTestBasicParent::OnMessageReceived","sym":"_ZN7mozilla9_ipdltest16PTestBasicParent17OnMessageReceivedERKN3IPC7MessageERNS_9UniquePtrIS3_NS_13DefaultDeleteIS3_EEEE","nestingRange":"116:0-119:0","type":"auto (const mozilla::_ipdltest::PTestBasicParent::Message &, UniquePtr<mozilla::_ipdltest::PTestBasicParent::Message> &) -> class PTestBasicParent::Result"}
{"loc":"00114:14-21","source":1,"syntax":"type,use","pretty":"type mozilla::_ipdltest::PTestBasicParent::Message","sym":"T_mozilla::_ipdltest::PTestBasicParent::Message","type":"mozilla::_ipdltest::PTestBasicParent::Message","typesym":"T_IPC::Message"}
{"loc":"00114:23-28","source":1,"syntax":"","pretty":"variable msg__","sym":"V_fe9c1f805a27c74b_a6144ff013,V_fa2c2dfe47846456_a6144ff013,V_472f1b36f658d82b_a6144ff013,V_fdc8c7f8d0861239_a6144ff013","no_crossref":1,"type":"const mozilla::_ipdltest::PTestBasicParent::Message &"}
{"loc":"00115:8-17","source":1,"syntax":"type,use","pretty":"type mozilla::_ipdltest::PTestBasicParent::UniquePtr","sym":"T_mozilla::_ipdltest::PTestBasicParent::UniquePtr"}
{"loc":"00115:18-25","source":1,"syntax":"type,use","pretty":"type mozilla::_ipdltest::PTestBasicParent::Message","sym":"T_mozilla::_ipdltest::PTestBasicParent::Message","type":"mozilla::_ipdltest::PTestBasicParent::Message","typesym":"T_IPC::Message"}
{"loc":"00115:28-35","source":1,"syntax":"","pretty":"variable reply__","sym":"V_55652f805a27c74b_f83bfee36b0d,V_51f43dfe47846456_f83bfee36b0d,V_ade72b36f658d82b_f83bfee36b0d,V_5491d7f8d0861239_f83bfee36b0d","no_crossref":1,"type":"UniquePtr<mozilla::_ipdltest::PTestBasicParent::Message> &"}
{"loc":"00115:40-56","source":1,"syntax":"type,use","pretty":"type mozilla::_ipdltest::PTestBasicParent","sym":"T_mozilla::_ipdltest::PTestBasicParent","type":"class mozilla::_ipdltest::PTestBasicParent","typesym":"T_mozilla::_ipdltest::PTestBasicParent"}
{"loc":"00115:58-64","source":1,"syntax":"type,use","pretty":"type mozilla::ipc::HasResultCodes::Result","sym":"T_mozilla::ipc::HasResultCodes::Result","type":"enum mozilla::ipc::HasResultCodes::Result","typesym":"T_mozilla::ipc::HasResultCodes::Result"}
{"loc":"00117:4-26","source":1,"syntax":"macro,use","pretty":"macro MOZ_ASSERT_UNREACHABLE","sym":"M_0d9562839add515c"}
{"loc":"00118:11-22","source":1,"syntax":"enum,use","pretty":"enum mozilla::ipc::HasResultCodes::Result::MsgNotKnown","sym":"E_<T_mozilla::ipc::HasResultCodes::Result>_MsgNotKnown","type":"enum mozilla::ipc::HasResultCodes::Result","typesym":"T_mozilla::ipc::HasResultCodes::Result"}
{"loc":"00121:5-21","source":1,"syntax":"type,use","pretty":"type mozilla::_ipdltest::PTestBasicParent","sym":"T_mozilla::_ipdltest::PTestBasicParent","type":"class mozilla::_ipdltest::PTestBasicParent","typesym":"T_mozilla::_ipdltest::PTestBasicParent"}
{"loc":"00121:23-37","source":1,"syntax":"def,function","pretty":"function mozilla::_ipdltest::PTestBasicParent::OnCallReceived","sym":"_ZN7mozilla9_ipdltest16PTestBasicParent14OnCallReceivedERKN3IPC7MessageERNS_9UniquePtrIS3_NS_13DefaultDeleteIS3_EEEE","nestingRange":"124:0-127:0","type":"auto (const mozilla::_ipdltest::PTestBasicParent::Message &, UniquePtr<mozilla::_ipdltest::PTestBasicParent::Message> &) -> class PTestBasicParent::Result"}
{"loc":"00122:14-21","source":1,"syntax":"type,use","pretty":"type mozilla::_ipdltest::PTestBasicParent::Message","sym":"T_mozilla::_ipdltest::PTestBasicParent::Message","type":"mozilla::_ipdltest::PTestBasicParent::Message","typesym":"T_IPC::Message"}
{"loc":"00122:23-28","source":1,"syntax":"","pretty":"variable msg__","sym":"V_ea9c20905a27c74b_a6144ff013,V_e62c3efe47846456_a6144ff013,V_332f2c36f658d82b_a6144ff013,V_e9c8d8f8d0861239_a6144ff013","no_crossref":1,"type":"const mozilla::_ipdltest::PTestBasicParent::Message &"}
{"loc":"00123:8-17","source":1,"syntax":"type,use","pretty":"type mozilla::_ipdltest::PTestBasicParent::UniquePtr","sym":"T_mozilla::_ipdltest::PTestBasicParent::UniquePtr"}
{"loc":"00123:18-25","source":1,"syntax":"type,use","pretty":"type mozilla::_ipdltest::PTestBasicParent::Message","sym":"T_mozilla::_ipdltest::PTestBasicParent::Message","type":"mozilla::_ipdltest::PTestBasicParent::Message","typesym":"T_IPC::Message"}
{"loc":"00123:28-35","source":1,"syntax":"","pretty":"variable reply__","sym":"V_416530905a27c74b_f83bfee36b0d,V_4de44efe47846456_f83bfee36b0d,V_99e73c36f658d82b_f83bfee36b0d,V_4091e8f8d0861239_f83bfee36b0d","no_crossref":1,"type":"UniquePtr<mozilla::_ipdltest::PTestBasicParent::Message> &"}
{"loc":"00123:40-56","source":1,"syntax":"type,use","pretty":"type mozilla::_ipdltest::PTestBasicParent","sym":"T_mozilla::_ipdltest::PTestBasicParent","type":"class mozilla::_ipdltest::PTestBasicParent","typesym":"T_mozilla::_ipdltest::PTestBasicParent"}
{"loc":"00123:58-64","source":1,"syntax":"type,use","pretty":"type mozilla::ipc::HasResultCodes::Result","sym":"T_mozilla::ipc::HasResultCodes::Result","type":"enum mozilla::ipc::HasResultCodes::Result","typesym":"T_mozilla::ipc::HasResultCodes::Result"}
{"loc":"00125:4-26","source":1,"syntax":"macro,use","pretty":"macro MOZ_ASSERT_UNREACHABLE","sym":"M_0d9562839add515c"}
{"loc":"00126:11-22","source":1,"syntax":"enum,use","pretty":"enum mozilla::ipc::HasResultCodes::Result::MsgNotKnown","sym":"E_<T_mozilla::ipc::HasResultCodes::Result>_MsgNotKnown","type":"enum mozilla::ipc::HasResultCodes::Result","typesym":"T_mozilla::ipc::HasResultCodes::Result"}
{"loc":"00129:5-21","source":1,"syntax":"type,use","pretty":"type mozilla::_ipdltest::PTestBasicParent","sym":"T_mozilla::_ipdltest::PTestBasicParent","type":"class mozilla::_ipdltest::PTestBasicParent","typesym":"T_mozilla::_ipdltest::PTestBasicParent"}
{"loc":"00129:23-37","source":1,"syntax":"def,function","pretty":"function mozilla::_ipdltest::PTestBasicParent::OnChannelClose","sym":"_ZN7mozilla9_ipdltest16PTestBasicParent14OnChannelCloseEv","nestingRange":"130:0-137:0","type":"auto (void) -> void"}
{"loc":"00131:4-18","source":1,"syntax":"function,use","pretty":"function mozilla::ipc::IProtocol::DestroySubtree","sym":"_ZN7mozilla3ipc9IProtocol14DestroySubtreeENS1_18ActorDestroyReasonE","type":"void"}
{"loc":"00131:19-33","source":1,"syntax":"enum,use","pretty":"enum mozilla::ipc::IProtocol::ActorDestroyReason::NormalShutdown","sym":"E_<T_mozilla::ipc::IProtocol::ActorDestroyReason>_NormalShutdown","type":"enum mozilla::ipc::IProtocol::ActorDestroyReason","typesym":"T_mozilla::ipc::IProtocol::ActorDestroyReason"}
{"loc":"00132:4-16","source":1,"syntax":"function,use","pretty":"function mozilla::_ipdltest::PTestBasicParent::ClearSubtree","sym":"_ZN7mozilla9_ipdltest16PTestBasicParent12ClearSubtreeEv","type":"void"}
{"loc":"00133:4-17","source":1,"syntax":"function,use","pretty":"function mozilla::ipc::IToplevelProtocol::DeallocShmems","sym":"_ZN7mozilla3ipc17IToplevelProtocol13DeallocShmemsEv","type":"void"}
{"loc":"00134:8-25","source":1,"syntax":"function,use","pretty":"function mozilla::ipc::IProtocol::GetLifecycleProxy","sym":"_ZN7mozilla3ipc9IProtocol17GetLifecycleProxyEv","type":"class mozilla::ipc::ActorLifecycleProxy *"}
{"loc":"00135:8-25","source":1,"syntax":"function,use","pretty":"function mozilla::ipc::IProtocol::GetLifecycleProxy","sym":"_ZN7mozilla3ipc9IProtocol17GetLifecycleProxyEv","type":"class mozilla::ipc::ActorLifecycleProxy *"}
{"loc":"00135:29-36","source":1,"syntax":"function,use","pretty":"function mozilla::ipc::ActorLifecycleProxy::Release","sym":"_ZN7mozilla3ipc19ActorLifecycleProxy7ReleaseEv","type":"MozExternalRefCountType"}
{"loc":"00139:5-21","source":1,"syntax":"type,use","pretty":"type mozilla::_ipdltest::PTestBasicParent","sym":"T_mozilla::_ipdltest::PTestBasicParent","type":"class mozilla::_ipdltest::PTestBasicParent","typesym":"T_mozilla::_ipdltest::PTestBasicParent"}
{"loc":"00139:23-37","source":1,"syntax":"def,function","pretty":"function mozilla::_ipdltest::PTestBasicParent::OnChannelError","sym":"_ZN7mozilla9_ipdltest16PTestBasicParent14OnChannelErrorEv","nestingRange":"140:0-147:0","type":"auto (void) -> void"}
{"loc":"00141:4-18","source":1,"syntax":"function,use","pretty":"function mozilla::ipc::IProtocol::DestroySubtree","sym":"_ZN7mozilla3ipc9IProtocol14DestroySubtreeENS1_18ActorDestroyReasonE","type":"void"}
{"loc":"00141:19-35","source":1,"syntax":"enum,use","pretty":"enum mozilla::ipc::IProtocol::ActorDestroyReason::AbnormalShutdown","sym":"E_<T_mozilla::ipc::IProtocol::ActorDestroyReason>_AbnormalShutdown","type":"enum mozilla::ipc::IProtocol::ActorDestroyReason","typesym":"T_mozilla::ipc::IProtocol::ActorDestroyReason"}
{"loc":"00142:4-16","source":1,"syntax":"function,use","pretty":"function mozilla::_ipdltest::PTestBasicParent::ClearSubtree","sym":"_ZN7mozilla9_ipdltest16PTestBasicParent12ClearSubtreeEv","type":"void"}
{"loc":"00143:4-17","source":1,"syntax":"function,use","pretty":"function mozilla::ipc::IToplevelProtocol::DeallocShmems","sym":"_ZN7mozilla3ipc17IToplevelProtocol13DeallocShmemsEv","type":"void"}
{"loc":"00144:8-25","source":1,"syntax":"function,use","pretty":"function mozilla::ipc::IProtocol::GetLifecycleProxy","sym":"_ZN7mozilla3ipc9IProtocol17GetLifecycleProxyEv","type":"class mozilla::ipc::ActorLifecycleProxy *"}
{"loc":"00145:8-25","source":1,"syntax":"function,use","pretty":"function mozilla::ipc::IProtocol::GetLifecycleProxy","sym":"_ZN7mozilla3ipc9IProtocol17GetLifecycleProxyEv","type":"class mozilla::ipc::ActorLifecycleProxy *"}
{"loc":"00145:29-36","source":1,"syntax":"function,use","pretty":"function mozilla::ipc::ActorLifecycleProxy::Release","sym":"_ZN7mozilla3ipc19ActorLifecycleProxy7ReleaseEv","type":"MozExternalRefCountType"}
{"loc":"00149:5-21","source":1,"syntax":"type,use","pretty":"type mozilla::_ipdltest::PTestBasicParent","sym":"T_mozilla::_ipdltest::PTestBasicParent","type":"class mozilla::_ipdltest::PTestBasicParent","typesym":"T_mozilla::_ipdltest::PTestBasicParent"}
{"loc":"00149:23-35","source":1,"syntax":"def,function","pretty":"function mozilla::_ipdltest::PTestBasicParent::ClearSubtree","sym":"_ZN7mozilla9_ipdltest16PTestBasicParent12ClearSubtreeEv","nestingRange":"150:0-151:0","type":"auto (void) -> void"}
{"loc":"00157:10-13","source":1,"syntax":"def,namespace","pretty":"namespace IPC","sym":"NS_IPC","nestingRange":"157:15-204:0"}
{"loc":"00158:5-16","source":1,"syntax":"type,use","pretty":"type IPC::ParamTraits","sym":"T_IPC::ParamTraits"}
{"loc":"00158:37-53","source":1,"syntax":"type,use","pretty":"type mozilla::_ipdltest::PTestBasicParent","sym":"T_mozilla::_ipdltest::PTestBasicParent","type":"class mozilla::_ipdltest::PTestBasicParent","typesym":"T_mozilla::_ipdltest::PTestBasicParent"}
{"loc":"00158:57-62","source":1,"syntax":"def,function","pretty":"function IPC::ParamTraits<mozilla::_ipdltest::PTestBasicParent *>::Write","sym":"_ZN3IPC11ParamTraitsIPN7mozilla9_ipdltest16PTestBasicParentEE5WriteEPNS_13MessageWriterERKS4_","nestingRange":"161:0-184:0","type":"auto (IPC::MessageWriter *, const IPC::ParamTraits<class mozilla::_ipdltest::PTestBasicParent *>::paramType &) -> void"}
{"loc":"00159:13-26","source":1,"syntax":"type,use","pretty":"type IPC::MessageWriter","sym":"T_IPC::MessageWriter","type":"class IPC::MessageWriter","typesym":"T_IPC::MessageWriter"}
{"loc":"00159:28-35","source":1,"syntax":"","pretty":"variable aWriter","sym":"V_dd9ec3905a27c74b_369ebc101b0d,V_d92ed10f47846456_369ebc101b0d,V_2621df36f658d82b_369ebc101b0d,V_dcca7cf8d0861239_369ebc101b0d","no_crossref":1,"type":"IPC::MessageWriter *"}
{"loc":"00160:14-23","source":1,"syntax":"type,use","pretty":"type IPC::ParamTraits<mozilla::_ipdltest::PTestBasicParent *>::paramType","sym":"T_IPC::ParamTraits<mozilla::_ipdltest::PTestBasicParent_*>::paramType","type":"IPC::ParamTraits<class mozilla::_ipdltest::PTestBasicParent *>::paramType"}
{"loc":"00160:25-29","source":1,"syntax":"","pretty":"variable aVar","sym":"V_2f21a4905a27c74b_f0bb39c71,V_2bb0b20f47846456_f0bb39c71,V_77b3a046f658d82b_f0bb39c71,V_2e5d4df8d0861239_f0bb39c71","no_crossref":1,"type":"const IPC::ParamTraits<class mozilla::_ipdltest::PTestBasicParent *>::paramType &"}
{"loc":"00162:4-22","source":1,"syntax":"macro,use","pretty":"macro MOZ_RELEASE_ASSERT","sym":"M_96f821839add515c"}
{"loc":"00163:8-15","source":1,"syntax":"","pretty":"variable aWriter","sym":"V_dd9ec3905a27c74b_369ebc101b0d,V_d92ed10f47846456_369ebc101b0d,V_2621df36f658d82b_369ebc101b0d,V_dcca7cf8d0861239_369ebc101b0d","no_crossref":1,"type":"IPC::MessageWriter *"}
{"loc":"00163:17-25","source":1,"syntax":"function,use","pretty":"function IPC::MessageWriter::GetActor","sym":"_ZNK3IPC13MessageWriter8GetActorEv","type":"mozilla::ipc::IProtocol *"}
{"loc":"00166:4-11","source":1,"syntax":"type,use","pretty":"type int32_t","sym":"T_int32_t","type":"int32_t"}
{"loc":"00166:12-14","source":1,"syntax":"","pretty":"variable id","sym":"V_41d5d4905a27c74b_238795,V_4d55e20f47846456_238795,V_9958d046f658d82b_238795,V_40028df8d0861239_238795","no_crossref":1,"type":"int32_t"}
{"loc":"00167:9-13","source":1,"syntax":"","pretty":"variable aVar","sym":"V_2f21a4905a27c74b_f0bb39c71,V_2bb0b20f47846456_f0bb39c71,V_77b3a046f658d82b_f0bb39c71,V_2e5d4df8d0861239_f0bb39c71","no_crossref":1,"type":"const IPC::ParamTraits<class mozilla::_ipdltest::PTestBasicParent *>::paramType &"}
{"loc":"00168:8-10","source":1,"syntax":"","pretty":"variable id","sym":"V_41d5d4905a27c74b_238795,V_4d55e20f47846456_238795,V_9958d046f658d82b_238795,V_40028df8d0861239_238795","no_crossref":1,"type":"int32_t"}
{"loc":"00170:8-10","source":1,"syntax":"","pretty":"variable id","sym":"V_41d5d4905a27c74b_238795,V_4d55e20f47846456_238795,V_9958d046f658d82b_238795,V_40028df8d0861239_238795","no_crossref":1,"type":"int32_t"}
{"loc":"00170:13-17","source":1,"syntax":"","pretty":"variable aVar","sym":"V_2f21a4905a27c74b_f0bb39c71,V_2bb0b20f47846456_f0bb39c71,V_77b3a046f658d82b_f0bb39c71,V_2e5d4df8d0861239_f0bb39c71","no_crossref":1,"type":"const IPC::ParamTraits<class mozilla::_ipdltest::PTestBasicParent *>::paramType &"}
{"loc":"00170:19-21","source":1,"syntax":"function,use","pretty":"function mozilla::ipc::IProtocol::Id","sym":"_ZNK7mozilla3ipc9IProtocol2IdEv","type":"int32_t"}
{"loc":"00171:12-14","source":1,"syntax":"","pretty":"variable id","sym":"V_41d5d4905a27c74b_238795,V_4d55e20f47846456_238795,V_9958d046f658d82b_238795,V_40028df8d0861239_238795","no_crossref":1,"type":"int32_t"}
{"loc":"00172:12-16","source":1,"syntax":"","pretty":"variable aVar","sym":"V_2f21a4905a27c74b_f0bb39c71,V_2bb0b20f47846456_f0bb39c71,V_77b3a046f658d82b_f0bb39c71,V_2e5d4df8d0861239_f0bb39c71","no_crossref":1,"type":"const IPC::ParamTraits<class mozilla::_ipdltest::PTestBasicParent *>::paramType &"}
{"loc":"00172:18-28","source":1,"syntax":"function,use","pretty":"function mozilla::ipc::IProtocol::FatalError","sym":"_ZNK7mozilla3ipc9IProtocol10FatalErrorEPKc","type":"void"}
{"loc":"00174:8-26","source":1,"syntax":"macro,use","pretty":"macro MOZ_RELEASE_ASSERT","sym":"M_96f821839add515c"}
{"loc":"00175:12-19","source":1,"syntax":"","pretty":"variable aWriter","sym":"V_dd9ec3905a27c74b_369ebc101b0d,V_d92ed10f47846456_369ebc101b0d,V_2621df36f658d82b_369ebc101b0d,V_dcca7cf8d0861239_369ebc101b0d","no_crossref":1,"type":"IPC::MessageWriter *"}
{"loc":"00175:21-29","source":1,"syntax":"function,use","pretty":"function IPC::MessageWriter::GetActor","sym":"_ZNK3IPC13MessageWriter8GetActorEv","type":"mozilla::ipc::IProtocol *"}
{"loc":"00175:33-46","source":1,"syntax":"function,use","pretty":"function mozilla::ipc::IProtocol::GetIPCChannel","sym":"_ZN7mozilla3ipc9IProtocol13GetIPCChannelEv","type":"class mozilla::ipc::MessageChannel *"}
{"loc":"00175:52-56","source":1,"syntax":"","pretty":"variable aVar","sym":"V_2f21a4905a27c74b_f0bb39c71,V_2bb0b20f47846456_f0bb39c71,V_77b3a046f658d82b_f0bb39c71,V_2e5d4df8d0861239_f0bb39c71","no_crossref":1,"type":"const IPC::ParamTraits<class mozilla::_ipdltest::PTestBasicParent *>::paramType &"}
{"loc":"00175:58-71","source":1,"syntax":"function,use","pretty":"function mozilla::ipc::IToplevelProtocol::GetIPCChannel","sym":"_ZN7mozilla3ipc17IToplevelProtocol13GetIPCChannelEv","type":"class mozilla::ipc::MessageChannel *"}
{"loc":"00178:8-26","source":1,"syntax":"macro,use","pretty":"macro MOZ_RELEASE_ASSERT","sym":"M_96f821839add515c"}
{"loc":"00179:12-16","source":1,"syntax":"","pretty":"variable aVar","sym":"V_2f21a4905a27c74b_f0bb39c71,V_2bb0b20f47846456_f0bb39c71,V_77b3a046f658d82b_f0bb39c71,V_2e5d4df8d0861239_f0bb39c71","no_crossref":1,"type":"const IPC::ParamTraits<class mozilla::_ipdltest::PTestBasicParent *>::paramType &"}
{"loc":"00179:18-25","source":1,"syntax":"function,use","pretty":"function mozilla::ipc::IProtocol::CanSend","sym":"_ZNK7mozilla3ipc9IProtocol7CanSendEv","type":"_Bool"}
{"loc":"00183:9-19","source":1,"syntax":"function,use","pretty":"function IPC::WriteParam","sym":"_ZN3IPCL10WriteParamEPNS_13MessageWriterEOT_","type":"void (class IPC::MessageWriter *, int &)"}
{"loc":"00183:20-27","source":1,"syntax":"","pretty":"variable aWriter","sym":"V_dd9ec3905a27c74b_369ebc101b0d,V_d92ed10f47846456_369ebc101b0d,V_2621df36f658d82b_369ebc101b0d,V_dcca7cf8d0861239_369ebc101b0d","no_crossref":1,"type":"IPC::MessageWriter *"}
{"loc":"00183:29-31","source":1,"syntax":"","pretty":"variable id","sym":"V_41d5d4905a27c74b_238795,V_4d55e20f47846456_238795,V_9958d046f658d82b_238795,V_40028df8d0861239_238795","no_crossref":1,"type":"int32_t"}
{"loc":"00186:5-16","source":1,"syntax":"type,use","pretty":"type IPC::ParamTraits","sym":"T_IPC::ParamTraits"}
{"loc":"00186:37-53","source":1,"syntax":"type,use","pretty":"type mozilla::_ipdltest::PTestBasicParent","sym":"T_mozilla::_ipdltest::PTestBasicParent","type":"class mozilla::_ipdltest::PTestBasicParent","typesym":"T_mozilla::_ipdltest::PTestBasicParent"}
{"loc":"00186:57-61","source":1,"syntax":"def,function","pretty":"function IPC::ParamTraits<mozilla::_ipdltest::PTestBasicParent *>::Read","sym":"_ZN3IPC11ParamTraitsIPN7mozilla9_ipdltest16PTestBasicParentEE4ReadEPNS_13MessageReaderEPS4_","nestingRange":"189:0-202:0","type":"auto (IPC::MessageReader *, IPC::ParamTraits<class mozilla::_ipdltest::PTestBasicParent *>::paramType *) -> _Bool"}
{"loc":"00187:13-26","source":1,"syntax":"type,use","pretty":"type IPC::MessageReader","sym":"T_IPC::MessageReader","type":"class IPC::MessageReader","typesym":"T_IPC::MessageReader"}
{"loc":"00187:28-35","source":1,"syntax":"","pretty":"variable aReader","sym":"V_e9a127905a27c74b_9983235f0b0d,V_e531350f47846456_9983235f0b0d,V_32342346f658d82b_9983235f0b0d,V_e8ddcff8d0861239_9983235f0b0d","no_crossref":1,"type":"IPC::MessageReader *"}
{"loc":"00188:8-17","source":1,"syntax":"type,use","pretty":"type IPC::ParamTraits<mozilla::_ipdltest::PTestBasicParent *>::paramType","sym":"T_IPC::ParamTraits<mozilla::_ipdltest::PTestBasicParent_*>::paramType","type":"IPC::ParamTraits<class mozilla::_ipdltest::PTestBasicParent *>::paramType"}
{"loc":"00188:19-23","source":1,"syntax":"","pretty":"variable aVar","sym":"V_fd6a27905a27c74b_f0bb39c71,V_f9f9350f47846456_f0bb39c71,V_46fc2346f658d82b_f0bb39c71,V_fc96dff8d0861239_f0bb39c71","no_crossref":1,"type":"IPC::ParamTraits<class mozilla::_ipdltest::PTestBasicParent *>::paramType *"}
{"loc":"00190:4-22","source":1,"syntax":"macro,use","pretty":"macro MOZ_RELEASE_ASSERT","sym":"M_96f821839add515c"}
{"loc":"00191:8-15","source":1,"syntax":"","pretty":"variable aReader","sym":"V_e9a127905a27c74b_9983235f0b0d,V_e531350f47846456_9983235f0b0d,V_32342346f658d82b_9983235f0b0d,V_e8ddcff8d0861239_9983235f0b0d","no_crossref":1,"type":"IPC::MessageReader *"}
{"loc":"00191:17-25","source":1,"syntax":"function,use","pretty":"function IPC::MessageReader::GetActor","sym":"_ZNK3IPC13MessageReader8GetActorEv","type":"mozilla::ipc::IProtocol *"}
{"loc":"00194:13-18","source":1,"syntax":"type,use","pretty":"type mozilla::Maybe","sym":"T_mozilla::Maybe"}
{"loc":"00194:33-42","source":1,"syntax":"type,use","pretty":"type mozilla::ipc::IProtocol","sym":"T_mozilla::ipc::IProtocol","type":"class mozilla::ipc::IProtocol","typesym":"T_mozilla::ipc::IProtocol"}
{"loc":"00194:45-50","source":1,"syntax":"","pretty":"variable actor","sym":"V_b3e828905a27c74b_ebe821f013,V_bf68360f47846456_ebe821f013,V_0c6b2446f658d82b_ebe821f013,V_b215d009d0861239_ebe821f013","no_crossref":1,"type":"mozilla::Maybe<mozilla::ipc::IProtocol *>","typesym":"T_mozilla::Maybe"}
{"loc":"00195:8-15","source":1,"syntax":"","pretty":"variable aReader","sym":"V_e9a127905a27c74b_9983235f0b0d,V_e531350f47846456_9983235f0b0d,V_32342346f658d82b_9983235f0b0d,V_e8ddcff8d0861239_9983235f0b0d","no_crossref":1,"type":"IPC::MessageReader *"}
{"loc":"00195:17-25","source":1,"syntax":"function,use","pretty":"function IPC::MessageReader::GetActor","sym":"_ZNK3IPC13MessageReader8GetActorEv","type":"mozilla::ipc::IProtocol *"}
{"loc":"00195:29-38","source":1,"syntax":"function,use","pretty":"function mozilla::ipc::IProtocol::ReadActor","sym":"_ZN7mozilla3ipc9IProtocol9ReadActorEPN3IPC13MessageReaderEbPKci","type":"Maybe<class mozilla::ipc::IProtocol *>","typesym":"T_mozilla::Maybe"}
{"loc":"00195:39-46","source":1,"syntax":"","pretty":"variable aReader","sym":"V_e9a127905a27c74b_9983235f0b0d,V_e531350f47846456_9983235f0b0d,V_32342346f658d82b_9983235f0b0d,V_e8ddcff8d0861239_9983235f0b0d","no_crossref":1,"type":"IPC::MessageReader *"}
{"loc":"00195:68-86","source":1,"syntax":"enum,use","pretty":"enum IPCMessageStart::PTestBasicMsgStart","sym":"E_<T_IPCMessageStart>_PTestBasicMsgStart","type":"enum IPCMessageStart","typesym":"T_IPCMessageStart"}
{"loc":"00196:8-13","source":1,"syntax":"","pretty":"variable actor","sym":"V_b3e828905a27c74b_ebe821f013,V_bf68360f47846456_ebe821f013,V_0c6b2446f658d82b_ebe821f013,V_b215d009d0861239_ebe821f013","no_crossref":1,"type":"mozilla::Maybe<mozilla::ipc::IProtocol *>","typesym":"T_mozilla::Maybe"}
{"loc":"00196:14-23","source":1,"syntax":"function,use","pretty":"function mozilla::Maybe::isNothing","sym":"_ZNK7mozilla5Maybe9isNothingEv","type":"_Bool"}
{"loc":"00200:5-9","source":1,"syntax":"","pretty":"variable aVar","sym":"V_fd6a27905a27c74b_f0bb39c71,V_f9f9350f47846456_f0bb39c71,V_46fc2346f658d82b_f0bb39c71,V_fc96dff8d0861239_f0bb39c71","no_crossref":1,"type":"IPC::ParamTraits<class mozilla::_ipdltest::PTestBasicParent *>::paramType *"}
{"loc":"00200:44-60","source":1,"syntax":"type,use","pretty":"type mozilla::_ipdltest::PTestBasicParent","sym":"T_mozilla::_ipdltest::PTestBasicParent","type":"class mozilla::_ipdltest::PTestBasicParent","typesym":"T_mozilla::_ipdltest::PTestBasicParent"}
{"loc":"00200:63-68","source":1,"syntax":"","pretty":"variable actor","sym":"V_b3e828905a27c74b_ebe821f013,V_bf68360f47846456_ebe821f013,V_0c6b2446f658d82b_ebe821f013,V_b215d009d0861239_ebe821f013","no_crossref":1,"type":"mozilla::Maybe<mozilla::ipc::IProtocol *>","typesym":"T_mozilla::Maybe"}
{"loc":"00200:69-74","source":1,"syntax":"function,use","pretty":"function mozilla::Maybe::value","sym":"_ZNKR7mozilla5Maybe5valueEv","type":"class mozilla::ipc::IProtocol *"}
{"loc":"00186:57-61","structured":1,"pretty":"IPC::ParamTraits<mozilla::_ipdltest::PTestBasicParent *>::Read","sym":"_ZN3IPC11ParamTraitsIPN7mozilla9_ipdltest16PTestBasicParentEE4ReadEPNS_13MessageReaderEPS4_","kind":"method","parentsym":"T_IPC::ParamTraits","implKind":"","sizeBytes":null,"supers":[],"methods":[],"fields":[],"overrides":[],"props":["static","user"]}
{"loc":"00158:57-62","structured":1,"pretty":"IPC::ParamTraits<mozilla::_ipdltest::PTestBasicParent *>::Write","sym":"_ZN3IPC11ParamTraitsIPN7mozilla9_ipdltest16PTestBasicParentEE5WriteEPNS_13MessageWriterERKS4_","kind":"method","parentsym":"T_IPC::ParamTraits","implKind":"","sizeBytes":null,"supers":[],"methods":[],"fields":[],"overrides":[],"props":["static","user"]}
{"loc":"00149:23-35","structured":1,"pretty":"mozilla::_ipdltest::PTestBasicParent::ClearSubtree","sym":"_ZN7mozilla9_ipdltest16PTestBasicParent12ClearSubtreeEv","kind":"method","parentsym":"T_mozilla::_ipdltest::PTestBasicParent","implKind":"","sizeBytes":null,"supers":[],"methods":[],"fields":[],"overrides":[],"props":["instance","user"]}
{"loc":"00079:23-36","structured":1,"pretty":"mozilla::_ipdltest::PTestBasicParent::RemoveManagee","sym":"_ZN7mozilla9_ipdltest16PTestBasicParent13RemoveManageeEiPNS_3ipc9IProtocolE","kind":"method","parentsym":"T_mozilla::_ipdltest::PTestBasicParent","implKind":"","sizeBytes":null,"supers":[],"methods":[],"fields":[],"overrides":[{"pretty":"mozilla::ipc::IProtocol::RemoveManagee","sym":"_ZN7mozilla3ipc9IProtocol13RemoveManageeEiPS1_"}],"props":["instance","virtual","user"]}
{"loc":"00087:23-37","structured":1,"pretty":"mozilla::_ipdltest::PTestBasicParent::DeallocManagee","sym":"_ZN7mozilla9_ipdltest16PTestBasicParent14DeallocManageeEiPNS_3ipc9IProtocolE","kind":"method","parentsym":"T_mozilla::_ipdltest::PTestBasicParent","implKind":"","sizeBytes":null,"supers":[],"methods":[],"fields":[],"overrides":[{"pretty":"mozilla::ipc::IProtocol::DeallocManagee","sym":"_ZN7mozilla3ipc9IProtocol14DeallocManageeEiPS1_"}],"props":["instance","virtual","user"]}
{"loc":"00121:23-37","structured":1,"pretty":"mozilla::_ipdltest::PTestBasicParent::OnCallReceived","sym":"_ZN7mozilla9_ipdltest16PTestBasicParent14OnCallReceivedERKN3IPC7MessageERNS_9UniquePtrIS3_NS_13DefaultDeleteIS3_EEEE","kind":"method","parentsym":"T_mozilla::_ipdltest::PTestBasicParent","implKind":"","sizeBytes":null,"supers":[],"methods":[],"fields":[],"overrides":[{"pretty":"mozilla::ipc::IProtocol::OnCallReceived","sym":"_ZN7mozilla3ipc9IProtocol14OnCallReceivedERKN3IPC7MessageERNS_9UniquePtrIS3_NS_13DefaultDeleteIS3_EEEE"}],"props":["instance","virtual","user"]}
{"loc":"00129:23-37","structured":1,"pretty":"mozilla::_ipdltest::PTestBasicParent::OnChannelClose","sym":"_ZN7mozilla9_ipdltest16PTestBasicParent14OnChannelCloseEv","kind":"method","parentsym":"T_mozilla::_ipdltest::PTestBasicParent","implKind":"","sizeBytes":null,"supers":[],"methods":[],"fields":[],"overrides":[{"pretty":"mozilla::ipc::IToplevelProtocol::OnChannelClose","sym":"_ZN7mozilla3ipc17IToplevelProtocol14OnChannelCloseEv"}],"props":["instance","virtual","user"]}
{"loc":"00139:23-37","structured":1,"pretty":"mozilla::_ipdltest::PTestBasicParent::OnChannelError","sym":"_ZN7mozilla9_ipdltest16PTestBasicParent14OnChannelErrorEv","kind":"method","parentsym":"T_mozilla::_ipdltest::PTestBasicParent","implKind":"","sizeBytes":null,"supers":[],"methods":[],"fields":[],"overrides":[{"pretty":"mozilla::ipc::IToplevelProtocol::OnChannelError","sym":"_ZN7mozilla3ipc17IToplevelProtocol14OnChannelErrorEv"}],"props":["instance","virtual","user"]}
{"loc":"00025:23-38","structured":1,"pretty":"mozilla::_ipdltest::PTestBasicParent::ProcessingError","sym":"_ZN7mozilla9_ipdltest16PTestBasicParent15ProcessingErrorENS_3ipc14HasResultCodes6ResultEPKc","kind":"method","parentsym":"T_mozilla::_ipdltest::PTestBasicParent","implKind":"","sizeBytes":null,"supers":[],"methods":[],"fields":[],"overrides":[{"pretty":"mozilla::ipc::IToplevelProtocol::ProcessingError","sym":"_ZN7mozilla3ipc17IToplevelProtocol15ProcessingErrorENS0_14HasResultCodes6ResultEPKc"}],"props":["instance","virtual","user"]}
{"loc":"00095:23-40","structured":1,"pretty":"mozilla::_ipdltest::PTestBasicParent::OnMessageReceived","sym":"_ZN7mozilla9_ipdltest16PTestBasicParent17OnMessageReceivedERKN3IPC7MessageE","kind":"method","parentsym":"T_mozilla::_ipdltest::PTestBasicParent","implKind":"","sizeBytes":null,"supers":[],"methods":[],"fields":[],"overrides":[{"pretty":"mozilla::ipc::IProtocol::OnMessageReceived","sym":"_ZN7mozilla3ipc9IProtocol17OnMessageReceivedERKN3IPC7MessageE"}],"props":["instance","virtual","user"]}
{"loc":"00113:23-40","structured":1,"pretty":"mozilla::_ipdltest::PTestBasicParent::OnMessageReceived","sym":"_ZN7mozilla9_ipdltest16PTestBasicParent17OnMessageReceivedERKN3IPC7MessageERNS_9UniquePtrIS3_NS_13DefaultDeleteIS3_EEEE","kind":"method","parentsym":"T_mozilla::_ipdltest::PTestBasicParent","implKind":"","sizeBytes":null,"supers":[],"methods":[],"fields":[],"overrides":[{"pretty":"mozilla::ipc::IProtocol::OnMessageReceived","sym":"_ZN7mozilla3ipc9IProtocol17OnMessageReceivedERKN3IPC7MessageERNS_9UniquePtrIS3_NS_13DefaultDeleteIS3_EEEE"}],"props":["instance","virtual","user"]}
{"loc":"00031:23-53","structured":1,"pretty":"mozilla::_ipdltest::PTestBasicParent::ShouldContinueFromReplyTimeout","sym":"_ZN7mozilla9_ipdltest16PTestBasicParent30ShouldContinueFromReplyTimeoutEv","kind":"method","parentsym":"T_mozilla::_ipdltest::PTestBasicParent","implKind":"","sizeBytes":null,"supers":[],"methods":[],"fields":[],"overrides":[{"pretty":"mozilla::ipc::IToplevelProtocol::ShouldContinueFromReplyTimeout","sym":"_ZN7mozilla3ipc17IToplevelProtocol30ShouldContinueFromReplyTimeoutEv"}],"props":["instance","virtual","user"]}
{"loc":"00054:23-32","structured":1,"pretty":"mozilla::_ipdltest::PTestBasicParent::SendHello","sym":"_ZN7mozilla9_ipdltest16PTestBasicParent9SendHelloEv","kind":"method","parentsym":"T_mozilla::_ipdltest::PTestBasicParent","implKind":"","sizeBytes":null,"supers":[],"methods":[],"fields":[],"overrides":[],"props":["instance","user"]}
{"loc":"00036:31-47","structured":1,"pretty":"mozilla::_ipdltest::PTestBasicParent::PTestBasicParent","sym":"_ZN7mozilla9_ipdltest16PTestBasicParentC1Ev","kind":"method","parentsym":"T_mozilla::_ipdltest::PTestBasicParent","implKind":"","sizeBytes":null,"supers":[],"methods":[],"fields":[],"overrides":[],"props":["instance","user"]}
{"loc":"00042:19-35","structured":1,"pretty":"mozilla::_ipdltest::PTestBasicParent::~PTestBasicParent","sym":"_ZN7mozilla9_ipdltest16PTestBasicParentD1Ev","kind":"method","parentsym":"T_mozilla::_ipdltest::PTestBasicParent","implKind":"","sizeBytes":null,"supers":[],"methods":[],"fields":[],"overrides":[{"pretty":"mozilla::ipc::IToplevelProtocol::~IToplevelProtocol","sym":"_ZN7mozilla3ipc17IToplevelProtocolD1Ev"}],"props":["instance","virtual","user"]}
{"loc":"00047:23-39","structured":1,"pretty":"mozilla::_ipdltest::PTestBasicParent::AllManagedActors","sym":"_ZNK7mozilla9_ipdltest16PTestBasicParent16AllManagedActorsER8nsTArrayI6RefPtrINS_3ipc19ActorLifecycleProxyEEE","kind":"method","parentsym":"T_mozilla::_ipdltest::PTestBasicParent","implKind":"","sizeBytes":null,"supers":[],"methods":[],"fields":[],"overrides":[{"pretty":"mozilla::ipc::IProtocol::AllManagedActors","sym":"_ZNK7mozilla3ipc9IProtocol16AllManagedActorsER8nsTArrayI6RefPtrINS0_19ActorLifecycleProxyEEE"}],"props":["instance","virtual","user"]}

```

## tests/tests/refresh-mozilla-central-stuff
```
#!/usr/bin/env bash

set -eu # Errors/undefined vars are fatal
set -o pipefail # Check all commands in a pipeline

# This scripts pull down interesting build byproducts from mozilla-central
# searchfox runs that we want to have available (and checked into the tests
# repo) so that we can have as much testing fidelity as possible in this repo,
# but without creating maintenance hassles where we try and replicate the
# mozilla-central build system or parts of it.
#
# File handling:
# - Source files (which are source files in mozilla-central) get reflected into
#   `files` here to be source files we check in.
# - Generated files get checked in under `mc-generated` here and will be copied
#   into `$OBJDIR` by `build`.
# - Analysis files get checked in under `mc-analysis` here and will be copied
#   into the `$INDEX_ROOT/analysis` by `build`.  These are the analysis files
#   for the generated files above, so the mapping is the same.
#
# How we get the data:
# - We copy it from a running searchfox web-server that has the mozilla-central
#   data we need.  We need data that's in the `objdir` and currently searchfox
#   doesn't directly expose that data to the web.
# - Note that we gzip up all the analysis files, so we need to grab the `.gz`
#   version and then un-gzip them.

if [[ $# -ne 1 ]]; then
    echo "Usage: $0 <webserver instance id you're already sshed into>"
    echo " In another shell, run ssh.py <instance-id> first to modify security"
    echo " permissions, then run this script."
    echo " "
    echo " e.g.: $0 i-0000000000"
    exit 1
fi
SERVER_ID=$1

SSH_SCRIPT=$(dirname $0)/../../infrastructure/aws/scp-while-sshed.py
TEST_REPO_FILES=$(dirname $0)/files
TEST_SAVED_GENERATED_FILES=$(dirname $0)/mc-generated
TEST_SAVED_ANALYSIS_FILES=$(dirname $0)/mc-analysis

REMOTE_MC_BASE=index/mozilla-central

# Maps source file relative path to target directory (sans filename)
declare -A SOURCE_FILES
SOURCE_FILES=(
  # Note that we do add some core helper files back after processing the below
  # into GENERATED_FILES
  [js/xpconnect/tests/idl/xpctest_attributes.idl]=xpidl/
  [js/xpconnect/tests/idl/xpctest_bug809674.idl]=xpidl/
  [js/xpconnect/tests/idl/xpctest_cenums.idl]=xpidl/
  [js/xpconnect/tests/idl/xpctest_interfaces.idl]=xpidl/
  [js/xpconnect/tests/idl/xpctest_params.idl]=xpidl/
  [js/xpconnect/tests/idl/xpctest_returncode.idl]=xpidl/
  [js/xpconnect/tests/idl/xpctest_utils.idl]=xpidl/
  [ipc/ipdl/test/gtest/PTestBasic.ipdl]=ipdl/
)

# Ugh, so, for some source files, we may want to just steal the analysis
# files from the server too.  This feels more sketchy than us doing this for the
# generated files, but it is, in fact, the same exact amount of sketchy!
declare -A SOURCE_ANALYSIS_FILES
SOURCE_ANALYSIS_FILES=(
  [ipc/ipdl/test/gtest/TestBasic.cpp]=ipdl/
  # These normally live next to TestBasic.cpp above but they get installed to
  # this location so this is where the includes hardcode.
  [ipc/ipdl/test/gtest/TestBasicChild.h]=mozilla/_ipdltest
  [ipc/ipdl/test/gtest/TestBasicParent.h]=mozilla/_ipdltest
)
# copy these into SOURCE_FILES now.
for src_file in "${!SOURCE_ANALYSIS_FILES[@]}"; do
  SOURCE_FILES[$src_file]=${SOURCE_ANALYSIS_FILES[$src_file]}
done


# Maps generated/analysis file relative path to target directory (sans filename)
declare -A GENERATED_FILES
# Note that at least the C++ header files are hard-coded to be assumed reside in
# dist/include.
GENERATED_FILES=(
  # XPIDL, IPDL Now automatically derived below!  But if you wanted manual stuff:
  #[dist/include/xpctest_params.h]=dist/include
  #[dist/xpcrs/bt/xpctest_params.rs]=dist/xpcrs/bt
  #[dist/xpcrs/rt/xpctest_params.rs]=dist/xpcrs/rt
)
# Derive
for src_file in "${!SOURCE_FILES[@]}"; do
  filename=$(basename $src_file)
  RE_IDL='\.idl$'
  if [[ $filename =~ $RE_IDL ]]; then
    GENERATED_FILES["dist/include/${filename%.idl}.h"]=dist/include
    GENERATED_FILES["dist/xpcrs/bt/${filename%.idl}.rs"]=dist/xpcrs/bt
    GENERATED_FILES["dist/xpcrs/rt/${filename%.idl}.rs"]=dist/xpcrs/rt
  fi
  # IPDL: This is very specific to the current gtests used.
  RE_IPDL='\.ipdl$'
  if [[ $filename =~ $RE_IPDL ]]; then
    GENERATED_FILES["ipc/ipdl/${filename%.ipdl}.cpp"]=ipdl
    GENERATED_FILES["ipc/ipdl/${filename%.ipdl}Child.cpp"]=ipdl
    GENERATED_FILES["ipc/ipdl/${filename%.ipdl}Parent.cpp"]=ipdl
    GENERATED_FILES["ipc/ipdl/_ipdlheaders/mozilla/_ipdltest/${filename%.ipdl}.h"]=ipc/ipdl/_ipdlheaders/mozilla/_ipdltest
    GENERATED_FILES["ipc/ipdl/_ipdlheaders/mozilla/_ipdltest/${filename%.ipdl}Child.h"]=ipc/ipdl/_ipdlheaders/mozilla/_ipdltest
    GENERATED_FILES["ipc/ipdl/_ipdlheaders/mozilla/_ipdltest/${filename%.ipdl}Parent.h"]=ipc/ipdl/_ipdlheaders/mozilla/_ipdltest
  fi
done

# These are files we want the source for but absolutely do not want the actual
# generated header files for because they increase the set of stubs we need.
# That said, feel free to consider adding them back to the initial SOURCE_FILES
# mapping.
SOURCE_FILES[xpcom/base/nsISupports.idl]=xpidl/
SOURCE_FILES[xpcom/base/nsrootidl.idl]=xpidl/

set -x # Show commands from here on out as the scp command is not usefully chatty.

## Source Files
for src_file in "${!SOURCE_FILES[@]}"; do
  targ_dir=${SOURCE_FILES[$src_file]}
  mkdir -p ${TEST_REPO_FILES}/${targ_dir}
  $SSH_SCRIPT "$1" "${REMOTE_MC_BASE}/git/${src_file}" ${TEST_REPO_FILES}/${targ_dir}
done

## Generated Files from objdir
for gen_file in "${!GENERATED_FILES[@]}"; do
  targ_dir=${GENERATED_FILES[$gen_file]}
  mkdir -p ${TEST_SAVED_GENERATED_FILES}/${targ_dir}
  $SSH_SCRIPT "$1" "${REMOTE_MC_BASE}/objdir/${gen_file}" ${TEST_SAVED_GENERATED_FILES}/${targ_dir}
done

## Analysis files from some of the source files
for src_file in "${!SOURCE_ANALYSIS_FILES[@]}"; do
  targ_dir=${SOURCE_ANALYSIS_FILES[$src_file]}
  mkdir -p ${TEST_SAVED_ANALYSIS_FILES}/${targ_dir}
  $SSH_SCRIPT "$1" "${REMOTE_MC_BASE}/analysis/${src_file}.gz" ${TEST_SAVED_ANALYSIS_FILES}/${targ_dir}
  gunzip -f "${TEST_SAVED_ANALYSIS_FILES}/${targ_dir}/$(basename $src_file).gz"
done

## Analysis data derived from the generated files.
#
# Note that these files are likely to be zero-length.
for gen_file in "${!GENERATED_FILES[@]}"; do
  targ_dir=${GENERATED_FILES[$gen_file]}
  mkdir -p "${TEST_SAVED_ANALYSIS_FILES}/__GENERATED__/${targ_dir}"
  $SSH_SCRIPT "$1" "${REMOTE_MC_BASE}/analysis/__GENERATED__/${gen_file}.gz" "${TEST_SAVED_ANALYSIS_FILES}/__GENERATED__/${targ_dir}"
  gunzip -f "${TEST_SAVED_ANALYSIS_FILES}/__GENERATED__/${targ_dir}/$(basename $gen_file).gz"
done

```

## tests/help.html
```
<div class="intro">

<h1>Welcome to Searchfox [testing]</h1>

<h3>
Direct link to: <a href="/tests/source/">tests</a> | <a href="/searchfox/source/">searchfox</a>
</h3>

<p>
Searchfox is a source code indexing tool for Mozilla Firefox. It
indexes C++ and JavaScript code. This is the help page for Searchfox.
</p>

```

## extension/background.js
```
browser.omnibox.onInputChanged.addListener((text, suggest) => {
  console.log("ENTERED", text);

  if (text.length < 3) {
    return;
  }

  function reqListener() {
    console.log("RESPONSE", this.responseText);
    let response = JSON.parse(this.responseText);
    suggest(response.map(result => { return { content: "symbol:" + result.symbol, description: result.id};  }));
  }

  let xhr = new XMLHttpRequest();
  xhr.addEventListener("load", reqListener);
  xhr.open("GET", "http://localhost:8001/mozilla-central/complete/" + text, true);
  xhr.send();
});

browser.omnibox.onInputEntered.addListener((text, disposition) => {
  let url = "https://searchfox.org/mozilla-central/search?q=" + encodeURIComponent(text);
  switch (disposition) {
    case "currentTab":
      browser.tabs.update({url});
      break;
    case "newForegroundTab":
      browser.tabs.create({url});
      break;
    case "newBackgroundTab":
      browser.tabs.create({url, active: false});
      break;
  }
});

```

## extension/manifest.json
```
{
  "name": "SearchFox++",
  "description": "Provides a better experience for searchfox.org",
  "author": "Bill McCloskey",
  "developer": {
    "name": "Bill McCloskey",
    "url": "https://searchfox.org"
  },
  "version": "1.0",
  "manifest_version": 2,

  "background": {
    "scripts": ["background.js"]
  },

  "omnibox": {
    "keyword": "sf"
  },

  "permissions": [
    "*://searchfox.org/",
    "*://localhost:8001/*",
    "<all_urls>"
  ]
}

```

## tree-configs/README.md
```
Put config files for the trees you symlinked in `/trees` in here.  The
assumed default config (which can list multiple trees) is `config.json`.

A good starting point is probably `tests/searchfox-config.json`, noting
that you probably want to increment the `codesearch_port` to start at
port 8082 and keep incrementing from there.  The `tests` repo uses port
8080 and `searchfox` uses 8081, so this avoids edge cases if you are
switching between what is indexed.

You can then build the trees via `make build-trees` from `/vagrant`
inside of your docker image if your config file is the default of
`config.json`.  If your config file is named something else, then
you can set the `CONFIG` env variable by doing something like
`CONFIG=my-config.json make build-trees`.  Note that although make
can accept variable assignments as part of its arguments, that's not
how this is intended to work, and so maybe it won't work!

```

